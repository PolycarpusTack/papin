This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.github/workflows/ci.yml
.github/workflows/release.yml
.gitignore
benches/performance_bench.rs
Cargo.toml
docs/ARCHITECTURE.md
docs/INSTALLATION.md
docs/local_llm_integration.md
docs/local_llm_integration/developer_documentation.md
docs/local_llm_integration/provider_comparison.md
docs/local_llm_integration/troubleshooting_guide.md
docs/local_llm_integration/user_guide.md
docs/OBSERVABILITY.md
docs/plugin_system_design.md
docs/README.md
docs/RELEASE_PROCESS.md
docs/SECURITY_PRIVACY.md
docs/TELEMETRY_ANALYSIS.md
docs/TESTING_AND_DEPLOYMENT.md
docs/USER_GUIDE.md
frontend-only.ps1
implementation-summary.md
installers/linux-build.sh
installers/macos-build.sh
installers/windows-build.ps1
Makefile
MCP_ARCHITECTURE.md
MCP_README.md
plugins/examples/github-snippets/Cargo.toml
plugins/examples/github-snippets/manifest.json
plugins/examples/github-snippets/src/main.rs
plugins/examples/meeting-summarizer/Cargo.toml
plugins/examples/meeting-summarizer/manifest.json
plugins/examples/meeting-summarizer/src/main.rs
plugins/examples/translation-plugin/Cargo.toml
plugins/examples/translation-plugin/manifest.json
plugins/examples/translation-plugin/src/main.rs
plugins/README.md
PROJECT_STATUS.md
README.md
scripts/build-linux.sh
scripts/build-macos.sh
scripts/build-windows.ps1
src-cli/Cargo.toml
src-cli/README.md
src-cli/src/commands/chat.rs
src-cli/src/commands/delete.rs
src-cli/src/commands/export.rs
src-cli/src/commands/interactive.rs
src-cli/src/commands/list.rs
src-cli/src/commands/mod.rs
src-cli/src/commands/model.rs
src-cli/src/commands/new.rs
src-cli/src/commands/setup.rs
src-cli/src/commands/show.rs
src-cli/src/commands/system.rs
src-cli/src/display/formatter.rs
src-cli/src/display/mod.rs
src-cli/src/display/printer.rs
src-cli/src/display/spinner.rs
src-cli/src/display/table.rs
src-cli/src/error.rs
src-cli/src/main.rs
src-common/Cargo.toml
src-common/README.md
src-common/src/config/mod.rs
src-common/src/config/settings.rs
src-common/src/config/storage.rs
src-common/src/error.rs
src-common/src/feature_flags/mod.rs
src-common/src/lib.rs
src-common/src/models/conversation.rs
src-common/src/models/message.rs
src-common/src/models/mod.rs
src-common/src/models/model.rs
src-common/src/models/tool.rs
src-common/src/observability/canary.rs
src-common/src/observability/logging.rs
src-common/src/observability/metrics.rs
src-common/src/observability/mod.rs
src-common/src/observability/telemetry.rs
src-common/src/platform/mod.rs
src-common/src/protocol/mcp.rs
src-common/src/protocol/mod.rs
src-common/src/protocol/websocket.rs
src-common/src/service/chat.rs
src-common/src/service/mcp.rs
src-common/src/service/mod.rs
src-common/src/utils/mod.rs
src-common/src/utils/security.rs
src-common/src/utils/text.rs
src-frontend/implementation-summary.md
src-frontend/index.html
src-frontend/package.json
src-frontend/src/__tests__/collaboration/CollaborationContext.test.tsx
src-frontend/src/__tests__/collaboration/CursorOverlay.test.tsx
src-frontend/src/accessibility/A11yButton.tsx
src-frontend/src/accessibility/accessibility.css
src-frontend/src/accessibility/AccessibilityProvider.tsx
src-frontend/src/accessibility/index.ts
src-frontend/src/animation/AnimationProvider.tsx
src-frontend/src/animation/animations.css
src-frontend/src/animation/index.ts
src-frontend/src/App.css
src-frontend/src/App.tsx
src-frontend/src/components/AppShell.css
src-frontend/src/components/AppShell.tsx
src-frontend/src/components/collaboration/call/CallControls.tsx
src-frontend/src/components/collaboration/CollaborationPanel.tsx
src-frontend/src/components/collaboration/context/CollaborationContext.tsx
src-frontend/src/components/collaboration/index.ts
src-frontend/src/components/collaboration/presence/CursorOverlay.tsx
src-frontend/src/components/collaboration/presence/SelectionOverlay.tsx
src-frontend/src/components/collaboration/presence/UserBadge.tsx
src-frontend/src/components/collaboration/settings/CollaborationSettings.tsx
src-frontend/src/components/collaboration/UserList.tsx
src-frontend/src/components/collaboration/whiteboard/CollaborativeWhiteboard.tsx
src-frontend/src/components/CommandPalette.css
src-frontend/src/components/CommandPalette.tsx
src-frontend/src/components/dashboard/llm/LLMPerformanceDashboard.css
src-frontend/src/components/dashboard/llm/LLMPerformanceDashboard.tsx
src-frontend/src/components/dashboard/ResourceDashboard.css
src-frontend/src/components/dashboard/ResourceDashboard.tsx
src-frontend/src/components/help/ExtendedGuide.css
src-frontend/src/components/help/ExtendedGuide.tsx
src-frontend/src/components/help/HelpButton.css
src-frontend/src/components/help/HelpButton.tsx
src-frontend/src/components/help/HelpCenter.css
src-frontend/src/components/help/HelpCenter.tsx
src-frontend/src/components/help/helpContent.ts
src-frontend/src/components/help/HelpGuide.css
src-frontend/src/components/help/HelpGuide.tsx
src-frontend/src/components/help/HelpSearch.css
src-frontend/src/components/help/HelpSearch.tsx
src-frontend/src/components/help/HelpTopic.css
src-frontend/src/components/help/HelpTopic.tsx
src-frontend/src/components/menu/HelpMenu.tsx
src-frontend/src/components/menu/Menu.css
src-frontend/src/components/offline/LLMMetricsPrivacyNotice.tsx
src-frontend/src/components/offline/OfflineSettings.css
src-frontend/src/components/offline/OfflineSettings.tsx
src-frontend/src/components/security/DataFlowVisualization.css
src-frontend/src/components/security/DataFlowVisualization.tsx
src-frontend/src/components/security/PermissionsManager.css
src-frontend/src/components/security/PermissionsManager.tsx
src-frontend/src/components/security/SecuritySettings.css
src-frontend/src/components/security/SecuritySettings.tsx
src-frontend/src/components/settings/PrivacySettings.tsx
src-frontend/src/components/Shell.css
src-frontend/src/components/Shell.tsx
src-frontend/src/components/ui/Button.css
src-frontend/src/components/ui/Button.tsx
src-frontend/src/components/ui/index.ts
src-frontend/src/components/ui/Input.css
src-frontend/src/components/ui/Input.tsx
src-frontend/src/contexts/FeatureFlagContext.tsx
src-frontend/src/disclosure/disclosure.css
src-frontend/src/disclosure/FeatureBadge.tsx
src-frontend/src/disclosure/FeatureDiscovery.tsx
src-frontend/src/disclosure/index.ts
src-frontend/src/disclosure/LockedFeatureMessage.tsx
src-frontend/src/disclosure/ProgressiveDisclosure.tsx
src-frontend/src/help/help.css
src-frontend/src/help/HelpButton.tsx
src-frontend/src/help/HelpProvider.tsx
src-frontend/src/help/HelpTrigger.tsx
src-frontend/src/help/index.ts
src-frontend/src/hooks/useCollaboration.ts
src-frontend/src/hooks/useCommandPalette.ts
src-frontend/src/index.css
src-frontend/src/interactions/feedback.css
src-frontend/src/interactions/index.ts
src-frontend/src/interactions/useFeedback.ts
src-frontend/src/interactions/useHoverEffect.ts
src-frontend/src/interactions/useMicroInteraction.ts
src-frontend/src/interactions/usePressEffect.ts
src-frontend/src/interactions/useRippleEffect.tsx
src-frontend/src/keyboard/index.ts
src-frontend/src/keyboard/KeyboardNavigation.css
src-frontend/src/keyboard/KeyboardNavigation.tsx
src-frontend/src/lazy/Chat.css
src-frontend/src/lazy/Chat.tsx
src-frontend/src/lazy/Header.css
src-frontend/src/lazy/Header.tsx
src-frontend/src/lazy/Settings.css
src-frontend/src/lazy/Settings.tsx
src-frontend/src/lazy/Sidebar.css
src-frontend/src/lazy/Sidebar.tsx
src-frontend/src/main.tsx
src-frontend/src/styles/collaboration.css
src-frontend/src/styles/observability.css
src-frontend/src/styles/whiteboard.css
src-frontend/src/theme/index.ts
src-frontend/src/theme/ThemeContext.tsx
src-frontend/src/theme/ThemeToggle.css
src-frontend/src/theme/ThemeToggle.tsx
src-frontend/src/theme/variables.css
src-frontend/src/tours/index.ts
src-frontend/src/tours/TourButton.tsx
src-frontend/src/tours/TourList.tsx
src-frontend/src/tours/TourProvider.tsx
src-frontend/src/tours/tours.css
src-frontend/src/utils/batch.ts
src-frontend/src/utils/lazyLoad.tsx
src-frontend/src/utils/retry.ts
src-frontend/src/utils/throttle.ts
src-frontend/ui-enhancements-summary.md
src-frontend/vite.config.ts
src-tauri/Cargo.toml
src-tauri/src/commands/mod.rs
src-tauri/src/commands/offline.rs
src-tauri/src/main.rs
src-tauri/src/monitoring/resources.rs
src-tauri/src/offline/llm/factory.rs
src-tauri/src/offline/llm/mod.rs
src-tauri/src/offline/llm/ollama.rs
src-tauri/src/offline/llm/provider.rs
src-tauri/src/offline/llm/providers/localai.rs
src-tauri/src/offline/llm/providers/mod.rs
src-tauri/src/offline/llm/providers/ollama.rs
src-tauri/src/offline/llm/types.rs
src-tauri/src/offline/mod.rs
src-tauri/tauri.conf.json
src-tui/Cargo.toml
src-tui/README.md
src-tui/src/app/mod.rs
src-tui/src/error.rs
src-tui/src/event.rs
src-tui/src/main.rs
src-tui/src/ui/mod.rs
src-tui/src/util/mod.rs
src/ai/claude/api.rs
src/ai/claude/mcp.rs
src/ai/claude/mod.rs
src/ai/claude/streaming.rs
src/ai/local/inference.rs
src/ai/local/mod.rs
src/ai/local/models.rs
src/ai/mod.rs
src/ai/router/mod.rs
src/auto_update/mod.rs
src/collaboration/mod.rs
src/collaboration/presence.rs
src/collaboration/rtc.rs
src/collaboration/sessions.rs
src/collaboration/sync.rs
src/commands/ai.rs
src/commands/auth.rs
src/commands/chat.rs
src/commands/collaboration/mod.rs
src/commands/collaboration/whiteboard.rs
src/commands/llm_metrics.rs
src/commands/mcp.rs
src/commands/mod.rs
src/commands/offline.rs
src/commands/offline/llm.rs
src/commands/offline/mod.rs
src/commands/optimization.rs
src/commands/security.rs
src/commands/update.rs
src/feature_flags.rs
src/main.rs
src/models/messages.rs
src/models/mod.rs
src/observability/metrics/llm.rs
src/observability/metrics/mod.rs
src/observability/mod.rs
src/offline/checkpointing/mod.rs
src/offline/llm/discovery.rs
src/offline/llm/migration.rs
src/offline/llm/mod.rs
src/offline/mod.rs
src/offline/sync/mod.rs
src/optimization/cache/mod.rs
src/optimization/memory/mod.rs
src/optimization/mod.rs
src/plugins/discovery/mod.rs
src/plugins/hooks/mod.rs
src/plugins/loader/mod.rs
src/plugins/mod.rs
src/plugins/permissions/mod.rs
src/plugins/registry/mod.rs
src/plugins/sandbox/mod.rs
src/plugins/types.rs
src/plugins/ui/mod.rs
src/protocols/mcp/client.rs
src/protocols/mcp/config.rs
src/protocols/mcp/error.rs
src/protocols/mcp/message.rs
src/protocols/mcp/mod.rs
src/protocols/mcp/protocol.rs
src/protocols/mcp/session.rs
src/protocols/mcp/types.rs
src/protocols/mcp/websocket.rs
src/protocols/mod.rs
src/security/credentials.rs
src/security/data_flow.rs
src/security/e2ee.rs
src/security/mod.rs
src/security/permissions.rs
src/services/ai.rs
src/services/api.rs
src/services/auth.rs
src/services/chat.rs
src/services/mcp.rs
src/services/mod.rs
src/shell_loader.rs
src/telemetry/mod.rs
src/updater.rs
src/utils/config.rs
src/utils/events.rs
src/utils/lazy_loader.rs
src/utils/mod.rs
start-app.ps1
tests/e2e/end_to_end_test.js
tests/e2e/playwright.config.js
tests/integration/api_integration_test.rs
tests/integration/auto_update_test.rs
tests/integration/performance_test.rs
tests/integration/ui_integration_test.rs
tests/integration/updater_test.rs
tests/unit/auto_update_test.rs
tests/unit/offline_test.rs
tests/unit/optimization_test.rs
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src-common/src/platform/mod.rs">
pub mod fs;
</file>

<file path=".github/workflows/ci.yml">
name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

env:
  CARGO_TERM_COLOR: always

jobs:
  lint:
    name: Lint
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          profile: minimal
          toolchain: stable
          override: true
          components: rustfmt, clippy
      
      - name: Cache Rust dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-
      
      - name: Check formatting
        uses: actions-rs/cargo@v1
        with:
          command: fmt
          args: -- --check
      
      - name: Run Clippy
        uses: actions-rs/clippy-check@v1
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          args: -- -D warnings

  test:
    name: Test
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
    
    steps:
      - uses: actions/checkout@v3
      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          profile: minimal
          toolchain: stable
          override: true
      
      - name: Cache Rust dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-
      
      - name: Install dependencies (Ubuntu)
        if: matrix.os == 'ubuntu-latest'
        run: |
          sudo apt-get update
          sudo apt-get install -y libwebkit2gtk-4.0-dev build-essential libssl-dev libgtk-3-dev libayatana-appindicator3-dev librsvg2-dev
      
      - name: Install Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 16
      
      - name: Install frontend dependencies
        working-directory: ./src-frontend
        run: npm ci
      
      - name: Build frontend
        working-directory: ./src-frontend
        run: npm run build
      
      - name: Run unit tests
        uses: actions-rs/cargo@v1
        with:
          command: test
          args: --lib
      
      - name: Run integration tests
        uses: actions-rs/cargo@v1
        with:
          command: test
          args: --test '*'

  benchmarks:
    name: Benchmarks
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          profile: minimal
          toolchain: stable
          override: true
      
      - name: Cache Rust dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-
      
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libwebkit2gtk-4.0-dev build-essential libssl-dev libgtk-3-dev libayatana-appindicator3-dev librsvg2-dev
      
      - name: Run benchmarks
        uses: actions-rs/cargo@v1
        with:
          command: bench
          args: --features benchmarking -- --output-format bencher | tee benchmark-results.txt
      
      - name: Store benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: benchmark-results.txt

  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: [lint, test]
    
    steps:
      - uses: actions/checkout@v3
      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          profile: minimal
          toolchain: stable
          override: true
      
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libwebkit2gtk-4.0-dev build-essential libssl-dev libgtk-3-dev libayatana-appindicator3-dev librsvg2-dev
      
      - name: Install Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 16
      
      - name: Install frontend dependencies
        working-directory: ./src-frontend
        run: npm ci
      
      - name: Install Playwright
        working-directory: ./src-frontend
        run: npx playwright install --with-deps
      
      - name: Build app for E2E tests
        run: |
          cargo build
          npm run build:tauri
      
      - name: Run E2E tests
        working-directory: ./src-frontend
        run: npm run test:e2e
      
      - name: Upload E2E test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: playwright-report
          path: ./src-frontend/playwright-report/
          retention-days: 30

  build:
    name: Build
    runs-on: ${{ matrix.os }}
    needs: [lint, test]
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        include:
          - os: ubuntu-latest
            rust_target: x86_64-unknown-linux-gnu
            artifact_name: linux
            asset_name: mcp-client-x86_64-linux
          - os: windows-latest
            rust_target: x86_64-pc-windows-msvc
            artifact_name: windows
            asset_name: mcp-client-x86_64-windows
          - os: macos-latest
            rust_target: x86_64-apple-darwin
            artifact_name: macos
            asset_name: mcp-client-x86_64-macos
    
    steps:
      - uses: actions/checkout@v3
      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          profile: minimal
          toolchain: stable
          target: ${{ matrix.rust_target }}
          override: true
      
      - name: Cache Rust dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-
      
      - name: Install dependencies (Ubuntu)
        if: matrix.os == 'ubuntu-latest'
        run: |
          sudo apt-get update
          sudo apt-get install -y libwebkit2gtk-4.0-dev build-essential libssl-dev libgtk-3-dev libayatana-appindicator3-dev librsvg2-dev
      
      - name: Install Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 16
      
      - name: Install frontend dependencies
        working-directory: ./src-frontend
        run: npm ci
      
      - name: Build frontend
        working-directory: ./src-frontend
        run: npm run build
      
      - name: Build (Linux)
        if: matrix.os == 'ubuntu-latest'
        run: bash ./installers/linux-build.sh
      
      - name: Build (Windows)
        if: matrix.os == 'windows-latest'
        run: powershell -File ./installers/windows-build.ps1
      
      - name: Build (macOS)
        if: matrix.os == 'macos-latest'
        run: bash ./installers/macos-build.sh
      
      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.artifact_name }}
          path: |
            ./installers/${{ matrix.artifact_name }}/
            ./dist/${{ matrix.asset_name }}
          retention-days: 7
</file>

<file path=".github/workflows/release.yml">
name: Release

on:
  push:
    tags:
      - 'v*'

env:
  CARGO_TERM_COLOR: always

jobs:
  create-release:
    runs-on: ubuntu-latest
    outputs:
      upload_url: ${{ steps.create-release.outputs.upload_url }}
      version: ${{ steps.get-version.outputs.version }}
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Get version
        id: get-version
        run: echo "version=${GITHUB_REF#refs/tags/v}" >> $GITHUB_OUTPUT
      
      - name: Create Release
        id: create-release
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: ${{ github.ref }}
          release_name: MCP Client ${{ steps.get-version.outputs.version }}
          draft: false
          prerelease: false
          body: |
            # MCP Client ${{ steps.get-version.outputs.version }}
            
            Please refer to the [CHANGELOG.md](https://github.com/your-org/mcp-client/blob/main/CHANGELOG.md) for details.
            
            ## Installation
            
            ### Windows
            - Download the MSI installer and run it
            - Alternatively, download the portable EXE if you prefer not to install
            
            ### macOS
            - Download the DMG file, open it, and drag the application to your Applications folder
            
            ### Linux
            - For Debian/Ubuntu: `sudo apt install ./mcp-client_${{ steps.get-version.outputs.version }}_amd64.deb`
            - For Fedora/RHEL: `sudo rpm -i mcp-client-${{ steps.get-version.outputs.version }}.x86_64.rpm`
            - For other distributions, use the AppImage

  build-and-upload:
    needs: create-release
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        include:
          - os: ubuntu-latest
            rust_target: x86_64-unknown-linux-gnu
            artifacts_folder: linux
          - os: windows-latest
            rust_target: x86_64-pc-windows-msvc
            artifacts_folder: windows
          - os: macos-latest
            rust_target: x86_64-apple-darwin
            artifacts_folder: macos
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          profile: minimal
          toolchain: stable
          target: ${{ matrix.rust_target }}
          override: true
      
      - name: Cache Rust dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-
      
      - name: Install dependencies (Ubuntu)
        if: matrix.os == 'ubuntu-latest'
        run: |
          sudo apt-get update
          sudo apt-get install -y libwebkit2gtk-4.0-dev build-essential libssl-dev libgtk-3-dev libayatana-appindicator3-dev librsvg2-dev
      
      - name: Install Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 16
      
      - name: Install frontend dependencies
        working-directory: ./src-frontend
        run: npm ci
      
      - name: Build frontend
        working-directory: ./src-frontend
        run: npm run build
      
      - name: Update version
        run: |
          VERSION=${{ needs.create-release.outputs.version }}
          # Update version in Cargo.toml
          sed -i 's/^version = ".*"/version = "'$VERSION'"/' Cargo.toml
          # Update version in package.json
          cd src-frontend
          npm version $VERSION --no-git-tag-version
          cd ..
          # Update version in tauri.conf.json
          sed -i 's/"version": ".*"/"version": "'$VERSION'"/' src-tauri/tauri.conf.json
      
      - name: Build (Linux)
        if: matrix.os == 'ubuntu-latest'
        run: bash ./installers/linux-build.sh
      
      - name: Build (Windows)
        if: matrix.os == 'windows-latest'
        run: powershell -File ./installers/windows-build.ps1
      
      - name: Build (macOS)
        if: matrix.os == 'macos-latest'
        run: bash ./installers/macos-build.sh
      
      - name: Upload Windows MSI
        if: matrix.os == 'windows-latest'
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ needs.create-release.outputs.upload_url }}
          asset_path: ./installers/windows/MCP-Client-${{ needs.create-release.outputs.version }}-x64.msi
          asset_name: MCP-Client-${{ needs.create-release.outputs.version }}-x64.msi
          asset_content_type: application/octet-stream
      
      - name: Upload Windows Portable EXE
        if: matrix.os == 'windows-latest'
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ needs.create-release.outputs.upload_url }}
          asset_path: ./installers/windows/MCP-Client-Portable.exe
          asset_name: MCP-Client-${{ needs.create-release.outputs.version }}-Portable.exe
          asset_content_type: application/octet-stream
      
      - name: Upload macOS DMG (Universal)
        if: matrix.os == 'macos-latest'
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ needs.create-release.outputs.upload_url }}
          asset_path: ./installers/macos/MCP-Client-Universal.dmg
          asset_name: MCP-Client-${{ needs.create-release.outputs.version }}-Universal.dmg
          asset_content_type: application/octet-stream
      
      - name: Upload macOS DMG (Intel)
        if: matrix.os == 'macos-latest'
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ needs.create-release.outputs.upload_url }}
          asset_path: ./installers/macos/MCP-Client-Intel.dmg
          asset_name: MCP-Client-${{ needs.create-release.outputs.version }}-Intel.dmg
          asset_content_type: application/octet-stream
      
      - name: Upload macOS DMG (Apple Silicon)
        if: matrix.os == 'macos-latest'
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ needs.create-release.outputs.upload_url }}
          asset_path: ./installers/macos/MCP-Client-AppleSilicon.dmg
          asset_name: MCP-Client-${{ needs.create-release.outputs.version }}-AppleSilicon.dmg
          asset_content_type: application/octet-stream
      
      - name: Upload Linux DEB
        if: matrix.os == 'ubuntu-latest'
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ needs.create-release.outputs.upload_url }}
          asset_path: ./installers/linux/mcp-client_${{ needs.create-release.outputs.version }}_amd64.deb
          asset_name: mcp-client_${{ needs.create-release.outputs.version }}_amd64.deb
          asset_content_type: application/octet-stream
      
      - name: Upload Linux RPM
        if: matrix.os == 'ubuntu-latest'
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ needs.create-release.outputs.upload_url }}
          asset_path: ./installers/linux/mcp-client-${{ needs.create-release.outputs.version }}.x86_64.rpm
          asset_name: mcp-client-${{ needs.create-release.outputs.version }}.x86_64.rpm
          asset_content_type: application/octet-stream
      
      - name: Upload Linux AppImage
        if: matrix.os == 'ubuntu-latest'
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ needs.create-release.outputs.upload_url }}
          asset_path: ./installers/linux/MCP-Client-${{ needs.create-release.outputs.version }}.AppImage
          asset_name: MCP-Client-${{ needs.create-release.outputs.version }}.AppImage
          asset_content_type: application/octet-stream

  publish-update-manifest:
    needs: [create-release, build-and-upload]
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Create update manifests
        run: |
          VERSION=${{ needs.create-release.outputs.version }}
          
          # Create Windows update manifest
          cat > update-win.json << EOL
          {
            "version": "$VERSION",
            "notes": "See the release page for details: https://github.com/your-org/mcp-client/releases/tag/v$VERSION",
            "pub_date": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "platforms": {
              "windows-x86_64": {
                "signature": "",
                "url": "https://github.com/your-org/mcp-client/releases/download/v$VERSION/MCP-Client-$VERSION-x64.msi"
              }
            }
          }
          EOL
          
          # Create macOS update manifest
          cat > update-macos.json << EOL
          {
            "version": "$VERSION",
            "notes": "See the release page for details: https://github.com/your-org/mcp-client/releases/tag/v$VERSION",
            "pub_date": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "platforms": {
              "darwin-x86_64": {
                "signature": "",
                "url": "https://github.com/your-org/mcp-client/releases/download/v$VERSION/MCP-Client-$VERSION-Intel.dmg"
              },
              "darwin-aarch64": {
                "signature": "",
                "url": "https://github.com/your-org/mcp-client/releases/download/v$VERSION/MCP-Client-$VERSION-AppleSilicon.dmg"
              }
            }
          }
          EOL
          
          # Create Linux update manifest
          cat > update-linux.json << EOL
          {
            "version": "$VERSION",
            "notes": "See the release page for details: https://github.com/your-org/mcp-client/releases/tag/v$VERSION",
            "pub_date": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "platforms": {
              "linux-x86_64": {
                "signature": "",
                "url": "https://github.com/your-org/mcp-client/releases/download/v$VERSION/MCP-Client-$VERSION.AppImage"
              }
            }
          }
          EOL
      
      - name: Deploy update manifests
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./
          publish_branch: updates
          destination_dir: ./updates
          keep_files: true
          user_name: 'github-actions[bot]'
          user_email: 'github-actions[bot]@users.noreply.github.com'
          commit_message: 'Update release manifest for v${{ needs.create-release.outputs.version }}'
          include_files: 'update-win.json,update-macos.json,update-linux.json'

  notify:
    needs: [create-release, build-and-upload, publish-update-manifest]
    runs-on: ubuntu-latest
    
    steps:
      - name: Send release notification
        uses: rtCamp/action-slack-notify@v2
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
          SLACK_CHANNEL: releases
          SLACK_COLOR: '#00BFA5'
          SLACK_ICON: https://github.com/your-org/mcp-client/raw/main/assets/app-icon.png
          SLACK_MESSAGE: ':rocket: MCP Client v${{ needs.create-release.outputs.version }} has been released! <https://github.com/your-org/mcp-client/releases/tag/v${{ needs.create-release.outputs.version }}|View Release>'
          SLACK_TITLE: New Release
          SLACK_USERNAME: GitHub Actions
</file>

<file path=".gitignore">
# Generated files
**/node_modules/
**/dist/
**/.tauri/
**/target/

# Environment variables
.env
.env.local
.env.development.local
.env.test.local
.env.production.local

# Editor directories and files
.vscode/*
!.vscode/extensions.json
.idea
.DS_Store
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

# Build artifacts
**/*.rs.bk
*.pdb

# Logs
npm-debug.log*
yarn-debug.log*
yarn-error.log*
*.log

# Local configuration
config.json
</file>

<file path="benches/performance_bench.rs">
use criterion::{criterion_group, criterion_main, Criterion};

fn bench_dummy(c: &mut Criterion) {
    c.bench_function("dummy", |b| b.iter(|| 1 + 1));
}

criterion_group!(benches, bench_dummy);
criterion_main!(benches);
</file>

<file path="docs/ARCHITECTURE.md">
# MCP Client Architecture

This document provides an overview of the MCP Client architecture, explaining the key components, their relationships, and the design decisions.

## System Architecture

The MCP Client is built with a hybrid architecture:

- **Frontend**: React with TypeScript
- **Backend**: Rust with Tauri
- **Bridge**: Tauri commands for communication between frontend and backend

```
┌───────────────────────────────────────┐
│             MCP Client                │
│                                       │
│  ┌─────────────┐     ┌─────────────┐  │
│  │             │     │             │  │
│  │   Frontend  │◄───►│   Backend   │  │
│  │   (React)   │     │   (Rust)    │  │
│  │             │     │             │  │
│  └─────────────┘     └─────────────┘  │
│                                       │
└───────────────────────────────────────┘
```

## Component Overview

### Frontend

The frontend is built with React and TypeScript, providing a responsive and intuitive user interface.

Key frontend components:
- **Conversation UI**: Rich text conversations with markdown support
- **Settings UI**: Configuration interface for various application settings
- **Resource Dashboard**: Visualization of system resources and metrics
- **Offline Mode UI**: Interface for managing offline capabilities

### Backend

The backend is built with Rust and Tauri, providing the core functionality and performance optimizations.

Key backend components:
- **Commands**: Tauri commands for communication with the frontend
- **Observability**: Metrics, logging, and telemetry systems
- **Offline**: Local-first architecture with embedded LLMs
- **Auto-Update**: Background update system with progressive rollouts
- **Optimization**: Memory management and caching

## Directory Structure

```
mcp-client/
├── src/                   # Backend Rust code
│   ├── auto_update/       # Auto-update functionality
│   ├── commands/          # Tauri commands for frontend communication
│   ├── observability/     # Metrics, logging, and telemetry
│   │   ├── metrics.rs     # Metrics collection
│   │   ├── logging.rs     # Logging system
│   │   ├── telemetry.rs   # Telemetry system
│   │   └── canary.rs      # Canary release infrastructure
│   ├── offline/           # Offline capabilities
│   │   ├── llm/           # Local LLM support
│   │   ├── checkpointing/ # Checkpointing system
│   │   └── sync/          # Synchronization mechanism
│   └── optimization/      # Performance optimizations
│       ├── memory/        # Memory management
│       └── cache/         # Caching system
├── src-frontend/          # Frontend React code
│   ├── src/               # Source code
│   │   ├── components/    # React components
│   │   ├── hooks/         # Custom React hooks
│   │   ├── context/       # React context providers
│   │   ├── utils/         # Utility functions
│   │   ├── services/      # API and backend services
│   │   ├── styles/        # CSS and styling
│   │   └── types/         # TypeScript type definitions
│   ├── public/            # Public assets
│   └── package.json       # Frontend dependencies
├── src-tauri/             # Tauri configuration
│   ├── Cargo.toml         # Tauri dependencies
│   ├── tauri.conf.json    # Tauri configuration
│   └── icons/             # Application icons
├── installers/            # Platform-specific installers
│   ├── windows-build.ps1  # Windows build script
│   ├── macos-build.sh     # macOS build script
│   └── linux-build.sh     # Linux build script
├── dist/                  # Distribution output
├── docs/                  # Documentation
│   ├── README.md          # Overview
│   ├── INSTALLATION.md    # Installation guide
│   ├── USER_GUIDE.md      # User guide
│   └── API.md             # API documentation
├── tests/                 # Tests
│   ├── unit/              # Unit tests
│   └── integration/       # Integration tests
├── Cargo.toml             # Rust dependencies
├── package.json           # Project configuration
└── README.md              # Project overview
```

## Key Components

### Observability System

The observability system consists of three main components:

1. **Metrics Collection**: Collects performance metrics and statistics
2. **Enhanced Logging**: Structured logging with different log levels
3. **Telemetry System**: Privacy-focused telemetry for tracking usage and errors

```
┌───────────────────────────────────────┐
│           Observability               │
│                                       │
│  ┌─────────────┐     ┌─────────────┐  │
│  │             │     │             │  │
│  │   Metrics   │     │   Logging   │  │
│  │             │     │             │  │
│  └─────────────┘     └─────────────┘  │
│                                       │
│          ┌─────────────┐              │
│          │             │              │
│          │  Telemetry  │              │
│          │             │              │
│          └─────────────┘              │
│                                       │
└───────────────────────────────────────┘
```

### Offline System

The offline system enables the application to function without an internet connection:

1. **Local LLM**: Embedded language models for offline inference
2. **Checkpointing**: System for saving and restoring conversation state
3. **Synchronization**: Two-way sync mechanism for seamless transitions

```
┌───────────────────────────────────────┐
│            Offline System             │
│                                       │
│  ┌─────────────┐     ┌─────────────┐  │
│  │             │     │             │  │
│  │  Local LLM  │     │Checkpointing│  │
│  │             │     │             │  │
│  └─────────────┘     └─────────────┘  │
│                                       │
│          ┌─────────────┐              │
│          │             │              │
│          │     Sync    │              │
│          │             │              │
│          └─────────────┘              │
│                                       │
└───────────────────────────────────────┘
```

### Optimization System

The optimization system improves performance and resource usage:

1. **Memory Management**: Intelligent memory usage with garbage collection
2. **Caching System**: API and resource caching for faster responses

```
┌───────────────────────────────────────┐
│          Optimization System          │
│                                       │
│  ┌─────────────┐     ┌─────────────┐  │
│  │             │     │             │  │
│  │   Memory    │     │    Cache    │  │
│  │ Management  │     │    System   │  │
│  │             │     │             │  │
│  └─────────────┘     └─────────────┘  │
│                                       │
└───────────────────────────────────────┘
```

### Auto-Update System

The auto-update system ensures the application stays current:

1. **Update Checker**: Periodically checks for updates
2. **Download Manager**: Downloads updates in the background
3. **Installer**: Installs updates when appropriate

```
┌───────────────────────────────────────┐
│          Auto-Update System           │
│                                       │
│  ┌─────────────┐     ┌─────────────┐  │
│  │             │     │             │  │
│  │   Update    │     │  Download   │  │
│  │   Checker   │     │   Manager   │  │
│  │             │     │             │  │
│  └─────────────┘     └─────────────┘  │
│                                       │
│          ┌─────────────┐              │
│          │             │              │
│          │  Installer  │              │
│          │             │              │
│          └─────────────┘              │
│                                       │
└───────────────────────────────────────┘
```

## Data Flow

### Frontend to Backend Communication

Communication between the frontend and backend is handled through Tauri commands:

1. Frontend calls a Tauri command
2. Backend processes the command
3. Backend returns the result to the frontend

```
┌───────────────┐          ┌───────────────┐
│               │          │               │
│   Frontend    │  Command │    Backend    │
│    (React)    │─────────►│    (Rust)     │
│               │          │               │
│               │  Result  │               │
│               │◄─────────│               │
└───────────────┘          └───────────────┘
```

### Offline Data Flow

When in offline mode, the data flow changes:

1. Frontend sends a request
2. Backend checks if offline mode is active
3. If offline, the request is processed locally
4. If online, the request is sent to the server

```
┌───────────────┐          ┌───────────────┐
│               │          │               │
│   Frontend    │  Request │    Backend    │
│    (React)    │─────────►│    (Rust)     │
│               │          │               │
│               │          │   ┌─────────┐ │
│               │          │   │ Offline │ │
│               │          │   │ Check   │ │
│               │          │   └─────────┘ │
│               │          │       │       │
│               │          │       ▼       │
│               │          │ ┌─────────────┐
│               │ Response │ │             │
│               │◄─────────┤ │ Local / API │
│               │          │ │             │
└───────────────┘          └─────────────┘
```

## Scalability and Performance

The MCP Client is designed for scalability and performance:

- **Memory Management**: Efficient memory usage with configurable limits
- **Caching**: API and resource caching for faster response times
- **Local LLMs**: Optimized local language models for offline inference
- **Async Processing**: Non-blocking operations for responsive UI

## Security

The MCP Client implements several security measures:

- **Secure Updates**: Updates are verified using cryptographic signatures
- **Data Encryption**: Sensitive data is encrypted both in transit and at rest
- **Permission Model**: Minimal required permissions for system operations
- **Privacy-Focused Telemetry**: Only essential usage data is collected with user consent

## Error Handling

The application implements a robust error handling strategy:

- **Graceful Degradation**: Continues functioning with reduced capabilities when errors occur
- **Offline Fallback**: Falls back to offline mode when network errors occur
- **Error Reporting**: Detailed error logging with optional telemetry
- **Recovery Mechanisms**: Automatic recovery from non-critical errors

## Testing Strategy

The testing strategy includes several layers:

- **Unit Tests**: Test individual components in isolation
- **Integration Tests**: Test interactions between components
- **Performance Tests**: Benchmark performance and resource usage
- **End-to-End Tests**: Test complete user workflows

## Future Enhancements

Planned future enhancements include:

- **Multiple Model Support**: Support for switching between different local models
- **Local Fine-tuning**: Allow users to fine-tune local models on their data
- **Enhanced Conflict Resolution**: More sophisticated conflict resolution strategies
- **Differential Sync**: More efficient synchronization using diffs
- **Custom Dashboards**: Allow users to create custom dashboards for metrics
- **Alerting System**: Add alerts for critical metrics and events
- **Distributed Tracing**: Implement end-to-end request tracing
</file>

<file path="docs/INSTALLATION.md">
# MCP Client Installation Guide

This guide provides detailed instructions for installing the MCP Client on various platforms.

## Windows Installation

### System Requirements

- Windows 10 or later (64-bit)
- 4GB RAM minimum (8GB recommended)
- 1GB free disk space
- Administrator privileges for installation

### Using the MSI Installer

1. Download the latest MSI installer from the [releases page](https://github.com/your-org/mcp-client/releases)
2. Right-click the downloaded file and select "Properties"
3. Check "Unblock" if available, then click "OK"
4. Double-click the installer to start the installation process
5. Follow the on-screen instructions
6. Choose the installation location (default is recommended)
7. Select whether to create desktop shortcuts
8. Click "Install" to begin the installation
9. If prompted by UAC (User Account Control), click "Yes" to allow the installation
10. Once installation is complete, click "Finish"

### Using the Portable Version

1. Download the portable EXE from the [releases page](https://github.com/your-org/mcp-client/releases)
2. Create a folder where you want to keep the application (e.g., `C:\Programs\MCP-Client`)
3. Move the downloaded EXE to this folder
4. (Optional) Create a shortcut on the desktop or taskbar
5. Double-click the EXE to run the application

### Silent Installation

For enterprise deployment, you can install silently using:

```batch
msiexec /i MCP-Client-1.0.0-x64.msi /quiet
```

## macOS Installation

### System Requirements

- macOS 10.15 (Catalina) or later
- Intel or Apple Silicon processor
- 4GB RAM minimum (8GB recommended)
- 1GB free disk space

### Using the DMG Installer

1. Download the latest DMG file from the [releases page](https://github.com/your-org/mcp-client/releases)
2. Double-click the downloaded DMG file to mount it
3. Drag the MCP Client application to the Applications folder
4. Eject the mounted DMG
5. Open your Applications folder and right-click on the MCP Client
6. Select "Open"
7. On first launch, you may see a security warning. Click "Open" to proceed
8. If prompted about allowing system access, follow the on-screen instructions

### Apple Silicon vs Intel Macs

- Universal DMG: Works on both Intel and Apple Silicon Macs (recommended)
- Intel DMG: Optimized for Intel Macs
- Apple Silicon DMG: Optimized for Apple Silicon Macs (M1/M2)

### Homebrew Installation

If you use Homebrew, you can install MCP Client with:

```bash
brew install --cask mcp-client
```

## Linux Installation

### System Requirements

- Ubuntu 20.04 or later, Fedora 34 or later, or equivalent
- X11 or Wayland display server
- 4GB RAM minimum (8GB recommended)
- 1GB free disk space

### Debian/Ubuntu Installation (DEB)

1. Download the latest DEB package from the [releases page](https://github.com/your-org/mcp-client/releases)
2. Open a terminal and navigate to the download location
3. Install using apt:

```bash
sudo apt install ./mcp-client_1.0.0_amd64.deb
```

4. Launch from your applications menu or via terminal:

```bash
mcp-client
```

### Fedora/RHEL Installation (RPM)

1. Download the latest RPM package from the [releases page](https://github.com/your-org/mcp-client/releases)
2. Open a terminal and navigate to the download location
3. Install using dnf:

```bash
sudo dnf install ./mcp-client-1.0.0.x86_64.rpm
```

4. Launch from your applications menu or via terminal:

```bash
mcp-client
```

### AppImage Installation

1. Download the latest AppImage from the [releases page](https://github.com/your-org/mcp-client/releases)
2. Open a terminal and navigate to the download location
3. Make the AppImage executable:

```bash
chmod +x MCP-Client-1.0.0.AppImage
```

4. Run the AppImage:

```bash
./MCP-Client-1.0.0.AppImage
```

5. (Optional) Integrate with your desktop:

```bash
./MCP-Client-1.0.0.AppImage --install
```

## Verifying Installation

After installation, verify that the application is working correctly:

1. Launch the MCP Client
2. You should see the login screen or welcome page
3. Check the version number in Help > About to ensure you have the latest version

## Troubleshooting

### Windows Issues

- **Installation Fails**: Ensure you have administrator privileges and check Windows logs
- **Application Crashes**: Update your graphics drivers and ensure DirectX is up to date
- **Missing DLLs**: Install the latest Visual C++ Redistributable packages

### macOS Issues

- **"App is damaged"**: Right-click the app, choose "Open", then click "Open" in the dialog
- **"Unidentified Developer"**: Go to System Preferences > Security & Privacy and click "Open Anyway"
- **App Won't Open**: Check if your macOS version is supported (10.15+)

### Linux Issues

- **Missing Libraries**: Install required dependencies:

```bash
# Debian/Ubuntu
sudo apt install libwebkit2gtk-4.0-37 libgtk-3-0 libsoup2.4-1

# Fedora
sudo dnf install webkit2gtk3 gtk3 libsoup
```

- **AppImage Won't Run**: Check file permissions and ensure FUSE is installed:

```bash
sudo apt install fuse libfuse2   # Debian/Ubuntu
sudo dnf install fuse libfuse    # Fedora
```

## Uninstallation

### Windows

- Use Add/Remove Programs in Control Panel
- Or uninstall via command line: `msiexec /x MCP-Client-1.0.0-x64.msi`

### macOS

- Drag the application from Applications to Trash
- To remove all associated files:

```bash
rm -rf ~/Library/Application\ Support/MCP-Client
```

### Linux

#### Debian/Ubuntu:

```bash
sudo apt remove mcp-client
```

#### Fedora/RHEL:

```bash
sudo dnf remove mcp-client
```

#### AppImage:

- Simply delete the AppImage file
- If you used `--install`, run:

```bash
./MCP-Client-1.0.0.AppImage --remove-appimage-desktop-integration
```

## Enterprise Deployment

### Windows MSI Properties

For enterprise deployment, you can use the following MSI properties:

```
INSTALLDIR - Installation directory
CREATEDESKTOPSHORTCUT - "1" to create a desktop shortcut, "0" to skip
STARTMENUSHORTCUT - "1" to create a start menu shortcut, "0" to skip
AUTOSTARTUP - "1" to start on boot, "0" to skip
```

Example:

```batch
msiexec /i MCP-Client-1.0.0-x64.msi INSTALLDIR="C:\Programs\MCP-Client" CREATEDESKTOPSHORTCUT="1" AUTOSTARTUP="0" /quiet
```

### macOS Deployment

For mass deployment on macOS, consider using:

- Jamf Pro
- Munki
- Apple Remote Desktop

### Linux Deployment

- Consider using your distribution's package management system
- For containerized environments, Docker images are available
</file>

<file path="docs/local_llm_integration.md">
# Local LLM Integration

This document provides comprehensive information about the local LLM (Large Language Model) integration in the MCP Client, including user guides, developer documentation, and troubleshooting information.

## Overview

The MCP Client supports local LLM providers to enable offline capabilities and reduce dependency on remote API services. Local LLMs can be used for:

- Text generation when offline
- Chat conversations without internet connectivity
- Reduced latency for certain operations
- Privacy-sensitive workloads that should not leave the device
- Fallback when cloud services are unavailable

The implementation follows a provider-based approach, allowing different LLM backends to be integrated through a common interface. This architecture enables users to choose the best solution for their specific hardware, needs, and preferences.

## User Guide

### Setting Up Local LLM Providers

#### Enabling Offline Mode

1. Open the MCP Client application
2. Navigate to Settings > Offline
3. Toggle "Enable Offline Mode" to enable local LLM capabilities
4. Optionally, enable "Auto-switch based on connectivity" to automatically switch between local and cloud models based on your network status

#### Understanding Provider Selection

The MCP Client supports various LLM providers, each with different characteristics:

- **Ollama**: User-friendly, wide model support, good performance
- **LocalAI**: OpenAI-compatible API, extensive customization
- **llama.cpp**: Optimal performance, specialized for local inference
- **Custom**: For advanced users with custom LLM implementations

Choose the provider that best suits your needs based on:
- Ease of use
- Performance requirements
- Model availability
- Hardware capabilities

#### Configuring LLM Providers

Each provider requires its own setup:

##### Ollama

[Ollama](https://ollama.ai/) is a user-friendly tool for running various LLMs locally.

1. **Installation**:
   - Download and install Ollama from [https://ollama.ai/download](https://ollama.ai/download)
   - Follow the installation instructions for your platform
   - Ensure Ollama is running (it typically starts automatically)

2. **Configuration in MCP Client**:
   - In Settings > Offline, select "Ollama" as your Local LLM Provider
   - The default endpoint is `http://localhost:11434` (usually no change needed)
   - No API key is required for Ollama

3. **Downloading Models**:
   - In the "Models" tab, browse available models
   - Click the download icon next to a model to download it
   - Wait for the download to complete (this may take time depending on the model size)
   - Optionally, set a default model to use when in offline mode

##### LocalAI

[LocalAI](https://github.com/go-skynet/LocalAI) is an API compatible with OpenAI's API but running locally.

1. **Installation**:
   - Follow the installation instructions at [LocalAI GitHub repository](https://github.com/go-skynet/LocalAI)
   - Start the LocalAI server using the provided scripts or Docker commands

2. **Configuration in MCP Client**:
   - In Settings > Offline, select "LocalAI" as your Local LLM Provider
   - Set the endpoint URL (default is `http://localhost:8080`)
   - No API key is required by default, but you can configure one if you've set up authentication

3. **Downloading Models**:
   - LocalAI models are managed through the LocalAI interface
   - In the "Models" tab of MCP Client, you'll see available models detected from LocalAI
   - Download models using the LocalAI interface, and they will appear in MCP Client

##### llama.cpp

[llama.cpp](https://github.com/ggerganov/llama.cpp) is a popular implementation of LLaMA models optimized for CPU usage.

1. **Installation**:
   - Clone and build the repository from [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
   - Start the server with `./server -m /path/to/model.gguf`

2. **Configuration in MCP Client**:
   - In Settings > Offline, select "LlamaCpp" as your Local LLM Provider
   - Set the endpoint URL (default is `http://localhost:8000`)
   - No API key is required

3. **Downloading Models**:
   - Download GGUF models from [Hugging Face](https://huggingface.co/)
   - Point llama.cpp server to your model file
   - MCP Client will detect the model from the server

##### Custom Provider

For advanced users who have custom LLM servers:

1. **Setup**:
   - Ensure your custom provider has a REST API endpoint
   - Configure authentication if required

2. **Configuration in MCP Client**:
   - In Settings > Offline, select "Custom" as your Local LLM Provider
   - Enter the endpoint URL for your custom provider
   - Configure API key if required
   - Use Advanced Configuration to fine-tune provider-specific settings

### Managing Models

#### Downloading Models

1. In Settings > Offline, browse to the Models tab
2. View available models for your selected provider
3. Click the download icon to start downloading a model
4. Monitor the download progress in the UI
5. Downloaded models will appear in the "Downloaded Models" tab

#### Setting a Default Model

1. Go to Settings > Offline
2. In the Local LLM Provider section, find the "Default Model" dropdown
3. Select a model from your downloaded models
4. This model will be used by default for offline operations

#### Managing Downloads

- **Cancel Download**: Click the pause icon during an in-progress download
- **Delete Model**: Click the trash icon next to a downloaded model to remove it
- **Model Info**: Click the info icon to view detailed information about the model

### Using Local LLMs

Once configured, local LLMs are used automatically when:

1. You are in offline mode (either manually enabled or auto-switched due to connectivity)
2. The client needs to generate text or respond to chat messages

You can see which model is being used in the conversation interface, which will indicate "Using local model: [model name]" when in offline mode.

### Performance Metrics

MCP Client includes a dashboard for monitoring LLM performance:

1. Go to Dashboard > LLM to view the performance metrics
2. The dashboard shows:
   - Generation counts and success rates
   - Throughput (tokens per second)
   - Latency metrics (average, p90, p99)
   - Model-specific performance statistics
   - Resource usage during inference

Note: Metrics collection is privacy-respecting and opt-in. You can configure it in the Settings > Offline section.

## Provider Comparison

| Feature | Ollama | LocalAI | llama.cpp | Custom |
|---------|--------|---------|-----------|--------|
| **Ease of Setup** | ★★★★★ | ★★★☆☆ | ★★☆☆☆ | ★☆☆☆☆ |
| **Performance** | ★★★★☆ | ★★★★☆ | ★★★★★ | Varies |
| **Model Variety** | ★★★★☆ | ★★★★★ | ★★★☆☆ | Varies |
| **Memory Usage** | ★★★☆☆ | ★★★★☆ | ★★★★★ | Varies |
| **API Compatibility** | Ollama API | OpenAI API | Simple API | Custom |
| **Quantization Options** | Limited | Extensive | Extensive | Varies |
| **GPU Support** | Yes | Yes | Yes | Varies |
| **Default Port** | 11434 | 8080 | 8000 | Custom |
| **UI for Management** | Yes | No | No | Varies |
| **Auto-discovery** | Yes | Yes | Limited | No |
| **Embedding Support** | Yes | Yes | Limited | Varies |

### Detailed Comparison

#### Ollama
- **Strengths**: Easy to set up and use, good selection of models, automatic model management
- **Weaknesses**: Less control over model loading parameters, limited quantization options
- **Best for**: Beginners, users who want a simple experience with good performance
- **Hardware Requirements**: 8GB RAM minimum, 16GB recommended, GPU optional but beneficial
- **Models**: Llama 2, Mistral, Vicuna, and many others with easy download commands

#### LocalAI
- **Strengths**: OpenAI API compatibility, extensive customization options, multi-modal support
- **Weaknesses**: More complex setup, requires more technical knowledge
- **Best for**: Users who need OpenAI compatibility or want extensive customization
- **Hardware Requirements**: 8GB RAM minimum, 16GB recommended, GPU optional
- **Models**: Compatible with GGUF, GGML, and other common formats

#### llama.cpp
- **Strengths**: Best performance, highly optimized, extensive control over parameters
- **Weaknesses**: More technical to set up, requires command-line knowledge
- **Best for**: Performance-focused users, limited hardware, technical users
- **Hardware Requirements**: 4GB RAM minimum (for small models), scales with model size
- **Models**: Primarily GGUF format models

#### Custom Providers
- **Strengths**: Complete flexibility, can integrate any LLM backend
- **Weaknesses**: Requires implementation and maintenance by user
- **Best for**: Advanced users, specialized use cases, research
- **Hardware Requirements**: Depends on implementation
- **Models**: Depends on implementation

### Recommendations

- **For beginners**: Start with Ollama, which offers the simplest setup experience
- **For best performance**: llama.cpp provides the most optimized inference
- **For OpenAI compatibility**: LocalAI offers the most compatible API
- **For advanced/custom needs**: Use Custom provider with your specific implementation

## Troubleshooting

### Common Issues

#### Provider Not Available

**Symptoms**:
- "Provider not available" warning in the UI
- Unable to see or download models

**Solutions**:
1. Ensure the provider software is installed and running
2. Check that the endpoint URL is correct
3. Verify network connectivity to the local service (try opening the URL in a browser)
4. Check for any firewalls blocking the connection
5. Restart the provider service and MCP Client

#### Model Download Failures

**Symptoms**:
- Download starts but fails before completion
- Error message in the download status

**Solutions**:
1. Check available disk space
2. Ensure you have stable internet connection during download
3. Try downloading a smaller model first
4. Check provider logs for specific errors:
   - Ollama: `~/.ollama/logs` or system logs
   - LocalAI: Check the terminal where LocalAI is running
   - llama.cpp: Check the terminal output
5. Restart the provider service and try again

#### High Memory Usage

**Symptoms**:
- System becomes slow when using local models
- Application crashes during text generation
- Out of memory errors

**Solutions**:
1. Use smaller models or models with higher quantization (e.g., q4_K_M instead of f16)
2. Close other memory-intensive applications
3. Increase system swap/page file size
4. For advanced users, adjust context window size in model parameters
5. Try a different provider that may have better memory management

#### Slow Generation Speed

**Symptoms**:
- Very slow responses when using local models
- Generation takes significantly longer than expected

**Solutions**:
1. Check CPU/GPU usage during generation
2. Try a smaller or more optimized model
3. Ensure you're using GPU acceleration if available:
   - Ollama automatically uses GPU if available
   - LocalAI may need specific configuration
   - llama.cpp needs to be compiled with appropriate flags
4. Reduce the model's context window size if configurable
5. Check for other processes using significant CPU/GPU resources

#### Offline Mode Not Working

**Symptoms**:
- Still using cloud APIs despite being offline
- Error messages when trying to generate text offline

**Solutions**:
1. Ensure offline mode is enabled in Settings
2. Verify a default model is selected
3. Check that the selected model is downloaded and available
4. Restart the provider service
5. Restart MCP Client
6. Check logs for specific errors

### Provider-Specific Troubleshooting

#### Ollama

**Common issues**:
- Service stops unexpectedly
- Model downloads incomplete

**Troubleshooting**:
1. Update Ollama to the latest version
2. Check logs: `~/.ollama/logs`
3. Run `ollama serve` manually to see console output
4. Remove and re-add problematic models: `ollama rm <model>`
5. Check GitHub issues for known problems

#### LocalAI

**Common issues**:
- Model compatibility
- API errors
- Configuration complexity

**Troubleshooting**:
1. Run LocalAI with verbose logging: `LOCAL_AI_DEBUG=1`
2. Check model formats are compatible
3. Verify configuration file has correct model paths
4. Update to the latest LocalAI version
5. Test API directly using curl before using through MCP Client

#### llama.cpp

**Common issues**:
- Compilation problems
- Model format compatibility
- Server configuration

**Troubleshooting**:
1. Ensure you're using GGUF model format (newer versions)
2. Check compilation flags match your hardware
3. Run server with verbose logging
4. Test direct API calls to isolate issues
5. Try different server parameters (threads, context size)

### Diagnostic Information

When reporting issues, please include:

1. MCP Client version
2. Provider type and version
3. Model name and size
4. System specifications (OS, CPU, RAM, GPU)
5. Relevant log entries
6. Steps to reproduce the issue

## Developer Documentation

### Architecture

The offline LLM capabilities are implemented with a provider-based architecture:

```
┌─────────────────────────┐
│    UI (Offline Settings)│
└───────────┬─────────────┘
            │
┌───────────▼─────────────┐       ┌───────────────────┐
│    Provider Manager     │◄──────►   Discovery Service│
└───────────┬─────────────┘       └───────────────────┘
            │
┌───────────▼─────────────┐
│     Provider Interface  │
└───────────┬─────────────┘
            │
     ┌──────┴─────────────────┐
     │                        │
┌────▼───┐  ┌─────────┐  ┌────▼───┐  ┌─────────┐
│ Ollama │  │ LocalAI │  │llama.cpp│  │ Custom  │
└────────┘  └─────────┘  └─────────┘  └─────────┘
```

Key components:

1. **Provider Interface**: Common interface for all LLM providers
2. **Provider Manager**: Manages provider configurations and selection
3. **Discovery Service**: Auto-detection of installed providers
4. **Provider Implementations**: Specific implementations for each supported provider
5. **Metrics Collection**: Performance monitoring for local models
6. **UI Components**: User interface for configuration and monitoring

### Key Files

#### Backend (Rust)

- **Provider Interface and Manager**:  
  `src/commands/offline/llm.rs` - Provider types and manager implementation

- **Provider Discovery**:  
  `src/offline/llm/discovery.rs` - Provider discovery logic  
  `src/offline/llm/migration.rs` - Migration from legacy systems

- **Local LLM Implementation**:  
  `src/offline/llm/mod.rs` - Core LLM functionality

- **Metrics Collection**:  
  `src/observability/metrics/llm.rs` - LLM-specific metrics

#### Frontend (TypeScript/React)

- **Settings UI**:  
  `src-frontend/src/components/offline/OfflineSettings.tsx` - Offline settings UI  
  `src-frontend/src/components/offline/LLMMetricsPrivacyNotice.tsx` - Privacy notice for metrics

- **Dashboard**:  
  `src-frontend/src/components/dashboard/llm/LLMPerformanceDashboard.tsx` - LLM metrics dashboard

### Provider Interface

The provider interface defines common functionality that all LLM providers must implement:

```rust
pub trait Provider: Send + Sync {
    /// Get provider type
    fn get_type(&self) -> ProviderType;
    
    /// Get provider name
    fn get_name(&self) -> String;
    
    /// Get provider description
    fn get_description(&self) -> String;
    
    /// Get provider version
    fn get_version(&self) -> String;
    
    /// Check if provider is available
    fn is_available(&self) -> Result<bool>;
    
    /// List available models
    fn list_available_models(&self) -> Result<Vec<ModelInfo>>;
    
    /// List downloaded models
    fn list_downloaded_models(&self) -> Result<Vec<ModelInfo>>;
    
    /// Download a model
    fn download_model(&self, model_id: &str) -> Result<()>;
    
    /// Cancel a model download
    fn cancel_download(&self, model_id: &str) -> Result<()>;
    
    /// Get download status
    fn get_download_status(&self, model_id: &str) -> Result<DownloadStatus>;
    
    /// Delete a model
    fn delete_model(&self, model_id: &str) -> Result<()>;
    
    /// Generate text using a specified model
    fn generate_text(&self, request: GenerationRequest) -> Result<GenerationResponse>;
}
```

### Adding a New Provider

To add a new LLM provider:

1. **Create a new provider implementation**:
   ```rust
   pub struct MyProvider {
       // Provider-specific fields
   }
   
   impl Provider for MyProvider {
       // Implement required methods
   }
   ```

2. **Register the provider in the provider manager**:
   ```rust
   // In provider_manager.rs
   pub fn register_provider(&mut self, provider: Box<dyn Provider>) {
       let provider_type = provider.get_type().to_string();
       self.providers.insert(provider_type.clone(), provider);
   }
   ```

3. **Add provider-specific UI configuration**:
   - Update `OfflineSettings.tsx` to handle your provider configuration
   - Add any provider-specific settings

4. **Add metrics collection**:
   - Update metrics collection for provider-specific metrics

5. **Update documentation**:
   - Add provider details to this documentation
   - Include setup instructions and troubleshooting

### Performance Metrics Collection

The LLM metrics system collects the following information:

1. **Generation Metrics**:
   - Latency (time to complete generation)
   - Throughput (tokens per second)
   - Time to first token
   - Success/failure rates

2. **Provider Metrics**:
   - Available models
   - Success/failure of operations
   - Resource usage (CPU, memory)

3. **Model Metrics**:
   - Model-specific performance
   - Usage counts
   - Error rates

Metrics collection is privacy-focused and opt-in. The collected data is used to:

1. Provide dashboard visualizations for users
2. Optimize the application performance
3. Identify issues with specific models or providers

### Best Practices for Implementation

1. **Error Handling**:
   - Gracefully handle provider unavailability
   - Provide clear error messages to users
   - Fallback mechanisms when local inference fails

2. **Resource Management**:
   - Monitor memory usage during inference
   - Implement timeouts for operations
   - Release resources when not in use

3. **Configuration**:
   - Validate user-provided endpoints and settings
   - Store provider configurations securely
   - Implement sensible defaults

4. **Performance**:
   - Benchmark your provider implementation
   - Optimize for both speed and resource usage
   - Use async operations where appropriate

## Examples

### Using the API

#### Example 1: Basic Chat Generation with Ollama

```typescript
// In a React component
const handleLocalGeneration = async (prompt: string) => {
  try {
    const response = await invoke('generate_text', {
      provider_type: 'Ollama',
      model_id: 'llama2',
      prompt: prompt,
      max_tokens: 1000,
      temperature: 0.7,
      options: {}
    });
    
    setResponse(response.data);
    
  } catch (error) {
    console.error('Generation failed:', error);
    setError('Failed to generate text locally');
  }
};
```

#### Example 2: Streaming Generation with LocalAI

```typescript
// In a React component
const streamLocalGeneration = async (prompt: string) => {
  try {
    // Start streaming
    await invoke('stream_text', {
      provider_type: 'LocalAI',
      model_id: 'gpt-3.5-turbo',
      prompt: prompt,
      max_tokens: 1000,
      temperature: 0.7,
      options: {}
    });
    
    // Set up a listener for streaming events
    const unlisten = await listen('generation_chunk', (event) => {
      const chunk = event.payload as string;
      
      if (chunk === '[DONE]') {
        // Stream complete
        unlisten();
      } else {
        // Append chunk to the UI
        setResponse(prev => prev + chunk);
      }
    });
    
  } catch (error) {
    console.error('Streaming failed:', error);
    setError('Failed to stream text locally');
  }
};
```

#### Example 3: Advanced Configuration with llama.cpp

```typescript
// In a React component
const generateWithAdvancedConfig = async (prompt: string) => {
  try {
    const response = await invoke('generate_text', {
      provider_type: 'LlamaCpp',
      model_id: 'wizardlm-13b',
      prompt: prompt,
      max_tokens: 2000,
      temperature: 0.5,
      options: {
        // Provider-specific parameters
        context_size: 4096,
        repeat_penalty: 1.1,
        top_k: 40,
        top_p: 0.9,
        threads: 4,
        seed: 42
      }
    });
    
    setResponse(response.data);
    
  } catch (error) {
    console.error('Generation failed:', error);
    setError('Failed to generate text locally');
  }
};
```

### UI Integration Examples

#### Example 1: Showing Provider Status

```tsx
// In a React component
const ProviderStatus = ({ providerType }: { providerType: string }) => {
  const [isAvailable, setIsAvailable] = useState<boolean>(false);
  const [version, setVersion] = useState<string>('');
  
  useEffect(() => {
    const checkAvailability = async () => {
      try {
        const response = await invoke('check_provider_availability', { 
          provider_type: providerType 
        });
        
        if (response.success && response.data) {
          setIsAvailable(response.data.available);
          setVersion(response.data.version || 'Unknown');
        }
      } catch (error) {
        console.error('Failed to check provider:', error);
        setIsAvailable(false);
      }
    };
    
    checkAvailability();
  }, [providerType]);
  
  return (
    <div className="provider-status">
      <div className={`status-indicator ${isAvailable ? 'available' : 'unavailable'}`} />
      <div className="provider-info">
        <span className="provider-name">{providerType}</span>
        {isAvailable && <span className="provider-version">v{version}</span>}
      </div>
      <div className="status-text">
        {isAvailable ? 'Available' : 'Unavailable'}
      </div>
    </div>
  );
};
```

#### Example 2: Model Selection Dropdown

```tsx
// In a React component
const ModelSelector = ({ 
  providerType, 
  onModelSelect 
}: { 
  providerType: string;
  onModelSelect: (modelId: string) => void;
}) => {
  const [models, setModels] = useState<ModelInfo[]>([]);
  const [loading, setLoading] = useState<boolean>(true);
  
  useEffect(() => {
    const fetchModels = async () => {
      try {
        setLoading(true);
        const response = await invoke('list_downloaded_models', {
          provider_type: providerType
        });
        
        if (response.success && response.data) {
          setModels(response.data);
        }
      } catch (error) {
        console.error('Failed to fetch models:', error);
      } finally {
        setLoading(false);
      }
    };
    
    fetchModels();
  }, [providerType]);
  
  return (
    <div className="model-selector">
      <label>Select Model:</label>
      {loading ? (
        <div className="loading">Loading models...</div>
      ) : (
        <select 
          onChange={(e) => onModelSelect(e.target.value)}
          disabled={models.length === 0}
        >
          <option value="">Select a model</option>
          {models.map((model) => (
            <option key={model.id} value={model.id}>
              {model.name}
            </option>
          ))}
        </select>
      )}
      {models.length === 0 && !loading && (
        <div className="no-models">
          No models available. Please download models in Settings.
        </div>
      )}
    </div>
  );
};
```

## Conclusion

The local LLM integration in MCP Client provides a powerful way to use language models without relying on cloud services. The provider-based approach allows flexibility in choosing the right solution for your needs while maintaining a consistent interface.

For users, this means more control over their data and the ability to work offline. For developers, it offers a well-structured framework for extending functionality with new providers and capabilities.

As the field of local LLMs continues to evolve rapidly, this architecture allows the MCP Client to adapt and incorporate new advancements while maintaining backward compatibility and a consistent user experience.

## Resources

- [Ollama Documentation](https://github.com/ollama/ollama/blob/main/README.md)
- [LocalAI Documentation](https://github.com/go-skynet/LocalAI/blob/master/README.md)
- [llama.cpp Documentation](https://github.com/ggerganov/llama.cpp/blob/master/README.md)
- [GGUF Format Specification](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
- [MCP Client Offline Mode Documentation](./offline_capabilities.md)
</file>

<file path="docs/local_llm_integration/developer_documentation.md">
# Developer Documentation: Local LLM Architecture

This document provides comprehensive technical information for developers who want to understand, extend, or modify the local LLM integration in the MCP Client.

## Architecture Overview

The local LLM system is designed with a provider-based architecture that allows for flexibility, extensibility, and easy integration of different LLM backends.

### Key Components

![Architecture Diagram](../assets/images/llm_architecture.png)

1. **Provider Interface**: A common interface that all LLM providers must implement
2. **Provider Manager**: Central component that manages provider configurations and selection
3. **Discovery Service**: Auto-detection and configuration of installed providers
4. **Provider Implementations**: Specific implementations for each supported provider (Ollama, LocalAI, llama.cpp, Custom)
5. **Migration System**: Handles migration from legacy LLM systems
6. **Metrics Collection**: Performance monitoring and telemetry
7. **Frontend Components**: UI elements for configuring and using local LLMs

### Design Principles

The architecture follows these key principles:

- **Abstraction**: Common interface hiding implementation details
- **Loose Coupling**: Components interact through well-defined interfaces
- **Extensibility**: Easy addition of new provider types
- **Configuration**: Flexible configuration options for each provider
- **Auto-discovery**: Automatic detection of installed providers
- **Fallback**: Graceful degradation when providers are unavailable

## Core Components

### Provider Interface

The core of the system is the `Provider` trait, which defines the contract that all LLM providers must implement:

```rust
pub trait Provider: Send + Sync {
    /// Get provider type
    fn get_type(&self) -> ProviderType;
    
    /// Get provider name
    fn get_name(&self) -> String;
    
    /// Get provider description
    fn get_description(&self) -> String;
    
    /// Get provider version
    fn get_version(&self) -> String;
    
    /// Check if provider is available
    fn is_available(&self) -> Result<bool>;
    
    /// List available models
    fn list_available_models(&self) -> Result<Vec<ModelInfo>>;
    
    /// List downloaded models
    fn list_downloaded_models(&self) -> Result<Vec<ModelInfo>>;
    
    /// Download a model
    fn download_model(&self, model_id: &str) -> Result<()>;
    
    /// Cancel a model download
    fn cancel_download(&self, model_id: &str) -> Result<()>;
    
    /// Get download status
    fn get_download_status(&self, model_id: &str) -> Result<DownloadStatus>;
    
    /// Delete a model
    fn delete_model(&self, model_id: &str) -> Result<()>;
    
    /// Generate text using a specified model
    fn generate_text(&self, request: GenerationRequest) -> Result<GenerationResponse>;
}
```

This interface ensures that all providers offer a consistent set of capabilities regardless of their underlying implementation.

### Provider Types

The system defines a `ProviderType` enum to identify different types of providers:

```rust
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub enum ProviderType {
    Ollama,
    LocalAI,
    LlamaCpp,
    Custom(String),
}
```

Custom providers can be dynamically added by specifying a name in the `Custom` variant.

### Provider Manager

The `ProviderManager` is the central component that manages provider configurations and handles requests:

```rust
pub struct ProviderManager {
    /// Available providers
    providers: Mutex<HashMap<String, ProviderInfo>>,
    /// Provider availability status
    availability: Mutex<HashMap<String, AvailabilityResult>>,
    /// Active provider
    active_provider: Mutex<Option<ProviderType>>,
    /// Provider configurations
    configs: Mutex<HashMap<String, ProviderConfig>>,
    /// LLM instances for each provider
    llm_instances: Mutex<HashMap<String, Arc<LocalLLM>>>,
}
```

Key responsibilities:
- Managing provider registration and availability
- Storing and retrieving provider configurations
- Selecting the active provider
- Routing text generation requests to the appropriate provider
- Managing model downloads and status

### Discovery Service

The `DiscoveryService` is responsible for detecting installed LLM providers:

```rust
pub struct DiscoveryService {
    /// Configuration for the discovery service
    config: Mutex<DiscoveryConfig>,
    /// Detected providers
    installations: Mutex<HashMap<String, InstallationInfo>>,
    /// Provider suggestions
    suggestions: Mutex<Vec<ProviderSuggestion>>,
    /// Whether a scan is currently running
    scanning: Mutex<bool>,
    /// Last scan timestamp
    last_scan: Mutex<Instant>,
    /// Is the background scanner running
    scanner_running: Mutex<bool>,
}
```

Key capabilities:
- Scanning the system for installed providers
- Creating provider configurations for detected providers
- Suggesting providers that could be installed
- Background scanning to detect new or updated providers

### Migration System

The `MigrationService` handles migration from legacy LLM systems:

```rust
pub struct MigrationService {
    /// Current migration status
    status: Arc<Mutex<MigrationStatus>>,
    /// Migration configuration
    config: Arc<Mutex<MigrationConfig>>,
    /// Legacy models found
    legacy_models: Arc<Mutex<HashMap<String, ModelInfo>>>,
    /// Legacy config found
    legacy_config: Arc<Mutex<Option<LLMConfig>>>,
    /// Model mappings
    model_mappings: Arc<Mutex<Vec<LegacyModelMapping>>>,
    /// Provider mappings
    provider_mappings: Arc<Mutex<Vec<ProviderMapping>>>,
    /// Legacy store type
    store_type: Arc<Mutex<Option<LegacyStoreType>>>,
    /// Legacy fallback provider
    fallback_provider: Arc<Mutex<Option<LocalLLM>>>,
}
```

This service:
- Detects legacy LLM systems
- Maps legacy configurations to new provider configurations
- Migrates models from legacy to new system
- Provides fallback options when migration fails

## Implementation Details

### Provider Implementations

Each provider implementation must satisfy the `Provider` trait. Here are the key details for each built-in provider:

#### Ollama Provider

The Ollama provider integrates with the [Ollama API](https://github.com/ollama/ollama/blob/main/docs/api.md):

- **API Endpoints**:
  - List models: `GET http://localhost:11434/api/tags`
  - Pull model: `POST http://localhost:11434/api/pull`
  - Generate: `POST http://localhost:11434/api/generate`

- **Model Handling**:
  - Models are identified by their tag (e.g., `llama2:7b`)
  - Model download is managed through the Ollama API
  - Model files are stored in Ollama's own directory structure

#### LocalAI Provider

The LocalAI provider integrates with the [LocalAI API](https://github.com/go-skynet/LocalAI/blob/master/docs/openai-compatibility.md), which is compatible with the OpenAI API:

- **API Endpoints**:
  - List models: `GET http://localhost:8080/models`
  - Completion: `POST http://localhost:8080/v1/completions`
  - Chat: `POST http://localhost:8080/v1/chat/completions`

- **Model Handling**:
  - Models must be manually placed in LocalAI's model directory
  - The provider detects available models from the API
  - Model files are not directly managed by the provider

#### llama.cpp Provider

The llama.cpp provider interacts with the [llama.cpp server API](https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md):

- **API Endpoints**:
  - Completion: `POST http://localhost:8000/completion`
  - Info: `GET http://localhost:8000/info`

- **Model Handling**:
  - Models are loaded directly by the llama.cpp server
  - Only one model can be loaded at a time
  - Model switching requires restarting the server

#### Custom Provider

The Custom provider allows for integration with any compatible LLM API:

- **Configuration**:
  - Custom endpoint URL
  - Optional API key
  - Provider-specific parameters in advanced config

- **Implementation**:
  - Adapts to the API of the custom service
  - Provides flexible request and response mapping

### Data Structures

#### Provider Configuration

Provider configurations are stored in the `ProviderConfig` struct:

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProviderConfig {
    /// Provider type identifier
    pub provider_type: String,
    /// Endpoint URL for the provider
    pub endpoint_url: String,
    /// API key for the provider (if required)
    pub api_key: Option<String>,
    /// Default model to use
    pub default_model: Option<String>,
    /// Whether to enable advanced configuration
    pub enable_advanced_config: bool,
    /// Advanced configuration options (provider-specific)
    pub advanced_config: HashMap<String, serde_json::Value>,
}
```

This configuration is stored in the application's settings and can be updated through the UI.

#### Model Information

Information about models is represented by the `EnhancedModelInfo` struct:

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EnhancedModelInfo {
    /// Model identifier
    pub id: String,
    /// Model name for display
    pub name: String,
    /// Model description
    pub description: String,
    /// Model size in bytes
    pub size_bytes: usize,
    /// Whether the model is downloaded
    pub is_downloaded: bool,
    /// Provider-specific metadata
    pub provider_metadata: HashMap<String, serde_json::Value>,
    /// Provider type
    pub provider: String,
    /// Whether the model supports text generation
    pub supports_text_generation: bool,
    /// Whether the model supports completion
    pub supports_completion: bool,
    /// Whether the model supports chat
    pub supports_chat: bool,
    /// Whether the model supports embeddings
    pub supports_embeddings: bool,
    /// Whether the model supports image generation
    pub supports_image_generation: bool,
    /// Quantization level (if applicable)
    pub quantization: Option<String>,
    /// Parameter count in billions
    pub parameter_count_b: Option<f32>,
    /// Context length in tokens
    pub context_length: Option<usize>,
    /// Model family/architecture
    pub model_family: Option<String>,
    /// When the model was created
    pub created_at: Option<String>,
    /// Model tags
    pub tags: Vec<String>,
    /// Model license
    pub license: Option<String>,
}
```

This structure provides comprehensive information about models, including capabilities, technical details, and metadata.

#### Download Status

The status of model downloads is tracked with the `EnhancedDownloadStatus` enum:

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EnhancedDownloadStatus {
    /// Status type 
    pub status: String,
    /// Not started status (empty object)
    pub NotStarted: Option<HashMap<String, serde_json::Value>>,
    /// In progress status
    pub InProgress: Option<InProgressStatus>,
    /// Completed status
    pub Completed: Option<CompletedStatus>,
    /// Failed status
    pub Failed: Option<FailedStatus>,
    /// Cancelled status
    pub Cancelled: Option<CancelledStatus>,
}
```

This structure allows for detailed tracking of download progress, errors, and completions.

### Command API

The system exposes several Tauri commands for interacting with the LLM providers:

#### Provider Management Commands

- `get_all_providers`: Get all available providers
- `get_all_provider_availability`: Check availability of all providers
- `check_provider_availability`: Check availability of a specific provider
- `get_active_provider`: Get the active provider
- `set_active_provider`: Set the active provider
- `get_provider_config`: Get the configuration for a provider
- `update_provider_config`: Update the configuration for a provider

#### Model Management Commands

- `list_available_models`: List available models for a provider
- `list_downloaded_models`: List downloaded models for a provider
- `get_download_status`: Get download status for a model
- `download_model`: Download a model
- `cancel_download`: Cancel a model download
- `delete_model`: Delete a model

#### Text Generation Commands

- `generate_text`: Generate text using a model
- `stream_text`: Stream text generation (for UI responsiveness)

#### Discovery Commands

- `scan_for_providers`: Scan for LLM providers
- `get_discovery_status`: Get provider discovery status
- `get_provider_suggestions`: Get suggestions for providers to install
- `get_discovery_config`: Get discovery service configuration
- `update_discovery_config`: Update discovery service configuration
- `auto_configure_providers`: Auto-configure detected providers

#### Migration Commands

- `check_legacy_system`: Check for legacy LLM system
- `get_migration_status`: Get migration status
- `run_migration`: Run migration
- `get_migration_config`: Get migration configuration
- `update_migration_config`: Update migration configuration
- `opt_out_of_migration`: Opt out of migration

## Frontend Integration

### Offline Settings Component

The primary UI for configuring local LLMs is the `OfflineSettings` component:

```tsx
const OfflineSettings: React.FC = () => {
  // State for offline mode
  const [isOfflineMode, setIsOfflineMode] = useState<boolean>(false);
  const [autoSwitchMode, setAutoSwitchMode] = useState<boolean>(true);
  
  // State for providers
  const [availableProviders, setAvailableProviders] = useState<ProviderInfo[]>([]);
  const [selectedProviderType, setSelectedProviderType] = useState<string>('Ollama');
  
  // State for models
  const [availableModels, setAvailableModels] = useState<ModelInfo[]>([]);
  const [downloadedModels, setDownloadedModels] = useState<ModelInfo[]>([]);
  
  // Additional state and functions...
  
  return (
    // UI implementation...
  );
};
```

This component:
- Displays offline mode settings
- Shows provider selection and configuration
- Lists available and downloaded models
- Handles model downloads and management
- Displays model information and status

### LLM Performance Dashboard

The `LLMPerformanceDashboard` component displays metrics about local LLM performance:

```tsx
const LLMPerformanceDashboard: React.FC = () => {
  // State for metrics
  const [metrics, setMetrics] = useState<LLMMetrics | null>(null);
  
  // Fetch metrics on mount and periodically
  useEffect(() => {
    const fetchMetrics = async () => {
      try {
        const response = await invoke('get_llm_metrics');
        if (response.success && response.data) {
          setMetrics(response.data);
        }
      } catch (error) {
        console.error('Failed to fetch metrics:', error);
      }
    };
    
    fetchMetrics();
    const interval = setInterval(fetchMetrics, 5000);
    
    return () => clearInterval(interval);
  }, []);
  
  return (
    // UI implementation with charts and stats...
  );
};
```

This dashboard shows:
- Generation throughput and latency
- Model-specific performance stats
- Resource usage metrics
- Success and error rates

## Extension Guide

### Adding a New Provider

To add a new LLM provider:

1. **Create Provider Implementation**:

```rust
pub struct MyProvider {
    endpoint: String,
    api_key: Option<String>,
    http_client: reqwest::Client,
}

impl Provider for MyProvider {
    fn get_type(&self) -> ProviderType {
        ProviderType::Custom("MyProvider".to_string())
    }
    
    fn get_name(&self) -> String {
        "My Custom Provider".to_string()
    }
    
    // Implement remaining methods...
}
```

2. **Register the Provider**:

```rust
// In provider_manager.rs
impl ProviderManager {
    pub fn register_providers(&mut self) {
        // Register built-in providers
        self.register_provider(Box::new(OllamaProvider::new()));
        self.register_provider(Box::new(LocalAIProvider::new()));
        self.register_provider(Box::new(LlamaCppProvider::new()));
        
        // Register your custom provider
        self.register_provider(Box::new(MyProvider::new()));
    }
}
```

3. **Add Provider Configuration UI**:

Update the `OfflineSettings.tsx` component to include configuration options for your provider:

```tsx
{selectedProviderType === 'MyProvider' && (
  <>
    <FormControl fullWidth sx={{ mb: 2 }}>
      <FormLabel>My Provider Endpoint</FormLabel>
      <TextField
        value={providerEndpoint}
        onChange={(e) => setProviderEndpoint(e.target.value)}
        placeholder="http://localhost:9000"
      />
    </FormControl>
    
    {/* Additional configuration fields */}
  </>
)}
```

4. **Update Documentation**:

Add information about your provider to the documentation, including:
- Installation instructions
- Configuration details
- Supported models
- Performance characteristics
- Troubleshooting tips

### Custom Model Formats

To support custom model formats:

1. **Update the Model Information Structure**:

```rust
pub struct ModelInfo {
    // Existing fields...
    
    // Add custom format information
    pub format: Option<String>,
    pub format_version: Option<String>,
    pub format_specific_data: Option<HashMap<String, serde_json::Value>>,
}
```

2. **Implement Format-Specific Handling**:

```rust
impl MyProvider {
    fn handle_custom_format(&self, model_id: &str, format: &str) -> Result<()> {
        match format {
            "my-format" => {
                // Format-specific handling
                Ok(())
            },
            _ => Err(anyhow!("Unsupported format: {}", format)),
        }
    }
}
```

### Metrics Collection

To collect additional metrics for your provider:

1. **Define Custom Metrics**:

```rust
pub struct MyProviderMetrics {
    pub inference_time_ms: f64,
    pub memory_usage_mb: f64,
    pub tokens_per_second: f64,
    pub custom_metric: f64,
}
```

2. **Implement Metrics Collection**:

```rust
impl MyProvider {
    fn collect_metrics(&self, start: Instant, tokens: usize) -> MyProviderMetrics {
        let elapsed = start.elapsed();
        let inference_time_ms = elapsed.as_millis() as f64;
        let tokens_per_second = tokens as f64 / (inference_time_ms / 1000.0);
        
        MyProviderMetrics {
            inference_time_ms,
            memory_usage_mb: self.measure_memory_usage(),
            tokens_per_second,
            custom_metric: self.get_custom_metric(),
        }
    }
    
    fn measure_memory_usage(&self) -> f64 {
        // Implementation-specific memory measurement
        0.0
    }
    
    fn get_custom_metric(&self) -> f64 {
        // Custom metric measurement
        0.0
    }
}
```

3. **Register Metrics**:

```rust
// In metrics.rs
pub fn register_provider_metrics(
    registry: &mut MetricsRegistry,
    provider_type: &str,
    metrics: &MyProviderMetrics,
) {
    registry.register_gauge(
        format!("llm.{}.inference_time_ms", provider_type),
        metrics.inference_time_ms,
    );
    
    // Register other metrics...
}
```

## Testing

### Unit Testing

When implementing a new provider, ensure comprehensive unit tests:

```rust
#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_provider_creation() {
        let provider = MyProvider::new(
            "http://localhost:9000".to_string(),
            None,
        );
        
        assert_eq!(provider.get_type().to_string(), "Custom(MyProvider)");
        assert_eq!(provider.get_name(), "My Custom Provider");
    }
    
    #[tokio::test]
    async fn test_provider_availability() {
        let provider = MyProvider::new(
            "http://localhost:9000".to_string(),
            None,
        );
        
        // Mock HTTP client for testing
        let mock_client = MockHttpClient::new()
            .with_response("/info", json!({"version": "1.0.0"}));
        provider.set_http_client(mock_client);
        
        let result = provider.is_available().await;
        assert!(result.is_ok());
        assert!(result.unwrap());
    }
    
    // Additional tests...
}
```

### Integration Testing

Verify the integration with the provider manager:

```rust
#[tokio::test]
async fn test_provider_registration() {
    let mut manager = ProviderManager::default();
    
    // Register custom provider
    manager.register_provider(Box::new(MyProvider::new(
        "http://localhost:9000".to_string(),
        None,
    )));
    
    // Verify provider is registered
    let providers = manager.get_all_providers().await.unwrap();
    assert!(providers.iter().any(|p| p.provider_type == "Custom(MyProvider)"));
}
```

## Performance Considerations

When implementing a provider, consider these performance aspects:

1. **Memory Management**:
   - Use streaming responses where possible
   - Minimize copying of large data buffers
   - Consider memory usage during model loading and inference

2. **Concurrency**:
   - Implement non-blocking API calls
   - Handle multiple concurrent requests efficiently
   - Use appropriate thread pools for CPU-bound operations

3. **Timeouts and Error Handling**:
   - Implement timeouts for API calls
   - Provide clear error messages
   - Handle network failures gracefully

4. **Resource Management**:
   - Clean up resources when no longer needed
   - Release memory when models are unloaded
   - Implement proper shutdown sequences

## Security Considerations

When implementing providers, consider these security aspects:

1. **API Key Handling**:
   - Store API keys securely
   - Avoid logging API keys
   - Use environment variables where appropriate

2. **Network Security**:
   - Validate endpoint URLs
   - Use HTTPS for external endpoints
   - Implement proper certificate validation

3. **Input Validation**:
   - Validate all user inputs
   - Sanitize prompts where necessary
   - Implement appropriate content filtering

4. **Local File Access**:
   - Validate file paths
   - Prevent path traversal attacks
   - Use appropriate file permissions

## Conclusion

The provider-based architecture of the local LLM integration in MCP Client allows for flexible and extensible integration of different LLM backends. By following this documentation, developers can understand the existing implementation, extend it with new providers, and implement custom functionality.

Key takeaways:
- The `Provider` trait defines a common interface for all LLM backends
- The `ProviderManager` centralizes provider management and request routing
- The `DiscoveryService` automatically detects and configures installed providers
- The architecture supports easy extension with new provider types
- Comprehensive metrics collection helps monitor and optimize performance

As local LLMs continue to evolve rapidly, this architecture provides a solid foundation for incorporating new technologies and capabilities while maintaining backward compatibility and a consistent user experience.

## References

- [Ollama API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [LocalAI Documentation](https://github.com/go-skynet/LocalAI/blob/master/docs/openai-compatibility.md)
- [llama.cpp Server API](https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md)
- [Tauri Command API](https://tauri.app/v1/guides/features/command/)
- [React Hooks Documentation](https://reactjs.org/docs/hooks-intro.html)
</file>

<file path="docs/local_llm_integration/provider_comparison.md">
# Comparison of Supported LLM Providers

This document provides a detailed comparison of the different LLM providers supported by the MCP Client. Use this information to choose the provider that best fits your specific needs and hardware capabilities.

## Quick Comparison

| Feature | Ollama | LocalAI | llama.cpp | Custom |
|---------|--------|---------|-----------|--------|
| **Ease of Setup** | ★★★★★ | ★★★☆☆ | ★★☆☆☆ | ★☆☆☆☆ |
| **Performance** | ★★★★☆ | ★★★★☆ | ★★★★★ | Varies |
| **Model Variety** | ★★★★☆ | ★★★★★ | ★★★☆☆ | Varies |
| **Memory Usage** | ★★★☆☆ | ★★★★☆ | ★★★★★ | Varies |
| **API Compatibility** | Ollama API | OpenAI API | Simple API | Custom |
| **Quantization Options** | Limited | Extensive | Extensive | Varies |
| **GPU Support** | Yes | Yes | Yes | Varies |
| **Default Port** | 11434 | 8080 | 8000 | Custom |
| **UI for Management** | Yes | No | No | Varies |
| **Auto-discovery** | Yes | Yes | Limited | No |
| **Embedding Support** | Yes | Yes | Limited | Varies |

## Detailed Feature Comparison

### Installation and Setup

| Feature | Ollama | LocalAI | llama.cpp | Custom |
|---------|--------|---------|-----------|--------|
| **Installation Method** | Native installer | Docker or binary | Build from source | Varies |
| **Platforms** | Windows, macOS, Linux | Windows, macOS, Linux | Windows, macOS, Linux | Varies |
| **Dependencies** | Self-contained | Varies | Build tools | Varies |
| **Setup Complexity** | Very Low | Medium | High | Varies |
| **Auto-start** | Yes | No | No | Varies |
| **Configuration Files** | Minimal | YAML files | Command-line | Varies |

**Notes**:
- **Ollama** has the simplest setup with native installers for all platforms and minimal configuration required.
- **LocalAI** is easiest to set up using Docker, but also offers native binaries.
- **llama.cpp** requires compilation from source, which can be challenging for users without development experience.
- **Custom** providers vary widely in installation requirements.

### Performance and Resource Usage

| Feature | Ollama | LocalAI | llama.cpp | Custom |
|---------|--------|---------|-----------|--------|
| **CPU Performance** | Good | Good | Excellent | Varies |
| **GPU Support** | CUDA, Metal | CUDA, ROCm | CUDA, Metal, ROCm | Varies |
| **Memory Efficiency** | Moderate | Good | Excellent | Varies |
| **Disk Space Required** | High | Moderate | Low | Varies |
| **Tokens/sec (7B model)** | ~50-150 | ~40-140 | ~60-200 | Varies |
| **Tokens/sec (13B model)** | ~30-100 | ~25-90 | ~40-120 | Varies |
| **Startup Time** | Fast | Medium | Very Fast | Varies |
| **Multi-model Loading** | Yes | Yes | No | Varies |

**Notes**:
- **llama.cpp** provides the best performance and memory efficiency due to its highly optimized C++ implementation.
- **Ollama** offers good performance with less configuration.
- **LocalAI** balances performance with flexibility.
- Performance numbers are approximate and vary significantly based on hardware, model, and configuration.

### Model Support and Management

| Feature | Ollama | LocalAI | llama.cpp | Custom |
|---------|--------|---------|-----------|--------|
| **Model Formats** | Proprietary, GGUF | GGUF, GGML, ONNX, more | GGUF | Varies |
| **Model Discovery** | Built-in registry | Manual/API | Manual | Varies |
| **Model Download** | Built-in | Manual | Manual | Varies |
| **Supported Architectures** | Llama, Mistral, more | Many | Many | Varies |
| **Custom Model Support** | Yes (via Modelfile) | Yes | Yes | Varies |
| **Fine-tuning Support** | Limited | Yes | No | Varies |
| **Multi-model Serving** | Yes | Yes | No (one at a time) | Varies |
| **Model Versioning** | Yes | Limited | No | Varies |

**Notes**:
- **Ollama** excels at model management with a built-in registry and simple commands.
- **LocalAI** supports the widest variety of model formats.
- **llama.cpp** requires manual model management but works with standard GGUF files.
- **Custom** providers vary based on implementation.

### API and Integration

| Feature | Ollama | LocalAI | llama.cpp | Custom |
|---------|--------|---------|-----------|--------|
| **API Type** | REST (Ollama API) | REST (OpenAI-compatible) | REST (Simple) | Varies |
| **OpenAI Compatibility** | No | Yes | No | Varies |
| **Chat Completions** | Yes | Yes | Yes | Varies |
| **Text Completions** | Yes | Yes | Yes | Varies |
| **Embeddings** | Yes | Yes | Limited | Varies |
| **Function Calling** | Yes | Yes | No | Varies |
| **Streaming** | Yes | Yes | Yes | Varies |
| **Multi-modal Support** | Limited | Yes | No | Varies |
| **Authentication** | No | Optional | No | Varies |

**Notes**:
- **LocalAI** provides the best API compatibility, making it easy to integrate with existing OpenAI-based applications.
- **Ollama** has a simple but effective API designed for ease of use.
- **llama.cpp** has a minimal API focused on the core functionality.
- **Custom** providers can implement any API structure.

### Advanced Features

| Feature | Ollama | LocalAI | llama.cpp | Custom |
|---------|--------|---------|-----------|--------|
| **Prompt Templates** | Yes | Yes | No | Varies |
| **Context Window Control** | Yes | Yes | Yes | Varies |
| **Quantization Options** | Limited | Extensive | Extensive | Varies |
| **Parameter Tuning** | Moderate | Extensive | Extensive | Varies |
| **Caching** | Yes | Yes | Limited | Varies |
| **Prompt Compression** | No | Yes | No | Varies |
| **Multi-model Chaining** | No | Yes | No | Varies |
| **Image Understanding** | Limited | Yes | No | Varies |
| **Audio Processing** | No | Yes | No | Varies |

**Notes**:
- **LocalAI** provides the most advanced features, particularly for multi-modal and workflow capabilities.
- **llama.cpp** offers extensive low-level optimization options.
- **Ollama** focuses on simplicity but includes essential advanced features.
- **Custom** providers can implement any custom features.

## Hardware Requirements

### Minimum Requirements

| Provider | CPU | RAM | Storage | GPU (Optional) |
|----------|-----|-----|---------|----------------|
| **Ollama** | 4 cores | 8GB | 10GB | NVIDIA/AMD/Apple |
| **LocalAI** | 4 cores | 8GB | 8GB | NVIDIA/AMD/Apple |
| **llama.cpp** | 2 cores | 4GB | 5GB | NVIDIA/AMD/Apple |
| **Custom** | Varies | Varies | Varies | Varies |

### Recommended Requirements

| Provider | CPU | RAM | Storage | GPU (Optional) |
|----------|-----|-----|---------|----------------|
| **Ollama** | 8+ cores | 16GB+ | 50GB+ | 6GB+ VRAM |
| **LocalAI** | 8+ cores | 16GB+ | 40GB+ | 6GB+ VRAM |
| **llama.cpp** | 8+ cores | 16GB+ | 30GB+ | 6GB+ VRAM |
| **Custom** | Varies | Varies | Varies | Varies |

**Notes**:
- These requirements assume running medium-sized models (7B-13B parameters).
- Larger models (30B-70B) will require significantly more resources.
- GPU acceleration can dramatically improve performance, especially for larger models.
- Memory requirements depend heavily on model size, quantization, and context length.

## Use Case Recommendations

### Best for Beginners

**Recommendation: Ollama**

Ollama provides the most user-friendly experience with:
- Simple installation process
- Built-in model management
- Good defaults that work without configuration
- Native applications for all major platforms
- Automatic GPU utilization when available

**Example Setup**:
```bash
# Install Ollama
# On macOS/Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Run
ollama serve

# Pull a model
ollama pull llama2

# In MCP Client, select Ollama provider and llama2 model
```

### Best for Performance on Limited Hardware

**Recommendation: llama.cpp**

llama.cpp offers the best optimization for limited hardware:
- Highly optimized C++ implementation
- Extensive quantization options
- Fine-grained control over resource usage
- Lowest memory footprint
- Best CPU performance

**Example Setup**:
```bash
# Clone and build
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

# Download a quantized model (Q4_K_M for balance of quality and efficiency)
# Start server
./server -m models/mistral-7b-instruct-v0.1.Q4_K_M.gguf -c 2048 --threads 4

# In MCP Client, select LlamaCpp provider
```

### Best for OpenAI API Compatibility

**Recommendation: LocalAI**

LocalAI provides the best compatibility with existing OpenAI-based applications:
- Drop-in replacement for OpenAI API
- Works with existing tools and libraries
- Supports a wide range of models
- Extensive configuration options
- Multi-modal capabilities

**Example Setup**:
```bash
# Using Docker
docker run -p 8080:8080 -v /path/to/models:/models localai/localai:latest

# In MCP Client, select LocalAI provider
```

### Best for Advanced Users and Custom Solutions

**Recommendation: Custom Provider**

Custom providers allow for tailored solutions:
- Full control over implementation
- Integration with specialized models or services
- Organization-specific requirements
- Research and experimentation

**Implementation**:
- Develop a custom provider that implements the required API
- Configure MCP Client to use your custom provider endpoint

## Example Configurations

### Balanced Setup (16GB RAM)

**Provider**: Ollama
**Model**: Mistral-7B (Q4_K_M)
**Configuration**:
- Context size: 4096 tokens
- Use GPU acceleration if available

**Advantages**:
- Easy setup and management
- Good performance for most tasks
- Reasonable memory usage
- Works well for chat and completion

### Performance Setup (32GB+ RAM or Good GPU)

**Provider**: llama.cpp
**Model**: Mistral-7B (Q5_K_M)
**Configuration**:
- Context size: 8192 tokens
- GPU acceleration with larger batch size
- Higher thread count

**Advantages**:
- Maximum performance
- Higher quality outputs
- Can handle longer contexts
- Best tokens/sec throughput

### Integration Setup (API Compatibility)

**Provider**: LocalAI
**Model**: GPT4All or similar
**Configuration**:
- OpenAI-compatible endpoints
- Function calling enabled
- Multiple models loaded simultaneously

**Advantages**:
- Works with existing OpenAI-based code
- Supports complex workflows
- Good for applications transitioning from cloud to local

## Comparison Table of Popular Models

| Model | Size | Ollama | LocalAI | llama.cpp | Suitable For |
|-------|------|--------|---------|-----------|--------------|
| **Llama 2** | 7B | ✅ | ✅ | ✅ | General purpose, chat |
| **Llama 2** | 13B | ✅ | ✅ | ✅ | Better reasoning, instruction following |
| **Llama 2** | 70B | ✅ | ✅ | ✅* | Advanced tasks, near-SOTA performance |
| **Mistral** | 7B | ✅ | ✅ | ✅ | Strong performance, efficient |
| **Mixtral** | 8x7B | ✅ | ✅ | ✅* | Near-SOTA performance, mixture of experts |
| **Phi-2** | 2.7B | ✅ | ✅ | ✅ | Compact, efficient, code generation |
| **Falcon** | 7B/40B | ✅ | ✅ | ✅ | Research, various tasks |
| **Vicuna** | 7B/13B | ✅ | ✅ | ✅ | Instruction following, chat |
| **CodeLlama** | 7B/13B/34B | ✅ | ✅ | ✅ | Code generation, understanding |
| **Stable LM** | 3B | ✅ | ✅ | ✅ | Compact, general purpose |

*✅* = Supported 
*✅** = Supported but requires high-end hardware

**Notes**:
- Models marked with * require substantial hardware resources (32GB+ RAM or GPU with sufficient VRAM)
- Quantization can reduce memory requirements at some cost to quality
- Performance varies significantly depending on hardware and configuration

## Quantization Comparison

Quantization reduces model precision to save memory at some cost to quality:

| Quantization | Memory Usage | Quality | llama.cpp | Ollama | LocalAI | Notes |
|--------------|--------------|---------|-----------|--------|---------|-------|
| **F16 (No Quantization)** | 100% | Best | ✅ | ✅ | ✅ | Requires high RAM |
| **Q8_0** | ~50% | Excellent | ✅ | ❌ | ✅ | Minimal quality loss |
| **Q6_K** | ~38% | Very Good | ✅ | ❌ | ✅ | Good balance |
| **Q5_K_M** | ~31% | Very Good | ✅ | ❌ | ✅ | Popular choice |
| **Q4_K_M** | ~25% | Good | ✅ | ✅ | ✅ | Recommended default |
| **Q3_K_M** | ~19% | Fair | ✅ | ❌ | ✅ | Noticeable quality drop |
| **Q2_K** | ~13% | Poor | ✅ | ❌ | ✅ | Significant quality drop |

**Memory Usage Example** (Mistral 7B model):
- F16: ~14GB
- Q4_K_M: ~3.5GB
- Q2_K: ~1.8GB

## Conclusion

### Best Overall Provider

**Ollama** offers the best balance of ease of use, performance, and features for most users. It's the recommended starting point unless you have specific requirements.

### Best for Specific Needs

- **Best Performance**: llama.cpp
- **Best API Compatibility**: LocalAI
- **Best for Beginners**: Ollama
- **Best for Advanced Users**: Custom Provider

### Final Recommendations

1. **Start with Ollama** if you're new to local LLMs
2. **Switch to llama.cpp** if you need maximum performance on limited hardware
3. **Use LocalAI** if you need OpenAI API compatibility
4. **Implement a Custom Provider** for specialized needs

Each provider has its strengths and is better suited for different use cases. The MCP Client's provider-based architecture allows you to easily switch between providers as your needs evolve.
</file>

<file path="docs/local_llm_integration/troubleshooting_guide.md">
# Troubleshooting Guide for Local LLM Integration

This guide provides solutions to common issues you might encounter when using local LLM providers with the MCP Client.

## Provider Connection Issues

### Provider Not Available

**Symptoms**:
- "Provider not available" warning in the UI
- Unable to see or download models
- Red warning icon next to provider name

**Potential Causes**:
1. Provider software is not installed
2. Provider service is not running
3. Incorrect endpoint URL
4. Firewall blocking the connection
5. Provider crashed or is unresponsive

**Solutions**:

#### For Ollama:
1. **Verify Installation**:
   ```bash
   # Check if Ollama is in PATH
   which ollama  # on macOS/Linux
   where ollama  # on Windows
   ```

2. **Check if Ollama is Running**:
   ```bash
   # Check process
   ps aux | grep ollama  # on macOS/Linux
   tasklist | findstr ollama  # on Windows
   
   # Check if the API is responding
   curl http://localhost:11434/api/version
   ```

3. **Start Ollama Server**:
   ```bash
   ollama serve
   ```

4. **Check Logs**:
   - macOS/Linux: `~/.ollama/logs/` or `/var/log/ollama.log`
   - Windows: `%USERPROFILE%\.ollama\logs\`

#### For LocalAI:
1. **Check Docker Container** (if using Docker):
   ```bash
   docker ps | grep localai
   
   # If not running, start it
   docker run -p 8080:8080 localai/localai:latest
   ```

2. **Test API Endpoint**:
   ```bash
   curl http://localhost:8080/version
   ```

3. **Check Logs**:
   ```bash
   # If running as Docker container
   docker logs $(docker ps | grep localai | awk '{print $1}')
   
   # If running locally
   cat localai.log  # or check terminal where LocalAI is running
   ```

#### For llama.cpp:
1. **Verify Server is Running**:
   ```bash
   # Check process
   ps aux | grep llama-server  # on macOS/Linux
   tasklist | findstr llama-server  # on Windows
   
   # Test endpoint
   curl http://localhost:8000/info
   ```

2. **Start Server**:
   ```bash
   ./server -m /path/to/model.gguf -c 2048
   ```

3. **Check Terminal Output** for any errors

### Firewall Issues

If you suspect a firewall issue:

1. **Check Firewall Settings**:
   - Windows: Check Windows Defender Firewall settings
   - macOS: Check System Settings > Network > Firewall
   - Linux: Check `ufw` or `iptables` rules

2. **Add Exceptions** for:
   - Ollama: Port 11434
   - LocalAI: Port 8080
   - llama.cpp: Port 8000 (or your custom port)

3. **Test Local Connection**:
   ```bash
   # Simple connection test
   telnet localhost 11434  # For Ollama
   telnet localhost 8080   # For LocalAI
   telnet localhost 8000   # For llama.cpp
   ```

## Model Issues

### Model Download Failures

**Symptoms**:
- Download starts but fails before completion
- Error message in the download status
- Download stuck at a certain percentage

**Potential Causes**:
1. Insufficient disk space
2. Network interruption
3. Provider service issues
4. Permission problems
5. Corrupted download cache

**Solutions**:

1. **Check Disk Space**:
   ```bash
   # On macOS/Linux
   df -h
   
   # On Windows
   dir /a /-c
   ```

2. **Clear Download Cache**:
   - Ollama: 
     ```bash
     rm -rf ~/.ollama/cache/*  # macOS/Linux
     rd /s /q %USERPROFILE%\.ollama\cache  # Windows
     ```
   - LocalAI: Check the LocalAI models directory

3. **Check Provider Logs** for specific error messages

4. **Try a Smaller Model**: Start with a smaller model to test if the issue is related to file size

5. **Manual Download**:
   - Ollama: Use the CLI `ollama pull modelname`
   - LocalAI: Download model files directly to the models directory
   - llama.cpp: Download GGUF files manually from Hugging Face

### Model Not Found After Download

**Symptoms**:
- Download completed successfully, but model doesn't appear in the downloaded list
- "Model not found" error when trying to use a downloaded model

**Solutions**:

1. **Refresh Model List**:
   - Click the "Refresh" button in the Models tab
   - Restart the MCP Client
   - Restart the provider service

2. **Check Model Location**:
   - Ollama: `~/.ollama/models/` (macOS/Linux) or `%USERPROFILE%\.ollama\models\` (Windows)
   - LocalAI: Check your configured models directory
   - llama.cpp: Check the path used when starting the server

3. **Verify Model Format**:
   - Ensure the model is in a compatible format
   - For llama.cpp, make sure you're using GGUF format (not older GGML)

4. **Check File Permissions**:
   - Ensure the user running MCP Client has read access to the model files

## Performance Issues

### High Memory Usage

**Symptoms**:
- System becomes slow when using local models
- Application crashes during text generation
- Out of memory errors
- System swap usage increases significantly

**Solutions**:

1. **Use Lower Precision Models**:
   - Switch from f16 to Q4_K_M or Q2_K quantization
   - These models use significantly less memory with minimal quality loss

2. **Reduce Context Length**:
   - Set a smaller context window (e.g., 2048 instead of 4096)
   - For llama.cpp server, restart with `-c 2048` option

3. **Choose Smaller Models**:
   - Switch from 13B or 70B parameter models to 7B models
   - Smaller models require much less RAM

4. **Adjust System Configuration**:
   - Increase swap space/page file size
   - Close other memory-intensive applications before using local LLMs
   - For llama.cpp, use `--threads` option to limit the number of threads

5. **Check for Memory Leaks**:
   - Restart the provider service after extended use
   - Monitor memory usage with tools like Task Manager (Windows), Activity Monitor (macOS), or `top`/`htop` (Linux)

### Slow Generation Speed

**Symptoms**:
- Very slow responses when using local models
- Generation takes significantly longer than expected

**Solutions**:

1. **Enable GPU Acceleration** if available:
   - Ollama: Uses GPU automatically if available
   - LocalAI: Check GPU configuration in settings
   - llama.cpp: Compile with GPU support (CUDA, Metal, or ROCm)

2. **Optimize Thread Count**:
   - For llama.cpp, set `--threads` to match your CPU core count
   - Example: `./server -m model.gguf -c 2048 --threads 8`

3. **Monitor System Resource Usage**:
   - Check CPU/GPU usage during generation
   - Ensure other processes aren't consuming resources

4. **Use Batch Processing** for multiple generations:
   - Where supported, batch requests together
   - This can be more efficient than separate requests

5. **Check for Network Bottlenecks**:
   - Ensure you're using localhost connections without any proxies
   - Check if antivirus software is scanning network traffic

### GPU-Specific Issues

**Symptoms**:
- Model runs on CPU instead of GPU
- CUDA/Metal/ROCm errors
- Crashes when trying to use GPU acceleration

**Solutions**:

#### NVIDIA GPUs:
1. **Verify CUDA Installation**:
   ```bash
   nvcc --version
   nvidia-smi
   ```

2. **Check Provider GPU Support**:
   - Ollama: Check log for GPU detection
   - llama.cpp: Compile with `-DLLAMA_CUBLAS=ON`
   - LocalAI: Verify GPU configuration

3. **Memory Issues**:
   - Monitor VRAM usage with `nvidia-smi`
   - Try models that fit in available VRAM
   - Use mixed precision (INT8/INT4) for larger models

#### Apple Silicon:
1. **Verify Metal Support**:
   - llama.cpp: Compile with `-DLLAMA_METAL=ON`
   - Ollama: Uses Metal automatically on macOS

2. **Check Activity Monitor** for GPU usage

3. **Verify Model Compatibility** with Metal

## Offline Mode Issues

### Offline Mode Not Working

**Symptoms**:
- Still using cloud APIs despite being offline
- Error messages when trying to generate text offline

**Solutions**:

1. **Check Offline Mode Settings**:
   - Verify "Enable Offline Mode" is toggled ON
   - Check if "Auto-switch based on connectivity" is enabled

2. **Verify Default Model is Selected**:
   - Go to Settings > Offline and check "Default Model" dropdown
   - Make sure a downloaded model is selected

3. **Test Provider Connection**:
   - Ensure the provider service is running
   - Try connecting to provider API manually

4. **Check Network Status Detection**:
   - The application might be incorrectly detecting network status
   - Try toggling Offline Mode manually instead of relying on auto-switch

5. **Restart Components**:
   - Restart the provider service
   - Restart MCP Client
   - In extreme cases, restart your computer

### Automatic Switching Issues

**Symptoms**:
- Doesn't switch to offline mode when network is disconnected
- Stays in offline mode despite network being available

**Solutions**:

1. **Check Internet Connection Detection**:
   - Some network configurations can cause incorrect detection
   - Try using a manual switch instead of auto-switch

2. **Verify Provider Availability**:
   - Ensure the local provider is available and configured correctly
   - Check if the default model is selected

3. **Check Application Logs**:
   - Look for network status events and mode switching
   - Identify any errors in the switching logic

## Provider-Specific Troubleshooting

### Ollama Issues

**Common Problem: Model Fails to Pull**

**Symptoms**:
- `ollama pull` command fails
- Download starts but doesn't complete
- "Error pulling model" message

**Solutions**:
1. **Check Network Connection**:
   - Ensure stable internet during download
   - Try using a different network if available

2. **Check Disk Space**:
   - Models can be large (2-40GB)
   - Ensure sufficient free space

3. **Remove Partially Downloaded Model**:
   ```bash
   ollama rm modelname
   ```

4. **Update Ollama**:
   - Download and install the latest version

5. **Check Ollama Logs**:
   ```bash
   cat ~/.ollama/logs/ollama.log  # macOS/Linux
   type %USERPROFILE%\.ollama\logs\ollama.log  # Windows
   ```

**Common Problem: Ollama Service Crashes**

**Solutions**:
1. **Restart Ollama**:
   ```bash
   # macOS/Linux
   killall ollama
   ollama serve
   
   # Windows
   taskkill /F /IM ollama.exe
   # Then restart Ollama
   ```

2. **Check System Resources**:
   - Ensure sufficient RAM for models
   - Monitor CPU usage

3. **Check for Corrupt Models**:
   - Try removing and re-downloading problematic models

### LocalAI Issues

**Common Problem: Model Loading Errors**

**Symptoms**:
- Error message when trying to use a model
- Model appears in list but fails when used

**Solutions**:
1. **Check Model Format**:
   - Ensure the model is in a compatible format
   - LocalAI supports GGUF, GGML, ONNX, and other formats

2. **Verify Model Configuration**:
   - Check for proper model configuration in LocalAI
   - Ensure model is in the correct directory

3. **Run with Verbose Logging**:
   ```bash
   LOCAL_AI_DEBUG=1 localai serve
   ```

4. **Check Model Compatibility**:
   - Some models require specific LocalAI versions
   - Check the LocalAI documentation for compatibility

**Common Problem: API Compatibility Issues**

**Symptoms**:
- Error messages about API format
- Requests fail with HTTP errors

**Solutions**:
1. **Check API Format**:
   - LocalAI implements OpenAI-compatible API
   - Ensure requests follow OpenAI API specifications

2. **Verify Endpoint URLs**:
   - Use correct endpoints for different operations:
     - `/v1/chat/completions` for chat
     - `/v1/completions` for completion
     - `/v1/embeddings` for embeddings

3. **Check Headers and Authentication**:
   - If you've configured authentication, ensure proper headers

### llama.cpp Issues

**Common Problem: Compilation Issues**

**Symptoms**:
- Errors when building llama.cpp
- Missing dependencies messages

**Solutions**:
1. **Install Required Dependencies**:
   ```bash
   # Ubuntu/Debian
   sudo apt-get install build-essential cmake
   
   # macOS
   brew install cmake
   
   # Windows
   # Install Visual Studio with C++ workload or MinGW
   ```

2. **Build with Specific Options**:
   ```bash
   # Basic build
   make
   
   # With CUDA support
   make LLAMA_CUBLAS=1
   
   # With Metal support (macOS)
   make LLAMA_METAL=1
   ```

3. **Check for Compatible Compiler**:
   - Ensure you have a modern C++ compiler (supporting C++11 or later)

**Common Problem: Model Format Compatibility**

**Symptoms**:
- "Model format not recognized" error
- Server crashes when loading model

**Solutions**:
1. **Use GGUF Format**:
   - Newer versions require GGUF, not GGML
   - Convert GGML to GGUF if needed with `convert-llama-ggml-to-gguf`

2. **Check llama.cpp Version**:
   - Ensure it's compatible with your model format
   - Update to the latest version for best compatibility

3. **Command Line Options**:
   - Use correct parameters for your model:
     ```bash
     ./server -m model.gguf -c 2048 --host 0.0.0.0 --port 8000
     ```

## UI and Configuration Issues

### Settings Not Saving

**Symptoms**:
- Configuration changes don't persist after restarting the application
- Settings revert to previous values

**Solutions**:
1. **Check Permissions**:
   - Ensure the application has write permissions to its config directory
   - Run as administrator/sudo if necessary

2. **Clear Application Cache**:
   - Close the application
   - Find and rename/delete the application's cache directory
   - Restart the application

3. **Check for Configuration Conflicts**:
   - Another process might be overwriting settings
   - Check for multiple instances of the application

### UI Responsiveness Issues

**Symptoms**:
- UI becomes slow or unresponsive during model operations
- Freezing during model downloads or text generation

**Solutions**:
1. **Enable Streaming Responses**:
   - Where supported, use streaming mode for generation
   - This improves UI responsiveness

2. **Reduce UI Updates**:
   - Lower the frequency of progress updates for long operations
   - This reduces rendering overhead

3. **Check System Resources**:
   - Monitor CPU, RAM, and disk I/O
   - Close other resource-intensive applications

## Collecting Diagnostic Information

When reporting issues, include the following information:

### System Information
```
- OS: [Windows/macOS/Linux + version]
- CPU: [make/model]
- RAM: [amount]
- GPU: [if applicable, make/model]
- Disk Space: [free space available]
```

### MCP Client Information
```
- Version: [x.y.z]
- Offline Mode Enabled: [Yes/No]
- Auto-switch Enabled: [Yes/No]
- Selected Provider: [Ollama/LocalAI/llama.cpp/Custom]
- Provider Version: [version number]
```

### Provider Information
```
- Provider Type: [Ollama/LocalAI/llama.cpp/Custom]
- Provider Version: [version number]
- Endpoint URL: [URL]
- Selected Model: [model name]
- Model Size: [parameters/file size]
- Quantization: [if applicable]
```

### Log Files

**MCP Client Logs**:
- Location: [application-specific]
- Include relevant error messages or warnings

**Provider Logs**:
- Ollama: `~/.ollama/logs/ollama.log`
- LocalAI: Check terminal or Docker logs
- llama.cpp: Check terminal output

### Steps to Reproduce
```
1. Detailed step-by-step instructions to reproduce the issue
2. Include any specific text prompts or operations
3. Mention any relevant system conditions (e.g., low disk space, high CPU load)
```

## Advanced Troubleshooting

### Network Packet Capture

For API communication issues, capture network traffic:

1. **Using Wireshark**:
   - Filter by port: `tcp.port == 11434` (for Ollama)
   - Examine HTTP requests and responses

2. **Using tcpdump**:
   ```bash
   sudo tcpdump -i lo0 -A -s 0 port 11434 > ollama_traffic.txt
   ```

3. **Using Fiddler** (Windows):
   - Configure to capture localhost traffic
   - Examine requests to provider endpoints

### Debugging Provider API

Test provider APIs directly:

#### Ollama API
```bash
# List models
curl http://localhost:11434/api/tags

# Generate text
curl -X POST http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Hello, world!",
  "stream": false
}'
```

#### LocalAI API
```bash
# List models
curl http://localhost:8080/models

# Generate text
curl -X POST http://localhost:8080/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-3.5-turbo",
    "prompt": "Hello, world!",
    "max_tokens": 100
  }'
```

#### llama.cpp API
```bash
# Get model info
curl http://localhost:8000/info

# Generate text
curl -X POST http://localhost:8000/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Hello, world!",
    "n_predict": 100
  }'
```

## Further Assistance

If you're still experiencing issues after trying these troubleshooting steps:

1. **Check GitHub Repositories** for known issues:
   - MCP Client: [GitHub Repository]
   - Ollama: [https://github.com/ollama/ollama/issues](https://github.com/ollama/ollama/issues)
   - LocalAI: [https://github.com/go-skynet/LocalAI/issues](https://github.com/go-skynet/LocalAI/issues)
   - llama.cpp: [https://github.com/ggerganov/llama.cpp/issues](https://github.com/ggerganov/llama.cpp/issues)

2. **Join Community Forums**:
   - Ollama Discord
   - LocalAI Discord
   - llama.cpp discussions

3. **Submit Detailed Bug Reports**:
   - Include all diagnostic information listed above
   - Be specific about the issue and reproduction steps
   - Mention any workarounds you've tried

By following this troubleshooting guide, you should be able to resolve most common issues with local LLM integration in the MCP Client. If you discover new issues or solutions, please contribute them to help improve the application.
</file>

<file path="docs/local_llm_integration/user_guide.md">
# User Guide for Configuring LLM Providers

This guide provides step-by-step instructions for configuring different LLM providers in the MCP Client. It covers installation, configuration, model management, and usage details for each supported provider.

## Getting Started with Offline Mode

### Enabling Offline Capabilities

1. Open the MCP Client application
2. Navigate to Settings > Offline
3. You'll see the Offline Mode section at the top:
   - Toggle "Enable Offline Mode" to ON to activate local LLM capabilities
   - Toggle "Auto-switch based on connectivity" to ON if you want the application to automatically switch between online and offline modes based on your network status

![Offline Mode Settings](../assets/images/offline_mode_settings.png)

### Selecting a Provider

The MCP Client can automatically detect installed providers on your system. To select a provider:

1. In the Settings > Offline screen, under "Local LLM Provider", select your preferred provider from the dropdown menu
2. The application will verify if the provider is available
3. If available, you'll see a green checkmark indicating it's ready to use
4. If unavailable, you'll see a warning message with information on how to install or properly configure it

## Provider-Specific Setup

### Ollama

[Ollama](https://ollama.ai/) is one of the easiest providers to set up and offers a wide range of models with good performance.

#### Installation

1. **Windows**:
   - Download the installer from [ollama.ai/download](https://ollama.ai/download)
   - Run the installer and follow the on-screen instructions
   - Ollama will start automatically after installation

2. **macOS**:
   - Download the application from [ollama.ai/download](https://ollama.ai/download)
   - Drag the Ollama app to your Applications folder
   - Open Ollama from your Applications folder

3. **Linux**:
   - Run the installation script:
     ```bash
     curl -fsSL https://ollama.ai/install.sh | sh
     ```
   - Start the Ollama service:
     ```bash
     ollama serve
     ```

#### Configuration in MCP Client

1. In the Offline Settings, select "Ollama" from the provider dropdown
2. The default endpoint is `http://localhost:11434` (this should work without changes in most cases)
3. No API key is required for Ollama

#### Working with Models

Ollama makes model management simple:

1. In the "Models" tab, you'll see all available models for Ollama
2. Click the download icon next to any model to start downloading it
3. The progress bar will show the download status
4. Once downloaded, the model will appear in the "Downloaded Models" tab
5. To set a model as the default for offline use, select it in the "Default Model" dropdown

#### Available Models

Ollama provides a wide range of models, including:

- **Llama 2**: Meta's powerful open-source models (7B, 13B, 70B parameters)
- **Mistral**: High-performing 7B parameter model
- **Vicuna**: Fine-tuned LLaMA models
- **Phi-2**: Microsoft's compact but powerful model
- **Orca 2**: Microsoft Research models with strong reasoning capabilities
- **Many more**: Check the Models tab for the complete list

### LocalAI

[LocalAI](https://github.com/go-skynet/LocalAI) provides an OpenAI-compatible API for various open-source models.

#### Installation

1. **Using Docker** (recommended):
   ```bash
   docker run -p 8080:8080 localai/localai:latest
   ```

2. **Manual Installation**:
   - Follow the instructions on the [LocalAI GitHub repository](https://github.com/go-skynet/LocalAI)
   - Basic steps involve:
     ```bash
     git clone https://github.com/go-skynet/LocalAI
     cd LocalAI
     make build
     ./localai serve
     ```

#### Configuration in MCP Client

1. In the Offline Settings, select "LocalAI" from the provider dropdown
2. Set the endpoint URL to where your LocalAI server is running:
   - Default for local installations: `http://localhost:8080`
   - If using Docker with a different port mapping, adjust accordingly
3. No API key is required unless you've configured authentication for your LocalAI server

#### Working with Models

LocalAI requires models to be placed in its models directory:

1. Download models from Hugging Face or other sources
2. Place them in the LocalAI models directory (default: `./models/`)
3. In the MCP Client, refresh the Models list to see the available models
4. Models should automatically appear in the list if they're in the correct directory
5. Select a model as default in the "Default Model" dropdown

#### Available Models

LocalAI supports various model formats:

- GGUF models (formerly GGML)
- ONNX models
- TensorFlow models
- PyTorch models
- And more

### llama.cpp

[llama.cpp](https://github.com/ggerganov/llama.cpp) is a highly optimized C/C++ implementation for running LLaMA models on CPUs.

#### Installation

1. **Clone and build the repository**:
   ```bash
   git clone https://github.com/ggerganov/llama.cpp
   cd llama.cpp
   make
   ```

2. **Start the server with a model**:
   ```bash
   # Download a model (example)
   wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf

   # Start server
   ./server -m mistral-7b-instruct-v0.1.Q4_K_M.gguf -c 2048
   ```

#### Configuration in MCP Client

1. In the Offline Settings, select "LlamaCpp" from the provider dropdown
2. Set the endpoint URL to where your llama.cpp server is running:
   - Default: `http://localhost:8000`
   - If you've specified a different port, adjust accordingly
3. No API key is required

#### Working with Models

llama.cpp requires you to manage models manually:

1. Download GGUF models from sources like Hugging Face
   - Popular model repositories: [TheBloke](https://huggingface.co/TheBloke)
2. Start the llama.cpp server with your chosen model
3. In MCP Client, the model will be automatically detected from the running server
4. You can switch models by stopping the server and restarting it with a different model

#### Model Selection

When choosing models for llama.cpp, consider:

1. **Quantization level**: 
   - Q4_K_M: Good balance of quality and memory usage
   - Q5_K_M: Better quality but higher memory
   - Q2_K: Lower quality but minimal memory
   
2. **Model size**:
   - 7B models: Work on most systems, even with limited RAM
   - 13B models: Require at least 8GB of RAM
   - 70B models: Require 16GB+ RAM or GPU acceleration

3. **Context length**:
   - Models support different context sizes (2k, 4k, 8k, etc.)
   - Larger contexts use more memory but can "remember" more

### Custom Provider

The Custom Provider option allows you to integrate a specialized or self-made LLM service.

#### Configuration

1. In the Offline Settings, select "Custom" from the provider dropdown
2. Enter the endpoint URL for your custom LLM service
3. Configure the API key if your service requires authentication
4. In the Advanced Configuration section, you can set provider-specific parameters

#### Requirements for Custom Providers

Your custom provider should implement a REST API with these minimum endpoints:

- `GET /models`: List available models
- `POST /generate`: Generate text from a prompt
- `GET /info`: Provide provider information

Refer to the Developer Documentation for more details on implementing a custom provider.

## Managing Default Model

To set a model as default for offline use:

1. Go to Settings > Offline
2. In the Local LLM Provider section, locate the "Default Model" dropdown
3. Select the model you want to use as default from your downloaded models
4. Click "Save Configuration" to apply the changes

The default model will be used automatically when:
- You're in offline mode
- You don't specify a particular model in your requests

## Advanced Features

### Automatic Provider Discovery

The MCP Client can automatically detect and configure providers installed on your system:

1. Go to Settings > Offline
2. Click "Refresh Providers" to scan for available providers
3. Newly discovered providers will appear in the provider dropdown
4. The system will attempt to auto-configure discovered providers

### Performance Metrics

You can monitor the performance of your local LLMs:

1. Go to Dashboard > LLM to view performance metrics
2. View metrics such as:
   - Tokens per second
   - Latency measurements
   - Memory usage
   - Success rate

### Offline Metrics Collection

The application can collect anonymous performance metrics to help improve the system:

1. In Settings > Offline, find the "LLM Performance Metrics" section
2. Review the privacy notice and what data is collected
3. Toggle the metrics collection option based on your preference

## Best Practices

### Choosing the Right Provider

Consider these factors when selecting a provider:

- **Ease of use**: Ollama is the simplest option for beginners
- **Performance**: llama.cpp offers the best optimization for limited hardware
- **Integration**: LocalAI provides the best OpenAI API compatibility
- **Model support**: Ollama and LocalAI support the widest range of models

### Optimizing for Your Hardware

To get the best performance:

1. **For systems with limited RAM** (8GB or less):
   - Use llama.cpp with Q4_K_M or Q2_K quantized models
   - Stick to 7B parameter models
   - Limit context size to 2048 tokens

2. **For mid-range systems** (16GB RAM):
   - Any provider works well
   - Can use 13B parameter models
   - Context sizes up to 4096 tokens

3. **For high-end systems** (32GB+ RAM or GPU):
   - Any provider with GPU acceleration
   - Can use 70B parameter models
   - Larger context sizes (8192+ tokens)

### Managing Model Downloads

1. **Disk space**: Models can range from 2GB to 40GB each
2. **Download order**: Start with smaller models first
3. **Organization**: Remove unused models to save space

## Conclusion

By following this guide, you should be able to configure and use any of the supported LLM providers in the MCP Client. Each provider has its strengths and is suited for different use cases and hardware configurations.

For troubleshooting common issues, refer to the Troubleshooting Guide in the documentation.
</file>

<file path="docs/OBSERVABILITY.md">
# MCP Client Observability System

This document provides an overview of the observability features implemented in the MCP Client.

## Overview

The MCP Client includes a comprehensive observability system with the following components:

1. **Metrics Collection System**: Collects performance metrics from all parts of the application
2. **Structured Logging System**: Provides detailed, context-rich logging with multiple severity levels
3. **Telemetry System**: Sends anonymized usage data to help improve the client (with strict privacy controls)
4. **Resource Monitoring Dashboard**: Visualizes system resource usage and application performance
5. **Canary Release Infrastructure**: Enables safe rollout of new features to small user groups

All components are integrated across the three interfaces (GUI, CLI, TUI) to provide a consistent observability experience.

## Components

### Metrics Collection System

The metrics collection system tracks various performance indicators throughout the application:

- **Counter Metrics**: Track occurrences of specific events (e.g., API requests)
- **Gauge Metrics**: Track values that can go up and down (e.g., memory usage)
- **Histogram Metrics**: Track the distribution of values (e.g., response times)
- **Timer Metrics**: Measure the duration of operations

Metrics are collected with configurable sampling rates to minimize performance impact.

**Files**:
- `src-common/src/observability/metrics.rs`: Core metrics implementation
- `src-tauri/src/monitoring/resources.rs`: Resource metrics collection

### Structured Logging System

The logging system provides rich, context-aware logs with different severity levels:

- **Trace**: Extremely detailed information for debugging
- **Debug**: Detailed information for developers
- **Info**: General information about application operation
- **Warn**: Potential issues that don't affect operation
- **Error**: Issues that affect operation but don't cause failure
- **Fatal**: Critical issues that cause application failure

Logs include structured context data and can be viewed in the application's log viewer.

**Files**:
- `src-common/src/observability/logging.rs`: Logging implementation

### Telemetry System

The telemetry system collects anonymized usage data to help improve the application:

- All telemetry is **opt-in** by default
- Users have **granular control** over what data is collected
- Data is **anonymized** with a random client ID
- Users can **delete their data** at any time

The telemetry system includes the following collection categories:

- **App Lifecycle**: Application startup and shutdown events
- **Feature Usage**: Which features are used and how often
- **Errors**: Error reports to help improve stability
- **Performance**: Performance metrics to optimize the application
- **User Actions**: UI interactions and workflow patterns
- **System Info**: Anonymous system configuration data
- **Logs**: Application logs for troubleshooting (opt-in only)

**Files**:
- `src-common/src/observability/telemetry.rs`: Telemetry implementation
- `src-frontend/src/components/settings/PrivacySettings.tsx`: Privacy controls UI

### Resource Monitoring Dashboard

The resource monitoring dashboard provides real-time visibility into system resource usage:

- **CPU Usage**: Real-time and historical CPU usage
- **Memory Usage**: Real-time and historical memory usage
- **API Latency**: Response times for API requests
- **Message Counts**: Number of messages processed
- **FPS**: Frames per second for UI rendering

The dashboard is available in the GUI interface and requires the `performance_dashboard` feature.

**Files**:
- `src-frontend/src/components/dashboard/ResourceDashboard.tsx`: Dashboard UI
- `src-tauri/src/monitoring/resources.rs`: Resource metrics collection

### Canary Release Infrastructure

The canary release infrastructure enables safe rollout of new features:

- **Feature Flags**: Control feature availability based on various criteria
- **Canary Groups**: Alpha, Beta, and Early Access user groups
- **Metrics Comparison**: Compare metrics between control and canary groups
- **Promotion Workflow**: Safely promote features from canary to general availability
- **Rollback Capability**: Quickly disable problematic features

**Files**:
- `src-common/src/feature_flags/mod.rs`: Feature flag system
- `src-common/src/observability/canary.rs`: Canary release infrastructure
- `src-frontend/src/components/canary/CanaryDashboard.tsx`: Canary UI
- `src-frontend/src/contexts/FeatureFlagContext.tsx`: Feature flag React context

## Configuration

The observability system is configurable through the application settings and feature flags:

- **Metrics Collection**: Configure sampling rates and buffer sizes
- **Logging**: Set log levels, file rotation, and console output
- **Telemetry**: Enable/disable collection categories and set privacy level
- **Canary Release**: Join canary groups and configure rollout percentages

## Feature Flags

The following feature flags control observability features:

- `advanced_telemetry`: Enable advanced telemetry collection (Canary Alpha)
- `performance_dashboard`: Enable the performance monitoring dashboard (Canary Beta)
- `debug_logging`: Enable verbose debug logging (Canary Alpha)
- `resource_monitoring`: Enable system resource monitoring (All Users)
- `crash_reporting`: Enable automatic crash reporting (50% Rollout)

## Implementation Notes

The observability system is designed with the following principles:

1. **Privacy First**: All user data collection is opt-in and transparent
2. **Low Overhead**: Sampling and buffering minimize performance impact
3. **Cross-Interface**: Consistent experience across GUI, CLI, and TUI
4. **Extensible**: Easy to add new metrics and logging contexts
5. **Safe Rollouts**: Feature flags and canary releases enable safe deployment

## Future Enhancements

Planned enhancements to the observability system:

1. **Custom Dashboards**: User-defined dashboards for specific metrics
2. **Alerting**: Proactive alerts for performance issues
3. **Distributed Tracing**: End-to-end request tracing
4. **Metrics Export**: Export metrics to external monitoring systems
5. **Advanced A/B Testing**: More sophisticated canary testing capabilities
</file>

<file path="docs/plugin_system_design.md">
# MCP Plugin System Design

This document outlines the design of the plugin system for the Claude MCP Client, including the WASM-based sandbox, plugin management, and permission system.

## Overview

The plugin system enables users to extend the functionality of the MCP client through custom plugins. These plugins run in a secure WebAssembly (WASM) sandbox to ensure they cannot harm the user's system or access unauthorized data.

## Core Components

1. **Plugin Registry**: Central registry for managing installed plugins
2. **Plugin Loader**: Loads and initializes plugins in a WASM sandbox
3. **Permission System**: Controls what resources plugins can access
4. **Plugin Discovery**: Finds and catalogs available plugins
5. **Plugin Management UI**: Interface for installing, updating, and configuring plugins
6. **Plugin SDK**: Tools and interfaces for developing plugins

## Architecture

```
┌─────────────────────────────┐
│                             │
│  Application                │
│                             │
│  ┌─────────────────────┐    │
│  │ Plugin Management   │    │
│  │ UI                  │    │
│  └──────────┬──────────┘    │
│             │               │
│             │               │
│  ┌──────────▼──────────┐    │
│  │                     │    │
│  │ Plugin Registry     │    │
│  │                     │    │
│  └──────────┬──────────┘    │
│             │               │
│             │               │
│  ┌──────────▼──────────┐    │
│  │                     │    │
│  │ Plugin Loader       │    │
│  │                     │    │
│  └──────────┬──────────┘    │
│             │               │
└─────────────┼───────────────┘
              │
┌─────────────▼───────────────┐
│                             │
│  WASM Sandbox               │
│                             │
│  ┌─────────────────────┐    │
│  │ Permission System   │    │
│  └──────────┬──────────┘    │
│             │               │
│             │               │
│  ┌──────────▼──────────┐    │
│  │                     │    │
│  │ Plugin 1            │    │
│  │                     │    │
│  └─────────────────────┘    │
│                             │
│  ┌─────────────────────┐    │
│  │                     │    │
│  │ Plugin 2            │    │
│  │                     │    │
│  └─────────────────────┘    │
│                             │
│  ┌─────────────────────┐    │
│  │                     │    │
│  │ Plugin 3            │    │
│  │                     │    │
│  └─────────────────────┘    │
│                             │
└─────────────────────────────┘
```

## Plugin Discovery and Installation

### Plugin Sources
- **Official Repository**: Curated plugins from the MCP developers
- **Community Repository**: Plugins from the community (with verification)
- **Local Plugins**: Custom plugins developed by the user
- **URL Installation**: Install plugins from a URL

### Plugin Manifest
Each plugin must include a manifest file that describes the plugin, its capabilities, and required permissions:

```json
{
  "name": "example-plugin",
  "displayName": "Example Plugin",
  "version": "1.0.0",
  "description": "An example plugin for the MCP client",
  "author": "Plugin Developer",
  "license": "MIT",
  "main": "plugin.wasm",
  "permissions": [
    "conversations:read",
    "conversations:write",
    "network:github.com"
  ],
  "hooks": [
    "message:pre-process",
    "message:post-process",
    "conversation:create"
  ],
  "config": {
    "settings": [
      {
        "name": "apiKey",
        "type": "string",
        "label": "API Key",
        "description": "Your API key for the service",
        "default": "",
        "secret": true
      }
    ]
  }
}
```

## Permission System

The permission system restricts what plugins can access, ensuring they can only interact with authorized resources.

### Permission Types

1. **Resource Permissions**:
   - `conversations:read` - Read conversations and messages
   - `conversations:write` - Create/modify conversations and messages
   - `models:read` - Read available models
   - `models:use` - Use specific models
   - `system:settings` - Access system settings
   - `user:preferences` - Access user preferences

2. **Network Permissions**:
   - `network:all` - Connect to any network resource
   - `network:{domain}` - Connect to a specific domain

3. **File System Permissions**:
   - `fs:read` - Read files (restricted to specific directories)
   - `fs:write` - Write files (restricted to specific directories)

4. **UI Permissions**:
   - `ui:display` - Show UI elements
   - `ui:interact` - Interact with user through UI elements

### Permission Levels

- **Minimal**: Basic functionality with minimal access
- **Standard**: Typical functionality for most plugins
- **Advanced**: Extended access for specialized plugins
- **Full**: Complete access (requires explicit user approval)

## Plugin Hooks

Plugins can hook into various events in the application:

- `message:pre-process`: Process a message before sending to Claude
- `message:post-process`: Process a message after receiving from Claude
- `conversation:create`: Called when a new conversation is created
- `conversation:open`: Called when a conversation is opened
- `conversation:close`: Called when a conversation is closed
- `application:start`: Called when the application starts
- `application:shutdown`: Called when the application shuts down
- `ui:render`: Custom UI rendering

## WebAssembly (WASM) Sandbox

Plugins run in a secure WASM sandbox with the following characteristics:

1. **Memory Isolation**: Plugins cannot access memory outside their sandbox
2. **Resource Limitations**: CPU and memory usage is restricted
3. **Controlled API Access**: Plugins can only access approved APIs
4. **Permission Enforcement**: Access to resources is controlled by permissions

### WASM Host Functions

The host provides functions to the WASM module:

- `registerPlugin(manifest)`: Register the plugin with the host
- `requestPermission(permission)`: Request additional permissions
- `registerHook(hookName, callbackPtr)`: Register a hook handler
- `logMessage(level, message)`: Log a message
- `getConversation(id)`: Get a conversation by ID (if permitted)
- `sendMessage(conversationId, message)`: Send a message (if permitted)
- `getModels()`: Get available models (if permitted)
- `httpRequest(url, method, headers, body)`: Make an HTTP request (if permitted)

## Plugin Development SDK

The SDK provides tools and interfaces for developing plugins:

1. **TypeScript/Rust Templates**: Starting points for plugin development
2. **Emulator**: Local environment for testing plugins
3. **Debugging Tools**: Tools for debugging WASM plugins
4. **Documentation**: API documentation and examples

## Example Plugins

1. **Translation Plugin**: Translates messages between languages
2. **GitHub Integration**: Fetches code snippets and repository information
3. **Meeting Summarizer**: Generates summaries of meeting transcripts
4. **Custom Prompt Templates**: Provides reusable prompt templates
5. **Conversation Export**: Exports conversations to various formats

## Implementation Phases

1. **Phase 1**: WASM sandbox and basic plugin loading
2. **Phase 2**: Permission system and plugin registry
3. **Phase 3**: Plugin discovery and installation
4. **Phase 4**: Plugin management UI
5. **Phase 5**: Example plugins and documentation

## Security Considerations

1. **Plugin Verification**: Verify plugin authors and code
2. **Sandboxing**: Run plugins in an isolated environment
3. **Resource Limits**: Prevent excessive resource usage
4. **Permission Prompts**: Clearly inform users about requested permissions
5. **Revocation**: Allow users to revoke permissions at any time
6. **Updates**: Secure update mechanism for plugins
</file>

<file path="docs/README.md">
# MCP Client

The MCP Client is a cross-platform desktop application for accessing and interacting with the MCP system. It provides a rich set of features, robust offline capabilities, and excellent performance.

## Features

### Core Features

- **Rich Text Conversations**: Fully-featured conversation UI with support for markdown, code blocks, and more
- **Offline Mode**: Continue working even without an internet connection
- **Local LLM Support**: Use embedded models for inference when offline
- **Performance Optimizations**: Efficient memory usage and caching for responsive experience
- **Auto-Updates**: Seamless background updates to keep the application current

### Performance and Optimization

- **Memory Management**: Intelligent memory usage with configurable limits and garbage collection
- **Caching System**: API and resource caching for faster response times
- **Resource Monitoring**: Real-time monitoring of system resources
- **Checkpointing**: Save and restore conversation state
- **Synchronization**: Two-way sync mechanism for seamless transitions between online and offline modes

### Observability

- **Metrics Collection**: Comprehensive metrics for monitoring application performance
- **Structured Logging**: Enhanced logging system with filtering and search capabilities
- **Telemetry**: Privacy-focused telemetry system for tracking feature usage and errors
- **Canary Releases**: Gradual rollout of new features with automatic rollback
- **Resource Dashboard**: Visual monitoring of system resources and performance

## System Requirements

- **Windows**: Windows 10 or later (64-bit)
- **macOS**: macOS 10.15 (Catalina) or later
- **Linux**: Ubuntu 20.04 or later, or equivalent

## Installation

### Windows

1. Download the MSI installer from the releases page
2. Run the installer and follow the on-screen instructions
3. Alternatively, download the portable EXE if you prefer not to install

### macOS

1. Download the DMG file from the releases page
2. Open the DMG file and drag the application to your Applications folder
3. First-time users may need to approve the application in System Preferences > Security & Privacy

### Linux

#### Debian/Ubuntu

```bash
sudo apt install ./mcp-client_1.0.0_amd64.deb
```

#### Fedora/RHEL

```bash
sudo rpm -i mcp-client-1.0.0.x86_64.rpm
```

#### AppImage

```bash
chmod +x MCP-Client-1.0.0.AppImage
./MCP-Client-1.0.0.AppImage
```

## Getting Started

1. Launch the MCP Client from your applications menu or desktop shortcut
2. Sign in with your MCP account credentials
3. For offline usage, go to Settings > Offline and enable offline mode
4. Configure auto-update settings in Settings > Updates

## Development

### Prerequisites

- Node.js 16 or later
- Rust 1.65 or later
- Tauri CLI

### Setup

1. Clone the repository:

```bash
git clone https://github.com/your-org/mcp-client.git
cd mcp-client
```

2. Install dependencies:

```bash
npm install
```

3. Start the development server:

```bash
npm run tauri dev
```

### Building

#### For all platforms:

```bash
npm run tauri build
```

#### For specific platforms:

- Windows: `./installers/windows-build.ps1`
- macOS: `./installers/macos-build.sh`
- Linux: `./installers/linux-build.sh`

## Architecture

The MCP Client is built with a hybrid architecture:

- **Frontend**: React with TypeScript
- **Backend**: Rust with Tauri
- **Bridge**: Tauri commands for communication between frontend and backend

Key components:

- **Observability**: Metrics, logging, and telemetry systems
- **Offline**: Local-first architecture with embedded LLMs
- **Auto-Update**: Background update system with progressive rollouts
- **Optimization**: Memory management and caching
- **Distribution**: Platform-specific installers and updates

## Configuration

### Memory Management

Memory management can be configured through the Settings > Performance menu or via the configuration file:

```json
{
  "memory": {
    "maxMemoryMb": 1024,
    "thresholdMemoryMb": 768,
    "maxContextTokens": 8192,
    "enabled": true,
    "checkIntervalSecs": 30
  }
}
```

### Caching

Caching can be configured through the Settings > Performance menu:

```json
{
  "apiCache": {
    "maxEntries": 1000,
    "ttlSeconds": 300,
    "persist": true,
    "enabled": true,
    "cleanupIntervalSecs": 60
  },
  "resourceCache": {
    "maxEntries": 200,
    "ttlSeconds": 3600,
    "persist": true,
    "enabled": true,
    "cleanupIntervalSecs": 300
  }
}
```

### Auto-Update

Auto-update can be configured through the Settings > Updates menu:

```json
{
  "updater": {
    "enabled": true,
    "checkInterval": 24,
    "autoDownload": true,
    "autoInstall": false
  }
}
```

## Troubleshooting

### Common Issues

#### Application Won't Start

1. Check system requirements
2. Verify installation integrity
3. Check system logs for errors

#### High Memory Usage

1. Adjust memory limits in Settings > Performance
2. Close other memory-intensive applications
3. Restart the application to clear caches

#### Offline Mode Not Working

1. Ensure offline mode is enabled in Settings > Offline
2. Check that local models are downloaded
3. Verify storage permissions

#### Updates Not Installing

1. Check internet connection
2. Verify update settings in Settings > Updates
3. Try manual update by downloading latest version

### Logs

Log files are located at:

- Windows: `%APPDATA%\MCP-Client\logs`
- macOS: `~/Library/Application Support/MCP-Client/logs`
- Linux: `~/.config/MCP-Client/logs`

## License

MCP Client is licensed under the [MIT License](LICENSE).
</file>

<file path="docs/RELEASE_PROCESS.md">
# MCP Client Release Process

This document outlines the release process for the MCP Client application, including version management, testing, building, and deployment.

## Release Checklist

### Pre-Release Preparation

1. **Update Dependencies**
   - [ ] Update Rust dependencies: `cargo update`
   - [ ] Update npm dependencies: `cd src-frontend && npm update`
   - [ ] Test application with updated dependencies
   - [ ] Commit dependency updates

2. **Code Freeze**
   - [ ] Announce code freeze to development team
   - [ ] Ensure all features for the release are complete
   - [ ] Create a release branch: `git checkout -b release/vX.Y.Z`

3. **Documentation Updates**
   - [ ] Update README.md with new features
   - [ ] Update CHANGELOG.md with detailed changes
   - [ ] Update API documentation if necessary
   - [ ] Verify installation instructions are current
   - [ ] Update screenshots if UI has changed

### Testing Phase

1. **Automated Testing**
   - [ ] Ensure all unit tests pass: `cargo test --lib`
   - [ ] Run integration tests: `cargo test --test '*'`
   - [ ] Run performance benchmarks: `cargo bench --features benchmarking`
   - [ ] Run end-to-end tests: `cd src-frontend && npm run test:e2e`
   - [ ] Fix any failing tests

2. **Manual Testing**
   - [ ] Test offline functionality
   - [ ] Test auto-update process
   - [ ] Test installer packages on all platforms
   - [ ] Test performance with large datasets
   - [ ] Verify memory usage is within acceptable limits

3. **Beta Release (Optional)**
   - [ ] Tag beta release: `git tag -a vX.Y.Z-beta.1 -m "MCP Client vX.Y.Z Beta 1"`
   - [ ] Push tag: `git push origin vX.Y.Z-beta.1`
   - [ ] CI system will build beta packages
   - [ ] Distribute to beta testers
   - [ ] Collect feedback and fix issues

### Release Process

1. **Version Bump**
   - [ ] Update version in Cargo.toml
   - [ ] Update version in package.json
   - [ ] Update version in src-tauri/tauri.conf.json
   - [ ] Commit version changes: `git commit -m "Bump version to vX.Y.Z"`

2. **Final Testing**
   - [ ] Run full test suite one final time
   - [ ] Build release packages locally to verify
   - [ ] Test update process from previous version

3. **Release Tagging**
   - [ ] Merge release branch to main: `git checkout main && git merge release/vX.Y.Z`
   - [ ] Tag release: `git tag -a vX.Y.Z -m "MCP Client vX.Y.Z"`
   - [ ] Push main and tag: `git push origin main && git push origin vX.Y.Z`

4. **CI/CD Release Build**
   - [ ] CI system detects tag and starts release workflow
   - [ ] Monitor build process in GitHub Actions
   - [ ] Verify all build artifacts are created
   - [ ] Check update manifests are published correctly

5. **Release Publication**
   - [ ] Verify GitHub release is created automatically
   - [ ] Edit release notes if necessary
   - [ ] Publish release

6. **Post-Release Steps**
   - [ ] Notify users through update notification
   - [ ] Publish announcement on relevant channels
   - [ ] Monitor telemetry for issues
   - [ ] Begin next development cycle on develop branch

## Version Numbering

MCP Client follows [Semantic Versioning](https://semver.org/):

- **MAJOR version (X)** for incompatible API changes
- **MINOR version (Y)** for new features in a backward-compatible manner
- **PATCH version (Z)** for backward-compatible bug fixes

Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.

## Release Cadence

- **Patch releases (X.Y.Z)**: As needed for bug fixes, typically every 1-2 weeks
- **Minor releases (X.Y.0)**: Every 1-2 months for new features
- **Major releases (X.0.0)**: Every 6-12 months for significant changes

## Release Artifacts

The CI/CD pipeline produces the following artifacts for each platform:

### Windows
- MSI installer for 64-bit systems
- Portable EXE for 64-bit systems
- Update manifest for auto-updates

### macOS
- Universal DMG (Intel + Apple Silicon)
- Intel-specific DMG
- Apple Silicon-specific DMG
- Update manifest for auto-updates

### Linux
- DEB package for Debian/Ubuntu
- RPM package for Fedora/RHEL
- AppImage for other distributions
- Update manifest for auto-updates

## Update Server

The update server is hosted on GitHub Pages and serves the update manifests for each platform. The CI/CD pipeline automatically publishes the update manifests to the `updates` branch.

Update endpoints:
- Windows: `https://update.mcp-client.com/windows/[version]`
- macOS: `https://update.mcp-client.com/macos/[version]`
- Linux: `https://update.mcp-client.com/linux/[version]`

## Emergency Hotfix Process

In case a critical bug is discovered in a release:

1. Create a hotfix branch from the release tag: `git checkout -b hotfix/vX.Y.Z+1 vX.Y.Z`
2. Fix the issue with minimal changes
3. Run focused tests to verify the fix
4. Update version to X.Y.Z+1
5. Tag and release following the normal release process
6. Backport the fix to the develop branch

## Telemetry Analysis

After each release, analyze telemetry data to identify issues:

1. Check error rates compared to previous version
2. Monitor performance metrics for degradation
3. Track crash reports for new crash types
4. Analyze user engagement with new features
5. Look for anomalies in usage patterns

If significant issues are detected, consider rolling out a hotfix release.
</file>

<file path="docs/SECURITY_PRIVACY.md">
# Security and Privacy Features

## Overview

The MCP Client includes comprehensive security and privacy features designed to protect user data and provide transparency about how information is used. This document describes the key components of the security and privacy system.

## Key Components

### 1. End-to-End Encryption (E2EE)

The end-to-end encryption system ensures that data synchronized between devices remains private and secure:

- Uses ChaCha20-Poly1305 for authenticated encryption
- Implements X25519 for key exchange
- Provides forward secrecy through a double ratchet algorithm
- Automatically rotates keys for enhanced security
- No plaintext data is ever sent to remote servers

Implementation: `src/security/e2ee.rs`

### 2. Secure Credential Storage

Credentials and sensitive information are protected using platform-specific secure enclaves:

- Windows: Windows Credential Manager
- macOS: Keychain
- Linux: Secret Service API (GNOME Keyring/KWallet)
- Fallback: Encrypted local storage with strong obfuscation
- Configurable memory caching with automatic expiration

Implementation: `src/security/credentials.rs`

### 3. Data Flow Visualization

The data flow tracking system provides complete transparency about how data moves through the application:

- Visual graph of all data flows between components
- Classification of data sensitivity levels
- Real-time monitoring of data movement
- Historical logs of all data operations
- Filtering by classification level
- Statistics and insights about data usage

Implementation: `src/security/data_flow.rs`

### 4. Granular Permission Management

The permission system gives users fine-grained control over application capabilities:

- Individual permissions for each feature and data access
- Four permission levels: Always Allow, Ask First Time, Ask Every Time, Never Allow
- Categorized permissions for easy management
- Usage statistics to show which permissions are most frequently used
- Easy reset to default settings
- Required permissions are clearly marked

Implementation: `src/security/permissions.rs`

### 5. Privacy Controls

Additional privacy features ensure user control over their data:

- Anonymized telemetry with opt-out option
- Local-first architecture for offline usage
- Automatic clipboard clearing for sensitive data
- Encrypted local storage
- Minimal data collection by default
- Transparency about all data usage

Implementation: Various security components

## User Interface

The security and privacy features are accessible through a dedicated interface:

- **General Settings**: Configure encryption, secure storage, and privacy options
- **Permissions Manager**: Control granular permissions for all app features
- **Data Flow Visualization**: View and understand how data moves through the system
- **Credentials Manager**: Securely store and manage sensitive information

Implementation: `src-frontend/src/components/security/`

## Implementation Details

### Security Manager

The central security manager coordinates all security and privacy features:

- Initializes security services on application startup
- Provides a unified API for all security operations
- Handles configuration changes and updates
- Manages permissions and credential requests
- Tracks data flows between components

Implementation: `src/security/mod.rs`

### Secure Configuration

Security and privacy settings are stored in an encrypted configuration file:

- Default settings prioritize security and privacy
- User changes are persisted securely
- Configuration is loaded at startup with fallbacks for errors
- Validation ensures settings cannot be maliciously modified

### Cross-Platform Support

All security features work consistently across platforms:

- Uses platform-specific secure storage when available
- Falls back to robust cross-platform implementations when needed
- Adapts to different operating system security models
- Consistent user experience across Windows, macOS, and Linux

## Future Enhancements

Planned improvements to the security and privacy system:

1. **Biometric Authentication**: Add support for fingerprint/face recognition
2. **Hardware Security Keys**: Add support for YubiKey and similar devices
3. **Enhanced Audit Logging**: More detailed security event logs
4. **Third-Party Security Reviews**: Independent security audits
5. **Export Controls**: Allow users to export and delete all their data

## Development Guidelines

When extending the MCP client, follow these security principles:

1. **Data Minimization**: Only collect and store what's absolutely necessary
2. **Secure by Default**: All features should be secure in their default configuration
3. **User Control**: Always give users clear choices about their data
4. **Transparency**: Make data usage clear and understandable
5. **Defense in Depth**: Multiple layers of security for critical features
</file>

<file path="docs/TELEMETRY_ANALYSIS.md">
# MCP Client Telemetry Analysis Guide

This document outlines the telemetry collection, analysis, and monitoring process for the MCP Client application. Telemetry helps us identify issues, understand usage patterns, and improve the application based on real-world data.

## Telemetry Collection Overview

### Types of Telemetry Collected

The MCP Client collects the following types of telemetry data:

1. **Application Events**
   - Application start/stop
   - Feature usage
   - Settings changes
   - User engagement metrics

2. **Performance Metrics**
   - Memory usage
   - API latency
   - Load times
   - LLM inference performance
   - Synchronization times

3. **Error Reporting**
   - Application errors
   - API call failures
   - Network issues
   - Business logic errors

4. **Crash Reporting**
   - Crashes with stack traces
   - Unhandled exceptions
   - Resource exhaustion events

### Privacy and Data Collection

- All telemetry collection is **opt-in** and disabled by default
- No personally identifiable information (PII) is collected
- Device and user IDs are anonymous and randomly generated
- Telemetry collection can be configured granularly (enable/disable specific types)
- Data is encrypted in transit using HTTPS

## Telemetry Infrastructure

### Data Flow

```
┌───────────────┐    ┌──────────────┐    ┌────────────────┐
│ MCP Client    │    │ Telemetry    │    │ Telemetry      │
│ Telemetry     │───►│ API Endpoint │───►│ Database       │
│ Collection    │    │              │    │                │
└───────────────┘    └──────────────┘    └────────────────┘
                                                 │
                                                 ▼
┌────────────────┐    ┌──────────────┐    ┌────────────────┐
│ Dashboards     │◄───│ Anomaly      │◄───│ Analytics      │
│ and Alerts     │    │ Detection    │    │ Processing     │
└────────────────┘    └──────────────┘    └────────────────┘
```

### Components

1. **Telemetry Service** (in MCP Client)
   - Collects telemetry events
   - Batches events for efficient transmission
   - Handles offline buffering and synchronization
   - Implements privacy controls

2. **Telemetry API**
   - Receives and validates telemetry data
   - Authenticates client applications
   - Processes and normalizes data
   - Forwards data to storage

3. **Telemetry Database**
   - Time-series database for efficient storage
   - Optimized for high-volume write operations
   - Supports complex query patterns
   - Implements data retention policies

4. **Analytics Processing**
   - Aggregates telemetry data
   - Calculates key metrics and statistics
   - Generates reports and insights
   - Feeds data to anomaly detection

5. **Anomaly Detection**
   - Identifies abnormal patterns
   - Detects significant deviations from baseline
   - Triggers alerts for potential issues
   - Uses machine learning for pattern recognition

6. **Dashboards and Alerts**
   - Real-time visualization of key metrics
   - Customizable alert thresholds
   - Notification system for critical issues
   - Historical trend analysis

## Analysis Techniques

### Real-time Monitoring

- **Key Metrics Dashboard**: Monitor critical metrics in real-time
- **Error Rate Tracking**: Track error rates by type and severity
- **Performance Monitoring**: Monitor API latency, memory usage, etc.
- **Usage Tracking**: Monitor feature usage and user engagement

### Post-Release Analysis

After each release, perform a comprehensive analysis:

1. **Compare with Previous Version**
   - Error rates and types
   - Performance metrics
   - Feature usage
   - User engagement

2. **Identify Regressions**
   - Performance degradation
   - Increased error rates
   - New crash types
   - Decreased user engagement

3. **Feature Adoption Analysis**
   - Usage of new features
   - Time spent using new features
   - Patterns of feature usage
   - Feature abandonment rate

### Anomaly Detection

Automated anomaly detection looks for:

1. **Error Spikes**
   - Sudden increase in error rates
   - New error types appearing frequently
   - Errors affecting many users

2. **Performance Degradation**
   - Significant increase in API latency
   - Memory usage approaching limits
   - Slow operation times
   - Increased load times

3. **Crash Patterns**
   - New crash signatures
   - Increased crash rates
   - Crashes affecting specific platforms or configurations

4. **Usage Anomalies**
   - Unusual patterns of feature usage
   - Unexpected user flows
   - Features being avoided
   - Session time decreases

## Reports and Dashboards

### Standard Reports

1. **Daily Health Report**
   - Overall application health metrics
   - Key performance indicators
   - Active user counts
   - Error and crash summary

2. **Weekly Trends Report**
   - Week-over-week metrics comparison
   - Feature usage trends
   - Performance trend analysis
   - Top reported issues

3. **Monthly Insights Report**
   - In-depth usage analysis
   - Feature adoption metrics
   - Long-term trends
   - Recommendations for improvements

### Dashboards

1. **Executive Dashboard**
   - High-level summary of key metrics
   - User growth and engagement
   - Overall application health
   - Major issues and resolutions

2. **Development Dashboard**
   - Detailed error and crash reports
   - Performance metrics by component
   - Technical diagnostic information
   - Issue prioritization data

3. **Product Dashboard**
   - Feature usage metrics
   - User engagement patterns
   - User journey mapping
   - A/B test results

## Using Telemetry Data

### Issue Detection and Resolution

1. **Early Warning System**
   - Detect issues before many users are affected
   - Identify patterns that may lead to future problems
   - Monitor for signs of performance degradation

2. **Issue Prioritization**
   - Use impact metrics to prioritize issues
   - Focus on problems affecting many users
   - Address issues causing significant user friction

3. **Root Cause Analysis**
   - Correlate telemetry data with issues
   - Identify common factors in error cases
   - Trace problems across system components

### Product Improvement

1. **Feature Evaluation**
   - Measure adoption and usage of new features
   - Identify features with low engagement
   - Understand how features are being used

2. **User Experience Optimization**
   - Identify friction points in user flows
   - Measure impact of UX changes
   - Understand user behavior patterns

3. **Performance Optimization**
   - Target optimization efforts based on real-world data
   - Measure impact of performance improvements
   - Identify performance bottlenecks

## Implementation Details

### Client-Side Implementation

The telemetry system in the MCP Client is implemented in the `src/telemetry` module and consists of:

1. **TelemetryService**: Main service handling telemetry collection and sending
2. **TelemetryConfig**: Configuration for telemetry collection
3. **TelemetryEvent**: Represents a single telemetry event
4. **TelemetryAnalyzer**: Client-side analysis of telemetry data (in-app dashboard)

### Server-Side Implementation

The server-side telemetry processing system includes:

1. **Telemetry API**: REST API for receiving telemetry data
2. **Telemetry Processor**: Processes incoming telemetry data
3. **Telemetry Database**: Stores telemetry data
4. **Analytics Engine**: Processes and analyzes telemetry data
5. **Anomaly Detector**: Identifies abnormal patterns
6. **Dashboard Backend**: Serves data for dashboards

### Integration with CI/CD

Telemetry is integrated with the CI/CD pipeline:

1. **Release Tagging**: Each release is tagged with a version
2. **Telemetry Correlation**: Telemetry data is correlated with release versions
3. **Automatic Monitoring**: Release monitoring automatically tracks telemetry metrics
4. **Rollback Triggers**: Significant issues can trigger automatic rollbacks

## Responsible Telemetry Use

Guidelines for responsible telemetry use:

1. **Privacy First**: Always prioritize user privacy
2. **Transparency**: Be transparent about what data is collected and why
3. **Consent**: Always obtain user consent before collecting telemetry
4. **Minimization**: Collect only what is necessary for analysis
5. **Security**: Ensure all telemetry data is securely stored and transmitted
6. **Retention**: Implement appropriate data retention policies
7. **Aggregation**: Use aggregated data whenever possible
8. **Benefit**: Use telemetry to benefit users through improvements

## Telemetry Analysis Tools

The following tools are used for telemetry analysis:

1. **InfluxDB**: Time-series database for telemetry data storage
2. **Grafana**: Visualization and dashboarding
3. **Prometheus**: Metrics collection and alerting
4. **Elasticsearch**: Log storage and search
5. **Kibana**: Log visualization and analysis
6. **Jupyter Notebooks**: Custom analysis and reporting
7. **TensorFlow**: Machine learning for anomaly detection

## Appendix: Telemetry Schema

### Common Fields

All telemetry events include:

- `id`: Unique event ID
- `event_type`: Type of event
- `name`: Event name
- `timestamp`: Event timestamp
- `session_id`: Session ID
- `user_id`: Anonymous user ID
- `app_version`: Application version
- `os`: Operating system
- `device_id`: Anonymous device ID

### Event-Specific Fields

Different event types include additional fields:

1. **Error Events**
   - `error_message`: Error message
   - `error_code`: Error code
   - `stack_trace`: Stack trace (if available)

2. **Performance Events**
   - `value`: Metric value
   - `unit`: Unit of measurement
   - `context`: Additional context

3. **Feature Usage Events**
   - `feature_id`: Feature identifier
   - `duration_seconds`: Usage duration
   - `interaction_count`: Number of interactions

4. **Network Events**
   - `url`: API endpoint URL
   - `status_code`: HTTP status code
   - `duration_ms`: Request duration
   - `bytes_transferred`: Data transferred
</file>

<file path="docs/TESTING_AND_DEPLOYMENT.md">
# MCP Client Testing and Deployment

This document provides an overview of the testing suite, CI/CD pipeline, and deployment strategy for the MCP Client application.

## Testing Strategy

The MCP Client implements a comprehensive testing strategy that includes multiple layers of testing to ensure code quality and application reliability.

### Unit Tests

Unit tests focus on testing individual components in isolation. These tests are written using Rust's built-in testing framework and the `mockall` crate for mocking dependencies.

Key unit test files:
- `tests/unit/auto_update_test.rs`: Tests the auto-update functionality
- `tests/unit/optimization_test.rs`: Tests the performance optimization components
- `tests/unit/offline_test.rs`: Tests the offline capabilities

Best practices for unit tests:
- Each function should have at least one test
- Use mocks for external dependencies
- Test edge cases and error conditions
- Keep tests fast and independent

### Integration Tests

Integration tests verify that components work together correctly. These tests use real implementations of components but may mock external systems.

Key integration test files:
- `tests/integration/api_integration_test.rs`: Tests the API client integration
- `tests/integration/ui_integration_test.rs`: Tests the UI components integration
- `tests/integration/performance_test.rs`: Tests performance characteristics

Best practices for integration tests:
- Focus on component interactions
- Test realistic scenarios
- Use dependency injection for testability
- Isolate from external systems when possible

### End-to-End Tests

End-to-End (E2E) tests verify the entire application functions correctly from a user's perspective. These tests are written using Playwright and simulate real user interactions.

Key E2E test files:
- `tests/e2e/end_to_end_test.js`: Tests core functionality and user flows
- `tests/e2e/playwright.config.js`: Playwright configuration

Best practices for E2E tests:
- Focus on user-facing functionality
- Test critical user flows
- Use selectors that are resistant to UI changes
- Keep tests independent and idempotent

### Performance Benchmarking

Performance benchmarks measure the performance characteristics of critical components. These tests are written using the `criterion` crate.

Key benchmark files:
- `benches/performance_bench.rs`: Benchmarks for performance-critical components

Benchmarking best practices:
- Measure representative workloads
- Establish performance baselines
- Test with realistic data volumes
- Monitor for performance regressions

## CI/CD Pipeline

The MCP Client uses GitHub Actions for continuous integration and deployment.

### CI Workflow

The CI workflow is defined in `.github/workflows/ci.yml` and includes the following jobs:

1. **Lint**
   - Rust formatting with `rustfmt`
   - Static analysis with `clippy`

2. **Test**
   - Unit tests on all platforms
   - Integration tests on all platforms

3. **Benchmarks**
   - Performance benchmarks
   - Store benchmark results as artifacts

4. **E2E Tests**
   - End-to-End tests with Playwright
   - Store test reports as artifacts

5. **Build**
   - Build application for all platforms
   - Store build artifacts

CI workflow triggers:
- On push to `main` and `develop` branches
- On pull requests to `main` and `develop` branches

### CD Workflow

The CD workflow is defined in `.github/workflows/release.yml` and handles the automatic release process:

1. **Create Release**
   - Triggered by version tags (e.g., `v1.0.0`)
   - Creates a GitHub release

2. **Build and Upload**
   - Builds installers for all platforms
   - Uploads artifacts to the GitHub release

3. **Publish Update Manifest**
   - Creates update manifests for auto-update
   - Publishes manifests to the update server

4. **Notify**
   - Sends notifications about the release

## Telemetry Analysis

The MCP Client includes a comprehensive telemetry system for monitoring performance, errors, and usage in production.

### Telemetry Collection

The telemetry collection system is implemented in the `src/telemetry` module and includes:

- `TelemetryService`: Collects and sends telemetry data
- `TelemetryConfig`: Configures telemetry collection
- `TelemetryEvent`: Represents telemetry events

Types of telemetry collected:
- Application events (start/stop)
- Feature usage
- Errors and crashes
- Performance metrics
- Network events

### Telemetry Analysis

The `TelemetryAnalyzer` provides tools for analyzing telemetry data:

- Error trends analysis
- Performance metrics analysis
- User engagement analysis
- Anomaly detection

Telemetry reports:
- Daily health reports
- Weekly trends reports
- Monthly insights reports

### Privacy and Security

The telemetry system is designed with privacy and security in mind:

- Opt-in by default
- No personal identifiable information
- Anonymous device and user IDs
- Encrypted transmission
- Configurable data collection

## Release Process

The MCP Client follows a structured release process outlined in `docs/RELEASE_PROCESS.md`:

1. **Pre-Release Preparation**
   - Update dependencies
   - Code freeze
   - Documentation updates

2. **Testing Phase**
   - Automated testing
   - Manual testing
   - Optional beta release

3. **Release Process**
   - Version bump
   - Final testing
   - Release tagging
   - CI/CD release build
   - Release publication

4. **Post-Release Steps**
   - User notification
   - Monitoring
   - Next development cycle

## Deployment Architecture

### Update Server

The MCP Client uses a custom update server for auto-updates:

- Hosted on GitHub Pages
- Serves update manifests for each platform
- Supports incremental updates
- Verifies update integrity with signatures

### Installer Distribution

Installers are distributed through multiple channels:

- GitHub Releases
- Official website
- (Optional) Platform-specific stores

### Installation Channels

The MCP Client supports multiple installation methods:

1. **Windows**
   - MSI installer
   - Portable EXE

2. **macOS**
   - DMG installer
   - Universal binary

3. **Linux**
   - DEB package
   - RPM package
   - AppImage

## Monitoring and Maintenance

### Production Monitoring

The MCP Client includes tools for monitoring the application in production:

- Error and crash reporting
- Performance monitoring
- Usage analytics
- Anomaly detection

### Hotfix Process

For critical issues, a hotfix process is defined:

1. Create hotfix branch from release tag
2. Fix the issue with minimal changes
3. Run focused tests
4. Update version
5. Release following the normal process
6. Backport to develop branch

## Best Practices and Guidelines

### Code Quality

- Follow the Rust API Guidelines
- Use strong typing and error handling
- Write comprehensive documentation
- Follow consistent coding style

### Testing

- Write tests for all new features
- Maintain high test coverage
- Use appropriate test types
- Test on all supported platforms

### CI/CD

- Keep build times under 10 minutes
- Automate as much as possible
- Make builds reproducible
- Sign all release artifacts

### Deployment

- Use semantic versioning
- Test update paths from previous versions
- Provide rollback mechanisms
- Monitor new releases closely

## Future Improvements

Planned improvements to the testing and deployment infrastructure:

1. **Testing Improvements**
   - Property-based testing
   - Visual regression testing
   - Expanded performance testing
   - Load and stress testing

2. **CI/CD Improvements**
   - Matrix testing across more configurations
   - Improved caching for faster builds
   - Automated dependency updates
   - Code coverage reporting

3. **Telemetry Improvements**
   - Real-time anomaly detection
   - Improved dashboarding
   - ML-based issue prediction
   - Enhanced offline support

4. **Deployment Improvements**
   - Staged rollouts
   - A/B testing infrastructure
   - Feature flags integration
   - Improved rollback mechanisms
</file>

<file path="docs/USER_GUIDE.md">
# MCP Client User Guide

## Introduction

The MCP Client is a powerful desktop application that provides access to the MCP system. This guide will help you navigate its features and get the most out of your experience.

## Getting Started

### First Launch

1. After installation, launch the MCP Client from your applications menu or desktop shortcut
2. On first launch, you'll be prompted to sign in with your MCP account
3. Enter your username and password
4. (Optional) Enable "Remember me" to stay signed in
5. Click "Sign In"

### Main Interface

The MCP Client interface consists of several key areas:

- **Sidebar**: Navigation between different sections and conversations
- **Main Content Area**: Displays conversations, settings, or other content
- **Toolbar**: Quick access to common actions and settings
- **Status Bar**: Shows connection status, offline mode indicator, and more

## Core Features

### Conversations

#### Starting a New Conversation

1. Click the "New Conversation" button in the sidebar
2. Enter your message in the input field at the bottom of the screen
3. Press Enter or click the send button to send your message

#### Conversation Features

- **Rich Text**: Use markdown syntax for formatting
- **Code Blocks**: Use triple backticks (```) to format code blocks
- **File Attachments**: Drag and drop files or use the attachment button
- **Images**: Include images in your conversations
- **Voice Input**: Use the microphone button for voice-to-text

#### Managing Conversations

- **Rename**: Click the conversation title to rename it
- **Delete**: Use the menu in the top-right of a conversation to delete it
- **Export**: Export conversations to various formats (PDF, Markdown, etc.)
- **Share**: Share conversations with other MCP users

### Offline Mode

#### Enabling Offline Mode

1. Go to Settings > Offline
2. Toggle "Enable Offline Mode"
3. Download required language models (if not already downloaded)
4. Configure offline settings as needed

#### Using Offline Mode

- When online, the MCP Client will sync conversations and resources
- When offline, the client will use local models for responses
- A status indicator in the status bar shows your current mode
- Conversations started in offline mode will sync when you're back online

#### Sync Management

1. Go to Settings > Offline > Sync
2. View sync status and history
3. Force sync manually if needed
4. Configure automatic sync options

### Performance Settings

#### Memory Management

1. Go to Settings > Performance > Memory
2. Adjust maximum memory usage
3. Configure cleanup frequency
4. Set token limits for conversations

#### Cache Management

1. Go to Settings > Performance > Cache
2. Configure API and resource cache sizes
3. Set TTL (Time To Live) for cached items
4. Clear caches manually if needed

## Advanced Features

### Resource Dashboard

1. Go to Tools > Resource Dashboard
2. View real-time performance metrics
3. Monitor memory usage, API latency, and more
4. Export performance logs for troubleshooting

### Local LLM Management

1. Go to Settings > Offline > Local Models
2. View installed models
3. Download new models
4. Remove unused models
5. Configure model settings

### Update Management

1. Go to Settings > Updates
2. View current version and update status
3. Check for updates manually
4. Configure automatic update settings
5. View update history

## Settings Reference

### General Settings

- **Appearance**: Light/Dark/System theme
- **Language**: Interface language
- **Notifications**: Configure notification behavior
- **Startup**: Launch on system startup, minimize to tray

### Offline Settings

- **Offline Mode**: Enable/disable offline capabilities
- **Local Models**: Manage local language models
- **Checkpointing**: Configure automatic checkpoints
- **Sync**: Manage synchronization settings

### Performance Settings

- **Memory**: Configure memory usage limits
- **Cache**: Configure API and resource caching
- **Resource Usage**: Set CPU and network usage limits
- **Optimization**: Advanced performance settings

### Updates

- **Check Frequency**: How often to check for updates
- **Automatic Download**: Download updates automatically
- **Automatic Install**: Install updates automatically
- **Update Channel**: Stable, Beta, or Alpha

### Privacy

- **Telemetry**: Configure usage data collection
- **Conversation Storage**: Local storage settings
- **Data Retention**: Configure automatic cleanup

## Keyboard Shortcuts

### General

- `Ctrl+N` (Windows/Linux) or `Cmd+N` (macOS): New conversation
- `Ctrl+O` (Windows/Linux) or `Cmd+O` (macOS): Open conversation
- `Ctrl+S` (Windows/Linux) or `Cmd+S` (macOS): Save/export conversation
- `Ctrl+P` (Windows/Linux) or `Cmd+P` (macOS): Print conversation
- `F1`: Open help

### Conversation

- `Ctrl+Enter` (Windows/Linux) or `Cmd+Enter` (macOS): Send message
- `Alt+Up/Down` (Windows/Linux) or `Option+Up/Down` (macOS): Navigate message history
- `Ctrl+Shift+C` (Windows/Linux) or `Cmd+Shift+C` (macOS): Copy selected message
- `Ctrl+Shift+V` (Windows/Linux) or `Cmd+Shift+V` (macOS): Paste without formatting

### Navigation

- `Ctrl+1-9` (Windows/Linux) or `Cmd+1-9` (macOS): Switch to sidebar section
- `Ctrl+Tab` (Windows/Linux) or `Cmd+Tab` (macOS): Switch between conversations
- `Ctrl+,` (Windows/Linux) or `Cmd+,` (macOS): Open settings

## Troubleshooting

### Connectivity Issues

If you're experiencing connection problems:

1. Check your internet connection
2. Go to Settings > Network and click "Test Connection"
3. Try enabling offline mode temporarily
4. Restart the application

### Performance Issues

If the application feels slow or unresponsive:

1. Go to Settings > Performance
2. Reduce memory limits if your system has limited RAM
3. Clear caches manually
4. Close other memory-intensive applications
5. Restart the application

### Sync Issues

If synchronization isn't working properly:

1. Check your internet connection
2. Go to Settings > Offline > Sync
3. Check the sync status and logs
4. Try forcing a manual sync
5. If problems persist, try signing out and back in

### Crash Recovery

If the application crashes:

1. Restart the application
2. Check if a crash report dialog appears
3. Submit the crash report if prompted
4. Check logs in Help > View Logs
5. If the issue persists, try reinstalling the application

## Advanced Topics

### Command Palette

Access the command palette with `Ctrl+Shift+P` (Windows/Linux) or `Cmd+Shift+P` (macOS) to quickly:

- Execute commands
- Navigate to settings
- Access advanced features
- Perform searches

### Developer Tools

For advanced users and troubleshooting:

1. Open developer tools with `Ctrl+Shift+I` (Windows/Linux) or `Cmd+Option+I` (macOS)
2. View console logs
3. Inspect application resources
4. Run diagnostic commands

### Custom Configuration

Advanced configuration options are available in:

- Windows: `%APPDATA%\MCP-Client\config.json`
- macOS: `~/Library/Application Support/MCP-Client/config.json`
- Linux: `~/.config/MCP-Client/config.json`

Edit these files only if you know what you're doing, as incorrect settings may cause instability.

## Getting Help

- **In-App Help**: Access help documentation through Help > Documentation
- **Support**: Contact support through Help > Contact Support
- **Community**: Join the MCP community forum for tips and assistance
- **Updates**: Ensure you're using the latest version for the best experience
</file>

<file path="frontend-only.ps1">
# Start only the frontend of the Papin application
Write-Host "Starting Papin frontend only..." -ForegroundColor Green

# Navigate to frontend directory
Set-Location C:\Projects\Papin\src-frontend

# Start the frontend
npm run dev

Write-Host "Frontend started successfully!" -ForegroundColor Green
</file>

<file path="implementation-summary.md">
# Collaboration Capabilities Implementation Summary

## Overview

We have successfully implemented comprehensive real-time collaboration capabilities for the MCP client, enabling multiple users to collaborate in real-time on the same conversation. This implementation includes cursor presence, session management, cross-device synchronization, and the foundation for audio/video communication. We've also added optimizations for performance and reliability, as well as a collaborative whiteboard feature.

## Implementation Details

### Backend Components

1. **Core Collaboration Module** (`src/collaboration/mod.rs`)
   - Main collaboration manager class
   - Session and user management
   - Configuration and initialization

2. **Presence Management** (`src/collaboration/presence.rs`)
   - Real-time cursor and selection tracking
   - User online status monitoring

3. **Session Management** (`src/collaboration/sessions.rs`)
   - Session creation, joining, and management
   - User roles and permissions

4. **Synchronization** (`src/collaboration/sync.rs`)
   - Two-way data synchronization
   - Conflict resolution
   - Operational transformation

5. **RTC Infrastructure** (`src/collaboration/rtc.rs`)
   - WebRTC-based audio/video call support
   - Media device management

6. **Tauri Commands** (`src/commands/collaboration/mod.rs`)
   - Interface for the frontend to access backend functionality
   - Whiteboard operation commands

### Frontend Components

1. **Context and State Management**
   - `CollaborationContext.tsx` - Context provider for collaboration state
   - `useCollaboration.ts` - Hook for accessing collaboration features

2. **Presence Visualization**
   - `CursorOverlay.tsx` - Shows remote user cursors with throttling for performance
   - `SelectionOverlay.tsx` - Displays remote user text selections
   - `UserBadge.tsx` - Provides user information

3. **User Interface**
   - `CollaborationPanel.tsx` - Main panel for collaboration features
   - `UserList.tsx` - Shows users in current session
   - `CallControls.tsx` - Interface for audio/video calls
   - `CollaborationSettings.tsx` - Settings for collaboration features

4. **Whiteboard**
   - `CollaborativeWhiteboard.tsx` - Shared whiteboard with real-time drawing

5. **Optimization Utilities**
   - `throttle.ts` - Throttles function calls to reduce network traffic
   - `batch.ts` - Batches multiple function calls to improve performance
   - `retry.ts` - Adds retry logic for network operations

6. **Integration**
   - `AppShell.tsx` - Sample integration into the main UI
   - CSS styles for all components

7. **Testing**
   - Unit tests for collaboration context and components

8. **Feature Flag**
   - Added `COLLABORATION` flag to the feature flags system

## Performance Optimizations

We've implemented several performance optimizations to ensure smooth collaboration even with many users:

1. **Cursor Update Throttling**
   - Limit cursor position updates to 50ms intervals to reduce network traffic
   - Smooth client-side animation between updates

2. **Batched Operations**
   - Group multiple selection updates into batches to reduce API calls
   - Process operations in efficient batches with configurable size and delay

3. **Network Resilience**
   - Added retry mechanism for critical operations
   - Exponential backoff for retries to avoid overloading the server
   - Smart retry conditions to only retry network-related errors

4. **Stale Data Handling**
   - Automatic cleanup of stale cursors and selections
   - Time-based filtering of outdated user presence data

## New Features

### Collaborative Whiteboard

We've implemented a collaborative whiteboard feature that allows users to draw together in real-time:

1. **Drawing Tools**
   - Pencil for freehand drawing
   - Line, rectangle, and circle tools for geometric shapes
   - Eraser tool for corrections
   - Color picker and size adjustment

2. **Real-Time Synchronization**
   - Shares drawing operations with all connected users
   - Efficient encoding of drawing data to minimize network traffic
   - Optimistic local updates for responsive UI

3. **Backend Support**
   - Tauri commands for sending and receiving operations
   - State management for whiteboard content
   - Image export functionality

## File Structure

```
src/
└── collaboration/                    # Backend collaboration code
    ├── mod.rs                        # Main collaboration manager
    ├── presence.rs                   # Cursor and selection tracking
    ├── rtc.rs                        # WebRTC infrastructure
    ├── sessions.rs                   # Session management
    └── sync.rs                       # Synchronization system

src/
└── commands/
    └── collaboration/                # Tauri commands for collaboration
        ├── mod.rs                    # Main command module
        └── whiteboard.rs             # Whiteboard-specific commands

src-frontend/
└── src/
    ├── components/
    │   ├── collaboration/            # Frontend collaboration components
    │   │   ├── context/
    │   │   │   └── CollaborationContext.tsx
    │   │   ├── presence/
    │   │   │   ├── CursorOverlay.tsx
    │   │   │   ├── SelectionOverlay.tsx
    │   │   │   └── UserBadge.tsx
    │   │   ├── call/
    │   │   │   └── CallControls.tsx
    │   │   ├── settings/
    │   │   │   └── CollaborationSettings.tsx
    │   │   ├── whiteboard/
    │   │   │   └── CollaborativeWhiteboard.tsx
    │   │   ├── CollaborationPanel.tsx
    │   │   ├── UserList.tsx
    │   │   └── index.ts
    │   └── AppShell.tsx              # Sample integration into main UI
    ├── hooks/
    │   └── useCollaboration.ts       # Collaboration hook with optimizations
    ├── utils/
    │   ├── throttle.ts               # Throttling utility
    │   ├── batch.ts                  # Batching utility
    │   └── retry.ts                  # Retry utility
    ├── styles/
    │   ├── collaboration.css         # Styles for collaboration components
    │   └── whiteboard.css            # Styles for whiteboard
    └── __tests__/
        └── collaboration/
            ├── CollaborationContext.test.tsx
            └── CursorOverlay.test.tsx
```

## Usage Guide

### Enabling Collaboration

Collaboration features can be enabled by setting the `COLLABORATION` feature flag:

```rust
// In code
let mut feature_manager = FeatureManager::default();
feature_manager.enable(FeatureFlags::COLLABORATION);

// Via environment variable
// CLAUDE_MCP_FEATURES=COLLABORATION
```

### Integration Steps

To integrate collaboration into an application:

1. **Wrap your app with the provider**:

```tsx
import { CollaborationProvider } from './components/collaboration';

function App() {
  return (
    <CollaborationProvider>
      {/* Your app content */}
    </CollaborationProvider>
  );
}
```

2. **Add the cursor and selection overlays**:

```tsx
import { CursorOverlay, SelectionOverlay } from './components/collaboration';

function MainContent() {
  const mainContentRef = useRef<HTMLDivElement>(null);
  const editorRef = useRef<HTMLDivElement>(null);

  return (
    <div ref={mainContentRef}>
      <div ref={editorRef}>
        {/* Content */}
      </div>
      
      <CursorOverlay containerRef={mainContentRef} />
      <SelectionOverlay editorRef={editorRef} />
    </div>
  );
}
```

3. **Add the collaboration panel**:

```tsx
import { CollaborationPanel } from './components/collaboration';

function Sidebar() {
  return (
    <div>
      <CollaborationPanel conversationId="your-conversation-id" />
    </div>
  );
}
```

4. **Using the optimized hooks**:

```tsx
import { useCollaboration } from './hooks/useCollaboration';

function YourComponent() {
  // Get optimized functions (throttled, batched, with retry)
  const { 
    createSessionWithRetry, 
    throttledUpdateCursor,
    batchUpdateSelection
  } = useCollaboration();
  
  // Use them in your component
  const handleMouseMove = (e) => {
    throttledUpdateCursor(x, y, elementId);
  };
  
  // ...
}
```

5. **Add the whiteboard**:

```tsx
import { CollaborativeWhiteboard } from './components/collaboration';

function Editor() {
  return (
    <div>
      <CollaborativeWhiteboard 
        sessionId="your-session-id"
        width={800}
        height={600}
      />
    </div>
  );
}
```

## Future Work

While we've implemented a comprehensive collaboration system, there are still some areas for further improvement:

1. **Enhanced Whiteboard**
   - Add text tool for adding text annotations
   - Implement undo/redo functionality
   - Add image import/export capabilities

2. **Screen Sharing**
   - Implement screen sharing capabilities using WebRTC
   - Add annotation tools for shared screens

3. **Conflict Resolution**
   - Improve the conflict resolution strategies for concurrent edits
   - Add visual indicators for conflicting edits

4. **User Interaction**
   - Add user presence awareness with "focus" indicators
   - Implement "raise hand" feature for meetings

5. **End-to-End Testing**
   - Add integration tests for multi-user scenarios
   - Performance testing with many concurrent users

## Conclusion

The collaboration capabilities implementation provides a robust foundation for real-time collaborative work in the MCP client. With the optimizations and new features added, it offers a smooth and responsive user experience even with many users. The system is designed to be modular and extensible, allowing for future enhancements as needed.
</file>

<file path="installers/linux-build.sh">
#!/bin/bash

# Build script for Linux distribution packages
# Supports: DEB, RPM, and AppImage

set -e

# Get the project directory
PROJECT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$PROJECT_DIR"

echo "Building MCP Client for Linux distributions..."

# Ensure required tools are installed
check_tools() {
  for tool in $@; do
    if ! command -v $tool &> /dev/null; then
      echo "Error: $tool is not installed. Please install it first."
      exit 1
    fi
  done
}

check_tools cargo npm rustup

# Update dependencies
echo "Updating dependencies..."
npm install
cargo update

# Build the application
echo "Building application..."
npm run build

# Build for specific Linux targets
echo "Building DEB package..."
cargo tauri build --target deb
echo "DEB package built successfully!"

echo "Building RPM package..."
cargo tauri build --target rpm
echo "RPM package built successfully!"

echo "Building AppImage..."
cargo tauri build --target appimage
echo "AppImage built successfully!"

# Move built packages to installers directory
echo "Moving packages to installers directory..."
mkdir -p "$PROJECT_DIR/installers/linux"
mv "$PROJECT_DIR/src-tauri/target/release/bundle/deb/"*.deb "$PROJECT_DIR/installers/linux/"
mv "$PROJECT_DIR/src-tauri/target/release/bundle/rpm/"*.rpm "$PROJECT_DIR/installers/linux/"
mv "$PROJECT_DIR/src-tauri/target/release/bundle/appimage/"*.AppImage "$PROJECT_DIR/installers/linux/"

# Generate checksums
echo "Generating checksums..."
cd "$PROJECT_DIR/installers/linux"
sha256sum *.deb *.rpm *.AppImage > checksums.txt

echo "Linux builds completed successfully!"
echo "Packages are available in the '$PROJECT_DIR/installers/linux' directory."
</file>

<file path="installers/macos-build.sh">
#!/bin/bash

# Build script for macOS DMG installers
# Supports Universal, Intel, and Apple Silicon builds

set -e

# Get the project directory
PROJECT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$PROJECT_DIR"

echo "Building MCP Client for macOS..."

# Ensure required tools are installed
check_tools() {
  for tool in $@; do
    if ! command -v $tool &> /dev/null; then
      echo "Error: $tool is not installed. Please install it first."
      exit 1
    fi
  done
}

check_tools cargo npm rustup

# Update dependencies
echo "Updating dependencies..."
npm install
cargo update

# Add Apple Silicon target if not already added
if ! rustup target list | grep -q "aarch64-apple-darwin"; then
  echo "Adding aarch64-apple-darwin target..."
  rustup target add aarch64-apple-darwin
fi

# Build the application
echo "Building application..."
npm run build

# Build for Intel x86_64
echo "Building for Intel (x86_64)..."
cargo tauri build --target x86_64-apple-darwin
echo "Intel build completed successfully!"

# Build for Apple Silicon
echo "Building for Apple Silicon (aarch64)..."
cargo tauri build --target aarch64-apple-darwin
echo "Apple Silicon build completed successfully!"

# Build Universal Binary
echo "Building Universal Binary..."
cargo tauri build --target universal-apple-darwin
echo "Universal Binary build completed successfully!"

# Move built packages to installers directory
echo "Moving packages to installers directory..."
mkdir -p "$PROJECT_DIR/installers/macos"

# Copy DMG files to installers directory
cp "$PROJECT_DIR/src-tauri/target/x86_64-apple-darwin/release/bundle/dmg/"*.dmg "$PROJECT_DIR/installers/macos/MCP-Client-Intel.dmg"
cp "$PROJECT_DIR/src-tauri/target/aarch64-apple-darwin/release/bundle/dmg/"*.dmg "$PROJECT_DIR/installers/macos/MCP-Client-AppleSilicon.dmg"
cp "$PROJECT_DIR/src-tauri/target/universal-apple-darwin/release/bundle/dmg/"*.dmg "$PROJECT_DIR/installers/macos/MCP-Client-Universal.dmg"

# Generate checksums
echo "Generating checksums..."
cd "$PROJECT_DIR/installers/macos"
shasum -a 256 *.dmg > checksums.txt

echo "macOS builds completed successfully!"
echo "Packages are available in the '$PROJECT_DIR/installers/macos' directory."
</file>

<file path="installers/windows-build.ps1">
# Windows Build Script for MCP Client
# Generates MSI installer and portable EXE

# Ensure script stops on errors
$ErrorActionPreference = "Stop"

Write-Host "Building MCP Client for Windows..." -ForegroundColor Green

# Get project directory
$ProjectDir = Split-Path -Parent (Split-Path -Parent $MyInvocation.MyCommand.Path)
Set-Location $ProjectDir

# Check for required tools
function Check-Command($cmdname) {
    return [bool](Get-Command -Name $cmdname -ErrorAction SilentlyContinue)
}

if (-not (Check-Command "cargo")) {
    Write-Host "Error: Cargo is not installed. Please install Rust and Cargo first." -ForegroundColor Red
    exit 1
}

if (-not (Check-Command "npm")) {
    Write-Host "Error: NPM is not installed. Please install Node.js first." -ForegroundColor Red
    exit 1
}

# Update dependencies
Write-Host "Updating dependencies..." -ForegroundColor Cyan
npm install
cargo update

# Build the application
Write-Host "Building application..." -ForegroundColor Cyan
npm run build

# Build MSI installer
Write-Host "Building MSI installer..." -ForegroundColor Cyan
cargo tauri build --target msi
Write-Host "MSI installer built successfully!" -ForegroundColor Green

# Build portable version
Write-Host "Building portable EXE..." -ForegroundColor Cyan
cargo tauri build
Write-Host "Portable EXE built successfully!" -ForegroundColor Green

# Create installers directory if it doesn't exist
$InstallerDir = Join-Path $ProjectDir "installers\windows"
if (-not (Test-Path $InstallerDir)) {
    New-Item -ItemType Directory -Path $InstallerDir -Force | Out-Null
}

# Move built packages to installers directory
Write-Host "Moving packages to installers directory..." -ForegroundColor Cyan
$MsiPath = Get-ChildItem -Path "$ProjectDir\src-tauri\target\release\bundle\msi\*.msi" | Select-Object -First 1
$ExePath = "$ProjectDir\src-tauri\target\release\mcp-client.exe"

Copy-Item $MsiPath -Destination $InstallerDir
Copy-Item $ExePath -Destination "$InstallerDir\MCP-Client-Portable.exe"

# Generate checksums
Write-Host "Generating checksums..." -ForegroundColor Cyan
Set-Location $InstallerDir
Get-FileHash -Algorithm SHA256 *.msi, *.exe | ForEach-Object {
    "$($_.Hash) $($_.Path.Split('\')[-1])"
} | Out-File -FilePath "checksums.txt" -Encoding utf8

Write-Host "Windows builds completed successfully!" -ForegroundColor Green
Write-Host "Packages are available in the '$InstallerDir' directory." -ForegroundColor Cyan
</file>

<file path="Makefile">
.PHONY: all clean build-common build-cli build-tui build-gui build-all install-cli install-tui install-gui install-all

# Variables
CARGO := cargo
CARGO_BUILD_ARGS := --release
RUST_BACKTRACE := 1
INSTALL_BIN_DIR := $(HOME)/.local/bin

# Default target
all: build-all

# Clean all targets
clean:
	@echo "Cleaning all targets..."
	$(CARGO) clean
	cd src-frontend && npm run clean || echo "No clean script in frontend"

# Build common library
build-common:
	@echo "Building common library..."
	cd src-common && $(CARGO) build $(CARGO_BUILD_ARGS)

# Build CLI
build-cli: build-common
	@echo "Building CLI..."
	cd src-cli && $(CARGO) build $(CARGO_BUILD_ARGS)

# Build TUI
build-tui: build-common
	@echo "Building TUI..."
	cd src-tui && $(CARGO) build $(CARGO_BUILD_ARGS)

# Build GUI (Tauri app)
build-gui: build-common
	@echo "Building GUI..."
	@echo "Installing frontend dependencies..."
	cd src-frontend && npm install
	@echo "Building Tauri app..."
	$(CARGO) tauri build

# Build all components
build-all: build-cli build-tui build-gui

# Install CLI
install-cli: build-cli
	@echo "Installing CLI..."
	mkdir -p $(INSTALL_BIN_DIR)
	cp target/release/mcp-cli $(INSTALL_BIN_DIR)/mcp
	@echo "CLI installed to $(INSTALL_BIN_DIR)/mcp"

# Install TUI
install-tui: build-tui
	@echo "Installing TUI..."
	mkdir -p $(INSTALL_BIN_DIR)
	cp target/release/mcp-tui $(INSTALL_BIN_DIR)/mcp-tui
	@echo "TUI installed to $(INSTALL_BIN_DIR)/mcp-tui"

# Install GUI
install-gui: build-gui
	@echo "Installing GUI..."
	@echo "To install the GUI, use the generated .deb or .AppImage file in src-tauri/target/release."

# Install all components
install-all: install-cli install-tui install-gui

# Run CLI
run-cli:
	@echo "Running CLI..."
	cd src-cli && $(CARGO) run

# Run TUI
run-tui:
	@echo "Running TUI..."
	cd src-tui && $(CARGO) run

# Run GUI in development mode
run-gui:
	@echo "Running GUI in development mode..."
	cd src-frontend && npm run dev
</file>

<file path="MCP_README.md">
# Model Context Protocol (MCP) Implementation

This document provides an overview of the Model Context Protocol implementation in the Claude MCP Client. The implementation enables real-time communication with MCP-compatible servers for AI model interactions.

## Core Components

### Protocol Architecture

The MCP implementation follows a layered architecture:

1. **Protocol Definition** - Core types and interfaces in `protocols/mod.rs`
2. **MCP Protocol** - Protocol-specific implementation in `protocols/mcp/`
3. **WebSocket Client** - Low-level WebSocket communication in `protocols/mcp/websocket.rs`
4. **Message Types** - Message structures in `models/messages.rs` and `protocols/mcp/message.rs`
5. **Service Layer** - High-level abstractions in `services/mcp.rs` and `services/chat.rs`
6. **Command Layer** - Tauri command interface in `commands/mcp.rs` and `commands/chat.rs`

### Protocol Handlers

The protocol is implemented using a series of handler traits:

- `ProtocolHandler` - Base trait for all protocol handlers
- `McpProtocolHandler` - MCP-specific implementation
- `ProtocolFactory` - Factory for creating protocol handlers
- `McpProtocolFactory` - Factory for MCP handlers

### Message Flow

1. **Client to Server**: Messages originate from the UI, pass through the service layer, get converted to MCP format, and are sent via WebSocket.
2. **Server to Client**: Messages arrive via WebSocket, are parsed into MCP format, converted to application format, and dispatched to subscribers.

## Message Protocol

### Message Types

The MCP protocol supports several message types:

- `CompletionRequest` - Request a completion from the model
- `CompletionResponse` - Response containing the completion
- `StreamingMessage` - Chunk of a streaming response
- `StreamingEnd` - End of a streaming response
- `CancelStream` - Request to cancel a streaming response
- `Error` - Error message
- `Ping/Pong` - Heartbeat messages
- `AuthRequest/AuthResponse` - Authentication messages

### Message Structure

Each MCP message contains:

- `id`: Unique message identifier
- `version`: Protocol version
- `type`: Message type
- `payload`: Type-specific content

## WebSocket Communication

The WebSocket client manages the low-level communication with the MCP server:

- Connection establishment and authentication
- Message serialization and deserialization
- Heartbeat and reconnection logic
- Error handling and recovery

## Streaming Implementation

The client supports streaming responses through the following mechanisms:

1. Client sends a completion request with `stream: true`
2. Server responds with a series of `StreamingMessage` events
3. Client accumulates these chunks into a complete response
4. Server signals the end of streaming with a `StreamingEnd` message
5. Client notifies UI of completion

## Authentication

Authentication with MCP servers is handled through:

1. API key validation during initial connection
2. Session token management for persistent connections
3. Secure credential storage using the system's config manager

## Service Integration

The MCP protocol is integrated with the application through:

- `McpService` - Core service for MCP interactions
- `ChatService` - High-level abstraction for conversation management
- Tauri commands that expose functionality to the UI

## Error Handling

The implementation includes a comprehensive error handling approach:

- Protocol-specific error types with detailed information
- Error propagation through all layers
- Recovery mechanisms for transient failures
- Graceful degradation on critical failures

## Extending the Protocol

To add new MCP message types:

1. Add the type to `McpMessageType` enum
2. Create corresponding payload structure in `McpMessagePayload`
3. Implement handling logic in `McpClient`
4. Expose through the service layer as needed

## Security Considerations

The implementation includes several security measures:

- API keys are never exposed to the frontend
- TLS encryption for all WebSocket communications
- API scopes for limiting access
- Session token rotation on reconnection
- Timeout handling for all requests

## Performance Optimizations

The implementation is optimized for performance:

- Asynchronous processing of all network operations
- Lazy loading of non-essential components
- Message buffering for resilience
- Efficient message serialization
- Connection pooling and reuse

## Cross-Platform Considerations

While designed for Linux, the implementation works across platforms:

- Platform-agnostic protocol implementation
- Abstracted file system operations
- Configurable paths for platform-specific storage
- Consistent error handling across platforms
</file>

<file path="plugins/examples/github-snippets/Cargo.toml">
[package]
name = "github-snippets"
version = "0.1.0"
edition = "2021"
description = "Fetch code snippets from GitHub repositories"
authors = ["Claude MCP Team"]
license = "MIT"

[lib]
crate-type = ["cdylib"]

[dependencies]
wasm-bindgen = "0.2"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
</file>

<file path="plugins/examples/github-snippets/manifest.json">
{
  "name": "github-snippets",
  "displayName": "GitHub Code Snippets",
  "version": "1.0.0",
  "description": "Fetch code snippets from GitHub repositories",
  "author": "Claude MCP Team",
  "license": "MIT",
  "main": "github-snippets.wasm",
  "permissions": [
    "conversations:read",
    "conversations:write",
    "network:github.com",
    "network:api.github.com"
  ],
  "hooks": [
    "message:pre-process",
    "message:post-process"
  ],
  "config": {
    "settings": [
      {
        "name": "githubToken",
        "type": "string",
        "label": "GitHub Token",
        "description": "Your GitHub personal access token (optional)",
        "default": "",
        "secret": true
      },
      {
        "name": "defaultBranch",
        "type": "string",
        "label": "Default Branch",
        "description": "Default branch to use when not specified",
        "default": "main"
      },
      {
        "name": "codeBlockStyle",
        "type": "string",
        "label": "Code Block Style",
        "description": "Style of code blocks",
        "default": "github",
        "enum_values": [
          {
            "value": "github",
            "label": "GitHub Style"
          },
          {
            "value": "simple",
            "label": "Simple Style"
          },
          {
            "value": "detailed",
            "label": "Detailed Style"
          }
        ]
      },
      {
        "name": "maxLineCount",
        "type": "number",
        "label": "Max Line Count",
        "description": "Maximum number of lines to fetch",
        "default": 100
      },
      {
        "name": "includeMetadata",
        "type": "boolean",
        "label": "Include Metadata",
        "description": "Include file metadata with code snippets",
        "default": true
      }
    ]
  }
}
</file>

<file path="plugins/examples/github-snippets/src/main.rs">
use wasm_bindgen::prelude::*;
use serde::{Serialize, Deserialize};
use serde_json::{json, Value};
use std::collections::HashMap;

// Plugin settings
#[derive(Serialize, Deserialize)]
struct Settings {
    github_token: String,
    default_branch: String,
    code_block_style: String,
    max_line_count: u32,
    include_metadata: bool,
}

// GitHub file content response
#[derive(Serialize, Deserialize)]
struct GitHubFileContent {
    content: String,
    encoding: String,
    url: String,
    sha: String,
    size: u32,
}

// GitHub error response
#[derive(Serialize, Deserialize)]
struct GitHubError {
    message: String,
    documentation_url: Option<String>,
}

// Plugin entry point
#[wasm_bindgen]
pub fn init() {
    // Register the plugin
    host::register_plugin();
    
    // Register hooks
    host::register_hook("message:pre-process", pre_process_message);
    host::register_hook("message:post-process", post_process_message);
    
    host::log_message("info", "GitHub Code Snippets plugin initialized");
}

// Pre-process message hook
fn pre_process_message(context_ptr: i32) -> i32 {
    // Parse context
    let context_str = host::read_memory(context_ptr);
    let context: Value = serde_json::from_str(&context_str).unwrap();
    
    // Get message content
    let message = context["data"]["message"].as_object().unwrap();
    let content = message["content"].as_str().unwrap();
    
    // Check for GitHub command
    if content.starts_with("/github") {
        // Get settings
        let settings_str = host::get_settings();
        let settings: Settings = serde_json::from_str(&settings_str).unwrap_or_else(|_| Settings {
            github_token: String::new(),
            default_branch: "main".to_string(),
            code_block_style: "github".to_string(),
            max_line_count: 100,
            include_metadata: true,
        });
        
        // Parse command
        let parts: Vec<&str> = content.split_whitespace().collect();
        
        // Check command format
        if parts.len() < 2 {
            let help_message = "Usage: /github [owner]/[repo]/[path/to/file] [branch/tag] [start_line-end_line]\n\
                               Example: /github microsoft/vscode/src/vs/editor/editor.main.ts main 10-20";
            
            // Create modified message
            let mut new_message = message.clone();
            new_message["content"] = json!(help_message);
            
            // Update context
            let mut new_context = context.clone();
            new_context["data"]["message"] = json!(new_message);
            
            // Write result to memory
            let result = serde_json::to_string(&new_context).unwrap();
            return host::write_memory(result.as_ptr(), result.len() as i32);
        }
        
        // Parse repository path
        let repo_path = parts[1];
        let repo_parts: Vec<&str> = repo_path.split('/').collect();
        
        // Validate repository path
        if repo_parts.len() < 3 {
            let error_message = format!("Invalid repository path: {}. Format should be [owner]/[repo]/[path/to/file]", repo_path);
            
            // Create modified message
            let mut new_message = message.clone();
            new_message["content"] = json!(error_message);
            
            // Update context
            let mut new_context = context.clone();
            new_context["data"]["message"] = json!(new_message);
            
            // Write result to memory
            let result = serde_json::to_string(&new_context).unwrap();
            return host::write_memory(result.as_ptr(), result.len() as i32);
        }
        
        // Extract owner and repo
        let owner = repo_parts[0];
        let repo = repo_parts[1];
        
        // Extract file path
        let file_path = repo_parts[2..].join("/");
        
        // Get branch (optional)
        let branch = if parts.len() > 2 {
            parts[2].to_string()
        } else {
            settings.default_branch.clone()
        };
        
        // Get line range (optional)
        let line_range = if parts.len() > 3 {
            parse_line_range(parts[3], settings.max_line_count)
        } else {
            (0, settings.max_line_count)
        };
        
        // Fetch code from GitHub
        let code_snippet = fetch_github_code(owner, repo, &file_path, &branch, line_range, &settings);
        
        // Create modified message
        let mut new_message = message.clone();
        new_message["content"] = json!(code_snippet);
        
        // Update context
        let mut new_context = context.clone();
        new_context["data"]["message"] = json!(new_message);
        
        // Write result to memory
        let result = serde_json::to_string(&new_context).unwrap();
        host::write_memory(result.as_ptr(), result.len() as i32)
    } else {
        // Not a GitHub command, return unchanged
        0
    }
}

// Post-process message hook
fn post_process_message(context_ptr: i32) -> i32 {
    // This hook doesn't modify post-processed messages
    0
}

// Parse line range
fn parse_line_range(range_str: &str, max_lines: u32) -> (u32, u32) {
    let parts: Vec<&str> = range_str.split('-').collect();
    
    if parts.len() == 2 {
        let start = parts[0].parse::<u32>().unwrap_or(1);
        let end = parts[1].parse::<u32>().unwrap_or(start + max_lines);
        
        // Ensure end is not too far from start
        let end = std::cmp::min(end, start + max_lines);
        
        (start, end)
    } else if parts.len() == 1 {
        let line = parts[0].parse::<u32>().unwrap_or(1);
        (line, line)
    } else {
        (1, max_lines)
    }
}

// Fetch code from GitHub
fn fetch_github_code(owner: &str, repo: &str, path: &str, branch: &str, line_range: (u32, u32), settings: &Settings) -> String {
    // Request permission for GitHub API
    if host::request_permission("network:api.github.com") == 0 {
        return format!("Error: Permission denied for accessing api.github.com");
    }
    
    // Create GitHub API URL
    let url = format!("https://api.github.com/repos/{}/{}/contents/{}", owner, repo, path);
    
    // Create headers
    let mut headers = HashMap::new();
    headers.insert("Accept".to_string(), "application/vnd.github.v3.raw".to_string());
    
    // Add authentication if token is provided
    if !settings.github_token.is_empty() {
        headers.insert("Authorization".to_string(), format!("token {}", settings.github_token));
    }
    
    // Convert headers to JSON
    let headers_json = serde_json::to_string(&headers).unwrap();
    
    // Add branch as query parameter
    let url_with_branch = format!("{}?ref={}", url, branch);
    
    // Make request to GitHub API
    // In a real plugin, this would be a real HTTP request
    // Here we mock the GitHub API
    let response = mock_github_api(&url_with_branch, &headers_json);
    
    // Check if request was successful
    if response.contains("error") {
        // Parse error
        let error: GitHubError = serde_json::from_str(&response).unwrap();
        
        return format!("GitHub API Error: {}", error.message);
    }
    
    // Extract file content
    let content = response;
    
    // Extract lines
    let lines: Vec<&str> = content.lines().collect();
    
    // Apply line range
    let start_line = line_range.0 as usize;
    let end_line = std::cmp::min(line_range.1 as usize, lines.len());
    
    let selected_lines = if start_line <= end_line && start_line <= lines.len() {
        let start_idx = start_line.saturating_sub(1); // Convert to 0-based index
        let end_idx = std::cmp::min(end_line, lines.len());
        
        lines[start_idx..end_idx].join("\n")
    } else {
        content.clone()
    };
    
    // Format code block based on style
    format_code_block(&selected_lines, path, owner, repo, branch, line_range, settings)
}

// Format code block
fn format_code_block(code: &str, path: &str, owner: &str, repo: &str, branch: &str, line_range: (u32, u32), settings: &Settings) -> String {
    // Detect language from file extension
    let language = detect_language(path);
    
    match settings.code_block_style.as_str() {
        "github" => {
            // GitHub style
            let mut result = String::new();
            
            if settings.include_metadata {
                result.push_str(&format!("```{} File: {}/{}/{}/blob/{}/{} (lines {}-{})\n", 
                                      language, owner, repo, path, branch, line_range.0, line_range.1));
            } else {
                result.push_str(&format!("```{}\n", language));
            }
            
            result.push_str(code);
            result.push_str("\n```");
            
            result
        }
        "detailed" => {
            // Detailed style
            let mut result = String::new();
            
            result.push_str(&format!("# Code from GitHub\n\n"));
            
            if settings.include_metadata {
                result.push_str(&format!("**Repository:** {}/{}\n", owner, repo));
                result.push_str(&format!("**File:** {}\n", path));
                result.push_str(&format!("**Branch:** {}\n", branch));
                result.push_str(&format!("**Lines:** {}-{}\n\n", line_range.0, line_range.1));
            }
            
            result.push_str(&format!("```{}\n", language));
            result.push_str(code);
            result.push_str("\n```");
            
            result
        }
        _ => {
            // Simple style
            let mut result = String::new();
            
            result.push_str(&format!("```{}\n", language));
            result.push_str(code);
            result.push_str("\n```");
            
            result
        }
    }
}

// Detect language from file extension
fn detect_language(path: &str) -> String {
    let parts: Vec<&str> = path.split('.').collect();
    
    if parts.len() < 2 {
        return "".to_string();
    }
    
    let extension = parts.last().unwrap().to_lowercase();
    
    match extension.as_str() {
        "rs" => "rust",
        "js" => "javascript",
        "ts" => "typescript",
        "py" => "python",
        "java" => "java",
        "c" => "c",
        "cpp" => "cpp",
        "h" => "c",
        "hpp" => "cpp",
        "cs" => "csharp",
        "go" => "go",
        "rb" => "ruby",
        "php" => "php",
        "html" => "html",
        "css" => "css",
        "md" => "markdown",
        "json" => "json",
        "yaml" | "yml" => "yaml",
        "toml" => "toml",
        "sql" => "sql",
        "sh" => "bash",
        "bat" => "batch",
        "ps1" => "powershell",
        _ => "",
    }.to_string()
}

// Mock GitHub API (for demonstration)
fn mock_github_api(url: &str, headers: &str) -> String {
    // In a real plugin, this would make an actual HTTP request
    // For now, we return a mock response
    
    // Extract owner, repo, and path from URL
    let url_parts: Vec<&str> = url.split('/').collect();
    
    if url_parts.len() < 7 {
        return json!({
            "error": true,
            "message": "Invalid URL format",
            "documentation_url": "https://docs.github.com/en/rest"
        }).to_string();
    }
    
    // Extract path
    let path = url_parts[7..].join("/");
    
    // Remove query parameters
    let path = path.split('?').next().unwrap();
    
    // Generate mock content based on path
    let extension = path.split('.').last().unwrap_or("");
    
    match extension {
        "rs" => "fn main() {\n    println!(\"Hello, world!\");\n}".to_string(),
        "js" => "function hello() {\n    console.log(\"Hello, world!\");\n}".to_string(),
        "py" => "def main():\n    print(\"Hello, world!\")\n\nif __name__ == \"__main__\":\n    main()".to_string(),
        _ => "// Sample content for demonstration\n// This would be the actual file content from GitHub".to_string(),
    }
}

// Host function imports
#[wasm_bindgen]
extern "C" {
    pub mod host {
        pub fn register_plugin() -> i32;
        pub fn register_hook(hook_name: &str, callback_ptr: fn(i32) -> i32) -> i32;
        pub fn log_message(level: &str, message: &str) -> i32;
        pub fn read_memory(ptr: i32) -> String;
        pub fn write_memory(ptr: i32, len: i32) -> i32;
        pub fn get_settings() -> String;
        pub fn request_permission(permission: &str) -> i32;
        pub fn http_request(url: &str, method: &str, headers: &str, body: &str) -> i32;
    }
}
</file>

<file path="plugins/examples/meeting-summarizer/Cargo.toml">
[package]
name = "meeting-summarizer"
version = "0.1.0"
edition = "2021"
description = "Generate structured summaries from meeting transcripts"
authors = ["Claude MCP Team"]
license = "MIT"

[lib]
crate-type = ["cdylib"]

[dependencies]
wasm-bindgen = "0.2"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
</file>

<file path="plugins/examples/meeting-summarizer/manifest.json">
{
  "name": "meeting-summarizer",
  "displayName": "Meeting Summarizer",
  "version": "1.0.0",
  "description": "Generate structured summaries from meeting transcripts",
  "author": "Claude MCP Team",
  "license": "MIT",
  "main": "meeting-summarizer.wasm",
  "permissions": [
    "conversations:read",
    "conversations:write",
    "models:use"
  ],
  "hooks": [
    "message:pre-process",
    "conversation:create"
  ],
  "config": {
    "settings": [
      {
        "name": "summaryFormat",
        "type": "string",
        "label": "Summary Format",
        "description": "Format of the generated meeting summary",
        "default": "detailed",
        "enum_values": [
          {
            "value": "detailed",
            "label": "Detailed"
          },
          {
            "value": "concise",
            "label": "Concise"
          },
          {
            "value": "bullet",
            "label": "Bullet Points"
          }
        ]
      },
      {
        "name": "includeActionItems",
        "type": "boolean",
        "label": "Include Action Items",
        "description": "Extract and include action items in summary",
        "default": true
      },
      {
        "name": "includeDecisions",
        "type": "boolean",
        "label": "Include Decisions",
        "description": "Extract and include decisions in summary",
        "default": true
      },
      {
        "name": "includeParticipants",
        "type": "boolean",
        "label": "Include Participants",
        "description": "Extract and include participants in summary",
        "default": true
      },
      {
        "name": "autoSummarize",
        "type": "boolean",
        "label": "Auto Summarize",
        "description": "Automatically detect and summarize meeting transcripts",
        "default": false
      }
    ]
  }
}
</file>

<file path="plugins/examples/meeting-summarizer/src/main.rs">
use wasm_bindgen::prelude::*;
use serde::{Serialize, Deserialize};
use serde_json::{json, Value};

// Plugin settings
#[derive(Serialize, Deserialize)]
struct Settings {
    summary_format: String,
    include_action_items: bool,
    include_decisions: bool,
    include_participants: bool,
    auto_summarize: bool,
}

// Meeting summary
#[derive(Serialize, Deserialize)]
struct MeetingSummary {
    title: String,
    summary: String,
    action_items: Vec<String>,
    decisions: Vec<String>,
    participants: Vec<String>,
}

// Plugin entry point
#[wasm_bindgen]
pub fn init() {
    // Register the plugin
    host::register_plugin();
    
    // Register hooks
    host::register_hook("message:pre-process", pre_process_message);
    host::register_hook("conversation:create", conversation_created);
    
    host::log_message("info", "Meeting Summarizer plugin initialized");
}

// Pre-process message hook
fn pre_process_message(context_ptr: i32) -> i32 {
    // Parse context
    let context_str = host::read_memory(context_ptr);
    let context: Value = serde_json::from_str(&context_str).unwrap();
    
    // Get message content
    let message = context["data"]["message"].as_object().unwrap();
    let content = message["content"].as_str().unwrap();
    
    // Check for summarize command
    if content.starts_with("/summarize") {
        // Get settings
        let settings_str = host::get_settings();
        let settings: Settings = serde_json::from_str(&settings_str).unwrap_or_else(|_| Settings {
            summary_format: "detailed".to_string(),
            include_action_items: true,
            include_decisions: true,
            include_participants: true,
            auto_summarize: false,
        });
        
        // Parse command
        let parts: Vec<&str> = content.split_whitespace().collect();
        
        // Get previous messages to summarize
        let conversation_id = context["data"]["conversation_id"].as_str().unwrap();
        let messages = get_conversation_messages(conversation_id);
        
        if messages.is_empty() {
            // No messages to summarize
            let help_message = "No messages found to summarize. Please provide a meeting transcript.";
            
            // Create modified message
            let mut new_message = message.clone();
            new_message["content"] = json!(help_message);
            
            // Update context
            let mut new_context = context.clone();
            new_context["data"]["message"] = json!(new_message);
            
            // Write result to memory
            let result = serde_json::to_string(&new_context).unwrap();
            return host::write_memory(result.as_ptr(), result.len() as i32);
        }
        
        // Get format parameter
        let format = if parts.len() > 1 {
            parts[1].to_string()
        } else {
            settings.summary_format.clone()
        };
        
        // Summarize the transcript
        let summary = summarize_transcript(&messages, &format, &settings);
        
        // Create modified message
        let mut new_message = message.clone();
        new_message["content"] = json!(summary);
        
        // Update context
        let mut new_context = context.clone();
        new_context["data"]["message"] = json!(new_message);
        
        // Write result to memory
        let result = serde_json::to_string(&new_context).unwrap();
        host::write_memory(result.as_ptr(), result.len() as i32)
    } else if content.starts_with("/meeting") {
        // Create new meeting transcript
        let meeting_prompt = "Let's start a new meeting. I'll help you take notes and generate a summary afterward.\n\n\
                             When you're done with the meeting, use the `/summarize` command to generate a summary.";
        
        // Create modified message
        let mut new_message = message.clone();
        new_message["content"] = json!(meeting_prompt);
        
        // Update context
        let mut new_context = context.clone();
        new_context["data"]["message"] = json!(new_message);
        
        // Write result to memory
        let result = serde_json::to_string(&new_context).unwrap();
        host::write_memory(result.as_ptr(), result.len() as i32)
    } else {
        // Check for auto-summarize
        let settings_str = host::get_settings();
        let settings: Settings = serde_json::from_str(&settings_str).unwrap_or_else(|_| Settings {
            summary_format: "detailed".to_string(),
            include_action_items: true,
            include_decisions: true,
            include_participants: true,
            auto_summarize: false,
        });
        
        if settings.auto_summarize && is_meeting_transcript(content) {
            // Auto-summarize meeting transcript
            let summary = summarize_transcript(&vec![content.to_string()], &settings.summary_format, &settings);
            
            // Create modified message
            let mut new_message = message.clone();
            new_message["content"] = json!(format!("{}\n\n{}", content, summary));
            
            // Update context
            let mut new_context = context.clone();
            new_context["data"]["message"] = json!(new_message);
            
            // Write result to memory
            let result = serde_json::to_string(&new_context).unwrap();
            return host::write_memory(result.as_ptr(), result.len() as i32);
        }
        
        // Not a known command, return unchanged
        0
    }
}

// Conversation created hook
fn conversation_created(context_ptr: i32) -> i32 {
    // Parse context
    let context_str = host::read_memory(context_ptr);
    let context: Value = serde_json::from_str(&context_str).unwrap();
    
    // Get conversation ID
    let conversation_id = context["data"]["conversation"]["id"].as_str().unwrap();
    
    host::log_message("info", &format!("New conversation created: {}", conversation_id));
    
    // No changes needed
    0
}

// Get messages from a conversation
fn get_conversation_messages(conversation_id: &str) -> Vec<String> {
    // In a real plugin, this would call the host function to get conversation messages
    // For now, we return a mock conversation
    
    vec![
        "John: Hi everyone, thanks for joining this meeting. Today we're going to discuss the Q2 roadmap.".to_string(),
        "Sarah: Sounds good. I have some ideas I'd like to share.".to_string(),
        "Michael: I think we should prioritize the new features over the bug fixes.".to_string(),
        "Sarah: I disagree. We have too many critical bugs that need to be fixed.".to_string(),
        "John: I see both points. Let's decide on a balance. How about we allocate 40% to new features and 60% to bug fixes?".to_string(),
        "Sarah: That sounds reasonable to me.".to_string(),
        "Michael: I can live with that. Let's go with 40/60.".to_string(),
        "John: Great! So the decision is 40% new features, 60% bug fixes for Q2.".to_string(),
        "John: Now, about the team expansion. I think we need to hire two more developers.".to_string(),
        "Sarah: Agreed. I'll start the recruitment process next week.".to_string(),
        "Michael: I'll prepare the onboarding materials.".to_string(),
        "John: Perfect. Sarah, please have the job postings ready by Monday. Michael, please have the onboarding materials ready by the end of the month.".to_string(),
        "Sarah: Will do.".to_string(),
        "Michael: Got it.".to_string(),
        "John: Any other topics we need to discuss?".to_string(),
        "Sarah: I think that covers everything for now.".to_string(),
        "John: Great! Thanks everyone for your time. Meeting adjourned.".to_string(),
    ]
}

// Check if a message appears to be a meeting transcript
fn is_meeting_transcript(content: &str) -> bool {
    // Simple heuristic: check if the message contains multiple speakers
    // and looks like a conversation
    
    // Count number of lines that appear to be speaker turns
    let lines: Vec<&str> = content.lines().collect();
    let mut speaker_turns = 0;
    
    for line in lines.iter() {
        if line.contains(":") {
            let parts: Vec<&str> = line.splitn(2, ':').collect();
            if parts.len() == 2 && !parts[0].is_empty() && !parts[0].contains(" ") {
                speaker_turns += 1;
            }
        }
    }
    
    // If we have at least 5 speaker turns, it's likely a meeting transcript
    speaker_turns >= 5
}

// Extract participants from transcript
fn extract_participants(transcript: &[String]) -> Vec<String> {
    // Extract names from the transcript
    let mut participants = Vec::new();
    
    for line in transcript {
        if line.contains(":") {
            let parts: Vec<&str> = line.splitn(2, ':').collect();
            if parts.len() == 2 && !parts[0].is_empty() {
                // Extract name
                let name = parts[0].trim();
                
                // Add if not already in the list
                if !participants.contains(&name.to_string()) {
                    participants.push(name.to_string());
                }
            }
        }
    }
    
    participants
}

// Extract action items from transcript
fn extract_action_items(transcript: &[String]) -> Vec<String> {
    // Look for phrases that indicate action items
    let mut action_items = Vec::new();
    
    let action_indicators = [
        "will",
        "need to",
        "should",
        "have to",
        "going to",
        "action item",
        "please",
        "task",
        "by tomorrow",
        "next week",
        "by monday",
    ];
    
    for line in transcript {
        let lowercase = line.to_lowercase();
        
        // Check if the line contains any action indicators
        if action_indicators.iter().any(|&indicator| lowercase.contains(indicator)) {
            // Extract the action item
            let parts: Vec<&str> = line.splitn(2, ':').collect();
            if parts.len() == 2 {
                let speaker = parts[0].trim();
                let content = parts[1].trim();
                
                action_items.push(format!("{}: {}", speaker, content));
            } else {
                action_items.push(line.to_string());
            }
        }
    }
    
    // Filter out duplicates
    action_items.dedup();
    
    action_items
}

// Extract decisions from transcript
fn extract_decisions(transcript: &[String]) -> Vec<String> {
    // Look for phrases that indicate decisions
    let mut decisions = Vec::new();
    
    let decision_indicators = [
        "decided",
        "agreed",
        "decision",
        "consensus",
        "agreement",
        "settled on",
        "concluded",
        "resolved",
        "finalized",
    ];
    
    for line in transcript {
        let lowercase = line.to_lowercase();
        
        // Check if the line contains any decision indicators
        if decision_indicators.iter().any(|&indicator| lowercase.contains(indicator)) {
            // Extract the decision
            let parts: Vec<&str> = line.splitn(2, ':').collect();
            if parts.len() == 2 {
                let speaker = parts[0].trim();
                let content = parts[1].trim();
                
                decisions.push(format!("{}: {}", speaker, content));
            } else {
                decisions.push(line.to_string());
            }
        }
    }
    
    // Filter out duplicates
    decisions.dedup();
    
    decisions
}

// Summarize a meeting transcript
fn summarize_transcript(transcript: &[String], format: &str, settings: &Settings) -> String {
    // Extract information from transcript
    let participants = if settings.include_participants {
        extract_participants(transcript)
    } else {
        Vec::new()
    };
    
    let action_items = if settings.include_action_items {
        extract_action_items(transcript)
    } else {
        Vec::new()
    };
    
    let decisions = if settings.include_decisions {
        extract_decisions(transcript)
    } else {
        Vec::new()
    };
    
    // Request model use permission
    if host::request_permission("models:use") == 0 {
        return format!("Error: Permission denied for using models");
    }
    
    // Create a summary based on the transcript
    // In a real plugin, this would use Claude to generate a summary
    // Here we use a mock function
    let summary = generate_summary(transcript, format);
    
    // Format the summary based on the requested format
    format_summary(
        &summary,
        &participants,
        &action_items,
        &decisions,
        format,
        settings,
    )
}

// Generate a summary from a transcript
fn generate_summary(transcript: &[String], format: &str) -> String {
    // In a real plugin, this would use Claude to generate a summary
    // For now, we generate a mock summary based on the transcript
    
    match format {
        "concise" => {
            "The team discussed the Q2 roadmap and decided on allocating 40% to new features and 60% to bug fixes. \
             They also agreed to hire two more developers, with Sarah handling recruitment and Michael preparing onboarding materials."
                .to_string()
        }
        "bullet" => {
            "• Team discussed Q2 roadmap\n\
             • Decided on 40% new features, 60% bug fixes\n\
             • Agreed to hire two more developers\n\
             • Sarah will handle recruitment\n\
             • Michael will prepare onboarding materials"
                .to_string()
        }
        _ => {
            // Detailed format
            "During this meeting, the team discussed the Q2 roadmap priorities. There was a debate about \
             whether to focus on new features or bug fixes. Michael preferred prioritizing new features while \
             Sarah advocated for addressing critical bugs. John suggested a compromise of 40% new features and \
             60% bug fixes, which everyone agreed to.\n\n\
             The team also discussed expanding the team by hiring two more developers. Sarah will start the \
             recruitment process next week and have job postings ready by Monday. Michael will prepare the \
             onboarding materials by the end of the month."
                .to_string()
        }
    }
}

// Format the summary
fn format_summary(
    summary: &str,
    participants: &[String],
    action_items: &[String],
    decisions: &[String],
    format: &str,
    settings: &Settings,
) -> String {
    let mut result = String::new();
    
    // Add title
    result.push_str("# Meeting Summary\n\n");
    
    // Add summary
    match format {
        "concise" => {
            result.push_str("## Summary\n\n");
            result.push_str(summary);
            result.push_str("\n\n");
        }
        "bullet" => {
            result.push_str("## Summary\n\n");
            result.push_str(summary);
            result.push_str("\n\n");
        }
        _ => {
            // Detailed format
            result.push_str("## Detailed Summary\n\n");
            result.push_str(summary);
            result.push_str("\n\n");
        }
    }
    
    // Add participants if requested
    if settings.include_participants && !participants.is_empty() {
        result.push_str("## Participants\n\n");
        for participant in participants {
            result.push_str(&format!("- {}\n", participant));
        }
        result.push_str("\n");
    }
    
    // Add decisions if requested
    if settings.include_decisions && !decisions.is_empty() {
        result.push_str("## Key Decisions\n\n");
        for decision in decisions {
            result.push_str(&format!("- {}\n", decision));
        }
        result.push_str("\n");
    }
    
    // Add action items if requested
    if settings.include_action_items && !action_items.is_empty() {
        result.push_str("## Action Items\n\n");
        for action in action_items {
            result.push_str(&format!("- {}\n", action));
        }
        result.push_str("\n");
    }
    
    result
}

// Host function imports
#[wasm_bindgen]
extern "C" {
    pub mod host {
        pub fn register_plugin() -> i32;
        pub fn register_hook(hook_name: &str, callback_ptr: fn(i32) -> i32) -> i32;
        pub fn log_message(level: &str, message: &str) -> i32;
        pub fn read_memory(ptr: i32) -> String;
        pub fn write_memory(ptr: i32, len: i32) -> i32;
        pub fn get_settings() -> String;
        pub fn request_permission(permission: &str) -> i32;
        pub fn http_request(url: &str, method: &str, headers: &str, body: &str) -> i32;
    }
}
</file>

<file path="plugins/examples/translation-plugin/Cargo.toml">
[package]
name = "translation-plugin"
version = "0.1.0"
edition = "2021"
description = "Translate messages between different languages"
authors = ["Claude MCP Team"]
license = "MIT"

[lib]
crate-type = ["cdylib"]

[dependencies]
wasm-bindgen = "0.2"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
</file>

<file path="plugins/examples/translation-plugin/manifest.json">
{
  "name": "translation-plugin",
  "displayName": "Translation Plugin",
  "version": "1.0.0",
  "description": "Translate messages between different languages",
  "author": "Claude MCP Team",
  "license": "MIT",
  "main": "translation-plugin.wasm",
  "permissions": [
    "conversations:read",
    "conversations:write",
    "network:translate-api.example.com"
  ],
  "hooks": [
    "message:pre-process",
    "message:post-process",
    "conversation:create"
  ],
  "config": {
    "settings": [
      {
        "name": "apiKey",
        "type": "string",
        "label": "API Key",
        "description": "Your translation API key",
        "default": "",
        "secret": true
      },
      {
        "name": "sourceLanguage",
        "type": "string",
        "label": "Source Language",
        "description": "Default source language",
        "default": "auto"
      },
      {
        "name": "targetLanguage",
        "type": "string",
        "label": "Target Language",
        "description": "Default target language",
        "default": "en",
        "enum_values": [
          {
            "value": "en",
            "label": "English"
          },
          {
            "value": "fr",
            "label": "French"
          },
          {
            "value": "es",
            "label": "Spanish"
          },
          {
            "value": "de",
            "label": "German"
          },
          {
            "value": "it",
            "label": "Italian"
          },
          {
            "value": "ja",
            "label": "Japanese"
          },
          {
            "value": "zh",
            "label": "Chinese"
          },
          {
            "value": "ru",
            "label": "Russian"
          }
        ]
      },
      {
        "name": "autoTranslate",
        "type": "boolean",
        "label": "Auto Translate",
        "description": "Automatically translate incoming messages",
        "default": false
      },
      {
        "name": "showOriginal",
        "type": "boolean",
        "label": "Show Original",
        "description": "Show original message alongside translation",
        "default": true
      }
    ]
  }
}
</file>

<file path="plugins/examples/translation-plugin/src/main.rs">
use wasm_bindgen::prelude::*;
use serde::{Serialize, Deserialize};
use serde_json::{json, Value};

// Plugin settings
#[derive(Serialize, Deserialize)]
struct Settings {
    api_key: String,
    source_language: String,
    target_language: String,
    auto_translate: bool,
    show_original: bool,
}

// Translation request
#[derive(Serialize, Deserialize)]
struct TranslationRequest {
    text: String,
    source_language: String,
    target_language: String,
    api_key: String,
}

// Translation response
#[derive(Serialize, Deserialize)]
struct TranslationResponse {
    translated_text: String,
    detected_language: Option<String>,
    confidence: Option<f32>,
}

// Message hook context
#[derive(Serialize, Deserialize)]
struct MessageContext {
    conversation_id: String,
    message_id: String,
    role: String,
    content: String,
}

// Plugin entry point
#[wasm_bindgen]
pub fn init() {
    // Register the plugin
    host::register_plugin();
    
    // Register hooks
    host::register_hook("message:pre-process", pre_process_message);
    host::register_hook("message:post-process", post_process_message);
    host::register_hook("conversation:create", conversation_created);
    
    host::log_message("info", "Translation plugin initialized");
}

// Pre-process message hook
fn pre_process_message(context_ptr: i32) -> i32 {
    // Parse context
    let context_str = host::read_memory(context_ptr);
    let context: Value = serde_json::from_str(&context_str).unwrap();
    
    // Get settings
    let settings_str = host::get_settings();
    let settings: Settings = serde_json::from_str(&settings_str).unwrap_or_else(|_| Settings {
        api_key: String::new(),
        source_language: "auto".to_string(),
        target_language: "en".to_string(),
        auto_translate: false,
        show_original: true,
    });
    
    // Check if auto-translate is enabled
    if !settings.auto_translate {
        // Auto-translate disabled, return unchanged
        return 0;
    }
    
    // Get message content
    let message = context["data"]["message"].as_object().unwrap();
    let content = message["content"].as_str().unwrap();
    
    // Check for translation command
    if content.starts_with("/translate") {
        // Parse command
        let parts: Vec<&str> = content.split_whitespace().collect();
        
        // Get target language
        let target_language = if parts.len() > 1 {
            parts[1].to_string()
        } else {
            settings.target_language.clone()
        };
        
        // Get text to translate
        let text = if parts.len() > 2 {
            parts[2..].join(" ")
        } else {
            // No text provided, return unchanged
            return 0;
        };
        
        // Translate the text
        let translated = translate(&text, &settings.source_language, &target_language, &settings.api_key);
        
        // Create modified message
        let mut new_message = message.clone();
        if settings.show_original {
            new_message["content"] = json!(format!("Translation ({} -> {}):\n\nOriginal: {}\nTranslated: {}", 
                                                settings.source_language, target_language, text, translated));
        } else {
            new_message["content"] = json!(translated);
        }
        
        // Update context
        let mut new_context = context.clone();
        new_context["data"]["message"] = json!(new_message);
        
        // Write result to memory
        let result = serde_json::to_string(&new_context).unwrap();
        host::write_memory(result.as_ptr(), result.len() as i32)
    } else {
        // Not a translation command, return unchanged
        0
    }
}

// Post-process message hook
fn post_process_message(context_ptr: i32) -> i32 {
    // Parse context
    let context_str = host::read_memory(context_ptr);
    let context: Value = serde_json::from_str(&context_str).unwrap();
    
    // Get settings
    let settings_str = host::get_settings();
    let settings: Settings = serde_json::from_str(&settings_str).unwrap_or_else(|_| Settings {
        api_key: String::new(),
        source_language: "auto".to_string(),
        target_language: "en".to_string(),
        auto_translate: false,
        show_original: true,
    });
    
    // Check if auto-translate is enabled
    if !settings.auto_translate {
        // Auto-translate disabled, return unchanged
        return 0;
    }
    
    // Get message content
    let message = context["data"]["message"].as_object().unwrap();
    let role = message["role"].as_str().unwrap();
    
    // Only translate assistant messages
    if role != "assistant" {
        return 0;
    }
    
    let content = message["content"].as_str().unwrap();
    
    // Translate the text
    let translated = translate(content, &settings.source_language, &settings.target_language, &settings.api_key);
    
    // Create modified message
    let mut new_message = message.clone();
    if settings.show_original {
        new_message["content"] = json!(format!("Original: {}\n\nTranslation ({} -> {}):\n{}", 
                                            content, settings.source_language, settings.target_language, translated));
    } else {
        new_message["content"] = json!(translated);
    }
    
    // Update context
    let mut new_context = context.clone();
    new_context["data"]["message"] = json!(new_message);
    
    // Write result to memory
    let result = serde_json::to_string(&new_context).unwrap();
    host::write_memory(result.as_ptr(), result.len() as i32)
}

// Conversation created hook
fn conversation_created(context_ptr: i32) -> i32 {
    // Parse context
    let context_str = host::read_memory(context_ptr);
    let context: Value = serde_json::from_str(&context_str).unwrap();
    
    // Get conversation ID
    let conversation_id = context["data"]["conversation"]["id"].as_str().unwrap();
    
    host::log_message("info", &format!("New conversation created: {}", conversation_id));
    
    // No changes needed
    0
}

// Translate text
fn translate(text: &str, source_language: &str, target_language: &str, api_key: &str) -> String {
    // Check if API key is provided
    if api_key.is_empty() {
        host::log_message("error", "Translation API key not provided");
        return format!("[Translation failed: API key not provided] {}", text);
    }
    
    // Create translation request
    let request = TranslationRequest {
        text: text.to_string(),
        source_language: source_language.to_string(),
        target_language: target_language.to_string(),
        api_key: api_key.to_string(),
    };
    
    // Convert to JSON
    let request_json = serde_json::to_string(&request).unwrap();
    
    // Send request to translation API
    // In a real plugin, this would make an actual HTTP request
    // Here we mock the translation service
    let response = mock_translation_service(&request);
    
    // Return translated text
    response.translated_text
}

// Mock translation service (for demonstration)
fn mock_translation_service(request: &TranslationRequest) -> TranslationResponse {
    // In a real plugin, this would call the actual translation API
    // For now, we just append a prefix indicating the translation
    TranslationResponse {
        translated_text: format!("[{} translation] {}", request.target_language, request.text),
        detected_language: Some("en".to_string()),
        confidence: Some(0.95),
    }
}

// Host function imports
#[wasm_bindgen]
extern "C" {
    pub mod host {
        pub fn register_plugin() -> i32;
        pub fn register_hook(hook_name: &str, callback_ptr: fn(i32) -> i32) -> i32;
        pub fn log_message(level: &str, message: &str) -> i32;
        pub fn read_memory(ptr: i32) -> String;
        pub fn write_memory(ptr: i32, len: i32) -> i32;
        pub fn get_settings() -> String;
        pub fn request_permission(permission: &str) -> i32;
        pub fn http_request(url: &str, method: &str, headers: &str, body: &str) -> i32;
    }
}
</file>

<file path="plugins/README.md">
# MCP Client Plugin System

This directory contains example plugins for the MCP Client. The plugin system enables extending the functionality of the MCP Client through WebAssembly (WASM) modules that run in a secure sandbox.

## Overview

The MCP Client plugin system allows developers to create custom extensions that can:

- Process messages before they are sent to Claude
- Process responses from Claude
- Add new commands and functionality
- Interact with external services
- Customize the UI

Plugins are implemented in Rust or other languages that compile to WebAssembly, and run in a secure sandbox with limited access to system resources.

## Plugin Architecture

Each plugin consists of:

1. A **manifest file** (manifest.json) that describes the plugin, its capabilities, and required permissions
2. A **WebAssembly module** (*.wasm) that contains the plugin code
3. Optional **assets** such as images, stylesheets, or other resources

Plugins interact with the host application through a defined API that provides:

- Hook registration for responding to events
- Permission-based access to resources
- API access for interacting with Claude and other services
- Configuration management
- Logging facilities

## Example Plugins

This directory contains the following example plugins:

### Translation Plugin

A plugin that translates messages between different languages:

- Features automatic translation of incoming/outgoing messages
- Supports multiple languages 
- Configurable translation settings

### GitHub Code Snippets

A plugin that fetches code snippets from GitHub repositories:

- Use `/github` command to fetch code
- Specify repositories, file paths, and line ranges
- Syntax highlighting and formatting options

### Meeting Summarizer

A plugin that generates structured summaries from meeting transcripts:

- Extract action items, decisions, and participants
- Multiple summary formats (detailed, concise, bullet points)
- Automatic detection of meeting content

## Plugin Development

To create your own plugin:

1. Create a new directory for your plugin
2. Create a manifest.json file with plugin metadata
3. Implement your plugin code in Rust or another language that compiles to WebAssembly
4. Build your plugin to produce a .wasm file
5. Test your plugin in the MCP Client

## Plugin Manifest

Each plugin requires a manifest.json file with the following structure:

```json
{
  "name": "plugin-name",
  "displayName": "Plugin Display Name",
  "version": "1.0.0",
  "description": "Description of the plugin",
  "author": "Your Name",
  "license": "MIT",
  "main": "plugin.wasm",
  "permissions": [
    "permissions:needed"
  ],
  "hooks": [
    "hooks:to:register"
  ],
  "config": {
    "settings": [
      {
        "name": "settingName",
        "type": "string",
        "label": "Setting Label",
        "description": "Setting Description",
        "default": "default value"
      }
    ]
  }
}
```

## Security Considerations

Plugins run in a secure WebAssembly sandbox with the following restrictions:

- Limited memory access (only to their own memory)
- No direct file system access
- No direct network access
- No access to system resources
- Permission-based access to APIs

All plugins must request appropriate permissions, and users must approve these permissions before they can be used.

## Available Permissions

Plugins can request the following permissions:

- `conversations:read` - Read conversations and messages
- `conversations:write` - Create/modify conversations and messages
- `models:read` - Read available models
- `models:use` - Use specific models
- `system:settings` - Access system settings
- `user:preferences` - Access user preferences
- `network:{domain}` - Connect to a specific domain
- `ui:display` - Show UI elements
- `ui:interact` - Interact with user through UI elements

## Available Hooks

Plugins can register handlers for the following hooks:

- `message:pre-process` - Process a message before sending to Claude
- `message:post-process` - Process a message after receiving from Claude
- `conversation:create` - Called when a new conversation is created
- `conversation:open` - Called when a conversation is opened
- `conversation:close` - Called when a conversation is closed
- `application:start` - Called when the application starts
- `application:shutdown` - Called when the application shuts down
- `ui:render` - Custom UI rendering

## Building Plugins

To build a plugin:

1. Set up a Rust project with appropriate dependencies:
   ```toml
   [package]
   name = "my-plugin"
   version = "0.1.0"
   edition = "2021"
   
   [lib]
   crate-type = ["cdylib"]
   
   [dependencies]
   wasm-bindgen = "0.2"
   serde = { version = "1.0", features = ["derive"] }
   serde_json = "1.0"
   ```

2. Implement your plugin code:
   ```rust
   use wasm_bindgen::prelude::*;
   
   #[wasm_bindgen]
   pub fn init() {
       // Register the plugin
       host::register_plugin();
       
       // Register hooks
       host::register_hook("message:pre-process", pre_process_message);
       
       host::log_message("info", "My plugin initialized");
   }
   
   fn pre_process_message(context_ptr: i32) -> i32 {
       // Process message
       0
   }
   
   #[wasm_bindgen]
   extern "C" {
       pub mod host {
           pub fn register_plugin() -> i32;
           pub fn register_hook(hook_name: &str, callback_ptr: fn(i32) -> i32) -> i32;
           pub fn log_message(level: &str, message: &str) -> i32;
           // Other host functions
       }
   }
   ```

3. Build your plugin:
   ```sh
   cargo build --target wasm32-unknown-unknown --release
   ```

4. Package your plugin:
   ```sh
   mkdir -p my-plugin
   cp manifest.json my-plugin/
   cp target/wasm32-unknown-unknown/release/my_plugin.wasm my-plugin/plugin.wasm
   zip -r my-plugin.zip my-plugin/
   ```

## Installing Plugins

To install a plugin in the MCP Client:

1. Open the Plugin Manager in the MCP Client
2. Click "Install from File" or "Browse Plugin Repository"
3. Select your plugin package (.zip file)
4. Review the requested permissions
5. Confirm installation

## Plugin API Reference

### Host Functions

The following host functions are available to plugins:

- `register_plugin()` - Register the plugin with the host
- `register_hook(hook_name, callback_ptr)` - Register a hook handler
- `log_message(level, message)` - Log a message
- `read_memory(ptr)` - Read from memory
- `write_memory(ptr, len)` - Write to memory
- `get_settings()` - Get plugin settings
- `request_permission(permission)` - Request additional permissions
- `http_request(url, method, headers, body)` - Make an HTTP request

### Hook Handlers

Hook handlers receive a pointer to a context object and return a status code:

- Return 0 to indicate no changes
- Return a pointer to modified context to replace the original
</file>

<file path="scripts/build-linux.sh">
#!/bin/bash
# Build script for Linux distributions

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[0;33m'
NC='\033[0m' # No Color

echo -e "${BLUE}Starting MCP Client build for Linux distributions...${NC}"

# Make sure we're in the project root
cd "$(dirname "$0")/.."
PROJECT_ROOT=$(pwd)

# Ensure dependencies are installed
echo -e "${YELLOW}Checking and installing dependencies...${NC}"
if [ -f "package.json" ]; then
  npm install
else
  echo -e "${RED}Error: package.json not found. Are you in the right directory?${NC}"
  exit 1
fi

# Build the frontend
echo -e "${YELLOW}Building frontend...${NC}"
npm run build

# Check if the build was successful
if [ $? -ne 0 ]; then
  echo -e "${RED}Frontend build failed!${NC}"
  exit 1
fi
echo -e "${GREEN}Frontend build successful!${NC}"

# Build for various Linux targets
echo -e "${YELLOW}Building for Debian...${NC}"
cargo tauri build --target deb

echo -e "${YELLOW}Building for RPM-based distributions...${NC}"
cargo tauri build --target rpm

echo -e "${YELLOW}Building AppImage...${NC}"
cargo tauri build --target appimage

# Move all built packages to the dist directory
echo -e "${YELLOW}Moving packages to dist directory...${NC}"
mkdir -p "$PROJECT_ROOT/dist"

# Find and move the built packages
find "$PROJECT_ROOT/src-tauri/target/release/bundle" -name "*.deb" -o -name "*.rpm" -o -name "*.AppImage" | while read file; do
  cp "$file" "$PROJECT_ROOT/dist/"
  echo -e "${GREEN}Copied $(basename "$file") to dist directory${NC}"
done

# Generate checksums
echo -e "${YELLOW}Generating checksums...${NC}"
cd "$PROJECT_ROOT/dist"
sha256sum *.deb *.rpm *.AppImage > checksums.sha256
echo -e "${GREEN}Checksums generated in dist/checksums.sha256${NC}"

echo -e "${GREEN}Build completed successfully!${NC}"
echo -e "${BLUE}Packages available in the dist directory:${NC}"
ls -la "$PROJECT_ROOT/dist"
</file>

<file path="scripts/build-macos.sh">
#!/bin/bash
# Build script for macOS

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[0;33m'
NC='\033[0m' # No Color

echo -e "${BLUE}Starting MCP Client build for macOS...${NC}"

# Make sure we're in the project root
cd "$(dirname "$0")/.."
PROJECT_ROOT=$(pwd)

# Ensure dependencies are installed
echo -e "${YELLOW}Checking and installing dependencies...${NC}"
if [ -f "package.json" ]; then
  npm install
else
  echo -e "${RED}Error: package.json not found. Are you in the right directory?${NC}"
  exit 1
fi

# Build the frontend
echo -e "${YELLOW}Building frontend...${NC}"
npm run build

# Check if the build was successful
if [ $? -ne 0 ]; then
  echo -e "${RED}Frontend build failed!${NC}"
  exit 1
fi
echo -e "${GREEN}Frontend build successful!${NC}"

# Build for macOS
echo -e "${YELLOW}Building for macOS (.app and .dmg)...${NC}"
cargo tauri build

# Move all built packages to the dist directory
echo -e "${YELLOW}Moving packages to dist directory...${NC}"
mkdir -p "$PROJECT_ROOT/dist"

# Find and move the built packages
find "$PROJECT_ROOT/src-tauri/target/release/bundle/dmg" -name "*.dmg" | while read file; do
  cp "$file" "$PROJECT_ROOT/dist/"
  echo -e "${GREEN}Copied $(basename "$file") to dist directory${NC}"
done

# For notarization (would be done in production environment)
echo -e "${YELLOW}NOTE: For distribution on macOS, the app should be notarized.${NC}"
echo -e "${YELLOW}This requires an Apple Developer account and is not included in this script.${NC}"

# Generate checksums
echo -e "${YELLOW}Generating checksums...${NC}"
cd "$PROJECT_ROOT/dist"
shasum -a 256 *.dmg > checksums.sha256
echo -e "${GREEN}Checksums generated in dist/checksums.sha256${NC}"

echo -e "${GREEN}Build completed successfully!${NC}"
echo -e "${BLUE}Packages available in the dist directory:${NC}"
ls -la "$PROJECT_ROOT/dist"
</file>

<file path="scripts/build-windows.ps1">
# Build script for Windows

# Colors for output
$Red = [System.ConsoleColor]::Red
$Green = [System.ConsoleColor]::Green
$Blue = [System.ConsoleColor]::Blue
$Yellow = [System.ConsoleColor]::Yellow

function Write-ColorOutput($ForegroundColor) {
    $fc = $host.UI.RawUI.ForegroundColor
    $host.UI.RawUI.ForegroundColor = $ForegroundColor
    if ($args) {
        Write-Output $args
    }
    else {
        $input | Write-Output
    }
    $host.UI.RawUI.ForegroundColor = $fc
}

Write-ColorOutput $Blue "Starting MCP Client build for Windows..."

# Make sure we're in the project root
$scriptPath = Split-Path -Parent $MyInvocation.MyCommand.Path
$projectRoot = (Get-Item $scriptPath).Parent.FullName
Set-Location $projectRoot

# Ensure dependencies are installed
Write-ColorOutput $Yellow "Checking and installing dependencies..."
if (Test-Path "package.json") {
    npm install
}
else {
    Write-ColorOutput $Red "Error: package.json not found. Are you in the right directory?"
    exit 1
}

# Build the frontend
Write-ColorOutput $Yellow "Building frontend..."
npm run build

# Check if the build was successful
if ($LASTEXITCODE -ne 0) {
    Write-ColorOutput $Red "Frontend build failed!"
    exit 1
}
Write-ColorOutput $Green "Frontend build successful!"

# Build for Windows
Write-ColorOutput $Yellow "Building for Windows (MSI)..."
cargo tauri build

# Move built package to the dist directory
Write-ColorOutput $Yellow "Moving package to dist directory..."
$distDir = Join-Path $projectRoot "dist"
if (-Not (Test-Path $distDir)) {
    New-Item -ItemType Directory -Path $distDir | Out-Null
}

# Find and move the built MSI package
$msiPackage = Get-ChildItem -Path (Join-Path $projectRoot "src-tauri\target\release\bundle\msi") -Filter "*.msi" | Select-Object -First 1
if ($msiPackage) {
    Copy-Item $msiPackage.FullName -Destination $distDir
    Write-ColorOutput $Green "Copied $($msiPackage.Name) to dist directory"
}
else {
    Write-ColorOutput $Red "No MSI package found!"
    exit 1
}

# Generate checksums
Write-ColorOutput $Yellow "Generating checksums..."
Set-Location $distDir
Get-FileHash -Algorithm SHA256 *.msi | ForEach-Object { "$($_.Hash) $($_.Path)" } | Out-File -FilePath "checksums.sha256" -Encoding ascii
Write-ColorOutput $Green "Checksums generated in dist/checksums.sha256"

Write-ColorOutput $Green "Build completed successfully!"
Write-ColorOutput $Blue "Packages available in the dist directory:"
Get-ChildItem $distDir | Format-Table Name, Length -AutoSize
</file>

<file path="src-cli/Cargo.toml">
[package]
name = "mcp-cli"
version = "0.1.0"
edition = "2021"
description = "Command Line Interface for the MCP client"

[dependencies]
# Common library
mcp-common = { path = "../src-common" }

# CLI framework
clap = { version = "4.4.4", features = ["derive"] }
dialoguer = "0.10.4"
indicatif = "0.17.7"
console = "0.15.7"

# Async runtime
tokio = { version = "1.32", features = ["full"] }
futures = "0.3.28"

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Logging and error handling
log = "0.4.20"
env_logger = "0.10.0"
thiserror = "1.0.49"
anyhow = "1.0.75"

# Utilities
chrono = { version = "0.4.29", features = ["serde"] }
once_cell = "1.18.0"
dirs = "5.0.1"
atty = "0.2.14"

[dependencies.uuid]
version = "1.4.1"
features = [
    "v4",                # Lets you generate random UUIDs
    "fast-rng",          # Use a faster (but still sufficiently random) RNG
    "macro-diagnostics", # Enable better diagnostics for compile-time UUIDs
]
</file>

<file path="src-cli/README.md">
# MCP CLI - Command Line Interface for Claude MCP

A fast, powerful command-line interface for interacting with Claude AI models through the Model Context Protocol (MCP).

## Features

- **Complete Command Set**: Full set of commands for managing conversations and interacting with Claude
- **Interactive Mode**: Real-time streaming chat with Claude in your terminal
- **Conversation Management**: Create, list, show, and delete conversations
- **Export/Import**: Export conversations to various formats (JSON, Markdown, plain text)
- **System Messages**: Set system messages for conversation context
- **Model Management**: List models, set default model, change models for conversations
- **Streaming Responses**: Real-time streaming of Claude's responses
- **Rich Formatting**: Color-coded output with various formatting options
- **MCP Protocol**: Full implementation of the Model Context Protocol

## Usage

```bash
# Send a message to Claude
mcp chat -m "What's the weather like today?"

# Start an interactive session
mcp interactive

# Create a new conversation
mcp new -t "Weather Discussion"

# List all conversations
mcp list

# Show a specific conversation
mcp show CONVERSATION_ID

# Delete a conversation
mcp delete CONVERSATION_ID

# Export a conversation to markdown
mcp export CONVERSATION_ID -f markdown -o conversation.md

# Set a system message for a conversation
mcp system CONVERSATION_ID -m "You are a weather expert"

# List available models
mcp model list

# Set default model
mcp model set-default claude-3-opus-20240229

# Change model for a conversation
mcp model set-for-conversation CONVERSATION_ID claude-3-opus-20240229
```

## Interactive Mode

Interactive mode provides a REPL-like interface for conversing with Claude:

```bash
$ mcp interactive
Welcome to Claude MCP Interactive Mode
Type '.help' to see available commands

You> What's the capital of France?
Claude> The capital of France is Paris.

You> .help

===== Available Commands =====
.history    - Show conversation history
.switch     - Switch to another conversation
.new        - Create a new conversation
.system     - Set a system message
.help       - Show this help
.quit       - Exit interactive mode
============================

You> .quit
Goodbye!
```

## Installation

### From Source

```bash
# Clone the repository
git clone https://github.com/your-username/claude-mcp.git
cd claude-mcp

# Build the CLI
cd src-cli
cargo build --release

# Install (optional)
cargo install --path .
```

### Using Pre-built Binaries

Download the appropriate binary for your platform from the [Releases](https://github.com/your-username/claude-mcp/releases) page.

## Configuration

On first run, the CLI will prompt you to configure your API key and other settings. 
You can also manually configure these settings by running:

```bash
mcp setup
```

Configuration is stored in the following location:
- Linux: `~/.config/mcp-cli/config.json`
- macOS: `~/Library/Application Support/mcp-cli/config.json`
- Windows: `%APPDATA%\mcp-cli\config.json`

## Environment Variables

- `MCP_API_KEY`: Your Claude API key (overrides config file)
- `MCP_DEFAULT_MODEL`: Default model to use (overrides config file)
- `MCP_CONFIG_PATH`: Custom path to config file

## Integration with Other Tools

The CLI is designed to work well with other command-line tools:

```bash
# Pipe content to Claude
cat document.txt | mcp chat

# Process Claude's response
mcp chat -m "Summarize this article" -c CONVERSATION_ID | jq .summary > summary.txt

# Use in scripts
mcp chat -m "Translate to French: Hello world" --no-stream > french.txt
```

## License

[MIT License](LICENSE)
</file>

<file path="src-cli/src/commands/chat.rs">
use dialoguer::Input;
use std::io::{self, Write};
use std::sync::Arc;
use tokio::sync::mpsc;

use crate::error::CliResult;
use crate::display::{format_message, print_error, print_info, MessageFormat, show_spinner};
use mcp_common::{error::McpResult, models::Message, service::ChatService};

/// Run the chat command
pub async fn run(
    chat_service: Arc<ChatService>,
    conversation_id: Option<String>,
    message: Option<String>,
    stream: bool,
) -> CliResult<()> {
    // Get conversation ID
    let conversation_id = match conversation_id {
        Some(id) => id,
        None => {
            // List available conversations
            let conversations = chat_service.list_conversations().await?;
            
            if conversations.is_empty() {
                print_info("No conversations found. Creating a new one...");
                let new_conversation = chat_service.create_conversation("New Conversation", None).await?;
                new_conversation.id
            } else {
                // Let user select from available conversations
                let mut options: Vec<String> = conversations
                    .iter()
                    .map(|c| format!("{} ({})", c.title, c.id))
                    .collect();
                
                options.push("Create a new conversation".to_string());
                
                let selection = dialoguer::Select::new()
                    .with_prompt("Select a conversation")
                    .items(&options)
                    .default(0)
                    .interact()?;
                
                if selection == conversations.len() {
                    // Create a new conversation
                    let title: String = Input::new()
                        .with_prompt("Enter a title for the new conversation")
                        .default("New Conversation".into())
                        .interact_text()?;
                    
                    let new_conversation = chat_service.create_conversation(&title, None).await?;
                    new_conversation.id
                } else {
                    // Extract ID from selected conversation
                    conversations[selection].id.clone()
                }
            }
        }
    };
    
    // Get message content
    let message_content = match message {
        Some(content) => content,
        None => {
            Input::new()
                .with_prompt("Enter your message")
                .interact_text()?
        }
    };
    
    // Send message
    let spinner = show_spinner();
    spinner.set_message("Sending message...");
    
    if stream {
        // Stream response
        let mut stream = chat_service
            .send_message_streaming(&conversation_id, &message_content)
            .await?;
        
        spinner.info("Response:");
        
        // Print user message
        println!("{}", format_message(&Message::user(&message_content), MessageFormat::Colored));
        println!();
        
        // Print assistant response as it streams
        let mut full_message = String::new();
        
        while let Some(result) = stream.recv().await {
            match result {
                Ok(message) => {
                    // Get the new text
                    let text = message.text();
                    
                    // Only print the new part since the last update
                    if text.len() > full_message.len() {
                        let new_text = &text[full_message.len()..];
                        print!("{}", new_text);
                        io::stdout().flush()?;
                        full_message = text;
                    }
                }
                Err(e) => {
                    print_error(&format!("Error receiving message: {}", e));
                    break;
                }
            }
        }
        
        println!("\n");
    } else {
        // Regular response
        match chat_service.send_message(&conversation_id, &message_content).await {
            Ok(response) => {
                spinner.success("Response received");
                
                // Print user message
                println!("{}", format_message(&Message::user(&message_content), MessageFormat::Colored));
                println!();
                
                // Print assistant response
                println!("{}", format_message(&response, MessageFormat::Colored));
                println!();
            }
            Err(e) => {
                spinner.error(&format!("Failed to send message: {}", e));
                return Err(e.into());
            }
        }
    }
    
    Ok(())
}
</file>

<file path="src-cli/src/commands/delete.rs">
use dialoguer::Confirm;
use std::sync::Arc;

use crate::display::{print_error, print_success, show_spinner};
use crate::error::CliResult;
use mcp_common::service::ChatService;

/// Run the delete command
pub async fn run(chat_service: Arc<ChatService>, conversation_id: String) -> CliResult<()> {
    // Confirm deletion
    if !Confirm::new()
        .with_prompt(format!("Are you sure you want to delete conversation {}?", conversation_id))
        .default(false)
        .interact()?
    {
        print_error("Operation cancelled");
        return Ok(());
    }
    
    let spinner = show_spinner();
    spinner.set_message(&format!("Deleting conversation {}...", conversation_id));
    
    // Delete the conversation
    match chat_service.delete_conversation(&conversation_id).await {
        Ok(_) => {
            spinner.success("Conversation deleted");
            print_success(&format!("Deleted conversation {}", conversation_id));
            Ok(())
        }
        Err(e) => {
            spinner.error(&format!("Failed to delete conversation: {}", e));
            Err(e.into())
        }
    }
}
</file>

<file path="src-cli/src/commands/export.rs">
use std::fs;
use std::io::{self, Write};
use std::path::Path;
use std::sync::Arc;

use crate::display::{format_conversation, print_error, print_success, show_spinner, MessageFormat};
use crate::error::CliResult;
use mcp_common::service::ChatService;

/// Run the export command
pub async fn run(
    chat_service: Arc<ChatService>,
    conversation_id: String,
    format: String,
    output: Option<String>,
) -> CliResult<()> {
    let spinner = show_spinner();
    spinner.set_message(&format!("Loading conversation {}...", conversation_id));
    
    // Load the conversation
    let conversation = match chat_service.get_conversation(&conversation_id).await {
        Ok(conv) => {
            spinner.success("Conversation loaded");
            conv
        }
        Err(e) => {
            spinner.error(&format!("Failed to load conversation: {}", e));
            return Err(e.into());
        }
    };
    
    // Determine format
    let format_mode = match format.to_lowercase().as_str() {
        "json" => MessageFormat::Json,
        "markdown" | "md" => MessageFormat::Markdown,
        "text" | "txt" => MessageFormat::Plain,
        _ => {
            print_error(&format!("Unknown format: {}", format));
            return Ok(());
        }
    };
    
    // Format the conversation
    let formatted = format_conversation(&conversation, format_mode);
    
    // Output the formatted conversation
    match output {
        Some(path) => {
            // Ensure parent directory exists
            if let Some(parent) = Path::new(&path).parent() {
                if !parent.exists() {
                    fs::create_dir_all(parent)?;
                }
            }
            
            // Write to file
            fs::write(&path, formatted)?;
            print_success(&format!("Conversation exported to {}", path));
        }
        None => {
            // Write to stdout
            io::stdout().write_all(formatted.as_bytes())?;
            println!();
        }
    }
    
    Ok(())
}
</file>

<file path="src-cli/src/commands/interactive.rs">
use console::{style, Term};
use dialoguer::{Input, Select};
use std::io::{self, Write};
use std::sync::Arc;

use crate::commands;
use crate::display::{format_message, print_error, print_info, print_success, MessageFormat};
use crate::error::CliResult;
use mcp_common::{error::McpResult, models::Message, service::ChatService};

// Commands available in interactive mode
enum InteractiveCommand {
    SendMessage,
    ShowHistory,
    SwitchConversation,
    NewConversation,
    SystemMessage,
    Help,
    Quit,
}

/// Run the interactive command
pub async fn run(
    chat_service: Arc<ChatService>,
    conversation_id: Option<String>,
) -> CliResult<()> {
    // Clear screen
    let term = Term::stdout();
    term.clear_screen()?;
    
    print_info("Welcome to Claude MCP Interactive Mode");
    print_info("Type '.help' to see available commands");
    println!();
    
    // Get or create conversation
    let mut current_conversation_id = match conversation_id {
        Some(id) => id,
        None => {
            // List available conversations
            let conversations = chat_service.list_conversations().await?;
            
            if conversations.is_empty() {
                print_info("No conversations found. Creating a new one...");
                let new_conversation = chat_service.create_conversation("New Conversation", None).await?;
                new_conversation.id
            } else {
                // Let user select from available conversations
                let mut options: Vec<String> = conversations
                    .iter()
                    .map(|c| format!("{} ({})", c.title, c.id))
                    .collect();
                
                options.push("Create a new conversation".to_string());
                
                let selection = Select::new()
                    .with_prompt("Select a conversation")
                    .items(&options)
                    .default(0)
                    .interact()?;
                
                if selection == conversations.len() {
                    // Create a new conversation
                    let title: String = Input::new()
                        .with_prompt("Enter a title for the new conversation")
                        .default("New Conversation".into())
                        .interact_text()?;
                    
                    let new_conversation = chat_service.create_conversation(&title, None).await?;
                    new_conversation.id
                } else {
                    // Extract ID from selected conversation
                    conversations[selection].id.clone()
                }
            }
        }
    };
    
    // Get conversation details
    let conversation = chat_service.get_conversation(&current_conversation_id).await?;
    print_success(&format!("Conversation: {} ({})", conversation.title, conversation.model.name));
    
    // Main interaction loop
    loop {
        // Get input from user
        let input: String = Input::new()
            .with_prompt(style("You").green().to_string())
            .interact_text()?;
        
        // Check if input is a command
        if input.starts_with('.') {
            match parse_command(&input) {
                InteractiveCommand::SendMessage => {
                    print_error("Not a valid command. Type '.help' to see available commands");
                }
                InteractiveCommand::ShowHistory => {
                    // Fetch and show conversation history
                    let conversation = chat_service.get_conversation(&current_conversation_id).await?;
                    
                    println!("\n===== Conversation History =====");
                    for message in &conversation.messages {
                        println!("{}\n", format_message(message, MessageFormat::Colored));
                    }
                    println!("================================\n");
                }
                InteractiveCommand::SwitchConversation => {
                    // List and select a conversation
                    let conversations = chat_service.list_conversations().await?;
                    
                    if conversations.is_empty() {
                        print_error("No conversations found");
                        continue;
                    }
                    
                    let options: Vec<String> = conversations
                        .iter()
                        .map(|c| format!("{} ({})", c.title, c.id))
                        .collect();
                    
                    let selection = Select::new()
                        .with_prompt("Select a conversation")
                        .items(&options)
                        .default(0)
                        .interact()?;
                    
                    // Switch to selected conversation
                    current_conversation_id = conversations[selection].id.clone();
                    print_success(&format!(
                        "Switched to conversation: {} ({})",
                        conversations[selection].title,
                        conversations[selection].model.name
                    ));
                }
                InteractiveCommand::NewConversation => {
                    // Create a new conversation
                    let result = commands::new::run(chat_service.clone(), None, None).await;
                    
                    if let Ok(()) = result {
                        // Get the newly created conversation
                        let conversations = chat_service.list_conversations().await?;
                        if let Some(newest) = conversations.first() {
                            current_conversation_id = newest.id.clone();
                            print_success(&format!(
                                "Now using conversation: {} ({})",
                                newest.title,
                                newest.model.name
                            ));
                        }
                    }
                }
                InteractiveCommand::SystemMessage => {
                    // Set system message
                    let content: String = Input::new()
                        .with_prompt("Enter system message")
                        .interact_text()?;
                    
                    chat_service
                        .set_system_message(&current_conversation_id, &content)
                        .await?;
                    
                    print_success("System message set");
                }
                InteractiveCommand::Help => {
                    show_help();
                }
                InteractiveCommand::Quit => {
                    print_info("Goodbye!");
                    break;
                }
            }
        } else {
            // Not a command, send as a message
            println!();
            
            match chat_service
                .send_message_streaming(&current_conversation_id, &input)
                .await
            {
                Ok(mut stream) => {
                    // Print assistant header
                    print!("{} ", style("Claude").blue().bold());
                    io::stdout().flush()?;
                    
                    // Print response as it streams
                    let mut full_message = String::new();
                    
                    while let Some(result) = stream.recv().await {
                        match result {
                            Ok(message) => {
                                // Get the new text
                                let text = message.text();
                                
                                // Only print the new part since the last update
                                if text.len() > full_message.len() {
                                    let new_text = &text[full_message.len()..];
                                    print!("{}", new_text);
                                    io::stdout().flush()?;
                                    full_message = text;
                                }
                            }
                            Err(e) => {
                                print_error(&format!("Error receiving message: {}", e));
                                break;
                            }
                        }
                    }
                    
                    println!("\n");
                }
                Err(e) => {
                    print_error(&format!("Failed to send message: {}", e));
                }
            }
        }
    }
    
    Ok(())
}

// Parse a command from user input
fn parse_command(input: &str) -> InteractiveCommand {
    let input = input.trim();
    
    match input {
        ".history" => InteractiveCommand::ShowHistory,
        ".switch" => InteractiveCommand::SwitchConversation,
        ".new" => InteractiveCommand::NewConversation,
        ".system" => InteractiveCommand::SystemMessage,
        ".help" => InteractiveCommand::Help,
        ".quit" | ".exit" => InteractiveCommand::Quit,
        _ => InteractiveCommand::SendMessage,
    }
}

// Display help information
fn show_help() {
    println!("\n===== Available Commands =====");
    println!(".history    - Show conversation history");
    println!(".switch     - Switch to another conversation");
    println!(".new        - Create a new conversation");
    println!(".system     - Set a system message");
    println!(".help       - Show this help");
    println!(".quit       - Exit interactive mode");
    println!("============================\n");
}
</file>

<file path="src-cli/src/commands/list.rs">
use console::Style;
use std::sync::Arc;

use crate::display::{print_info, show_spinner, TableColumn, print_table};
use crate::error::CliResult;
use mcp_common::service::ChatService;

/// Run the list command
pub async fn run(chat_service: Arc<ChatService>) -> CliResult<()> {
    let spinner = show_spinner();
    spinner.set_message("Loading conversations...");
    
    let conversations = chat_service.list_conversations().await?;
    
    if conversations.is_empty() {
        spinner.info("No conversations found");
        return Ok(());
    }
    
    spinner.success(&format!("Found {} conversations", conversations.len()));
    
    // Define table columns
    let columns = vec![
        TableColumn {
            title: "ID".to_string(),
            width: 12,
            style: Some(Style::new().dim()),
        },
        TableColumn {
            title: "Title".to_string(),
            width: 30,
            style: Some(Style::new().cyan()),
        },
        TableColumn {
            title: "Model".to_string(),
            width: 20,
            style: Some(Style::new().yellow()),
        },
        TableColumn {
            title: "Messages".to_string(),
            width: 8,
            style: None,
        },
        TableColumn {
            title: "Last Updated".to_string(),
            width: 20,
            style: None,
        },
    ];
    
    // Prepare rows
    let mut rows = Vec::new();
    
    for conversation in conversations {
        let updated_at = chrono::DateTime::<chrono::Local>::from(conversation.updated_at)
            .format("%Y-%m-%d %H:%M:%S")
            .to_string();
        
        rows.push(vec![
            conversation.id[0..10].to_string() + "..",
            conversation.title,
            conversation.model.name,
            conversation.messages.len().to_string(),
            updated_at,
        ]);
    }
    
    // Print table
    print_table(&columns, &rows)?;
    
    Ok(())
}
</file>

<file path="src-cli/src/commands/model.rs">
use std::sync::Arc;

use crate::display::{print_error, print_info, print_success, print_table, TableColumn};
use crate::error::CliResult;
use mcp_common::service::ChatService;

/// List available models
pub async fn list(chat_service: Arc<ChatService>) -> CliResult<()> {
    print_info("Fetching available models...");
    
    match chat_service.list_models().await {
        Ok(models) => {
            if models.is_empty() {
                print_info("No models available");
                return Ok(());
            }
            
            // Define table columns
            let columns = vec![
                TableColumn {
                    header: "Name".to_string(),
                    width: 30,
                },
                TableColumn {
                    header: "Provider".to_string(),
                    width: 15,
                },
                TableColumn {
                    header: "Max Tokens".to_string(),
                    width: 15,
                },
                TableColumn {
                    header: "Available".to_string(),
                    width: 10,
                },
            ];
            
            // Convert models to rows
            let rows: Vec<Vec<String>> = models
                .iter()
                .map(|model| {
                    vec![
                        model.name.clone(),
                        model.provider.clone().unwrap_or_else(|| "Unknown".to_string()),
                        model.max_tokens.map(|t| t.to_string()).unwrap_or_else(|| "-".to_string()),
                        if model.available { "Yes".to_string() } else { "No".to_string() },
                    ]
                })
                .collect();
            
            // Print models table
            print_table(&columns, &rows);
            
            print_success(&format!("Found {} models", models.len()));
        }
        Err(e) => {
            print_error(&format!("Failed to fetch models: {}", e));
            return Err(e.into());
        }
    }
    
    Ok(())
}

/// Set default model for new conversations
pub async fn set_default(chat_service: Arc<ChatService>, model_name: &str) -> CliResult<()> {
    print_info(&format!("Setting default model to '{}'...", model_name));
    
    match chat_service.set_default_model(model_name).await {
        Ok(_) => {
            print_success(&format!("Default model set to '{}'", model_name));
            Ok(())
        }
        Err(e) => {
            print_error(&format!("Failed to set default model: {}", e));
            Err(e.into())
        }
    }
}

/// Set model for an existing conversation
pub async fn set_for_conversation(
    chat_service: Arc<ChatService>,
    conversation_id: &str,
    model_name: &str,
) -> CliResult<()> {
    print_info(&format!(
        "Setting model for conversation '{}' to '{}'...",
        conversation_id, model_name
    ));
    
    match chat_service.set_conversation_model(conversation_id, model_name).await {
        Ok(_) => {
            print_success(&format!("Model set to '{}'", model_name));
            Ok(())
        }
        Err(e) => {
            print_error(&format!("Failed to set conversation model: {}", e));
            Err(e.into())
        }
    }
}
</file>

<file path="src-cli/src/commands/new.rs">
use dialoguer::Input;
use std::sync::Arc;

use crate::display::{print_success, show_spinner};
use crate::error::CliResult;
use mcp_common::{models::Model, service::ChatService};

/// Run the new command
pub async fn run(
    chat_service: Arc<ChatService>,
    title: Option<String>,
    model_id: Option<String>,
) -> CliResult<()> {
    // Get title
    let title = match title {
        Some(t) => t,
        None => {
            Input::new()
                .with_prompt("Enter a title for the new conversation")
                .default("New Conversation".into())
                .interact_text()?
        }
    };
    
    // Get model
    let model = if let Some(model_id) = model_id {
        // Get available models
        let models = chat_service.available_models().await?;
        
        // Find requested model
        models
            .into_iter()
            .find(|m| m.id == model_id)
            .unwrap_or_else(Model::default_claude)
    } else {
        // Get available models
        let spinner = show_spinner();
        spinner.set_message("Loading available models...");
        
        let models = chat_service.available_models().await?;
        
        if models.is_empty() {
            spinner.warning("No models available. Using default model.");
            Model::default_claude()
        } else {
            spinner.success(&format!("Found {} models", models.len()));
            
            // Let user select a model
            let options: Vec<String> = models
                .iter()
                .map(|m| format!("{} ({})", m.name, m.id))
                .collect();
            
            let selection = dialoguer::Select::new()
                .with_prompt("Select a model")
                .items(&options)
                .default(0)
                .interact()?;
            
            models[selection].clone()
        }
    };
    
    // Create the conversation
    let spinner = show_spinner();
    spinner.set_message(&format!("Creating conversation with {}...", model.name));
    
    let conversation = chat_service.create_conversation(&title, Some(model)).await?;
    
    spinner.success("Conversation created");
    print_success(&format!(
        "Created conversation '{}' with ID: {}",
        conversation.title, conversation.id
    ));
    
    Ok(())
}
</file>

<file path="src-cli/src/commands/setup.rs">
use dialoguer::{Confirm, Input, Select};
use std::sync::Arc;

use crate::display::{print_error, print_info, print_success, show_spinner};
use crate::error::CliResult;
use mcp_common::config::{get_settings, get_storage_manager};

/// Run the setup command
pub async fn run() -> CliResult<()> {
    print_info("MCP Client Setup");
    println!();
    
    // Get settings
    let settings = get_settings();
    let mut settings_guard = settings.lock().unwrap();
    
    // API key setup
    let current_api_key = settings_guard.get_api_key().unwrap_or(Ok(None)).unwrap_or(None);
    
    if current_api_key.is_some() {
        print_info("An API key is already configured");
        
        if !Confirm::new()
            .with_prompt("Do you want to replace it?")
            .default(false)
            .interact()?
        {
            print_info("Keeping existing API key");
        } else {
            // Replace API key
            let new_api_key: String = Input::new()
                .with_prompt("Enter your Anthropic API key")
                .interact_text()?;
            
            if new_api_key.is_empty() {
                print_error("API key cannot be empty");
            } else {
                settings_guard.set_api_key(&new_api_key)?;
                print_success("API key updated");
            }
        }
    } else {
        // No API key configured
        print_info("No API key configured");
        
        let new_api_key: String = Input::new()
            .with_prompt("Enter your Anthropic API key")
            .interact_text()?;
        
        if new_api_key.is_empty() {
            print_error("API key cannot be empty");
        } else {
            settings_guard.set_api_key(&new_api_key)?;
            print_success("API key saved");
        }
    }
    
    // Model settings
    print_info("\nModel Settings");
    
    // Default model
    let model_options = [
        "claude-3-opus-20240229",
        "claude-3-sonnet-20240229",
        "claude-3-haiku-20240307",
    ];
    
    let default_model_index = model_options
        .iter()
        .position(|&m| m == settings_guard.api.model)
        .unwrap_or(1); // Default to sonnet
    
    let model_selection = Select::new()
        .with_prompt("Select default model")
        .items(&model_options)
        .default(default_model_index)
        .interact()?;
    
    settings_guard.api.model = model_options[model_selection].to_string();
    
    // Temperature
    let temperature: f32 = Input::new()
        .with_prompt("Default temperature (0.0-1.0)")
        .default(settings_guard.model.temperature)
        .interact_text()?;
    
    settings_guard.model.temperature = temperature.clamp(0.0, 1.0);
    
    // Max tokens
    let max_tokens: u32 = Input::new()
        .with_prompt("Default max tokens")
        .default(settings_guard.model.max_tokens)
        .interact_text()?;
    
    settings_guard.model.max_tokens = max_tokens;
    
    // System prompt
    print_info("\nSystem Prompt");
    
    if let Some(prompt) = &settings_guard.model.system_prompt {
        println!("Current system prompt: {}", prompt);
        
        if Confirm::new()
            .with_prompt("Do you want to change it?")
            .default(false)
            .interact()?
        {
            let new_prompt: String = Input::new()
                .with_prompt("Enter new system prompt (leave empty to clear)")
                .interact_text()?;
            
            if new_prompt.is_empty() {
                settings_guard.model.system_prompt = None;
                print_info("System prompt cleared");
            } else {
                settings_guard.model.system_prompt = Some(new_prompt);
                print_success("System prompt updated");
            }
        }
    } else {
        print_info("No system prompt configured");
        
        if Confirm::new()
            .with_prompt("Do you want to set a system prompt?")
            .default(false)
            .interact()?
        {
            let new_prompt: String = Input::new()
                .with_prompt("Enter system prompt")
                .interact_text()?;
            
            if !new_prompt.is_empty() {
                settings_guard.model.system_prompt = Some(new_prompt);
                print_success("System prompt set");
            }
        }
    }
    
    // Save settings
    settings_guard.save()?;
    print_success("\nSettings saved");
    
    // Return success
    Ok(())
}
</file>

<file path="src-cli/src/commands/show.rs">
use std::sync::Arc;

use crate::display::{format_conversation, print_error, show_spinner, MessageFormat};
use crate::error::CliResult;
use mcp_common::service::ChatService;

/// Run the show command
pub async fn run(chat_service: Arc<ChatService>, conversation_id: String) -> CliResult<()> {
    let spinner = show_spinner();
    spinner.set_message(&format!("Loading conversation {}...", conversation_id));
    
    // Load the conversation
    match chat_service.get_conversation(&conversation_id).await {
        Ok(conversation) => {
            spinner.success("Conversation loaded");
            
            // Format and print the conversation
            let formatted = format_conversation(&conversation, MessageFormat::Colored);
            println!("{}", formatted);
            
            Ok(())
        }
        Err(e) => {
            spinner.error(&format!("Failed to load conversation: {}", e));
            Err(e.into())
        }
    }
}
</file>

<file path="src-cli/src/commands/system.rs">
use dialoguer::Input;
use std::sync::Arc;

use crate::display::{print_success, show_spinner};
use crate::error::CliResult;
use mcp_common::service::ChatService;

/// Run the system command
pub async fn run(
    chat_service: Arc<ChatService>,
    conversation_id: String,
    message: Option<String>,
) -> CliResult<()> {
    // Get system message content
    let content = match message {
        Some(text) => text,
        None => {
            Input::new()
                .with_prompt("Enter system message")
                .interact_text()?
        }
    };
    
    let spinner = show_spinner();
    spinner.set_message("Setting system message...");
    
    // Set the system message
    match chat_service.set_system_message(&conversation_id, &content).await {
        Ok(_) => {
            spinner.success("System message set");
            print_success(&format!(
                "System message set for conversation {}",
                conversation_id
            ));
            Ok(())
        }
        Err(e) => {
            spinner.error(&format!("Failed to set system message: {}", e));
            Err(e.into())
        }
    }
}
</file>

<file path="src-cli/src/display/formatter.rs">
use console::{style, Style};
use mcp_common::models::{Conversation, Message, MessageRole};

/// Message format options
pub enum MessageFormat {
    Plain,
    Colored,
    Markdown,
    Json,
}

/// Format a conversation based on the selected format
pub fn format_conversation(conversation: &Conversation, format: MessageFormat) -> String {
    match format {
        MessageFormat::Plain => format_conversation_plain(conversation),
        MessageFormat::Colored => format_conversation_colored(conversation),
        MessageFormat::Markdown => format_conversation_markdown(conversation),
        MessageFormat::Json => format_conversation_json(conversation),
    }
}

/// Format a message based on the selected format
pub fn format_message(message: &Message, format: MessageFormat) -> String {
    match format {
        MessageFormat::Plain => format_message_plain(message),
        MessageFormat::Colored => format_message_colored(message),
        MessageFormat::Markdown => format_message_markdown(message),
        MessageFormat::Json => format_message_json(message),
    }
}

/// Format metadata as a string
pub fn format_metadata(metadata: &serde_json::Value) -> String {
    let formatted = serde_json::to_string_pretty(metadata).unwrap_or_default();
    
    if formatted == "null" {
        String::new()
    } else {
        formatted
    }
}

// Format a conversation in plain text
fn format_conversation_plain(conversation: &Conversation) -> String {
    let mut result = String::new();
    
    result.push_str(&format!("Conversation: {}\n", conversation.title));
    result.push_str(&format!("Model: {}\n", conversation.model.name));
    result.push_str(&format!("ID: {}\n", conversation.id));
    result.push_str(&format!("Messages: {}\n\n", conversation.messages.len()));
    
    for message in &conversation.messages {
        result.push_str(&format_message_plain(message));
        result.push_str("\n\n");
    }
    
    result
}

// Format a conversation with colors
fn format_conversation_colored(conversation: &Conversation) -> String {
    let mut result = String::new();
    
    let title_style = Style::new().cyan().bold();
    let model_style = Style::new().yellow();
    let id_style = Style::new().dim();
    
    result.push_str(&format!("{}: {}\n", title_style.apply_to("Conversation"), conversation.title));
    result.push_str(&format!("{}: {}\n", style("Model").yellow(), model_style.apply_to(&conversation.model.name)));
    result.push_str(&format!("{}: {}\n", style("ID").dim(), id_style.apply_to(&conversation.id)));
    result.push_str(&format!("{}: {}\n\n", style("Messages").dim(), conversation.messages.len()));
    
    for message in &conversation.messages {
        result.push_str(&format_message_colored(message));
        result.push_str("\n\n");
    }
    
    result
}

// Format a conversation in markdown
fn format_conversation_markdown(conversation: &Conversation) -> String {
    let mut result = String::new();
    
    result.push_str(&format!("# {}\n\n", conversation.title));
    result.push_str(&format!("**Model**: {}\n\n", conversation.model.name));
    result.push_str(&format!("**ID**: {}\n\n", conversation.id));
    result.push_str(&format!("**Messages**: {}\n\n", conversation.messages.len()));
    
    for message in &conversation.messages {
        result.push_str(&format_message_markdown(message));
        result.push_str("\n\n");
    }
    
    result
}

// Format a conversation as JSON
fn format_conversation_json(conversation: &Conversation) -> String {
    match serde_json::to_string_pretty(conversation) {
        Ok(json) => json,
        Err(_) => String::from("Error: Could not serialize conversation to JSON"),
    }
}

// Format a message in plain text
fn format_message_plain(message: &Message) -> String {
    let role = match message.role {
        MessageRole::User => "User",
        MessageRole::Assistant => "Assistant",
        MessageRole::System => "System",
    };
    
    format!("[{}] {}\n{}", role, message.timestamp(), message.text())
}

// Format a message with colors
fn format_message_colored(message: &Message) -> String {
    let (role, style) = match message.role {
        MessageRole::User => ("User", Style::new().green().bold()),
        MessageRole::Assistant => ("Assistant", Style::new().blue().bold()),
        MessageRole::System => ("System", Style::new().yellow().bold()),
    };
    
    let timestamp = Style::new().dim().apply_to(message.timestamp());
    
    format!(
        "[{}] {}\n{}",
        style.apply_to(role),
        timestamp,
        message.text()
    )
}

// Format a message in markdown
fn format_message_markdown(message: &Message) -> String {
    let heading = match message.role {
        MessageRole::User => "## 👤 User",
        MessageRole::Assistant => "## 🤖 Assistant",
        MessageRole::System => "## ⚙️ System",
    };
    
    format!(
        "{} ({})\n\n{}",
        heading,
        message.timestamp(),
        message.text()
    )
}

// Format a message as JSON
fn format_message_json(message: &Message) -> String {
    match serde_json::to_string_pretty(message) {
        Ok(json) => json,
        Err(_) => String::from("Error: Could not serialize message to JSON"),
    }
}
</file>

<file path="src-cli/src/display/mod.rs">
mod formatter;
mod printer;
mod spinner;
mod table;

pub use formatter::{format_conversation, format_message, format_metadata, MessageFormat};
pub use printer::{print_error, print_info, print_success, print_warning};
pub use spinner::{show_spinner, show_spinner_with_message, SpinnerHandle};
pub use table::{print_table, TableColumn};
</file>

<file path="src-cli/src/display/printer.rs">
use console::style;
use std::io::{self, Write};

/// Print an informational message
pub fn print_info(message: &str) {
    let stdout = io::stdout();
    let mut handle = stdout.lock();
    let _ = writeln!(handle, "{} {}", style("[INFO]").cyan(), message);
}

/// Print a success message
pub fn print_success(message: &str) {
    let stdout = io::stdout();
    let mut handle = stdout.lock();
    let _ = writeln!(handle, "{} {}", style("[SUCCESS]").green(), message);
}

/// Print a warning message
pub fn print_warning(message: &str) {
    let stdout = io::stdout();
    let mut handle = stdout.lock();
    let _ = writeln!(handle, "{} {}", style("[WARNING]").yellow(), message);
}

/// Print an error message
pub fn print_error(message: &str) {
    let stderr = io::stderr();
    let mut handle = stderr.lock();
    let _ = writeln!(handle, "{} {}", style("[ERROR]").red().bold(), message);
}
</file>

<file path="src-cli/src/display/spinner.rs">
use indicatif::{ProgressBar, ProgressStyle};
use std::time::Duration;

/// Spinner handle
pub struct SpinnerHandle {
    bar: ProgressBar,
}

impl SpinnerHandle {
    /// Set a new message for the spinner
    pub fn set_message(&self, message: &str) {
        self.bar.set_message(message.to_string());
    }
    
    /// Finish with a success message
    pub fn success(self, message: &str) {
        self.bar.finish_with_message(format!("✓ {}", message));
    }
    
    /// Finish with an error message
    pub fn error(self, message: &str) {
        self.bar.finish_with_message(format!("✗ {}", message));
    }
    
    /// Finish with a warning message
    pub fn warning(self, message: &str) {
        self.bar.finish_with_message(format!("⚠ {}", message));
    }
    
    /// Finish with an info message
    pub fn info(self, message: &str) {
        self.bar.finish_with_message(format!("ℹ {}", message));
    }
    
    /// Abandon the spinner (finish without message)
    pub fn abandon(self) {
        self.bar.finish_and_clear();
    }
}

/// Show a spinner with default message "Processing..."
pub fn show_spinner() -> SpinnerHandle {
    show_spinner_with_message("Processing...")
}

/// Show a spinner with a custom message
pub fn show_spinner_with_message(message: &str) -> SpinnerHandle {
    let spinner_style = ProgressStyle::default_spinner()
        .tick_chars("⠋⠙⠹⠸⠼⠴⠦⠧⠇⠏")
        .template("{spinner} {msg}")
        .unwrap();
    
    let bar = ProgressBar::new_spinner();
    bar.set_style(spinner_style);
    bar.set_message(message.to_string());
    bar.enable_steady_tick(Duration::from_millis(100));
    
    SpinnerHandle { bar }
}
</file>

<file path="src-cli/src/display/table.rs">
use console::{style, Style, Term};
use std::io;

/// Table column definition
pub struct TableColumn {
    /// Column title
    pub title: String,
    
    /// Column width
    pub width: usize,
    
    /// Column style
    pub style: Option<Style>,
}

/// Print a table to the terminal
pub fn print_table(columns: &[TableColumn], rows: &[Vec<String>]) -> io::Result<()> {
    let term = Term::stdout();
    
    // Print header
    let mut header_line = String::new();
    
    for (i, column) in columns.iter().enumerate() {
        let title = format!("{:<width$}", column.title, width = column.width);
        
        if let Some(ref style) = column.style {
            header_line.push_str(&format!("{}", style.apply_to(title)));
        } else {
            header_line.push_str(&title);
        }
        
        if i < columns.len() - 1 {
            header_line.push_str("  ");
        }
    }
    
    term.write_line(&header_line)?;
    
    // Print separator
    let sep_line: String = header_line
        .chars()
        .map(|c| if c.is_whitespace() { ' ' } else { '─' })
        .collect();
    
    term.write_line(&sep_line)?;
    
    // Print rows
    for row in rows {
        let mut row_line = String::new();
        
        for (i, (column, value)) in columns.iter().zip(row.iter()).enumerate() {
            // Truncate value if it's too long
            let truncated_value = if value.len() > column.width {
                format!("{}…", &value[0..column.width - 1])
            } else {
                value.clone()
            };
            
            let padded_value = format!("{:<width$}", truncated_value, width = column.width);
            
            // Apply style if provided
            if let Some(ref style) = column.style {
                row_line.push_str(&format!("{}", style.apply_to(padded_value)));
            } else {
                row_line.push_str(&padded_value);
            }
            
            if i < columns.len() - 1 {
                row_line.push_str("  ");
            }
        }
        
        term.write_line(&row_line)?;
    }
    
    Ok(())
}
</file>

<file path="src-cli/src/error.rs">
use mcp_common::error::McpError;
use thiserror::Error;
use anyhow::Result;

/// CLI error type
#[derive(Error, Debug)]
pub enum CliError {
    #[error("MCP error: {0}")]
    McpError(#[from] McpError),
    
    #[error("I/O error: {0}")]
    IoError(#[from] std::io::Error),
    
    #[error("Serialization error: {0}")]
    SerializationError(#[from] serde_json::Error),
    
    #[error("Input error: {0}")]
    InputError(String),
    
    #[error("Invalid argument: {0}")]
    InvalidArgument(String),
    
    #[error("Operation cancelled")]
    Cancelled,
    
    #[error("Unknown error: {0}")]
    Unknown(String),
}

/// Result type for CLI operations
pub type CliResult<T> = Result<T, CliError>;

/// Convert any error to a CliError
pub fn to_cli_error<E: std::error::Error>(e: E) -> CliError {
    CliError::Unknown(e.to_string())
}
</file>

<file path="src-common/Cargo.toml">
[package]
name = "mcp-common"
version = "0.1.0"
edition = "2021"
description = "Shared components for MCP client applications"

[dependencies]
# Async runtime and utilities
tokio = { version = "1.32", features = ["full"] }
async-trait = "0.1.73"
futures = "0.3.28"

# Serialization/deserialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# WebSocket and HTTP client
tokio-tungstenite = { version = "0.20", features = ["native-tls"] }
reqwest = { version = "0.11", features = ["json", "native-tls"] }
url = "2.4.1"

# Utilities
log = "0.4.20"
env_logger = "0.10.0"
thiserror = "1.0.49"
uuid = { version = "1.4.1", features = ["v4", "serde"] }
once_cell = "1.18.0"
chrono = { version = "0.4.29", features = ["serde"] }
strum = { version = "0.25", features = ["derive"] }
regex = "1.9.5"

# Config and settings
config = "0.13.3"
directories = "5.0.1"

# Encryption
ring = "0.17.5"
base64 = "0.21.4"
</file>

<file path="src-common/README.md">
# MCP Common Library

A shared Rust library that provides common functionality for all MCP client interfaces (GUI, CLI, and TUI). This library implements the Model Context Protocol (MCP) and provides services for interacting with Claude AI models.

## Components

### MCP Protocol Implementation

- Full implementation of the Model Context Protocol (MCP)
- WebSocket-based real-time communication
- Message streaming support
- Authentication and session management
- Error handling and recovery

### Service Layer

- **McpService**: Core service for MCP interactions
- **ChatService**: High-level service for conversation management
- **AuthService**: Authentication and API key management
- **ModelService**: Model selection and management

### Data Models

- Conversation model with messages
- User, assistant, and system message types
- Model configurations
- Metadata structures

### Configuration Management

- Configuration file handling
- Environment variable support
- User settings management

### Utilities

- Error handling
- Logging
- Token management
- Path resolution

## Usage

This library is designed to be used as a dependency by the various MCP client interfaces. It provides a consistent API for interacting with Claude models, regardless of the interface being used.

```rust
use mcp_common::{init_mcp_service, service::ChatService};
use std::sync::Arc;

#[tokio::main]
async fn main() {
    // Initialize the MCP service
    let mcp_service = init_mcp_service();
    
    // Create a chat service
    let chat_service = Arc::new(ChatService::new(mcp_service));
    
    // Create a new conversation
    let conversation = chat_service
        .create_conversation("New Conversation", None)
        .await
        .expect("Failed to create conversation");
    
    // Send a message
    let response = chat_service
        .send_message(&conversation.id, "Hello, Claude!")
        .await
        .expect("Failed to send message");
    
    println!("Claude: {}", response.text());
}
```

## Message Streaming

The library supports real-time streaming of responses from Claude:

```rust
// Send a message with streaming response
let mut stream = chat_service
    .send_message_streaming(&conversation_id, "Tell me a story")
    .await
    .expect("Failed to start streaming");

// Process streaming updates
while let Some(result) = stream.recv().await {
    match result {
        Ok(message) => {
            println!("New content: {}", message.text());
        }
        Err(e) => {
            eprintln!("Error: {}", e);
            break;
        }
    }
}
```

## Offline Support

The library includes support for local models when offline:

```rust
// The router will automatically select a local model if disconnected
let response = chat_service
    .send_message(&conversation_id, "Are you there?")
    .await
    .expect("Failed to send message");

// Check if the response came from a local model
if let Some(model) = &response.model {
    println!("Response from model: {}", model.name);
}
```

## Thread Safety

All services are designed to be thread-safe and can be wrapped in an `Arc` for sharing between threads:

```rust
let chat_service = Arc::new(ChatService::new(mcp_service));

// Clone to share between threads
let service_clone = chat_service.clone();

tokio::spawn(async move {
    let conversations = service_clone.list_conversations().await.unwrap();
    // ...
});
```

## Error Handling

The library provides a comprehensive error type hierarchy:

```rust
match chat_service.send_message(&conversation_id, "Hello").await {
    Ok(response) => {
        println!("Claude: {}", response.text());
    }
    Err(e) => match e {
        McpError::Network(err) => {
            eprintln!("Network error: {}", err);
            // Handle network errors (try offline mode)
        }
        McpError::Authentication(err) => {
            eprintln!("Authentication error: {}", err);
            // Handle auth errors (prompt for API key)
        }
        McpError::Service(err) => {
            eprintln!("Service error: {}", err);
            // Handle service errors
        }
        _ => {
            eprintln!("Error: {}", e);
        }
    },
}
```

## Configuration

The library uses a shared configuration system that works across all interfaces:

```rust
use mcp_common::config::{Config, load_config, save_config};

// Load configuration
let mut config = load_config().unwrap_or_default();

// Update configuration
config.api_key = Some("your-api-key".to_string());
config.default_model = Some("claude-3-opus-20240229".to_string());

// Save configuration
save_config(&config).expect("Failed to save config");
```
</file>

<file path="src-common/src/config/mod.rs">
mod settings;
mod storage;

use once_cell::sync::OnceCell;
use std::path::PathBuf;
use std::sync::{Arc, Mutex};

pub use settings::Settings;
pub use storage::StorageManager;

/// Global settings instance
static SETTINGS: OnceCell<Arc<Mutex<Settings>>> = OnceCell::new();

/// Global storage manager instance
static STORAGE_MANAGER: OnceCell<Arc<StorageManager>> = OnceCell::new();

/// Get the global settings instance
pub fn get_settings() -> Arc<Mutex<Settings>> {
    SETTINGS.get_or_init(|| {
        Arc::new(Mutex::new(Settings::load().unwrap_or_default()))
    }).clone()
}

/// Get the global storage manager instance
pub fn get_storage_manager() -> Arc<StorageManager> {
    STORAGE_MANAGER.get_or_init(|| {
        Arc::new(StorageManager::new())
    }).clone()
}

/// Get the application config directory
pub fn get_config_dir() -> PathBuf {
    let proj_dirs = directories::ProjectDirs::from("com", "anthropic", "mcp-client")
        .expect("Failed to determine config directory");
    
    let config_dir = proj_dirs.config_dir().to_path_buf();
    
    // Create if it doesn't exist
    if !config_dir.exists() {
        std::fs::create_dir_all(&config_dir).expect("Failed to create config directory");
    }
    
    config_dir
}

/// Get the application data directory
pub fn get_data_dir() -> PathBuf {
    let proj_dirs = directories::ProjectDirs::from("com", "anthropic", "mcp-client")
        .expect("Failed to determine data directory");
    
    let data_dir = proj_dirs.data_dir().to_path_buf();
    
    // Create if it doesn't exist
    if !data_dir.exists() {
        std::fs::create_dir_all(&data_dir).expect("Failed to create data directory");
    }
    
    data_dir
}

/// Get a path within the config directory
pub fn config_path(filename: &str) -> PathBuf {
    let mut path = get_config_dir();
    path.push(filename);
    path
}

/// Get a path within the data directory
pub fn data_path(filename: &str) -> PathBuf {
    let mut path = get_data_dir();
    path.push(filename);
    path
}
</file>

<file path="src-common/src/config/settings.rs">
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::Path;

use super::config_path;
use crate::error::{McpError, McpResult};
use crate::utils::security;

const SETTINGS_FILE: &str = "settings.json";
const API_KEY_FILE: &str = "credentials.enc";

/// Application settings
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Settings {
    /// API configuration
    pub api: ApiSettings,
    
    /// UI configuration
    pub ui: UiSettings,
    
    /// Model configuration
    pub model: ModelSettings,
}

/// API settings
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ApiSettings {
    /// API endpoint URL
    pub url: String,
    
    /// Default model to use
    pub model: String,
    
    /// API version
    pub version: String,
}

/// UI settings
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UiSettings {
    /// Use dark mode
    pub dark_mode: bool,
    
    /// Font size
    pub font_size: u8,
    
    /// Enable animations
    pub animations: bool,
    
    /// Use system theme
    pub system_theme: bool,
}

/// Model settings
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelSettings {
    /// Default temperature
    pub temperature: f32,
    
    /// Default max tokens
    pub max_tokens: u32,
    
    /// Default system prompt
    pub system_prompt: Option<String>,
    
    /// Enable streaming
    pub streaming: bool,
}

impl Settings {
    /// Load settings from file
    pub fn load() -> McpResult<Self> {
        let path = config_path(SETTINGS_FILE);
        
        if path.exists() {
            let content = fs::read_to_string(&path)
                .map_err(|e| McpError::Io(e))?;
                
            let settings = serde_json::from_str(&content)
                .map_err(|e| McpError::Serialization(e))?;
                
            Ok(settings)
        } else {
            // Create default settings
            let settings = Self::default();
            settings.save()?;
            Ok(settings)
        }
    }
    
    /// Save settings to file
    pub fn save(&self) -> McpResult<()> {
        let path = config_path(SETTINGS_FILE);
        
        let content = serde_json::to_string_pretty(self)
            .map_err(|e| McpError::Serialization(e))?;
            
        fs::write(path, content)
            .map_err(|e| McpError::Io(e))?;
            
        Ok(())
    }
    
    /// Get API key (will be decrypted)
    pub fn get_api_key(&self) -> McpResult<Option<String>> {
        let path = config_path(API_KEY_FILE);
        
        if path.exists() {
            let encrypted = fs::read(&path)
                .map_err(|e| McpError::Io(e))?;
                
            let key = security::decrypt(&encrypted)
                .map_err(|e| McpError::Config(format!("Failed to decrypt API key: {}", e)))?;
                
            Ok(Some(key))
        } else {
            Ok(None)
        }
    }
    
    /// Set API key (will be encrypted)
    pub fn set_api_key(&self, api_key: &str) -> McpResult<()> {
        let path = config_path(API_KEY_FILE);
        
        let encrypted = security::encrypt(api_key)
            .map_err(|e| McpError::Config(format!("Failed to encrypt API key: {}", e)))?;
            
        fs::write(path, encrypted)
            .map_err(|e| McpError::Io(e))?;
            
        Ok(())
    }
}

impl Default for Settings {
    fn default() -> Self {
        Self {
            api: ApiSettings {
                url: "wss://api.anthropic.com/v1/messages".to_string(),
                model: "claude-3-sonnet-20240229".to_string(),
                version: "v1".to_string(),
            },
            ui: UiSettings {
                dark_mode: false,
                font_size: 14,
                animations: true,
                system_theme: true,
            },
            model: ModelSettings {
                temperature: 0.7,
                max_tokens: 4096,
                system_prompt: None,
                streaming: true,
            },
        }
    }
}
</file>

<file path="src-common/src/config/storage.rs">
use std::fs;
use std::path::{Path, PathBuf};
use std::sync::{Arc, Mutex};

use crate::error::{McpError, McpResult};
use crate::models::Conversation;
use super::{data_path, get_data_dir};

/// Storage manager
pub struct StorageManager {
    /// Conversations directory
    conversations_dir: PathBuf,
}

impl StorageManager {
    /// Create a new storage manager
    pub fn new() -> Self {
        let mut conversations_dir = get_data_dir();
        conversations_dir.push("conversations");
        
        // Create if it doesn't exist
        if !conversations_dir.exists() {
            fs::create_dir_all(&conversations_dir).expect("Failed to create conversations directory");
        }
        
        Self {
            conversations_dir,
        }
    }
    
    /// Get path for a conversation file
    pub fn conversation_path(&self, conversation_id: &str) -> PathBuf {
        let mut path = self.conversations_dir.clone();
        path.push(format!("{}.json", conversation_id));
        path
    }
    
    /// Save a conversation
    pub fn save_conversation(&self, conversation: &Conversation) -> McpResult<()> {
        let path = self.conversation_path(&conversation.id);
        
        let content = serde_json::to_string_pretty(conversation)
            .map_err(|e| McpError::Serialization(e))?;
            
        fs::write(path, content)
            .map_err(|e| McpError::Io(e))?;
            
        Ok(())
    }
    
    /// Load a conversation
    pub fn load_conversation(&self, conversation_id: &str) -> McpResult<Conversation> {
        let path = self.conversation_path(conversation_id);
        
        if !path.exists() {
            return Err(McpError::Unknown(format!("Conversation {} not found", conversation_id)));
        }
        
        let content = fs::read_to_string(&path)
            .map_err(|e| McpError::Io(e))?;
            
        let conversation = serde_json::from_str(&content)
            .map_err(|e| McpError::Serialization(e))?;
            
        Ok(conversation)
    }
    
    /// Delete a conversation
    pub fn delete_conversation(&self, conversation_id: &str) -> McpResult<()> {
        let path = self.conversation_path(conversation_id);
        
        if path.exists() {
            fs::remove_file(path)
                .map_err(|e| McpError::Io(e))?;
        }
        
        Ok(())
    }
    
    /// List all conversations
    pub fn list_conversations(&self) -> McpResult<Vec<Conversation>> {
        let mut conversations = Vec::new();
        
        for entry in fs::read_dir(&self.conversations_dir)
            .map_err(|e| McpError::Io(e))?
        {
            let entry = entry.map_err(|e| McpError::Io(e))?;
            let path = entry.path();
            
            if path.is_file() && path.extension().map_or(false, |ext| ext == "json") {
                // Read the conversation file
                if let Ok(content) = fs::read_to_string(&path) {
                    if let Ok(conversation) = serde_json::from_str::<Conversation>(&content) {
                        conversations.push(conversation);
                    }
                }
            }
        }
        
        // Sort by last updated
        conversations.sort_by(|a, b| b.updated_at.cmp(&a.updated_at));
        
        Ok(conversations)
    }
}

impl Default for StorageManager {
    fn default() -> Self {
        Self::new()
    }
}
</file>

<file path="src-common/src/error.rs">
use thiserror::Error;

/// MCP client error types
#[derive(Debug, Error)]
pub enum McpError {
    #[error("Protocol error: {0}")]
    Protocol(String),
    
    #[error("Connection error: {0}")]
    Connection(String),
    
    #[error("Authentication error: {0}")]
    Authentication(String),
    
    #[error("Message error: {0}")]
    Message(#[from] crate::models::MessageError),
    
    #[error("Configuration error: {0}")]
    Config(String),
    
    #[error("I/O error: {0}")]
    Io(#[from] std::io::Error),
    
    #[error("Serialization error: {0}")]
    Serialization(#[from] serde_json::Error),
    
    #[error("Invalid request: {0}")]
    InvalidRequest(String),
    
    #[error("Rate limited: {0}")]
    RateLimit(String),
    
    #[error("Unknown error: {0}")]
    Unknown(String),
}

/// Result type alias for MCP operations
pub type McpResult<T> = Result<T, McpError>;
</file>

<file path="src-common/src/feature_flags/mod.rs">
use serde::{Serialize, Deserialize};
use std::collections::HashMap;
use std::sync::{Arc, RwLock};
use rand::Rng;
use uuid::Uuid;
use chrono::Utc;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum RolloutStrategy {
    AllUsers,
    PercentageRollout(f64),
    UserGroups(Vec<String>),
    CanaryGroup(String, f64),
    DeviceTypes(Vec<String>),
    DateRange { start: i64, end: Option<i64> },
    Expression(String),
}

impl std::fmt::Display for RolloutStrategy {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            RolloutStrategy::AllUsers => write!(f, "All Users"),
            RolloutStrategy::PercentageRollout(pct) => write!(f, "{}% of Users", pct * 100.0),
            RolloutStrategy::UserGroups(groups) => write!(f, "User Groups: {}", groups.join(", ")),
            RolloutStrategy::CanaryGroup(group, pct) => write!(f, "Canary Group: {} ({}%)", group, pct * 100.0),
            RolloutStrategy::DeviceTypes(devices) => write!(f, "Device Types: {}", devices.join(", ")),
            RolloutStrategy::DateRange { start, end } => {
                if let Some(end_date) = end {
                    write!(f, "Date Range: {} to {}", start, end_date)
                } else {
                    write!(f, "Date Range: {} onwards", start)
                }
            },
            RolloutStrategy::Expression(expr) => write!(f, "Custom: {}", expr),
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FeatureFlag {
    pub id: String,
    pub name: String,
    pub description: String,
    pub enabled: bool,
    pub rollout_strategy: RolloutStrategy,
    pub dependencies: Vec<String>,
    pub created_at: i64,
    pub updated_at: i64,
    pub metadata: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FeatureFlagConfig {
    pub client_id: String,
    pub user_id: Option<String>,
    pub user_groups: Vec<String>,
    pub device_type: String,
    pub canary_groups: HashMap<String, bool>,
    pub canary_percentage: f64,
    pub environment: String,
    pub version: String,
}

pub struct FeatureFlagManager {
    pub config: RwLock<FeatureFlagConfig>,
    pub flags: RwLock<HashMap<String, FeatureFlag>>,
}

impl FeatureFlagManager {
    pub fn new(config: FeatureFlagConfig) -> Self {
        Self {
            config: RwLock::new(config),
            flags: RwLock::new(HashMap::new()),
        }
    }
    
    pub fn load_flags(&self, flags: Vec<FeatureFlag>) {
        let mut flag_map = self.flags.write().unwrap();
        *flag_map = flags.into_iter().map(|flag| (flag.id.clone(), flag)).collect();
    }
    
    pub fn is_enabled(&self, flag_id: &str) -> bool {
        let flags = self.flags.read().unwrap();
        let config = self.config.read().unwrap();
        
        if let Some(flag) = flags.get(flag_id) {
            if !flag.enabled {
                return false;
            }
            
            // Check dependencies
            for dep_id in &flag.dependencies {
                if !self.is_enabled(dep_id) {
                    return false;
                }
            }
            
            // Check rollout strategy
            match &flag.rollout_strategy {
                RolloutStrategy::AllUsers => true,
                
                RolloutStrategy::PercentageRollout(percentage) => {
                    // Use client ID as seed for deterministic randomness
                    let mut hasher = std::collections::hash_map::DefaultHasher::new();
                    std::hash::Hash::hash_slice(config.client_id.as_bytes(), &mut hasher);
                    std::hash::Hash::hash_slice(flag_id.as_bytes(), &mut hasher);
                    let hash = std::hash::Hasher::finish(&hasher);
                    
                    // Convert to 0-1 range
                    let normalized = (hash as f64) / (u64::MAX as f64);
                    
                    normalized < *percentage
                },
                
                RolloutStrategy::UserGroups(groups) => {
                    if let Some(user_id) = &config.user_id {
                        for group in groups {
                            if config.user_groups.contains(group) {
                                return true;
                            }
                        }
                    }
                    false
                },
                
                RolloutStrategy::CanaryGroup(group_name, percentage) => {
                    if let Some(is_in_group) = config.canary_groups.get(group_name) {
                        if *is_in_group {
                            // Use client ID as seed for deterministic randomness
                            let mut hasher = std::collections::hash_map::DefaultHasher::new();
                            std::hash::Hash::hash_slice(config.client_id.as_bytes(), &mut hasher);
                            std::hash::Hash::hash_slice(flag_id.as_bytes(), &mut hasher);
                            let hash = std::hash::Hasher::finish(&hasher);
                            
                            // Convert to 0-1 range
                            let normalized = (hash as f64) / (u64::MAX as f64);
                            
                            return normalized < *percentage;
                        }
                    }
                    false
                },
                
                RolloutStrategy::DeviceTypes(device_types) => {
                    device_types.contains(&config.device_type)
                },
                
                RolloutStrategy::DateRange { start, end } => {
                    let now = Utc::now().timestamp();
                    now >= *start && (end.is_none() || now <= end.unwrap())
                },
                
                RolloutStrategy::Expression(expr) => {
                    // In a real implementation, this would evaluate a condition expression
                    // For this example, we'll just return false
                    false
                },
            }
        } else {
            false
        }
    }
    
    pub fn get_flag(&self, flag_id: &str) -> Option<FeatureFlag> {
        let flags = self.flags.read().unwrap();
        flags.get(flag_id).cloned()
    }
    
    pub fn update_config(&self, new_config: FeatureFlagConfig) {
        let mut config = self.config.write().unwrap();
        *config = new_config;
    }
    
    pub fn get_enabled_flags(&self) -> Vec<FeatureFlag> {
        let flags = self.flags.read().unwrap();
        flags.values()
            .filter(|flag| self.is_enabled(&flag.id))
            .cloned()
            .collect()
    }
    
    pub fn opt_into_canary_group(&self, group_name: &str) {
        let mut config = self.config.write().unwrap();
        config.canary_groups.insert(group_name.to_string(), true);
    }
    
    pub fn opt_out_of_canary_group(&self, group_name: &str) {
        let mut config = self.config.write().unwrap();
        config.canary_groups.insert(group_name.to_string(), false);
    }
    
    pub fn set_canary_percentage(&self, percentage: f64) {
        let mut config = self.config.write().unwrap();
        config.canary_percentage = percentage.clamp(0.0, 1.0);
    }
    
    pub fn create_flag(&self, name: &str, description: &str, 
                       rollout_strategy: RolloutStrategy, 
                       dependencies: Vec<String>) -> FeatureFlag {
        let flag = FeatureFlag {
            id: Uuid::new_v4().to_string(),
            name: name.to_string(),
            description: description.to_string(),
            enabled: true,
            rollout_strategy,
            dependencies,
            created_at: Utc::now().timestamp(),
            updated_at: Utc::now().timestamp(),
            metadata: HashMap::new(),
        };
        
        let mut flags = self.flags.write().unwrap();
        flags.insert(flag.id.clone(), flag.clone());
        
        flag
    }
    
    pub fn update_flag(&self, flag: FeatureFlag) -> Result<(), String> {
        let mut flags = self.flags.write().unwrap();
        
        if !flags.contains_key(&flag.id) {
            return Err(format!("Flag with ID {} not found", flag.id));
        }
        
        flags.insert(flag.id.clone(), flag);
        Ok(())
    }
    
    pub fn delete_flag(&self, flag_id: &str) -> Result<(), String> {
        let mut flags = self.flags.write().unwrap();
        
        if !flags.contains_key(flag_id) {
            return Err(format!("Flag with ID {} not found", flag_id));
        }
        
        flags.remove(flag_id);
        Ok(())
    }
    
    pub fn toggle_flag(&self, flag_id: &str, enabled: bool) -> Result<(), String> {
        let mut flags = self.flags.write().unwrap();
        
        if let Some(flag) = flags.get_mut(flag_id) {
            flag.enabled = enabled;
            flag.updated_at = Utc::now().timestamp();
            Ok(())
        } else {
            Err(format!("Flag with ID {} not found", flag_id))
        }
    }
    
    pub fn get_all_flags(&self) -> Vec<FeatureFlag> {
        let flags = self.flags.read().unwrap();
        flags.values().cloned().collect()
    }
}

// Create a global instance
lazy_static::lazy_static! {
    pub static ref FEATURE_FLAG_MANAGER: Arc<FeatureFlagManager> = {
        // Create default config
        let config = FeatureFlagConfig {
            client_id: Uuid::new_v4().to_string(),
            user_id: None,
            user_groups: Vec::new(),
            device_type: detect_device_type(),
            canary_groups: HashMap::new(),
            canary_percentage: 0.0, // By default, not in canary program
            environment: "production".to_string(),
            version: env!("CARGO_PKG_VERSION").to_string(),
        };
        
        Arc::new(FeatureFlagManager::new(config))
    };
}

fn detect_device_type() -> String {
    #[cfg(target_os = "windows")]
    return "windows".to_string();
    
    #[cfg(target_os = "macos")]
    return "macos".to_string();
    
    #[cfg(target_os = "linux")]
    return "linux".to_string();
    
    #[cfg(not(any(target_os = "windows", target_os = "macos", target_os = "linux")))]
    return "unknown".to_string();
}

// Feature flag check macro
#[macro_export]
macro_rules! feature_enabled {
    ($flag_id:expr) => {
        crate::feature_flags::FEATURE_FLAG_MANAGER.is_enabled($flag_id)
    };
}

// Predefined canary groups
pub const CANARY_GROUP_ALPHA: &str = "alpha";
pub const CANARY_GROUP_BETA: &str = "beta";
pub const CANARY_GROUP_EARLY_ACCESS: &str = "early_access";
pub const CANARY_GROUP_INTERNAL: &str = "internal";

// Feature flag IDs for observability features
pub const FLAG_ADVANCED_TELEMETRY: &str = "advanced_telemetry";
pub const FLAG_PERFORMANCE_DASHBOARD: &str = "performance_dashboard";
pub const FLAG_DEBUG_LOGGING: &str = "debug_logging";
pub const FLAG_RESOURCE_MONITORING: &str = "resource_monitoring";
pub const FLAG_CRASH_REPORTING: &str = "crash_reporting";

// Tauri commands
#[cfg(feature = "tauri")]
#[tauri::command]
pub fn get_feature_flags() -> Vec<FeatureFlag> {
    FEATURE_FLAG_MANAGER.get_all_flags()
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn toggle_feature_flag(flag_id: String, enabled: bool) -> Result<(), String> {
    FEATURE_FLAG_MANAGER.toggle_flag(&flag_id, enabled)
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn create_feature_flag(
    name: String, 
    description: String, 
    rollout_strategy: RolloutStrategy, 
    dependencies: Option<Vec<String>>
) -> FeatureFlag {
    FEATURE_FLAG_MANAGER.create_flag(
        &name, 
        &description, 
        rollout_strategy, 
        dependencies.unwrap_or_default()
    )
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn delete_feature_flag(flag_id: String) -> Result<(), String> {
    FEATURE_FLAG_MANAGER.delete_flag(&flag_id)
}
</file>

<file path="src-common/src/lib.rs">
pub mod config;
pub mod error;
pub mod models;
pub mod protocol;
pub mod service;
pub mod utils;

use once_cell::sync::OnceCell;
use std::sync::Arc;

use crate::service::mcp::McpService;

/// Global MCP service instance
static MCP_SERVICE: OnceCell<Arc<McpService>> = OnceCell::new();

/// Initialize the MCP service
pub fn init_mcp_service() -> Arc<McpService> {
    // Create a shared MCP service instance
    let service = Arc::new(McpService::new());
    
    // Store in global cell if not already set
    if let Err(_) = MCP_SERVICE.set(service.clone()) {
        // Already initialized, just return the new instance
    }
    
    service
}

/// Get the global MCP service instance
pub fn get_mcp_service() -> Arc<McpService> {
    MCP_SERVICE.get_or_init(|| Arc::new(McpService::new())).clone()
}
</file>

<file path="src-common/src/models/conversation.rs">
use serde::{Deserialize, Serialize};
use std::time::{Duration, SystemTime};
use uuid::Uuid;

use super::model::Model;
use super::message::Message;

/// Represents a conversation with a model
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Conversation {
    /// Unique conversation identifier
    pub id: String,
    
    /// User-friendly title
    pub title: String,
    
    /// When the conversation was created
    pub created_at: SystemTime,
    
    /// When the conversation was last modified
    pub updated_at: SystemTime,
    
    /// Model used for this conversation
    pub model: Model,
    
    /// Conversation metadata
    pub metadata: serde_json::Value,
    
    /// Messages in this conversation
    #[serde(default)]
    pub messages: Vec<Message>,
}

/// Implementation for Conversation
impl Conversation {
    /// Create a new conversation
    pub fn new(title: impl Into<String>, model: Model) -> Self {
        let now = SystemTime::now();
        Self {
            id: Uuid::new_v4().to_string(),
            title: title.into(),
            created_at: now,
            updated_at: now,
            model,
            metadata: serde_json::Value::Object(serde_json::Map::new()),
            messages: Vec::new(),
        }
    }
    
    /// Set conversation title
    pub fn set_title(&mut self, title: impl Into<String>) {
        self.title = title.into();
        self.updated_at = SystemTime::now();
    }
    
    /// Add a message to the conversation
    pub fn add_message(&mut self, message: Message) {
        self.messages.push(message);
        self.updated_at = SystemTime::now();
    }
    
    /// Calculate conversation age
    pub fn age(&self) -> Duration {
        SystemTime::now()
            .duration_since(self.created_at)
            .unwrap_or(Duration::from_secs(0))
    }
    
    /// Get a summary of the conversation
    pub fn summary(&self) -> String {
        let msg_count = self.messages.len();
        let last_update = chrono::DateTime::<chrono::Local>::from(self.updated_at)
            .format("%Y-%m-%d %H:%M")
            .to_string();
            
        format!(
            "{} ({}, {} messages, updated {})",
            self.title,
            self.model.name,
            msg_count,
            last_update
        )
    }
}
</file>

<file path="src-common/src/models/message.rs">
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::time::{Duration, SystemTime};
use thiserror::Error;
use uuid::Uuid;

use super::tool::ToolCall;

/// Message role
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum MessageRole {
    User,
    Assistant,
    System,
}

/// Message content type
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(tag = "type", content = "value")]
pub enum ContentType {
    Text { text: String },
    Image { url: String, alt_text: Option<String> },
    ToolCalls { calls: Vec<ToolCall> },
    ToolResults { results: Vec<serde_json::Value> },
}

/// Message content
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct MessageContent {
    pub parts: Vec<ContentType>,
}

/// Message structure
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Message {
    /// Unique message identifier
    pub id: String,
    
    /// Role (user, assistant, system)
    pub role: MessageRole,
    
    /// Content of the message
    pub content: MessageContent,
    
    /// Optional metadata
    #[serde(skip_serializing_if = "Option::is_none")]
    pub metadata: Option<HashMap<String, serde_json::Value>>,
    
    /// When the message was created
    pub created_at: SystemTime,
}

/// Message error types
#[derive(Debug, Error)]
pub enum MessageError {
    #[error("Message sending timed out after {0:?}")]
    Timeout(Duration),
    
    #[error("Network error: {0}")]
    Network(String),
    
    #[error("Authentication error: {0}")]
    Auth(String),
    
    #[error("Rate limited: {0}")]
    RateLimit(String),
    
    #[error("Bad request: {0}")]
    BadRequest(String),
    
    #[error("Unknown error: {0}")]
    Unknown(String),
}

impl Message {
    /// Create a new text message from a user
    pub fn user(text: impl Into<String>) -> Self {
        Self {
            id: Uuid::new_v4().to_string(),
            role: MessageRole::User,
            content: MessageContent {
                parts: vec![ContentType::Text { text: text.into() }],
            },
            metadata: None,
            created_at: SystemTime::now(),
        }
    }
    
    /// Create a new text message from the assistant
    pub fn assistant(text: impl Into<String>) -> Self {
        Self {
            id: Uuid::new_v4().to_string(),
            role: MessageRole::Assistant,
            content: MessageContent {
                parts: vec![ContentType::Text { text: text.into() }],
            },
            metadata: None,
            created_at: SystemTime::now(),
        }
    }
    
    /// Create a new system message
    pub fn system(text: impl Into<String>) -> Self {
        Self {
            id: Uuid::new_v4().to_string(),
            role: MessageRole::System,
            content: MessageContent {
                parts: vec![ContentType::Text { text: text.into() }],
            },
            metadata: None,
            created_at: SystemTime::now(),
        }
    }
    
    /// Get the text content of the message
    pub fn text(&self) -> String {
        let mut result = String::new();
        
        for part in &self.content.parts {
            if let ContentType::Text { text } = part {
                result.push_str(text);
            }
        }
        
        result
    }
    
    /// Get a formatted timestamp for the message
    pub fn timestamp(&self) -> String {
        chrono::DateTime::<chrono::Local>::from(self.created_at)
            .format("%H:%M:%S")
            .to_string()
    }
    
    /// Check if this message has tool calls
    pub fn has_tool_calls(&self) -> bool {
        self.content.parts.iter().any(|part| {
            if let ContentType::ToolCalls { .. } = part {
                true
            } else {
                false
            }
        })
    }
}
</file>

<file path="src-common/src/models/mod.rs">
pub mod conversation;
pub mod message;
pub mod model;
pub mod tool;

pub use conversation::Conversation;
pub use message::{Message, MessageContent, MessageError, MessageRole};
pub use model::{Model, ModelCapabilities};
pub use tool::{Tool, ToolCall, ToolResult};
</file>

<file path="src-common/src/models/model.rs">
use serde::{Deserialize, Serialize};

/// Information about a model
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub struct Model {
    /// Model identifier (e.g., "claude-3-opus-20240229")
    pub id: String,
    
    /// Provider name (e.g., "anthropic")
    pub provider: String,
    
    /// User-friendly name (e.g., "Claude 3 Opus")
    pub name: String,
    
    /// Model version
    pub version: String,
    
    /// Model capabilities
    pub capabilities: ModelCapabilities,
}

/// Model capabilities
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub struct ModelCapabilities {
    /// Can process images
    pub vision: bool,
    
    /// Maximum context length
    pub max_context_length: usize,
    
    /// Supports functions/tools
    pub functions: bool,
    
    /// Supports streamed responses
    pub streaming: bool,
}

/// Implementation for Model
impl Model {
    /// Create a new Claude model
    pub fn claude(variant: &str, version: &str) -> Self {
        let (name, display_name) = match variant {
            "opus" => ("claude-3-opus", "Claude 3 Opus"),
            "sonnet" => ("claude-3-sonnet", "Claude 3 Sonnet"),
            "haiku" => ("claude-3-haiku", "Claude 3 Haiku"),
            _ => (format!("claude-3-{}", variant).as_str(), format!("Claude 3 {}", variant.to_string()).as_str()),
        };
        
        let capabilities = match variant {
            "opus" => ModelCapabilities {
                vision: true,
                max_context_length: 200_000,
                functions: true,
                streaming: true,
            },
            "sonnet" => ModelCapabilities {
                vision: true,
                max_context_length: 180_000,
                functions: true,
                streaming: true,
            },
            "haiku" => ModelCapabilities {
                vision: true,
                max_context_length: 150_000,
                functions: true,
                streaming: true,
            },
            _ => ModelCapabilities {
                vision: false,
                max_context_length: 100_000,
                functions: false,
                streaming: true,
            },
        };
        
        Self {
            id: format!("{}-{}", name, version),
            provider: "anthropic".to_string(),
            name: display_name.to_string(),
            version: version.to_string(),
            capabilities,
        }
    }
    
    /// Get all available Claude models
    pub fn available_claude_models() -> Vec<Self> {
        vec![
            Self::claude("opus", "20240229"),
            Self::claude("sonnet", "20240229"),
            Self::claude("haiku", "20240307"),
        ]
    }
    
    /// Get default Claude model
    pub fn default_claude() -> Self {
        Self::claude("sonnet", "20240229")
    }
}

impl Default for Model {
    fn default() -> Self {
        Self::default_claude()
    }
}
</file>

<file path="src-common/src/models/tool.rs">
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

/// Tool definition
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Tool {
    /// Tool name
    pub name: String,
    
    /// Tool description
    pub description: String,
    
    /// Schema in JSON Schema format
    pub schema: serde_json::Value,
}

/// Tool call
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ToolCall {
    /// Tool call ID
    pub id: String,
    
    /// Tool name
    pub name: String,
    
    /// Tool arguments
    pub arguments: serde_json::Value,
}

/// Tool result
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ToolResult {
    /// Tool call ID
    pub tool_call_id: String,
    
    /// Tool name
    pub name: String,
    
    /// Tool result
    pub result: serde_json::Value,
}

impl Tool {
    /// Create a new tool definition
    pub fn new(name: impl Into<String>, description: impl Into<String>, schema: serde_json::Value) -> Self {
        Self {
            name: name.into(),
            description: description.into(),
            schema,
        }
    }
    
    /// Create a simple function tool with string input and output
    pub fn simple_function(name: impl Into<String>, description: impl Into<String>) -> Self {
        let schema = serde_json::json!({
            "type": "object",
            "properties": {
                "input": {
                    "type": "string",
                    "description": "The input to the function"
                }
            },
            "required": ["input"]
        });
        
        Self::new(name, description, schema)
    }
    
    /// Get common tools
    pub fn common_tools() -> Vec<Self> {
        vec![
            Self::simple_function(
                "web_search",
                "Search the web for information"
            ),
            Self::simple_function(
                "calculator",
                "Perform mathematical calculations"
            ),
            Self::simple_function(
                "file_read",
                "Read the contents of a file"
            ),
            Self::simple_function(
                "file_write",
                "Write content to a file"
            ),
        ]
    }
}

impl ToolCall {
    /// Create a new tool call
    pub fn new(id: impl Into<String>, name: impl Into<String>, arguments: serde_json::Value) -> Self {
        Self {
            id: id.into(),
            name: name.into(),
            arguments,
        }
    }
}

impl ToolResult {
    /// Create a new tool result
    pub fn new(tool_call_id: impl Into<String>, name: impl Into<String>, result: serde_json::Value) -> Self {
        Self {
            tool_call_id: tool_call_id.into(),
            name: name.into(),
            result,
        }
    }
    
    /// Create a success result
    pub fn success(tool_call_id: impl Into<String>, name: impl Into<String>, data: impl Into<String>) -> Self {
        Self::new(
            tool_call_id,
            name,
            serde_json::json!({
                "status": "success",
                "data": data.into()
            })
        )
    }
    
    /// Create an error result
    pub fn error(tool_call_id: impl Into<String>, name: impl Into<String>, message: impl Into<String>) -> Self {
        Self::new(
            tool_call_id,
            name,
            serde_json::json!({
                "status": "error",
                "message": message.into()
            })
        )
    }
}
</file>

<file path="src-common/src/observability/mod.rs">
// Observability module for the MCP client
//
// This module contains components for monitoring, logging, telemetry,
// and canary release management.

pub mod metrics;
pub mod logging;
pub mod telemetry;
pub mod canary;

use serde::{Serialize, Deserialize};

// Shared configuration types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ObservabilityConfig {
    pub enabled: bool,
    pub telemetry: TelemetryConfig,
    pub logging: LoggingConfig,
    pub metrics: MetricsConfig,
    pub canary_release: CanaryConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryConfig {
    pub enabled: bool,
    pub server_url: String,
    pub client_id: Option<String>,
    pub batch_interval_seconds: u64,
    pub collection_categories: Vec<String>,
    pub privacy_level: PrivacyLevel,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LoggingConfig {
    pub min_level: logging::LogLevel,
    pub console_enabled: bool,
    pub file_enabled: bool,
    pub max_file_size_mb: u64,
    pub max_files: u32,
    pub log_dir: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricsConfig {
    pub enabled: bool,
    pub sampling_rate: f64,
    pub collection_interval_seconds: u64,
    pub buffer_size: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CanaryConfig {
    pub enabled: bool,
    pub group: Option<String>,
    pub opt_in_percentage: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum PrivacyLevel {
    Minimal,    // Only essential data (errors, crashes)
    Basic,      // Basic usage data (features used, performance metrics)
    Standard,   // More detailed usage and performance data
    Extended,   // Detailed telemetry including user flows and patterns
}

impl Default for ObservabilityConfig {
    fn default() -> Self {
        Self {
            enabled: true,
            telemetry: TelemetryConfig {
                enabled: false, // Disabled by default, requires opt-in
                server_url: "https://telemetry.mcp-client.example.com/v1/telemetry".to_string(),
                client_id: None, // Will be generated on first run
                batch_interval_seconds: 300, // 5 minutes
                collection_categories: vec![
                    "errors".to_string(),
                    "performance".to_string(),
                ],
                privacy_level: PrivacyLevel::Basic,
            },
            logging: LoggingConfig {
                min_level: logging::LogLevel::Info,
                console_enabled: true,
                file_enabled: true,
                max_file_size_mb: 10,
                max_files: 5,
                log_dir: None, // Will be set to default app data directory
            },
            metrics: MetricsConfig {
                enabled: true,
                sampling_rate: 0.1, // 10% sampling
                collection_interval_seconds: 60,
                buffer_size: 100,
            },
            canary_release: CanaryConfig {
                enabled: true,
                group: None, // No canary group by default
                opt_in_percentage: 0.05, // 5% of users will be asked to opt in
            },
        }
    }
}

// Initialize all observability systems
pub fn init(config: &ObservabilityConfig) {
    // Initialize logging
    let logging_config = metrics::ObservabilityConfig {
        metrics_enabled: config.metrics.enabled,
        sampling_rate: config.metrics.sampling_rate,
        buffer_size: config.metrics.buffer_size,
        min_log_level: Some(config.logging.min_level as u8),
        log_file_path: config.logging.log_dir.clone(),
        console_logging: Some(config.logging.console_enabled),
        telemetry_enabled: Some(config.telemetry.enabled),
        log_telemetry: Some(config.logging.file_enabled),
    };
    
    // Initialize logging first for other components to use
    logging::init_logger(&logging_config);
    
    // Initialize metrics
    metrics::init_metrics(&logging_config);
    
    // Log initialization
    log_info!("observability", "Observability systems initialized");
}
</file>

<file path="src-common/src/protocol/mcp.rs">
use async_trait::async_trait;
use log::{debug, error, info, warn};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::{mpsc, Mutex, RwLock};
use tokio_tungstenite::tungstenite::protocol::Message as WsMessage;
use uuid::Uuid;

use super::{ConnectionStatus, ProtocolConfig, ProtocolHandler, WebSocketClient, WebSocketConfig};
use crate::error::{McpError, McpResult};
use crate::models::{ContentType, Message, MessageContent, MessageRole};

/// MCP message types
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
#[serde(rename_all = "snake_case")]
pub enum McpMessageType {
    AuthRequest,
    AuthResponse,
    CompletionRequest,
    CompletionResponse,
    StreamingStart,
    StreamingMessage,
    StreamingEnd,
    CancelStream,
    Ping,
    Pong,
    Error,
}

/// MCP message structure
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct McpMessage {
    /// Message ID
    pub id: String,
    
    /// Protocol version
    pub version: String,
    
    /// Message type
    #[serde(rename = "type")]
    pub message_type: McpMessageType,
    
    /// Message payload
    pub payload: serde_json::Value,
}

/// MCP protocol configuration
#[derive(Debug, Clone)]
pub struct McpConfig {
    /// API key for authentication
    pub api_key: String,
    
    /// Server URL
    pub url: String,
    
    /// Protocol version
    pub version: String,
    
    /// Default model ID
    pub model: String,
}

/// MCP client
pub struct McpClient {
    /// Configuration
    config: McpConfig,
    
    /// WebSocket client
    ws_client: Arc<WebSocketClient>,
    
    /// Connection status
    status: Arc<RwLock<ConnectionStatus>>,
    
    /// Active streaming sessions
    streaming_sessions: Arc<Mutex<HashMap<String, mpsc::Sender<Message>>>>,
}

/// MCP protocol handler implementation
pub struct McpProtocolHandler {
    /// MCP client
    client: Arc<McpClient>,
    
    /// Shared connection status
    status: Arc<RwLock<ConnectionStatus>>,
}

impl McpMessage {
    /// Create a new MCP message
    pub fn new(message_type: McpMessageType, payload: serde_json::Value) -> Self {
        Self {
            id: Uuid::new_v4().to_string(),
            version: "v1".to_string(),
            message_type,
            payload,
        }
    }
    
    /// Create an authentication request message
    pub fn auth_request(api_key: &str) -> Self {
        Self::new(
            McpMessageType::AuthRequest,
            serde_json::json!({
                "api_key": api_key,
            }),
        )
    }
    
    /// Create a completion request message
    pub fn completion_request(
        model: &str,
        messages: &[Message],
        max_tokens: u32,
        temperature: f32,
        stream: bool,
    ) -> Self {
        // Convert messages to MCP format
        let mcp_messages = messages
            .iter()
            .map(|msg| {
                let content = msg.content.parts.iter().map(|part| {
                    match part {
                        ContentType::Text { text } => {
                            serde_json::json!({
                                "type": "text",
                                "text": text
                            })
                        }
                        ContentType::Image { url, alt_text } => {
                            serde_json::json!({
                                "type": "image",
                                "source": {
                                    "type": "url",
                                    "url": url
                                },
                                "alt_text": alt_text
                            })
                        }
                        _ => serde_json::json!(null),
                    }
                }).collect::<Vec<_>>();
                
                serde_json::json!({
                    "role": match msg.role {
                        MessageRole::User => "user",
                        MessageRole::Assistant => "assistant",
                        MessageRole::System => "system",
                    },
                    "content": content
                })
            })
            .collect::<Vec<_>>();
        
        Self::new(
            McpMessageType::CompletionRequest,
            serde_json::json!({
                "model": model,
                "messages": mcp_messages,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "stream": stream,
            }),
        )
    }
    
    /// Create a cancel stream message
    pub fn cancel_stream(stream_id: &str) -> Self {
        Self::new(
            McpMessageType::CancelStream,
            serde_json::json!({
                "stream_id": stream_id,
            }),
        )
    }
    
    /// Create a ping message
    pub fn ping() -> Self {
        Self::new(McpMessageType::Ping, serde_json::json!({}))
    }
}

impl McpConfig {
    /// Create a new MCP configuration with an API key
    pub fn with_api_key(api_key: String) -> Self {
        Self {
            api_key,
            url: "wss://api.anthropic.com/v1/messages".to_string(),
            version: "v1".to_string(),
            model: "claude-3-sonnet-20240229".to_string(),
        }
    }
    
    /// Set the server URL
    pub fn with_url(mut self, url: String) -> Self {
        self.url = url;
        self
    }
    
    /// Set the protocol version
    pub fn with_version(mut self, version: String) -> Self {
        self.version = version;
        self
    }
    
    /// Set the default model
    pub fn with_model(mut self, model: String) -> Self {
        self.model = model;
        self
    }
}

impl McpClient {
    /// Create a new MCP client
    pub fn new(config: McpConfig) -> Self {
        // Create websocket configuration
        let ws_config = WebSocketConfig {
            url: config.url.clone(),
            headers: vec![
                ("X-API-Key".to_string(), config.api_key.clone()),
                ("Content-Type".to_string(), "application/json".to_string()),
                ("Accept".to_string(), "application/json".to_string()),
            ],
            ..Default::default()
        };
        
        // Create websocket client
        let ws_client = Arc::new(WebSocketClient::new(ws_config));
        
        // Create MCP client
        Self {
            config,
            ws_client,
            status: Arc::new(RwLock::new(ConnectionStatus::Disconnected)),
            streaming_sessions: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    /// Get the current connection status
    pub fn connection_status(&self) -> ConnectionStatus {
        self.ws_client.status()
    }
    
    /// Connect to the MCP server
    pub async fn connect(&self) -> McpResult<()> {
        // Connect WebSocket
        self.ws_client.connect().await?;
        
        // Send authentication message
        let auth_message = McpMessage::auth_request(&self.config.api_key);
        self.send_message(&auth_message).await?;
        
        // Wait for authentication response
        let response = self.receive_message().await?;
        
        if response.message_type == McpMessageType::AuthResponse {
            // Check if authentication was successful
            if let Some(success) = response.payload.get("success") {
                if success.as_bool().unwrap_or(false) {
                    *self.status.write().await = ConnectionStatus::Connected;
                    Ok(())
                } else {
                    *self.status.write().await = ConnectionStatus::AuthFailed;
                    Err(McpError::Authentication("Authentication failed".to_string()))
                }
            } else {
                *self.status.write().await = ConnectionStatus::Error("Invalid auth response".to_string());
                Err(McpError::Protocol("Invalid authentication response".to_string()))
            }
        } else if response.message_type == McpMessageType::Error {
            // Authentication error
            *self.status.write().await = ConnectionStatus::AuthFailed;
            Err(McpError::Authentication(
                response
                    .payload
                    .get("message")
                    .and_then(|m| m.as_str())
                    .unwrap_or("Authentication failed")
                    .to_string(),
            ))
        } else {
            // Unexpected response
            *self.status.write().await = ConnectionStatus::Error("Unexpected response".to_string());
            Err(McpError::Protocol("Unexpected response type".to_string()))
        }
    }
    
    /// Disconnect from the MCP server
    pub async fn disconnect(&self) -> McpResult<()> {
        self.ws_client.disconnect().await?;
        *self.status.write().await = ConnectionStatus::Disconnected;
        Ok(())
    }
    
    /// Send an MCP message
    pub async fn send_message(&self, message: &McpMessage) -> McpResult<()> {
        // Serialize message
        let json = serde_json::to_string(message)
            .map_err(|e| McpError::Serialization(e))?;
            
        // Send via websocket
        self.ws_client
            .send(WsMessage::Text(json))
            .await
            .map_err(|e| McpError::Protocol(format!("Failed to send message: {}", e)))
    }
    
    /// Receive an MCP message with timeout
    pub async fn receive_message(&self) -> McpResult<McpMessage> {
        // Receive from websocket with timeout
        let message = self
            .ws_client
            .receive(Duration::from_secs(60))
            .await
            .map_err(|e| McpError::Protocol(format!("Failed to receive message: {}", e)))?;
            
        // Parse message
        if let WsMessage::Text(text) = message {
            let mcp_message: McpMessage = serde_json::from_str(&text)
                .map_err(|e| McpError::Serialization(e))?;
                
            Ok(mcp_message)
        } else {
            Err(McpError::Protocol("Unexpected message type".to_string()))
        }
    }
    
    /// Send a completion request
    pub async fn send_completion(
        &self,
        model: &str,
        messages: &[Message],
        max_tokens: u32,
        temperature: f32,
    ) -> McpResult<Message> {
        // Check if connected
        if !matches!(self.connection_status(), ConnectionStatus::Connected) {
            return Err(McpError::Connection("Not connected".to_string()));
        }
        
        // Create completion request
        let request = McpMessage::completion_request(
            model,
            messages,
            max_tokens,
            temperature,
            false, // No streaming
        );
        
        // Send request
        self.send_message(&request).await?;
        
        // Wait for response
        let response = self.receive_message().await?;
        
        if response.message_type == McpMessageType::CompletionResponse {
            // Parse response
            let content = response
                .payload
                .get("content")
                .ok_or_else(|| McpError::Protocol("Missing content in response".to_string()))?;
                
            // Convert to Message format
            let message = Message {
                id: response.id,
                role: MessageRole::Assistant,
                content: MessageContent {
                    parts: vec![ContentType::Text {
                        text: content
                            .as_str()
                            .ok_or_else(|| {
                                McpError::Protocol("Invalid content type in response".to_string())
                            })?
                            .to_string(),
                    }],
                },
                metadata: None,
                created_at: std::time::SystemTime::now(),
            };
            
            Ok(message)
        } else if response.message_type == McpMessageType::Error {
            // Error response
            Err(McpError::Protocol(
                response
                    .payload
                    .get("message")
                    .and_then(|m| m.as_str())
                    .unwrap_or("Unknown error")
                    .to_string(),
            ))
        } else {
            // Unexpected response
            Err(McpError::Protocol("Unexpected response type".to_string()))
        }
    }
    
    /// Start a streaming completion request
    pub async fn stream_completion(
        &self,
        model: &str,
        messages: &[Message],
        max_tokens: u32,
        temperature: f32,
    ) -> McpResult<mpsc::Receiver<Message>> {
        // Check if connected
        if !matches!(self.connection_status(), ConnectionStatus::Connected) {
            return Err(McpError::Connection("Not connected".to_string()));
        }
        
        // Create completion request
        let request = McpMessage::completion_request(
            model,
            messages,
            max_tokens,
            temperature,
            true, // Enable streaming
        );
        
        // Create channel for streaming
        let (tx, rx) = mpsc::channel::<Message>(32);
        
        // Store streaming session
        {
            let mut sessions = self.streaming_sessions.lock().await;
            sessions.insert(request.id.clone(), tx.clone());
        }
        
        // Send request
        self.send_message(&request).await?;
        
        // Start streaming task
        let client_clone = Arc::new(self.clone());
        let request_id = request.id.clone();
        
        tokio::spawn(async move {
            // Process streaming messages
            loop {
                match client_clone.receive_message().await {
                    Ok(message) => {
                        match message.message_type {
                            McpMessageType::StreamingStart => {
                                // Stream started - just log it
                                debug!("Streaming started for {}", request_id);
                            }
                            McpMessageType::StreamingMessage => {
                                // Process streaming message
                                if let Some(content) = message.payload.get("content") {
                                    if let Some(text) = content.as_str() {
                                        // Create message
                                        let chunk = Message {
                                            id: request_id.clone(),
                                            role: MessageRole::Assistant,
                                            content: MessageContent {
                                                parts: vec![ContentType::Text {
                                                    text: text.to_string(),
                                                }],
                                            },
                                            metadata: None,
                                            created_at: std::time::SystemTime::now(),
                                        };
                                        
                                        // Send to receiver
                                        if tx.send(chunk).await.is_err() {
                                            // Receiver dropped, stop streaming
                                            break;
                                        }
                                    }
                                }
                            }
                            McpMessageType::StreamingEnd => {
                                // Stream ended
                                debug!("Streaming ended for {}", request_id);
                                break;
                            }
                            McpMessageType::Error => {
                                // Error occurred
                                error!(
                                    "Streaming error: {}",
                                    message
                                        .payload
                                        .get("message")
                                        .and_then(|m| m.as_str())
                                        .unwrap_or("Unknown error")
                                );
                                break;
                            }
                            _ => {
                                // Ignore other message types
                            }
                        }
                    }
                    Err(e) => {
                        // Error receiving message
                        error!("Error receiving streaming message: {}", e);
                        break;
                    }
                }
            }
            
            // Remove streaming session
            let client = client_clone.as_ref();
            let mut sessions = client.streaming_sessions.lock().await;
            sessions.remove(&request_id);
        });
        
        Ok(rx)
    }
    
    /// Cancel a streaming completion request
    pub async fn cancel_streaming(&self, stream_id: &str) -> McpResult<()> {
        // Check if connected
        if !matches!(self.connection_status(), ConnectionStatus::Connected) {
            return Err(McpError::Connection("Not connected".to_string()));
        }
        
        // Create cancel message
        let cancel = McpMessage::cancel_stream(stream_id);
        
        // Send cancel message
        self.send_message(&cancel).await?;
        
        // Remove streaming session
        let mut sessions = self.streaming_sessions.lock().await;
        sessions.remove(stream_id);
        
        Ok(())
    }
}

impl Clone for McpClient {
    fn clone(&self) -> Self {
        Self {
            config: self.config.clone(),
            ws_client: self.ws_client.clone(),
            status: self.status.clone(),
            streaming_sessions: self.streaming_sessions.clone(),
        }
    }
}

impl McpProtocolHandler {
    /// Create a new MCP protocol handler
    pub fn new(config: McpConfig) -> Self {
        let client = Arc::new(McpClient::new(config));
        Self {
            client: client.clone(),
            status: client.status.clone(),
        }
    }
}

#[async_trait]
impl ProtocolHandler for McpProtocolHandler {
    fn protocol_name(&self) -> &'static str {
        "Model Context Protocol"
    }
    
    fn connection_status(&self) -> ConnectionStatus {
        self.client.connection_status()
    }
    
    async fn connect(&self) -> McpResult<()> {
        self.client.connect().await
    }
    
    async fn disconnect(&self) -> McpResult<()> {
        self.client.disconnect().await
    }
    
    async fn send_message(&self, message: Message) -> McpResult<()> {
        // Get model from message metadata or use default
        let model = message
            .metadata
            .as_ref()
            .and_then(|m| m.get("model"))
            .and_then(|m| m.as_str())
            .unwrap_or(&self.client.config.model)
            .to_string();
        
        // Get conversation history from message metadata or create new
        let history = message
            .metadata
            .as_ref()
            .and_then(|m| m.get("history"))
            .and_then(|m| m.as_array())
            .map(|h| {
                h.iter()
                    .filter_map(|m| serde_json::from_value::<Message>(m.clone()).ok())
                    .collect::<Vec<_>>()
            })
            .unwrap_or_else(|| vec![]);
        
        // Create message history including the new message
        let mut messages = history;
        messages.push(message.clone());
        
        // Send completion request
        let _response = self
            .client
            .send_completion(&model, &messages, 4096, 0.7)
            .await?;
        
        Ok(())
    }
    
    async fn receive_messages(&self) -> McpResult<Vec<Message>> {
        // This would normally process all pending messages
        // For now, just return an empty vector
        Ok(Vec::new())
    }
}

impl ProtocolConfig for McpConfig {
    fn validate(&self) -> McpResult<()> {
        if self.api_key.is_empty() {
            return Err(McpError::Config("API key is required".to_string()));
        }
        
        if self.url.is_empty() {
            return Err(McpError::Config("URL is required".to_string()));
        }
        
        if self.model.is_empty() {
            return Err(McpError::Config("Model ID is required".to_string()));
        }
        
        Ok(())
    }
}

impl Default for McpConfig {
    fn default() -> Self {
        Self {
            api_key: String::new(),
            url: "wss://api.anthropic.com/v1/messages".to_string(),
            version: "v1".to_string(),
            model: "claude-3-sonnet-20240229".to_string(),
        }
    }
}
</file>

<file path="src-common/src/protocol/mod.rs">
mod mcp;
mod websocket;

pub use mcp::{McpClient, McpConfig, McpMessage, McpMessageType};
pub use websocket::{ConnectionStatus, WebSocketClient};

use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::sync::Arc;

use crate::error::McpResult;
use crate::models::Message;

/// Base trait for all protocol handlers
#[async_trait]
pub trait ProtocolHandler: Send + Sync {
    /// Returns the protocol name
    fn protocol_name(&self) -> &'static str;
    
    /// Returns the current connection status
    fn connection_status(&self) -> ConnectionStatus;
    
    /// Establishes connection to the server
    async fn connect(&self) -> McpResult<()>;
    
    /// Disconnects from the server
    async fn disconnect(&self) -> McpResult<()>;
    
    /// Sends a message to the server
    async fn send_message(&self, message: Message) -> McpResult<()>;
    
    /// Receives messages from the server
    async fn receive_messages(&self) -> McpResult<Vec<Message>>;
    
    /// Checks if the handler is connected
    fn is_connected(&self) -> bool {
        matches!(self.connection_status(), ConnectionStatus::Connected)
    }
}

/// Factory for creating protocol handlers
pub trait ProtocolFactory: Send + Sync {
    /// Creates a new protocol handler instance
    fn create_handler(&self) -> Arc<dyn ProtocolHandler>;
    
    /// Returns the protocol name
    fn protocol_name(&self) -> &'static str;
    
    /// Returns the protocol description
    fn protocol_description(&self) -> &'static str;
}

/// Protocol configuration base trait
pub trait ProtocolConfig: Send + Sync {
    /// Validates the configuration
    fn validate(&self) -> McpResult<()>;
}
</file>

<file path="src-common/src/protocol/websocket.rs">
use async_trait::async_trait;
use futures::{SinkExt, StreamExt};
use log::{debug, error, info, warn};
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use std::time::Duration;
use tokio::net::TcpStream;
use tokio::sync::{mpsc, Mutex, RwLock};
use tokio::time::timeout;
use tokio_tungstenite::{
    connect_async, tungstenite::protocol::Message as WsMessage, MaybeTlsStream, WebSocketStream,
};
use url::Url;

use crate::error::{McpError, McpResult};

/// WebSocket connection status
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum ConnectionStatus {
    /// Not connected to any server
    Disconnected,
    
    /// Currently establishing connection
    Connecting,
    
    /// Connected and authenticated
    Connected,
    
    /// Connection established but authentication failed
    AuthFailed,
    
    /// Connection dropped or experiencing issues
    Error(String),
}

/// WebSocket connection configuration
#[derive(Clone, Debug)]
pub struct WebSocketConfig {
    /// Server URL
    pub url: String,
    
    /// Optional headers
    pub headers: Vec<(String, String)>,
    
    /// Timeout for connection
    pub connect_timeout: Duration,
    
    /// Heartbeat interval
    pub heartbeat_interval: Duration,
    
    /// Reconnection attempts
    pub max_reconnect_attempts: u32,
    
    /// Reconnection delay
    pub reconnect_delay: Duration,
}

/// WebSocket client
pub struct WebSocketClient {
    /// Configuration
    config: WebSocketConfig,
    
    /// Connection status
    status: Arc<RwLock<ConnectionStatus>>,
    
    /// Message sender channel
    sender: Arc<Mutex<Option<mpsc::Sender<WsMessage>>>>,
    
    /// Message receiver channel
    receiver: Arc<Mutex<mpsc::Receiver<WsMessage>>>,
    
    /// Message sender for the connection task
    connection_sender: mpsc::Sender<WsMessage>,
}

impl WebSocketClient {
    /// Create a new WebSocket client
    pub fn new(config: WebSocketConfig) -> Self {
        // Create channels for message passing
        let (connection_sender, connection_receiver) = mpsc::channel::<WsMessage>(32);
        let (message_sender, message_receiver) = mpsc::channel::<WsMessage>(32);
        
        let client = Self {
            config,
            status: Arc::new(RwLock::new(ConnectionStatus::Disconnected)),
            sender: Arc::new(Mutex::new(None)),
            receiver: Arc::new(Mutex::new(message_receiver)),
            connection_sender,
        };
        
        // Spawn task to handle messages
        let status_clone = client.status.clone();
        let sender_clone = client.sender.clone();
        let config_clone = client.config.clone();
        
        tokio::spawn(async move {
            Self::connection_task(
                status_clone,
                sender_clone,
                message_sender,
                connection_receiver,
                config_clone,
            )
            .await;
        });
        
        client
    }
    
    /// Get current connection status
    pub fn status(&self) -> ConnectionStatus {
        tokio::task::block_in_place(|| {
            tokio::runtime::Handle::current().block_on(async {
                self.status.read().await.clone()
            })
        })
    }
    
    /// Connect to the server
    pub async fn connect(&self) -> McpResult<()> {
        // Update status
        *self.status.write().await = ConnectionStatus::Connecting;
        
        // Send connect message to the connection task
        self.connection_sender
            .send(WsMessage::Text("CONNECT".to_string()))
            .await
            .map_err(|e| McpError::Connection(format!("Failed to send connect message: {}", e)))?;
            
        // Wait for connection to be established
        for _ in 0..10 {
            tokio::time::sleep(Duration::from_millis(100)).await;
            let status = self.status.read().await.clone();
            
            match status {
                ConnectionStatus::Connected => return Ok(()),
                ConnectionStatus::AuthFailed => {
                    return Err(McpError::Authentication("Authentication failed".to_string()));
                }
                ConnectionStatus::Error(e) => {
                    return Err(McpError::Connection(e));
                }
                _ => continue,
            }
        }
        
        Err(McpError::Connection("Connection timed out".to_string()))
    }
    
    /// Disconnect from the server
    pub async fn disconnect(&self) -> McpResult<()> {
        // Send disconnect message to the connection task
        self.connection_sender
            .send(WsMessage::Text("DISCONNECT".to_string()))
            .await
            .map_err(|e| McpError::Connection(format!("Failed to send disconnect message: {}", e)))?;
            
        // Update status
        *self.status.write().await = ConnectionStatus::Disconnected;
        
        Ok(())
    }
    
    /// Send a message to the server
    pub async fn send(&self, message: WsMessage) -> McpResult<()> {
        // Check if connected
        let status = self.status.read().await.clone();
        if status != ConnectionStatus::Connected {
            return Err(McpError::Connection("Not connected".to_string()));
        }
        
        // Get sender
        let sender = self.sender.lock().await.clone();
        if let Some(sender) = sender {
            // Send message
            sender
                .send(message)
                .await
                .map_err(|e| McpError::Connection(format!("Failed to send message: {}", e)))?;
                
            Ok(())
        } else {
            Err(McpError::Connection("No sender available".to_string()))
        }
    }
    
    /// Receive a message from the server with timeout
    pub async fn receive(&self, timeout_duration: Duration) -> McpResult<WsMessage> {
        // Check if connected
        let status = self.status.read().await.clone();
        if status != ConnectionStatus::Connected {
            return Err(McpError::Connection("Not connected".to_string()));
        }
        
        // Get receiver
        let mut receiver = self.receiver.lock().await;
        
        // Wait for message with timeout
        match timeout(timeout_duration, receiver.recv()).await {
            Ok(Some(message)) => Ok(message),
            Ok(None) => Err(McpError::Connection("Channel closed".to_string())),
            Err(_) => Err(McpError::Connection("Receive timed out".to_string())),
        }
    }
    
    /// Connection task
    async fn connection_task(
        status: Arc<RwLock<ConnectionStatus>>,
        sender: Arc<Mutex<Option<mpsc::Sender<WsMessage>>>>,
        message_sender: mpsc::Sender<WsMessage>,
        mut control_receiver: mpsc::Receiver<WsMessage>,
        config: WebSocketConfig,
    ) {
        // Websocket connection
        let mut ws_stream: Option<WebSocketStream<MaybeTlsStream<TcpStream>>> = None;
        let mut reconnect_attempts = 0;
        
        loop {
            // Check for control messages
            if let Ok(msg) = control_receiver.try_recv() {
                if let WsMessage::Text(text) = &msg {
                    match text.as_str() {
                        "CONNECT" => {
                            // Try to connect
                            match Self::do_connect(&config).await {
                                Ok(stream) => {
                                    ws_stream = Some(stream);
                                    *status.write().await = ConnectionStatus::Connected;
                                    reconnect_attempts = 0;
                                    
                                    // Create message channels
                                    let (tx, mut rx) = mpsc::channel::<WsMessage>(32);
                                    *sender.lock().await = Some(tx);
                                    
                                    // Spawn task to handle websocket messages
                                    let stream_clone = ws_stream.as_mut().unwrap().get_mut();
                                    let message_sender_clone = message_sender.clone();
                                    let status_clone = status.clone();
                                    
                                    tokio::spawn(async move {
                                        Self::handle_websocket(
                                            stream_clone,
                                            message_sender_clone,
                                            &mut rx,
                                            status_clone,
                                        )
                                        .await;
                                    });
                                }
                                Err(e) => {
                                    *status.write().await = ConnectionStatus::Error(e.to_string());
                                }
                            }
                        }
                        "DISCONNECT" => {
                            // Close connection
                            if let Some(stream) = &mut ws_stream {
                                let _ = stream.close(None).await;
                            }
                            ws_stream = None;
                            *status.write().await = ConnectionStatus::Disconnected;
                            *sender.lock().await = None;
                        }
                        _ => {
                            // Ignore other messages
                        }
                    }
                }
            }
            
            // Sleep a bit to avoid busy loop
            tokio::time::sleep(Duration::from_millis(100)).await;
        }
    }
    
    /// Connect to the WebSocket server
    async fn do_connect(
        config: &WebSocketConfig,
    ) -> McpResult<WebSocketStream<MaybeTlsStream<TcpStream>>> {
        // Parse URL
        let url = Url::parse(&config.url)
            .map_err(|e| McpError::Connection(format!("Invalid URL: {}", e)))?;
            
        // Connect with timeout
        let result = timeout(config.connect_timeout, connect_async(url)).await;
        
        match result {
            Ok(Ok((ws_stream, _))) => Ok(ws_stream),
            Ok(Err(e)) => Err(McpError::Connection(format!("WebSocket connect error: {}", e))),
            Err(_) => Err(McpError::Connection("Connection timed out".to_string())),
        }
    }
    
    /// Handle WebSocket messages
    async fn handle_websocket(
        ws_stream: &mut WebSocketStream<MaybeTlsStream<TcpStream>>,
        message_sender: mpsc::Sender<WsMessage>,
        message_receiver: &mut mpsc::Receiver<WsMessage>,
        status: Arc<RwLock<ConnectionStatus>>,
    ) {
        loop {
            tokio::select! {
                // Handle incoming messages from the WebSocket
                Some(msg) = ws_stream.next() => {
                    match msg {
                        Ok(msg) => {
                            if msg.is_close() {
                                debug!("WebSocket closed by server");
                                break;
                            }
                            
                            // Forward message to receiver
                            if let Err(e) = message_sender.send(msg).await {
                                error!("Failed to forward message: {}", e);
                                break;
                            }
                        }
                        Err(e) => {
                            error!("WebSocket error: {}", e);
                            *status.write().await = ConnectionStatus::Error(format!("WebSocket error: {}", e));
                            break;
                        }
                    }
                }
                
                // Handle outgoing messages to the WebSocket
                Some(msg) = message_receiver.recv() => {
                    if let Err(e) = ws_stream.send(msg).await {
                        error!("Failed to send message: {}", e);
                        *status.write().await = ConnectionStatus::Error(format!("Send error: {}", e));
                        break;
                    }
                }
                
                // Stop if all senders are dropped
                else => {
                    debug!("All message senders dropped");
                    break;
                }
            }
        }
        
        // Update status on exit
        let current_status = status.read().await.clone();
        if current_status == ConnectionStatus::Connected {
            *status.write().await = ConnectionStatus::Disconnected;
        }
    }
}

impl Default for WebSocketConfig {
    fn default() -> Self {
        Self {
            url: "wss://api.anthropic.com/v1/messages".to_string(),
            headers: Vec::new(),
            connect_timeout: Duration::from_secs(30),
            heartbeat_interval: Duration::from_secs(30),
            max_reconnect_attempts: 5,
            reconnect_delay: Duration::from_secs(2),
        }
    }
}
</file>

<file path="src-common/src/service/chat.rs">
use std::sync::Arc;
use tokio::sync::mpsc;
use log::{debug, error, info, warn};

use crate::error::{McpError, McpResult};
use crate::models::{Conversation, Message, Model};
use crate::service::mcp::McpService;

/// Service for managing chat interactions
pub struct ChatService {
    /// MCP service for communication
    mcp_service: Arc<McpService>,
}

impl ChatService {
    /// Create a new chat service
    pub fn new(mcp_service: Arc<McpService>) -> Self {
        Self { mcp_service }
    }
    
    /// Create a new conversation
    pub async fn create_conversation(&self, title: &str, model: Option<Model>) -> McpResult<Conversation> {
        // Use provided model or default
        let model = match model {
            Some(m) => m,
            None => {
                let models = self.mcp_service.available_models().await;
                models.into_iter().next().unwrap_or_else(Model::default_claude)
            }
        };
        
        self.mcp_service.create_conversation(title, &model).await
    }
    
    /// Get a conversation by ID
    pub async fn get_conversation(&self, id: &str) -> McpResult<Conversation> {
        self.mcp_service.get_conversation(id).await
    }
    
    /// List all conversations
    pub async fn list_conversations(&self) -> McpResult<Vec<Conversation>> {
        Ok(self.mcp_service.active_conversations().await)
    }
    
    /// Delete a conversation
    pub async fn delete_conversation(&self, id: &str) -> McpResult<()> {
        self.mcp_service.delete_conversation(id).await
    }
    
    /// Send a message in a conversation
    pub async fn send_message(&self, conversation_id: &str, content: &str) -> McpResult<Message> {
        // Create user message
        let message = Message::user(content);
        
        // Send via MCP service
        self.mcp_service.send_message(conversation_id, message).await
    }
    
    /// Send a message with streaming response
    pub async fn send_message_streaming(
        &self,
        conversation_id: &str,
        content: &str,
    ) -> McpResult<mpsc::Receiver<McpResult<Message>>> {
        // Create user message
        let message = Message::user(content);
        
        // Send via MCP service with streaming
        self.mcp_service.stream_message(conversation_id, message).await
    }
    
    /// Set a system message for a conversation
    pub async fn set_system_message(&self, conversation_id: &str, content: &str) -> McpResult<()> {
        // Get current conversation
        let mut conversation = self.mcp_service.get_conversation(conversation_id).await?;
        
        // Find existing system message, if any
        let has_system_message = conversation
            .messages
            .iter()
            .any(|msg| msg.role == crate::models::MessageRole::System);
        
        // If there's an existing system message, replace it
        if has_system_message {
            conversation.messages = conversation
                .messages
                .into_iter()
                .map(|msg| {
                    if msg.role == crate::models::MessageRole::System {
                        Message::system(content)
                    } else {
                        msg
                    }
                })
                .collect();
        } else {
            // Otherwise, add a new system message at the beginning
            let mut new_messages = vec![Message::system(content)];
            new_messages.extend(conversation.messages);
            conversation.messages = new_messages;
        }
        
        // Update the conversation
        self.mcp_service.update_conversation(conversation).await
    }
    
    /// Get available models
    pub async fn available_models(&self) -> McpResult<Vec<Model>> {
        Ok(self.mcp_service.available_models().await)
    }
}
</file>

<file path="src-common/src/service/mcp.rs">
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, SystemTime};
use tokio::sync::{mpsc, Mutex, RwLock};
use log::{debug, error, info, warn};

use crate::config::{get_settings, get_storage_manager};
use crate::error::{McpError, McpResult};
use crate::models::{Conversation, Message, Model};
use crate::protocol::{ConnectionStatus, McpClient, McpConfig};

/// Service for interacting with the MCP protocol
pub struct McpService {
    /// MCP client
    client: Arc<McpClient>,
    
    /// Available models
    models: Arc<RwLock<Vec<Model>>>,
    
    /// Active conversations
    conversations: Arc<RwLock<HashMap<String, Conversation>>>,
    
    /// Active streaming sessions
    streaming_sessions: Arc<Mutex<HashMap<String, mpsc::Sender<McpResult<Message>>>>>,
}

impl McpService {
    /// Create a new MCP service
    pub fn new() -> Self {
        // Load settings
        let settings = get_settings();
        let settings_guard = settings.lock().unwrap();
        
        // Get API key
        let api_key = settings_guard
            .get_api_key()
            .unwrap_or(Ok(None))
            .unwrap_or(None)
            .unwrap_or_default();
        
        // Create MCP configuration
        let mcp_config = McpConfig::with_api_key(api_key)
            .with_url(settings_guard.api.url.clone())
            .with_model(settings_guard.api.model.clone());
        
        // Create MCP client
        let client = Arc::new(McpClient::new(mcp_config));
        
        // Define available models
        let models = Model::available_claude_models();
        
        Self {
            client,
            models: Arc::new(RwLock::new(models)),
            conversations: Arc::new(RwLock::new(HashMap::new())),
            streaming_sessions: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    /// Initialize the service - load saved data
    pub async fn initialize(&self) -> McpResult<()> {
        // Load saved conversations
        let storage = get_storage_manager();
        let conversations = storage.list_conversations()?;
        
        // Store in memory
        {
            let mut conv_map = self.conversations.write().await;
            for conversation in conversations {
                conv_map.insert(conversation.id.clone(), conversation);
            }
        }
        
        Ok(())
    }
    
    /// Get the current connection status
    pub fn connection_status(&self) -> ConnectionStatus {
        self.client.connection_status()
    }
    
    /// Connect to the MCP server
    pub async fn connect(&self) -> McpResult<()> {
        self.client.connect().await
    }
    
    /// Disconnect from the MCP server
    pub async fn disconnect(&self) -> McpResult<()> {
        self.client.disconnect().await
    }
    
    /// Get available models
    pub async fn available_models(&self) -> Vec<Model> {
        self.models.read().await.clone()
    }
    
    /// Get active conversations
    pub async fn active_conversations(&self) -> Vec<Conversation> {
        self.conversations
            .read()
            .await
            .values()
            .cloned()
            .collect()
    }
    
    /// Create a new conversation
    pub async fn create_conversation(&self, title: &str, model: &Model) -> McpResult<Conversation> {
        let conversation = Conversation::new(title, model.clone());
        
        // Store conversation
        {
            let mut conversations = self.conversations.write().await;
            conversations.insert(conversation.id.clone(), conversation.clone());
        }
        
        // Save to storage
        let storage = get_storage_manager();
        storage.save_conversation(&conversation)?;
        
        Ok(conversation)
    }
    
    /// Get a conversation by ID
    pub async fn get_conversation(&self, id: &str) -> McpResult<Conversation> {
        // Try to get from memory
        {
            let conversations = self.conversations.read().await;
            if let Some(conv) = conversations.get(id) {
                return Ok(conv.clone());
            }
        }
        
        // Try to load from storage
        let storage = get_storage_manager();
        let conversation = storage.load_conversation(id)?;
        
        // Store in memory
        {
            let mut conversations = self.conversations.write().await;
            conversations.insert(conversation.id.clone(), conversation.clone());
        }
        
        Ok(conversation)
    }
    
    /// Update a conversation
    pub async fn update_conversation(&self, conversation: Conversation) -> McpResult<()> {
        // Store in memory
        {
            let mut conversations = self.conversations.write().await;
            conversations.insert(conversation.id.clone(), conversation.clone());
        }
        
        // Save to storage
        let storage = get_storage_manager();
        storage.save_conversation(&conversation)?;
        
        Ok(())
    }
    
    /// Delete a conversation
    pub async fn delete_conversation(&self, id: &str) -> McpResult<()> {
        // Remove from memory
        {
            let mut conversations = self.conversations.write().await;
            conversations.remove(id);
        }
        
        // Remove from storage
        let storage = get_storage_manager();
        storage.delete_conversation(id)?;
        
        Ok(())
    }
    
    /// Send a message in a conversation
    pub async fn send_message(&self, conversation_id: &str, message: Message) -> McpResult<Message> {
        // Get conversation
        let mut conversation = self.get_conversation(conversation_id).await?;
        
        // Add user message to conversation
        conversation.add_message(message.clone());
        
        // Save conversation with user message
        self.update_conversation(conversation.clone()).await?;
        
        // Check connection status
        if self.connection_status() != ConnectionStatus::Connected {
            self.connect().await?;
        }
        
        // Get settings
        let settings = get_settings();
        let settings_guard = settings.lock().unwrap();
        
        // Send message to MCP server
        let response = self
            .client
            .send_completion(
                &conversation.model.id,
                &conversation.messages,
                settings_guard.model.max_tokens,
                settings_guard.model.temperature,
            )
            .await?;
        
        // Add assistant response to conversation
        conversation.add_message(response.clone());
        
        // Save conversation with assistant response
        self.update_conversation(conversation).await?;
        
        Ok(response)
    }
    
    /// Start a streaming message in a conversation
    pub async fn stream_message(
        &self,
        conversation_id: &str,
        message: Message,
    ) -> McpResult<mpsc::Receiver<McpResult<Message>>> {
        // Get conversation
        let mut conversation = self.get_conversation(conversation_id).await?;
        
        // Add user message to conversation
        conversation.add_message(message.clone());
        
        // Save conversation with user message
        self.update_conversation(conversation.clone()).await?;
        
        // Check connection status
        if self.connection_status() != ConnectionStatus::Connected {
            self.connect().await?;
        }
        
        // Get settings
        let settings = get_settings();
        let settings_guard = settings.lock().unwrap();
        
        // Create streaming channel
        let (tx, rx) = mpsc::channel(32);
        
        // Store streaming session
        {
            let mut sessions = self.streaming_sessions.lock().await;
            sessions.insert(message.id.clone(), tx.clone());
        }
        
        // Start streaming
        let client_clone = self.client.clone();
        let model_id = conversation.model.id.clone();
        let messages = conversation.messages.clone();
        let max_tokens = settings_guard.model.max_tokens;
        let temperature = settings_guard.model.temperature;
        let session_id = message.id.clone();
        let conversation_id = conversation_id.to_string();
        let service = Arc::new(self.clone());
        
        tokio::spawn(async move {
            // Start streaming
            match client_clone
                .stream_completion(&model_id, &messages, max_tokens, temperature)
                .await
            {
                Ok(mut receiver) => {
                    let mut full_response = Message {
                        id: session_id.clone(),
                        role: crate::models::MessageRole::Assistant,
                        content: crate::models::MessageContent {
                            parts: vec![crate::models::ContentType::Text {
                                text: String::new(),
                            }],
                        },
                        metadata: None,
                        created_at: SystemTime::now(),
                    };
                    
                    // Process streaming chunks
                    while let Some(chunk) = receiver.recv().await {
                        // Accumulate text
                        if let crate::models::ContentType::Text { ref text } = chunk.content.parts[0] {
                            if let crate::models::ContentType::Text { ref mut text } = full_response.content.parts[0] {
                                *text = text.clone();
                            }
                        }
                        
                        // Send chunk to receiver
                        if tx.send(Ok(chunk)).await.is_err() {
                            // Receiver dropped, cancel streaming
                            let _ = client_clone.cancel_streaming(&session_id).await;
                            break;
                        }
                    }
                    
                    // Add the complete message to the conversation
                    let mut conversation = match service.get_conversation(&conversation_id).await {
                        Ok(conv) => conv,
                        Err(_) => return,
                    };
                    
                    conversation.add_message(full_response);
                    let _ = service.update_conversation(conversation).await;
                }
                Err(e) => {
                    // Send error to receiver
                    let _ = tx.send(Err(e)).await;
                }
            }
            
            // Remove streaming session
            let mut sessions = service.streaming_sessions.lock().await;
            sessions.remove(&session_id);
        });
        
        Ok(rx)
    }
    
    /// Cancel a streaming message
    pub async fn cancel_streaming(&self, message_id: &str) -> McpResult<()> {
        // Cancel streaming with MCP client
        let result = self.client.cancel_streaming(message_id).await;
        
        // Remove streaming session
        {
            let mut sessions = self.streaming_sessions.lock().await;
            sessions.remove(message_id);
        }
        
        result
    }
}

impl Clone for McpService {
    fn clone(&self) -> Self {
        Self {
            client: self.client.clone(),
            models: self.models.clone(),
            conversations: self.conversations.clone(),
            streaming_sessions: self.streaming_sessions.clone(),
        }
    }
}
</file>

<file path="src-common/src/service/mod.rs">
pub mod chat;
pub mod mcp;

// Re-export main services
pub use chat::ChatService;
pub use mcp::McpService;
</file>

<file path="src-common/src/utils/mod.rs">
pub mod security;
pub mod text;

use std::env;

/// Check if a feature flag is enabled
pub fn is_feature_enabled(name: &str) -> bool {
    env::var(format!("MCP_FEATURE_{}", name.to_uppercase()))
        .map(|val| val.to_lowercase() == "true" || val == "1")
        .unwrap_or(false)
}

/// Get environment variable or default
pub fn env_or(name: &str, default: &str) -> String {
    env::var(name).unwrap_or_else(|_| default.to_string())
}

/// Get application name
pub fn app_name() -> String {
    env_or("MCP_APP_NAME", "Claude MCP Client")
}

/// Get application version
pub fn app_version() -> String {
    env_or("MCP_APP_VERSION", "0.1.0")
}

/// Get application platform
pub fn app_platform() -> String {
    env_or("MCP_APP_PLATFORM", std::env::consts::OS)
}
</file>

<file path="src-common/src/utils/security.rs">
use base64::{engine::general_purpose::STANDARD, Engine};
use ring::aead::{Aad, BoundKey, Nonce, NonceSequence, OpeningKey, SealingKey, UnboundKey, CHACHA20_POLY1305};
use ring::error::Unspecified;
use ring::rand::{SecureRandom, SystemRandom};
use std::num::NonZeroU32;
use std::sync::atomic::{AtomicU32, Ordering};

use crate::config::config_path;

const NONCE_LEN: usize = 12;
const SALT_LEN: usize = 16;
const KEY_LEN: usize = 32;
const SALT_FILE: &str = "salt.bin";

/// Derive a key using PBKDF2
fn derive_key() -> Result<[u8; KEY_LEN], String> {
    let salt_path = config_path(SALT_FILE);
    let mut salt = [0u8; SALT_LEN];
    
    // Create or load salt
    if !salt_path.exists() {
        // Generate new salt
        let rng = SystemRandom::new();
        rng.fill(&mut salt)
            .map_err(|_| "Failed to generate random salt".to_string())?;
            
        // Save salt
        std::fs::write(&salt_path, &salt)
            .map_err(|e| format!("Failed to save salt: {}", e))?;
    } else {
        // Load existing salt
        let salt_data = std::fs::read(&salt_path)
            .map_err(|e| format!("Failed to read salt: {}", e))?;
            
        if salt_data.len() != SALT_LEN {
            return Err("Invalid salt length".to_string());
        }
        
        salt.copy_from_slice(&salt_data);
    }
    
    // Use application-specific "password" for encryption
    // This is not intended for high security, just basic obfuscation of API keys
    let app_key = format!("MCP_CLIENT_{}", std::env::consts::OS);
    
    // Derive key using PBKDF2
    let mut key = [0u8; KEY_LEN];
    ring::pbkdf2::derive(
        ring::pbkdf2::PBKDF2_HMAC_SHA256,
        NonZeroU32::new(100_000).unwrap(),
        &salt,
        app_key.as_bytes(),
        &mut key,
    );
    
    Ok(key)
}

/// Simple nonce sequence for encryption
struct CounterNonceSequence {
    counter: AtomicU32,
}

impl CounterNonceSequence {
    fn new(initial: u32) -> Self {
        Self {
            counter: AtomicU32::new(initial),
        }
    }
}

impl NonceSequence for CounterNonceSequence {
    fn advance(&self) -> Result<Nonce, Unspecified> {
        let mut nonce_bytes = [0u8; NONCE_LEN];
        let counter = self.counter.fetch_add(1, Ordering::Relaxed);
        
        for (i, byte) in counter.to_be_bytes().iter().take(4).enumerate() {
            nonce_bytes[i] = *byte;
        }
        
        Nonce::try_assume_unique_for_key(&nonce_bytes)
    }
}

/// Encrypt data
pub fn encrypt(plaintext: &str) -> Result<Vec<u8>, String> {
    let key = derive_key()?;
    let rng = SystemRandom::new();
    
    // Generate random initial counter
    let mut counter_bytes = [0u8; 4];
    rng.fill(&mut counter_bytes)
        .map_err(|_| "Failed to generate random counter".to_string())?;
    let counter = u32::from_be_bytes(counter_bytes);
    
    // Create encryption key
    let unbound_key = UnboundKey::new(&CHACHA20_POLY1305, &key)
        .map_err(|_| "Failed to create encryption key".to_string())?;
    let nonce_sequence = CounterNonceSequence::new(counter);
    let mut sealing_key = SealingKey::new(unbound_key, nonce_sequence);
    
    // Encrypt the data
    let mut in_out = plaintext.as_bytes().to_vec();
    sealing_key
        .seal_in_place_append_tag(Aad::empty(), &mut in_out)
        .map_err(|_| "Encryption failed".to_string())?;
    
    // Prepend the counter bytes for decryption
    let mut result = counter_bytes.to_vec();
    result.extend_from_slice(&in_out);
    
    Ok(result)
}

/// Decrypt data
pub fn decrypt(ciphertext: &[u8]) -> Result<String, String> {
    if ciphertext.len() < 4 + 16 {
        // 4 bytes counter + at least 16 bytes ciphertext (tag)
        return Err("Invalid ciphertext length".to_string());
    }
    
    let key = derive_key()?;
    
    // Extract counter
    let mut counter_bytes = [0u8; 4];
    counter_bytes.copy_from_slice(&ciphertext[0..4]);
    let counter = u32::from_be_bytes(counter_bytes);
    
    // Create decryption key
    let unbound_key = UnboundKey::new(&CHACHA20_POLY1305, &key)
        .map_err(|_| "Failed to create decryption key".to_string())?;
    let nonce_sequence = CounterNonceSequence::new(counter);
    let mut opening_key = OpeningKey::new(unbound_key, nonce_sequence);
    
    // Decrypt the data
    let mut in_out = ciphertext[4..].to_vec();
    let plaintext = opening_key
        .open_in_place(Aad::empty(), &mut in_out)
        .map_err(|_| "Decryption failed".to_string())?;
    
    String::from_utf8(plaintext.to_vec())
        .map_err(|_| "Invalid UTF-8 in decrypted data".to_string())
}
</file>

<file path="src-common/src/utils/text.rs">
use regex::Regex;

/// Wrap text to a specific width
pub fn wrap_text(text: &str, width: usize) -> String {
    let mut result = String::new();
    let mut current_width = 0;
    
    for word in text.split_whitespace() {
        if current_width + word.len() + 1 > width {
            // Add a newline and start a new line
            result.push('\n');
            result.push_str(word);
            current_width = word.len();
        } else if current_width == 0 {
            // First word on the line
            result.push_str(word);
            current_width = word.len();
        } else {
            // Add word with a space
            result.push(' ');
            result.push_str(word);
            current_width += word.len() + 1;
        }
    }
    
    result
}

/// Truncate string to a maximum length with ellipsis
pub fn truncate(text: &str, max_length: usize) -> String {
    if text.len() <= max_length {
        text.to_string()
    } else {
        // Find a good place to truncate - preferably at a word boundary
        let truncate_pos = text[..max_length].rfind(' ').unwrap_or(max_length);
        format!("{}...", &text[..truncate_pos])
    }
}

/// Extract the first line of text
pub fn first_line(text: &str) -> String {
    text.lines()
        .next()
        .unwrap_or("")
        .trim()
        .to_string()
}

/// Clean text by removing extra whitespace
pub fn clean_text(text: &str) -> String {
    let whitespace_regex = Regex::new(r"[ \t]+").unwrap();
    let newline_regex = Regex::new(r"\n{3,}").unwrap();
    
    let text = whitespace_regex.replace_all(text, " ");
    let text = newline_regex.replace_all(&text, "\n\n");
    
    text.trim().to_string()
}

/// Convert markdown to plain text
pub fn markdown_to_plain(markdown: &str) -> String {
    // This is a very simplified markdown to plain text converter
    let heading_regex = Regex::new(r"^(#+)\s+(.*)$").unwrap();
    let bullet_regex = Regex::new(r"^[-*+]\s+(.*)$").unwrap();
    let numbered_regex = Regex::new(r"^\d+\.\s+(.*)$").unwrap();
    let link_regex = Regex::new(r"\[([^\]]+)\]\([^)]+\)").unwrap();
    let emphasis_regex = Regex::new(r"(\*\*|__)(.*?)\1").unwrap();
    let italic_regex = Regex::new(r"(\*|_)(.*?)\1").unwrap();
    let code_regex = Regex::new(r"`([^`]+)`").unwrap();
    
    let mut lines: Vec<String> = Vec::new();
    
    for line in markdown.lines() {
        // Process headings
        let line = heading_regex.replace(line, "$2");
        
        // Process bullet points
        let line = bullet_regex.replace(&line, "• $1");
        
        // Process numbered lists
        let line = numbered_regex.replace(&line, "$1");
        
        // Process links
        let line = link_regex.replace_all(&line, "$1");
        
        // Process bold text
        let line = emphasis_regex.replace_all(&line, "$2");
        
        // Process italic text
        let line = italic_regex.replace_all(&line, "$2");
        
        // Process code
        let line = code_regex.replace_all(&line, "$1");
        
        lines.push(line.to_string());
    }
    
    lines.join("\n")
}
</file>

<file path="src-frontend/implementation-summary.md">
# MCP Client UI Implementation - Summary

## Components Implemented

### Core UI Framework
- **Theme System**: Implemented a theme provider with support for light, dark, and system modes
- **Component System**: Created a tree-shakeable component system with reusable UI elements
- **Shell**: Developed a lightweight application shell with loading states
- **App Structure**: Set up the main application structure with proper layout

### UI Components
- **Button**: Reusable button component with multiple variants and states
- **Input**: Flexible input component with validation states
- **Chat Interface**: Fully functional chat UI with message history and input
- **Command Palette**: Implemented a command palette for quick actions with keyboard shortcuts
- **Sidebar**: Context-aware sidebar that changes based on the current view
- **Header**: Application header with navigation and theme controls
- **Settings**: Complete settings panel with form controls

### State Management
- Added basic state management for UI state
- Prepared hooks for theme management and command palette
- Set up lazy loading for components to improve initial load time

### Styling
- Implemented a comprehensive CSS variable system for theming
- Created consistent styling across all components
- Added animations and transitions for a polished UI experience
- Ensured responsive design principles

## Technical Features

### Tree-Shaking Support
- Components are exported individually to allow tree-shaking
- Lazy-loaded components using React.lazy and Suspense
- Custom lazyLoad utility with error boundaries

### Theme System
- Theme context provider with system theme detection
- Theme toggle component
- CSS variables for theme-consistent styling

### Accessibility
- Proper focus states
- Keyboard navigation support
- ARIA attributes where needed

## Next Steps

### Backend Integration
- Connect the UI to the Tauri backend commands
- Implement real-time communication via MCP protocol
- Add proper authentication flow

### Additional Features
- File uploads and attachments
- Code syntax highlighting
- Markdown rendering for messages
- User preferences persistence

### Performance Optimizations
- Virtualized message list for large conversations
- Further code splitting optimizations
- Service worker for offline support
</file>

<file path="src-frontend/src/__tests__/collaboration/CollaborationContext.test.tsx">
import React from 'react';
import { render, screen, act, waitFor } from '@testing-library/react';
import { CollaborationProvider, useCollaboration } from '../../components/collaboration';
import { invoke } from '@tauri-apps/api/tauri';

// Mock Tauri invoke function
jest.mock('@tauri-apps/api/tauri', () => ({
  invoke: jest.fn(),
}));

// Helper component that uses the collaboration context
const TestComponent = () => {
  const { state, createSession, joinSession } = useCollaboration();
  
  return (
    <div>
      <div data-testid="connection-status">{state.connectionStatus}</div>
      <button 
        data-testid="create-session-btn" 
        onClick={() => createSession('Test Session', 'conversation-123')}
      >
        Create Session
      </button>
      <button 
        data-testid="join-session-btn" 
        onClick={() => joinSession('session-123')}
      >
        Join Session
      </button>
    </div>
  );
};

describe('CollaborationContext', () => {
  beforeEach(() => {
    jest.clearAllMocks();
  });
  
  it('initializes with default state', () => {
    render(
      <CollaborationProvider>
        <TestComponent />
      </CollaborationProvider>
    );
    
    expect(screen.getByTestId('connection-status')).toHaveTextContent('Disconnected');
  });
  
  it('creates a session successfully', async () => {
    // Mock the invoke function to return a successful response
    (invoke as jest.Mock).mockResolvedValueOnce({
      id: 'session-123',
      name: 'Test Session',
      active: true,
      created_at: new Date().toISOString(),
      updated_at: new Date().toISOString(),
      conversation_id: 'conversation-123',
      users: {},
      metadata: {},
    });
    
    // Mock get_session_users to return empty array
    (invoke as jest.Mock).mockResolvedValueOnce([]);
    
    render(
      <CollaborationProvider>
        <TestComponent />
      </CollaborationProvider>
    );
    
    // Click the create session button
    act(() => {
      screen.getByTestId('create-session-btn').click();
    });
    
    // Wait for the invoke call to be made
    await waitFor(() => {
      expect(invoke).toHaveBeenCalledWith('create_session', {
        name: 'Test Session',
        conversationId: 'conversation-123',
      });
    });
  });
  
  it('joins a session successfully', async () => {
    // Mock the invoke function to return a successful response
    (invoke as jest.Mock).mockResolvedValueOnce({
      id: 'session-123',
      name: 'Test Session',
      active: true,
      created_at: new Date().toISOString(),
      updated_at: new Date().toISOString(),
      conversation_id: 'conversation-123',
      users: {},
      metadata: {},
    });
    
    // Mock get_session_users to return empty array
    (invoke as jest.Mock).mockResolvedValueOnce([]);
    
    render(
      <CollaborationProvider>
        <TestComponent />
      </CollaborationProvider>
    );
    
    // Click the join session button
    act(() => {
      screen.getByTestId('join-session-btn').click();
    });
    
    // Wait for the invoke call to be made
    await waitFor(() => {
      expect(invoke).toHaveBeenCalledWith('join_session', {
        sessionId: 'session-123',
      });
    });
  });
  
  it('handles errors when creating a session', async () => {
    // Mock the invoke function to return an error
    (invoke as jest.Mock).mockRejectedValueOnce(new Error('Failed to create session'));
    
    render(
      <CollaborationProvider>
        <TestComponent />
      </CollaborationProvider>
    );
    
    // Click the create session button
    act(() => {
      screen.getByTestId('create-session-btn').click();
    });
    
    // Wait for the invoke call to be made
    await waitFor(() => {
      expect(invoke).toHaveBeenCalledWith('create_session', {
        name: 'Test Session',
        conversationId: 'conversation-123',
      });
    });
    
    // Error should be caught and state should remain the same
    expect(screen.getByTestId('connection-status')).toHaveTextContent('Disconnected');
  });
});
</file>

<file path="src-frontend/src/__tests__/collaboration/CursorOverlay.test.tsx">
import React, { useRef } from 'react';
import { render, fireEvent, screen } from '@testing-library/react';
import { CursorOverlay } from '../../components/collaboration';
import { CollaborationContext, ConnectionStatus, UserRole } from '../../components/collaboration/context/CollaborationContext';

// Mock the useCollaboration hook
jest.mock('../../../hooks/useCollaboration', () => ({
  useCollaboration: jest.fn(),
}));

// Mock the collaboration context with test data
const mockContextValue = {
  state: {
    initialized: true,
    config: {
      enabled: true,
      show_presence: true,
    },
    connectionStatus: ConnectionStatus.Connected,
    users: [
      {
        id: 'user-1',
        name: 'Test User',
        role: UserRole.Editor,
        color: '#ff0000',
        online: true,
        last_active: new Date().toISOString(),
        device_id: 'device-1',
        metadata: {},
      },
    ],
    cursors: {
      'user-1': {
        user_id: 'user-1',
        device_id: 'device-1',
        x: 0.5,
        y: 0.5,
        timestamp: new Date().toISOString(),
      },
    },
    currentUser: {
      id: 'current-user',
      name: 'Current User',
      role: UserRole.Owner,
      color: '#00ff00',
      online: true,
      last_active: new Date().toISOString(),
      device_id: 'device-current',
      metadata: {},
    },
    sessions: [],
    selections: {},
    mediaDevices: [],
    statistics: {
      session_count: 0,
      total_users: 0,
      active_sessions: 0,
      cursor_updates: 0,
      selection_updates: 0,
      messages_sent: 0,
      messages_received: 0,
      sync_operations: 0,
      conflicts_resolved: 0,
      calls_initiated: 0,
      call_duration_seconds: 0,
      connection_status: ConnectionStatus.Connected,
    },
  },
  updateCursorPosition: jest.fn(),
};

// Test component that wraps CursorOverlay with a ref
const TestComponent = () => {
  const containerRef = useRef<HTMLDivElement>(null);
  
  return (
    <div ref={containerRef} style={{ width: '500px', height: '500px' }}>
      <CursorOverlay containerRef={containerRef} />
    </div>
  );
};

describe('CursorOverlay', () => {
  beforeEach(() => {
    jest.clearAllMocks();
  });
  
  it('renders cursors for other users', () => {
    // Provide the mock context value
    render(
      <CollaborationContext.Provider value={mockContextValue as any}>
        <TestComponent />
      </CollaborationContext.Provider>
    );
    
    // Check if the cursor element is rendered
    const cursorElement = document.querySelector('.cursor-wrapper');
    expect(cursorElement).toBeInTheDocument();
  });
  
  it('does not render cursors when presence is disabled', () => {
    const disabledContext = {
      ...mockContextValue,
      state: {
        ...mockContextValue.state,
        config: {
          ...mockContextValue.state.config,
          show_presence: false,
        },
      },
    };
    
    render(
      <CollaborationContext.Provider value={disabledContext as any}>
        <TestComponent />
      </CollaborationContext.Provider>
    );
    
    // Check that no cursor elements are rendered
    const cursorElement = document.querySelector('.cursor-wrapper');
    expect(cursorElement).not.toBeInTheDocument();
  });
  
  it('calls updateCursorPosition on mouse move', () => {
    render(
      <CollaborationContext.Provider value={mockContextValue as any}>
        <TestComponent />
      </CollaborationContext.Provider>
    );
    
    // Simulate a mouse move event
    const container = document.querySelector('div[style]');
    
    // Mock getBoundingClientRect for the container
    container.getBoundingClientRect = jest.fn().mockReturnValue({
      width: 500,
      height: 500,
      left: 0,
      top: 0,
    });
    
    fireEvent.mouseMove(container, { clientX: 250, clientY: 250 });
    
    // Check if updateCursorPosition was called with normalized coordinates
    expect(mockContextValue.updateCursorPosition).toHaveBeenCalledWith(0.5, 0.5, undefined);
  });
  
  it('does not render stale cursors', () => {
    // Create a stale cursor (older than 10 seconds)
    const staleTimestamp = new Date();
    staleTimestamp.setSeconds(staleTimestamp.getSeconds() - 15);
    
    const staleContext = {
      ...mockContextValue,
      state: {
        ...mockContextValue.state,
        cursors: {
          'user-1': {
            ...mockContextValue.state.cursors['user-1'],
            timestamp: staleTimestamp.toISOString(),
          },
        },
      },
    };
    
    render(
      <CollaborationContext.Provider value={staleContext as any}>
        <TestComponent />
      </CollaborationContext.Provider>
    );
    
    // Check that no cursor elements are rendered
    const cursorElement = document.querySelector('.cursor-wrapper');
    expect(cursorElement).not.toBeInTheDocument();
  });
});
</file>

<file path="src-frontend/src/accessibility/A11yButton.tsx">
import React from 'react';
import { useAccessibility } from './AccessibilityProvider';

const A11yButton: React.FC = () => {
  const { toggleA11yPanel } = useAccessibility();

  return (
    <button 
      className="a11y-quick-access" 
      onClick={toggleA11yPanel}
      aria-label="Accessibility settings"
    >
      <span className="a11y-quick-access-tooltip">Accessibility Settings</span>
      <svg 
        xmlns="http://www.w3.org/2000/svg" 
        viewBox="0 0 24 24" 
        fill="none" 
        stroke="currentColor" 
        strokeWidth="2" 
        strokeLinecap="round" 
        strokeLinejoin="round"
      >
        <circle cx="12" cy="12" r="10" />
        <path d="M12 8v4M12 16h.01" />
      </svg>
    </button>
  );
};

export default A11yButton;
</file>

<file path="src-frontend/src/accessibility/accessibility.css">
/* Base accessibility styles */

/* High contrast mode */
.a11y-high-contrast {
  --color-primary: #0074d9;
  --color-primary-light: #0091ff;
  --color-primary-dark: #005eb0;
  
  --color-on-background: #000000;
  --color-on-surface: #000000;
  --color-on-surface-variant: #222222;
  
  --color-background: #ffffff;
  --color-surface: #f8f8f8;
  --color-surface-variant: #e0e0e0;
  
  --color-border: #666666;
  --color-divider: #444444;
  
  --shadow-sm: 0 2px 4px rgba(0, 0, 0, 0.15);
  --shadow-md: 0 4px 8px rgba(0, 0, 0, 0.2), 0 2px 4px rgba(0, 0, 0, 0.2);
  --shadow-lg: 0 8px 16px rgba(0, 0, 0, 0.25), 0 4px 8px rgba(0, 0, 0, 0.2);
}

/* Dark mode high contrast */
.dark-theme.a11y-high-contrast {
  --color-primary: #56b3ff;
  --color-primary-light: #99d1ff;
  --color-primary-dark: #0074d9;
  
  --color-on-background: #ffffff;
  --color-on-surface: #ffffff;
  --color-on-surface-variant: #dddddd;
  
  --color-background: #121212;
  --color-surface: #1e1e1e;
  --color-surface-variant: #2a2a2a;
  
  --color-border: #999999;
  --color-divider: #666666;
}

/* Large text mode */
.a11y-large-text {
  --font-size-xs: 0.875rem;   /* 14px */
  --font-size-sm: 1rem;       /* 16px */
  --font-size-md: 1.125rem;   /* 18px */
  --font-size-lg: 1.25rem;    /* 20px */
  --font-size-xl: 1.5rem;     /* 24px */
  --font-size-2xl: 1.75rem;   /* 28px */
  --font-size-3xl: 2.25rem;   /* 36px */
  
  /* Increase line height for better readability */
  line-height: 1.7;
  
  /* Increase spacing for better readability */
  letter-spacing: 0.01em;
}

/* Reduced motion mode */
.a11y-reduced-motion * {
  animation-duration: 0.001ms !important;
  transition-duration: 0.001ms !important;
  animation-iteration-count: 1 !important;
  scroll-behavior: auto !important;
}

/* Enhanced focus indicators */
.a11y-focus-indicators *:focus-visible {
  outline: 3px solid var(--color-primary) !important;
  outline-offset: 3px !important;
}

.a11y-focus-indicators button:focus-visible,
.a11y-focus-indicators a:focus-visible,
.a11y-focus-indicators [tabindex="0"]:focus-visible {
  outline: 3px solid var(--color-primary) !important;
  outline-offset: 3px !important;
  box-shadow: 0 0 0 3px rgba(var(--color-primary-rgb), 0.4) !important;
}

/* Dyslexic-friendly font */
.a11y-dyslexic-font {
  --font-family: 'Open Dyslexic', 'Comic Sans MS', 'Comic Sans', cursive, sans-serif;
  
  /* Improve readability */
  letter-spacing: 0.05em;
  word-spacing: 0.1em;
}

/* Increased text spacing */
.a11y-text-spacing {
  letter-spacing: 0.03em;
  word-spacing: 0.12em;
  line-height: 1.7;
  
  /* Increase paragraph spacing */
  p {
    margin-bottom: 1.5em;
  }
  
  /* Increase heading spacing */
  h1, h2, h3, h4, h5, h6 {
    margin-top: 1.5em;
    margin-bottom: 0.75em;
  }
}

/* Screen reader enhancements */
.a11y-screen-reader .sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  white-space: nowrap;
  border-width: 0;
}

/* ARIA role descriptions */
[role] {
  cursor: default;
}

[role="button"], [role="link"], [role="tab"] {
  cursor: pointer;
}

/* Accessibility panel styles */
.a11y-panel-overlay {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.5);
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: var(--z-modal);
  animation: fadeIn var(--animation-duration-fast) var(--animation-easing-standard);
}

.a11y-panel {
  width: 500px;
  max-width: 90vw;
  max-height: 90vh;
  background-color: var(--color-surface);
  border-radius: var(--radius-lg);
  box-shadow: var(--shadow-lg);
  display: flex;
  flex-direction: column;
  animation: scaleIn var(--animation-duration-normal) var(--animation-easing-spring);
  overflow: hidden;
}

.a11y-panel-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: var(--spacing-md) var(--spacing-lg);
  border-bottom: 1px solid var(--color-border);
}

.a11y-panel-header h2 {
  margin: 0;
  font-size: var(--font-size-xl);
  color: var(--color-on-surface);
}

.a11y-panel-close {
  background: none;
  border: none;
  font-size: 1.5rem;
  line-height: 1;
  padding: var(--spacing-xs);
  cursor: pointer;
  color: var(--color-on-surface-variant);
  transition: color var(--animation-duration-fast) var(--animation-easing-standard);
}

.a11y-panel-close:hover {
  color: var(--color-on-surface);
}

.a11y-panel-content {
  padding: var(--spacing-md) var(--spacing-lg);
  overflow-y: auto;
  flex: 1;
}

.a11y-setting {
  margin-bottom: var(--spacing-md);
  padding-bottom: var(--spacing-md);
  border-bottom: 1px solid var(--color-divider);
}

.a11y-setting:last-child {
  margin-bottom: 0;
  padding-bottom: 0;
  border-bottom: none;
}

.a11y-setting-label {
  display: flex;
  align-items: center;
  gap: var(--spacing-sm);
  font-size: var(--font-size-md);
  font-weight: 500;
  color: var(--color-on-surface);
  margin-bottom: var(--spacing-xxs);
  cursor: pointer;
}

.a11y-setting-label input[type="checkbox"] {
  width: 18px;
  height: 18px;
}

.a11y-setting-description {
  margin: 0 0 0 calc(18px + var(--spacing-sm));
  font-size: var(--font-size-sm);
  color: var(--color-on-surface-variant);
}

.a11y-panel-footer {
  padding: var(--spacing-md) var(--spacing-lg);
  border-top: 1px solid var(--color-border);
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: var(--spacing-sm);
}

.a11y-panel-reset {
  background: none;
  border: 1px solid var(--color-border);
  border-radius: var(--radius-md);
  padding: var(--spacing-xs) var(--spacing-md);
  font-size: var(--font-size-sm);
  color: var(--color-on-surface);
  cursor: pointer;
  transition: all var(--animation-duration-fast) var(--animation-easing-standard);
}

.a11y-panel-reset:hover {
  background-color: var(--color-surface-variant);
  border-color: var(--color-primary);
}

.a11y-panel-shortcut {
  font-size: var(--font-size-xs);
  color: var(--color-on-surface-variant);
}

.a11y-panel-shortcut kbd {
  display: inline-block;
  padding: 2px var(--spacing-xxs);
  font-family: var(--font-family);
  font-size: var(--font-size-xs);
  border: 1px solid var(--color-border);
  border-radius: var(--radius-sm);
  box-shadow: 0 1px 0 var(--color-border);
  background-color: var(--color-surface-variant);
  color: var(--color-on-surface);
}

/* Accessibility button that appears in the corner */
.a11y-quick-access {
  position: fixed;
  bottom: var(--spacing-md);
  right: var(--spacing-md);
  width: 3rem;
  height: 3rem;
  border-radius: 50%;
  background-color: var(--color-primary);
  color: white;
  display: flex;
  align-items: center;
  justify-content: center;
  box-shadow: var(--shadow-md);
  border: none;
  cursor: pointer;
  z-index: var(--z-dropdown);
  transition: all var(--animation-duration-fast) var(--animation-easing-standard);
}

.a11y-quick-access:hover {
  background-color: var(--color-primary-dark);
  transform: scale(1.05);
}

.a11y-quick-access svg {
  width: 1.5rem;
  height: 1.5rem;
}

.a11y-quick-access-tooltip {
  position: absolute;
  bottom: 100%;
  left: 50%;
  transform: translateX(-50%);
  margin-bottom: var(--spacing-xs);
  background-color: var(--color-surface);
  color: var(--color-on-surface);
  padding: var(--spacing-xs) var(--spacing-sm);
  border-radius: var(--radius-md);
  font-size: var(--font-size-xs);
  box-shadow: var(--shadow-sm);
  pointer-events: none;
  opacity: 0;
  transition: opacity var(--animation-duration-fast) var(--animation-easing-standard);
  white-space: nowrap;
}

.a11y-quick-access:hover .a11y-quick-access-tooltip {
  opacity: 1;
}
</file>

<file path="src-frontend/src/accessibility/AccessibilityProvider.tsx">
import React, { createContext, useContext, useState, useEffect, ReactNode } from 'react';
import './accessibility.css';

// Accessibility settings interface
interface AccessibilitySettings {
  highContrast: boolean;
  largeText: boolean;
  reducedMotion: boolean;
  screenReader: boolean;
  focusIndicators: boolean;
  dyslexicFont: boolean;
  textSpacing: boolean;
}

// Context interface
interface AccessibilityContextType {
  settings: AccessibilitySettings;
  updateSettings: (settings: Partial<AccessibilitySettings>) => void;
  resetSettings: () => void;
  showA11yPanel: boolean;
  toggleA11yPanel: () => void;
}

// Default settings
const defaultSettings: AccessibilitySettings = {
  highContrast: false,
  largeText: false,
  reducedMotion: false,
  screenReader: false,
  focusIndicators: true,
  dyslexicFont: false,
  textSpacing: false,
};

// Create context
const AccessibilityContext = createContext<AccessibilityContextType>({
  settings: defaultSettings,
  updateSettings: () => {},
  resetSettings: () => {},
  showA11yPanel: false,
  toggleA11yPanel: () => {},
});

export const useAccessibility = () => useContext(AccessibilityContext);

interface AccessibilityProviderProps {
  children: ReactNode;
}

export const AccessibilityProvider: React.FC<AccessibilityProviderProps> = ({ children }) => {
  // Load settings from localStorage or use defaults
  const getInitialSettings = (): AccessibilitySettings => {
    const savedSettings = localStorage.getItem('mcp-accessibility-settings');
    if (savedSettings) {
      try {
        return { ...defaultSettings, ...JSON.parse(savedSettings) };
      } catch (e) {
        console.error('Failed to parse accessibility settings:', e);
      }
    }
    return { ...defaultSettings };
  };

  const [settings, setSettings] = useState<AccessibilitySettings>(getInitialSettings);
  const [showA11yPanel, setShowA11yPanel] = useState<boolean>(false);

  // Update settings
  const updateSettings = (newSettings: Partial<AccessibilitySettings>) => {
    const updatedSettings = { ...settings, ...newSettings };
    setSettings(updatedSettings);
    localStorage.setItem('mcp-accessibility-settings', JSON.stringify(updatedSettings));
  };

  // Reset to defaults
  const resetSettings = () => {
    setSettings({ ...defaultSettings });
    localStorage.setItem('mcp-accessibility-settings', JSON.stringify(defaultSettings));
  };

  // Toggle accessibility panel
  const toggleA11yPanel = () => {
    setShowA11yPanel(prev => !prev);
  };

  // Apply settings to document
  useEffect(() => {
    const root = document.documentElement;
    
    // High contrast
    if (settings.highContrast) {
      root.classList.add('a11y-high-contrast');
    } else {
      root.classList.remove('a11y-high-contrast');
    }
    
    // Large text
    if (settings.largeText) {
      root.classList.add('a11y-large-text');
    } else {
      root.classList.remove('a11y-large-text');
    }
    
    // Reduced motion (this works alongside the AnimationProvider)
    if (settings.reducedMotion) {
      root.classList.add('a11y-reduced-motion');
    } else {
      root.classList.remove('a11y-reduced-motion');
    }
    
    // Screen reader - add ARIA role descriptions and enhanced keyboard support
    if (settings.screenReader) {
      root.classList.add('a11y-screen-reader');
    } else {
      root.classList.remove('a11y-screen-reader');
    }
    
    // Focus indicators - enhanced visibility for keyboard focus
    if (settings.focusIndicators) {
      root.classList.add('a11y-focus-indicators');
    } else {
      root.classList.remove('a11y-focus-indicators');
    }
    
    // Dyslexic-friendly font
    if (settings.dyslexicFont) {
      root.classList.add('a11y-dyslexic-font');
    } else {
      root.classList.remove('a11y-dyslexic-font');
    }
    
    // Text spacing
    if (settings.textSpacing) {
      root.classList.add('a11y-text-spacing');
    } else {
      root.classList.remove('a11y-text-spacing');
    }
  }, [settings]);

  // Check for OS/browser settings on initial load
  useEffect(() => {
    const checkOSSettings = () => {
      // Check prefers-reduced-motion
      const prefersReducedMotion = window.matchMedia('(prefers-reduced-motion: reduce)').matches;
      
      // Check prefers-contrast
      const prefersContrast = window.matchMedia('(prefers-contrast: more)').matches;
      
      // Apply OS settings if they're not already overridden by user preferences
      const savedSettings = localStorage.getItem('mcp-accessibility-settings');
      
      if (!savedSettings) {
        updateSettings({
          reducedMotion: prefersReducedMotion,
          highContrast: prefersContrast,
        });
      }
    };
    
    checkOSSettings();
    
    // Create keyboard shortcut to toggle accessibility panel (Alt+A)
    const handleKeyDown = (e: KeyboardEvent) => {
      if (e.altKey && e.key === 'a') {
        e.preventDefault();
        toggleA11yPanel();
      }
    };
    
    window.addEventListener('keydown', handleKeyDown);
    
    return () => {
      window.removeEventListener('keydown', handleKeyDown);
    };
  }, []);

  return (
    <AccessibilityContext.Provider
      value={{
        settings,
        updateSettings,
        resetSettings,
        showA11yPanel,
        toggleA11yPanel,
      }}
    >
      {children}
      {showA11yPanel && <AccessibilityPanel />}
    </AccessibilityContext.Provider>
  );
};

// Accessibility settings panel
const AccessibilityPanel: React.FC = () => {
  const { settings, updateSettings, resetSettings, toggleA11yPanel } = useAccessibility();

  return (
    <div className="a11y-panel-overlay" onClick={toggleA11yPanel}>
      <div className="a11y-panel" onClick={e => e.stopPropagation()}>
        <div className="a11y-panel-header">
          <h2>Accessibility Settings</h2>
          <button className="a11y-panel-close" onClick={toggleA11yPanel} aria-label="Close accessibility panel">
            &times;
          </button>
        </div>
        
        <div className="a11y-panel-content">
          <div className="a11y-setting">
            <label className="a11y-setting-label">
              <input
                type="checkbox"
                checked={settings.highContrast}
                onChange={e => updateSettings({ highContrast: e.target.checked })}
              />
              <span>High Contrast</span>
            </label>
            <p className="a11y-setting-description">Increases contrast for better readability</p>
          </div>
          
          <div className="a11y-setting">
            <label className="a11y-setting-label">
              <input
                type="checkbox"
                checked={settings.largeText}
                onChange={e => updateSettings({ largeText: e.target.checked })}
              />
              <span>Large Text</span>
            </label>
            <p className="a11y-setting-description">Increases font size throughout the application</p>
          </div>
          
          <div className="a11y-setting">
            <label className="a11y-setting-label">
              <input
                type="checkbox"
                checked={settings.reducedMotion}
                onChange={e => updateSettings({ reducedMotion: e.target.checked })}
              />
              <span>Reduced Motion</span>
            </label>
            <p className="a11y-setting-description">Minimizes animations and transitions</p>
          </div>
          
          <div className="a11y-setting">
            <label className="a11y-setting-label">
              <input
                type="checkbox"
                checked={settings.screenReader}
                onChange={e => updateSettings({ screenReader: e.target.checked })}
              />
              <span>Screen Reader Support</span>
            </label>
            <p className="a11y-setting-description">Enhances compatibility with screen readers</p>
          </div>
          
          <div className="a11y-setting">
            <label className="a11y-setting-label">
              <input
                type="checkbox"
                checked={settings.focusIndicators}
                onChange={e => updateSettings({ focusIndicators: e.target.checked })}
              />
              <span>Focus Indicators</span>
            </label>
            <p className="a11y-setting-description">Shows clear visual indicators when navigating with keyboard</p>
          </div>
          
          <div className="a11y-setting">
            <label className="a11y-setting-label">
              <input
                type="checkbox"
                checked={settings.dyslexicFont}
                onChange={e => updateSettings({ dyslexicFont: e.target.checked })}
              />
              <span>Dyslexic-Friendly Font</span>
            </label>
            <p className="a11y-setting-description">Uses a font designed to be easier to read with dyslexia</p>
          </div>
          
          <div className="a11y-setting">
            <label className="a11y-setting-label">
              <input
                type="checkbox"
                checked={settings.textSpacing}
                onChange={e => updateSettings({ textSpacing: e.target.checked })}
              />
              <span>Increased Text Spacing</span>
            </label>
            <p className="a11y-setting-description">Adds more space between letters, words, and lines</p>
          </div>
        </div>
        
        <div className="a11y-panel-footer">
          <button className="a11y-panel-reset" onClick={resetSettings}>
            Reset to Defaults
          </button>
          <p className="a11y-panel-shortcut">
            Press <kbd>Alt</kbd> + <kbd>A</kbd> to toggle this panel
          </p>
        </div>
      </div>
    </div>
  );
};

export default AccessibilityProvider;
</file>

<file path="src-frontend/src/accessibility/index.ts">
export * from './AccessibilityProvider';
export { default as A11yButton } from './A11yButton';
</file>

<file path="src-frontend/src/animation/AnimationProvider.tsx">
import React, { createContext, useContext, useState, useEffect, ReactNode } from 'react';
import './animations.css';

interface AnimationContextType {
  animationsEnabled: boolean;
  animationSpeed: 'normal' | 'slow' | 'fast';
  toggleAnimations: () => void;
  setAnimationSpeed: (speed: 'normal' | 'slow' | 'fast') => void;
}

const AnimationContext = createContext<AnimationContextType>({
  animationsEnabled: true,
  animationSpeed: 'normal',
  toggleAnimations: () => {},
  setAnimationSpeed: () => {},
});

export const useAnimation = () => useContext(AnimationContext);

interface AnimationProviderProps {
  children: ReactNode;
}

export const AnimationProvider: React.FC<AnimationProviderProps> = ({ children }) => {
  // Get saved preferences from localStorage or use defaults
  const getSavedAnimationPreference = (): boolean => {
    const savedPref = localStorage.getItem('mcp-animations-enabled');
    return savedPref !== null ? savedPref === 'true' : true;
  };

  const getSavedAnimationSpeed = (): 'normal' | 'slow' | 'fast' => {
    const savedSpeed = localStorage.getItem('mcp-animation-speed');
    return (savedSpeed as 'normal' | 'slow' | 'fast') || 'normal';
  };

  const [animationsEnabled, setAnimationsEnabled] = useState<boolean>(getSavedAnimationPreference());
  const [animationSpeed, setAnimationSpeed] = useState<'normal' | 'slow' | 'fast'>(getSavedAnimationSpeed());

  // Toggle animations on/off
  const toggleAnimations = () => {
    const newValue = !animationsEnabled;
    setAnimationsEnabled(newValue);
    localStorage.setItem('mcp-animations-enabled', String(newValue));
  };

  // Set animation speed and save to localStorage
  const handleSetAnimationSpeed = (speed: 'normal' | 'slow' | 'fast') => {
    setAnimationSpeed(speed);
    localStorage.setItem('mcp-animation-speed', speed);
  };

  // Apply animation settings to document
  useEffect(() => {
    const root = document.documentElement;
    
    // Handle animation enabling/disabling
    if (animationsEnabled) {
      document.body.classList.remove('animate-none');
    } else {
      document.body.classList.add('animate-none');
    }
    
    // Handle animation speed
    let multiplier: number;
    switch (animationSpeed) {
      case 'slow':
        multiplier = 1.5;
        break;
      case 'fast':
        multiplier = 0.6;
        break;
      default:
        multiplier = 1;
    }
    
    root.style.setProperty('--user-animation-multiplier', String(multiplier));
    
    // Also check prefers-reduced-motion
    const mediaQuery = window.matchMedia('(prefers-reduced-motion: reduce)');
    const handleReducedMotionChange = (e: MediaQueryListEvent) => {
      if (e.matches && animationsEnabled) {
        // Don't disable completely, but reduce speed
        root.style.setProperty('--user-animation-multiplier', '0.5');
      } else {
        // Restore the multiplier based on user preference
        root.style.setProperty('--user-animation-multiplier', String(multiplier));
      }
    };
    
    mediaQuery.addEventListener('change', handleReducedMotionChange);
    
    return () => {
      mediaQuery.removeEventListener('change', handleReducedMotionChange);
    };
  }, [animationsEnabled, animationSpeed]);

  return (
    <AnimationContext.Provider 
      value={{ 
        animationsEnabled, 
        animationSpeed, 
        toggleAnimations, 
        setAnimationSpeed: handleSetAnimationSpeed 
      }}
    >
      {children}
    </AnimationContext.Provider>
  );
};

export default AnimationProvider;
</file>

<file path="src-frontend/src/animation/animations.css">
/* Base Animation Variables */
:root {
  --animation-duration-fastest: 100ms;
  --animation-duration-fast: 200ms;
  --animation-duration-normal: 300ms;
  --animation-duration-slow: 500ms;
  --animation-duration-slowest: 800ms;
  
  --animation-easing-standard: cubic-bezier(0.4, 0.0, 0.2, 1);
  --animation-easing-decelerate: cubic-bezier(0.0, 0.0, 0.2, 1);
  --animation-easing-accelerate: cubic-bezier(0.4, 0.0, 1, 1);
  --animation-easing-sharp: cubic-bezier(0.4, 0.0, 0.6, 1);
  --animation-easing-spring: cubic-bezier(0.43, 0.195, 0.02, 1.01);
  
  /* User preference adjustments */
  --user-animation-multiplier: 1;
}

/* Respect user preference for reduced motion */
@media (prefers-reduced-motion: reduce) {
  :root {
    --user-animation-multiplier: 0.5;
  }
  
  .animate-disabled * {
    animation-duration: 0ms !important;
    transition-duration: 0ms !important;
  }
}

/* Animation Durations considering user preferences */
.duration-fastest {
  transition-duration: calc(var(--animation-duration-fastest) * var(--user-animation-multiplier));
}

.duration-fast {
  transition-duration: calc(var(--animation-duration-fast) * var(--user-animation-multiplier));
}

.duration-normal {
  transition-duration: calc(var(--animation-duration-normal) * var(--user-animation-multiplier));
}

.duration-slow {
  transition-duration: calc(var(--animation-duration-slow) * var(--user-animation-multiplier));
}

.duration-slowest {
  transition-duration: calc(var(--animation-duration-slowest) * var(--user-animation-multiplier));
}

/* Easing Curves */
.ease-standard {
  transition-timing-function: var(--animation-easing-standard);
}

.ease-decelerate {
  transition-timing-function: var(--animation-easing-decelerate);
}

.ease-accelerate {
  transition-timing-function: var(--animation-easing-accelerate);
}

.ease-sharp {
  transition-timing-function: var(--animation-easing-sharp);
}

.ease-spring {
  transition-timing-function: var(--animation-easing-spring);
}

/* Fade Animations */
@keyframes fadeIn {
  from { opacity: 0; }
  to { opacity: 1; }
}

@keyframes fadeOut {
  from { opacity: 1; }
  to { opacity: 0; }
}

.fade-in {
  animation: fadeIn calc(var(--animation-duration-normal) * var(--user-animation-multiplier)) var(--animation-easing-standard) forwards;
}

.fade-out {
  animation: fadeOut calc(var(--animation-duration-normal) * var(--user-animation-multiplier)) var(--animation-easing-standard) forwards;
}

/* Scale Animations */
@keyframes scaleIn {
  from { transform: scale(0.9); opacity: 0; }
  to { transform: scale(1); opacity: 1; }
}

@keyframes scaleOut {
  from { transform: scale(1); opacity: 1; }
  to { transform: scale(0.9); opacity: 0; }
}

.scale-in {
  animation: scaleIn calc(var(--animation-duration-normal) * var(--user-animation-multiplier)) var(--animation-easing-spring) forwards;
}

.scale-out {
  animation: scaleOut calc(var(--animation-duration-normal) * var(--user-animation-multiplier)) var(--animation-easing-spring) forwards;
}

/* Slide Animations */
@keyframes slideInUp {
  from { transform: translateY(20px); opacity: 0; }
  to { transform: translateY(0); opacity: 1; }
}

@keyframes slideOutDown {
  from { transform: translateY(0); opacity: 1; }
  to { transform: translateY(20px); opacity: 0; }
}

@keyframes slideInDown {
  from { transform: translateY(-20px); opacity: 0; }
  to { transform: translateY(0); opacity: 1; }
}

@keyframes slideOutUp {
  from { transform: translateY(0); opacity: 1; }
  to { transform: translateY(-20px); opacity: 0; }
}

@keyframes slideInLeft {
  from { transform: translateX(-20px); opacity: 0; }
  to { transform: translateX(0); opacity: 1; }
}

@keyframes slideOutLeft {
  from { transform: translateX(0); opacity: 1; }
  to { transform: translateX(-20px); opacity: 0; }
}

@keyframes slideInRight {
  from { transform: translateX(20px); opacity: 0; }
  to { transform: translateX(0); opacity: 1; }
}

@keyframes slideOutRight {
  from { transform: translateX(0); opacity: 1; }
  to { transform: translateX(20px); opacity: 0; }
}

.slide-in-up {
  animation: slideInUp calc(var(--animation-duration-normal) * var(--user-animation-multiplier)) var(--animation-easing-decelerate) forwards;
}

.slide-out-down {
  animation: slideOutDown calc(var(--animation-duration-normal) * var(--user-animation-multiplier)) var(--animation-easing-accelerate) forwards;
}

.slide-in-down {
  animation: slideInDown calc(var(--animation-duration-normal) * var(--user-animation-multiplier)) var(--animation-easing-decelerate) forwards;
}

.slide-out-up {
  animation: slideOutUp calc(var(--animation-duration-normal) * var(--user-animation-multiplier)) var(--animation-easing-accelerate) forwards;
}

.slide-in-left {
  animation: slideInLeft calc(var(--animation-duration-normal) * var(--user-animation-multiplier)) var(--animation-easing-decelerate) forwards;
}

.slide-out-left {
  animation: slideOutLeft calc(var(--animation-duration-normal) * var(--user-animation-multiplier)) var(--animation-easing-accelerate) forwards;
}

.slide-in-right {
  animation: slideInRight calc(var(--animation-duration-normal) * var(--user-animation-multiplier)) var(--animation-easing-decelerate) forwards;
}

.slide-out-right {
  animation: slideOutRight calc(var(--animation-duration-normal) * var(--user-animation-multiplier)) var(--animation-easing-accelerate) forwards;
}

/* Attention-grabbing animations */
@keyframes pulse {
  0% { transform: scale(1); }
  50% { transform: scale(1.05); }
  100% { transform: scale(1); }
}

@keyframes shake {
  0%, 100% { transform: translateX(0); }
  10%, 30%, 50%, 70%, 90% { transform: translateX(-2px); }
  20%, 40%, 60%, 80% { transform: translateX(2px); }
}

@keyframes bounce {
  0%, 20%, 50%, 80%, 100% { transform: translateY(0); }
  40% { transform: translateY(-10px); }
  60% { transform: translateY(-5px); }
}

.pulse {
  animation: pulse calc(var(--animation-duration-normal) * var(--user-animation-multiplier)) var(--animation-easing-standard);
}

.shake {
  animation: shake calc(var(--animation-duration-normal) * var(--user-animation-multiplier)) var(--animation-easing-standard);
}

.bounce {
  animation: bounce calc(var(--animation-duration-slow) * var(--user-animation-multiplier)) var(--animation-easing-spring);
}

/* Microinteraction Animations */
@keyframes buttonPress {
  0% { transform: scale(1); }
  50% { transform: scale(0.96); }
  100% { transform: scale(1); }
}

.button-press {
  animation: buttonPress calc(var(--animation-duration-fastest) * var(--user-animation-multiplier)) var(--animation-easing-standard);
}

/* Loading indicators */
@keyframes spin {
  from { transform: rotate(0deg); }
  to { transform: rotate(360deg); }
}

@keyframes skeleton {
  0% { background-position: 100% 50%; }
  100% { background-position: 0% 50%; }
}

.spin {
  animation: spin 1s linear infinite;
}

.skeleton-loading {
  background: linear-gradient(
    90deg,
    var(--color-surface-variant) 25%,
    var(--color-surface) 50%,
    var(--color-surface-variant) 75%
  );
  background-size: 200% 100%;
  animation: skeleton 1.5s infinite;
}

/* Conditional class to disable all animations */
.animate-none, .animate-none * {
  animation: none !important;
  transition: none !important;
}
</file>

<file path="src-frontend/src/animation/index.ts">
export * from './AnimationProvider';
</file>

<file path="src-frontend/src/components/collaboration/call/CallControls.tsx">
// CallControls.tsx
//
// This component provides controls for audio/video calls in the collaboration session,
// including starting/ending calls and toggling audio/video.

import React, { useState, useEffect } from 'react';
import { useCollaboration } from '../../../hooks/useCollaboration';
import { Participant, MediaDevice } from '../context/CollaborationContext';

interface CallControlsProps {
  // Optional props can be added here
}

const CallControls: React.FC<CallControlsProps> = () => {
  const { state, startAudioCall, startVideoCall, endCall, toggleMute, toggleVideo } = useCollaboration();
  const { activeCall, mediaDevices, config } = state;
  
  const [isStartingCall, setIsStartingCall] = useState<boolean>(false);
  const [selectedAudioInput, setSelectedAudioInput] = useState<string>('');
  const [selectedAudioOutput, setSelectedAudioOutput] = useState<string>('');
  const [selectedVideoInput, setSelectedVideoInput] = useState<string>('');
  const [error, setError] = useState<string | null>(null);
  
  // Select default devices when list changes
  useEffect(() => {
    if (mediaDevices.length > 0) {
      // Select first audio input if none selected
      if (!selectedAudioInput) {
        const audioInput = mediaDevices.find(device => device.kind === 'audioinput');
        if (audioInput) {
          setSelectedAudioInput(audioInput.id);
        }
      }
      
      // Select first audio output if none selected
      if (!selectedAudioOutput) {
        const audioOutput = mediaDevices.find(device => device.kind === 'audiooutput');
        if (audioOutput) {
          setSelectedAudioOutput(audioOutput.id);
        }
      }
      
      // Select first video input if none selected
      if (!selectedVideoInput) {
        const videoInput = mediaDevices.find(device => device.kind === 'videoinput');
        if (videoInput) {
          setSelectedVideoInput(videoInput.id);
        }
      }
    }
  }, [mediaDevices, selectedAudioInput, selectedAudioOutput, selectedVideoInput]);
  
  // Handle starting an audio call
  const handleStartAudioCall = async () => {
    setIsStartingCall(true);
    setError(null);
    
    try {
      await startAudioCall();
    } catch (err) {
      setError(`Failed to start audio call: ${err}`);
    } finally {
      setIsStartingCall(false);
    }
  };
  
  // Handle starting a video call
  const handleStartVideoCall = async () => {
    setIsStartingCall(true);
    setError(null);
    
    try {
      await startVideoCall();
    } catch (err) {
      setError(`Failed to start video call: ${err}`);
    } finally {
      setIsStartingCall(false);
    }
  };
  
  // Handle ending a call
  const handleEndCall = async () => {
    try {
      await endCall();
    } catch (err) {
      setError(`Failed to end call: ${err}`);
    }
  };
  
  // Handle toggling mute
  const handleToggleMute = async () => {
    try {
      await toggleMute();
    } catch (err) {
      setError(`Failed to toggle mute: ${err}`);
    }
  };
  
  // Handle toggling video
  const handleToggleVideo = async () => {
    try {
      await toggleVideo();
    } catch (err) {
      setError(`Failed to toggle video: ${err}`);
    }
  };
  
  // Get current user's participant info
  const getCurrentParticipant = (): Participant | undefined => {
    if (!activeCall || !state.currentUser) return undefined;
    
    return activeCall.participants[state.currentUser.id];
  };
  
  // Filter devices by kind
  const getDevicesByKind = (kind: string): MediaDevice[] => {
    return mediaDevices.filter(device => device.kind === kind);
  };
  
  // Render device selection
  const renderDeviceSelection = () => {
    return (
      <div style={{ marginBottom: '20px' }}>
        <h4 style={{ margin: '0 0 10px 0' }}>Media Devices</h4>
        
        <div className="collaboration-form-group">
          <label 
            htmlFor="audioInput" 
            className="collaboration-label"
          >
            Microphone
          </label>
          <select
            id="audioInput"
            value={selectedAudioInput}
            onChange={(e) => setSelectedAudioInput(e.target.value)}
            className="collaboration-select"
          >
            {getDevicesByKind('audioinput').map(device => (
              <option key={device.id} value={device.id}>
                {device.name}
              </option>
            ))}
          </select>
        </div>
        
        <div className="collaboration-form-group">
          <label 
            htmlFor="audioOutput" 
            className="collaboration-label"
          >
            Speakers
          </label>
          <select
            id="audioOutput"
            value={selectedAudioOutput}
            onChange={(e) => setSelectedAudioOutput(e.target.value)}
            className="collaboration-select"
          >
            {getDevicesByKind('audiooutput').map(device => (
              <option key={device.id} value={device.id}>
                {device.name}
              </option>
            ))}
          </select>
        </div>
        
        <div className="collaboration-form-group">
          <label 
            htmlFor="videoInput" 
            className="collaboration-label"
          >
            Camera
          </label>
          <select
            id="videoInput"
            value={selectedVideoInput}
            onChange={(e) => setSelectedVideoInput(e.target.value)}
            className="collaboration-select"
          >
            {getDevicesByKind('videoinput').map(device => (
              <option key={device.id} value={device.id}>
                {device.name}
              </option>
            ))}
          </select>
        </div>
      </div>
    );
  };
  
  // Render call controls when not in a call
  const renderStartCallControls = () => {
    return (
      <div style={{ textAlign: 'center', marginTop: '20px' }}>
        <p>Start a call with users in this session</p>
        
        <div className="call-controls">
          <button
            onClick={handleStartAudioCall}
            disabled={isStartingCall}
            className="call-button primary"
          >
            <svg 
              width="20" 
              height="20" 
              viewBox="0 0 24 24" 
              fill="none" 
              stroke="currentColor" 
              strokeWidth="2" 
              strokeLinecap="round" 
              strokeLinejoin="round"
            >
              <path d="M22 16.92v3a2 2 0 0 1-2.18 2 19.79 19.79 0 0 1-8.63-3.07 19.5 19.5 0 0 1-6-6 19.79 19.79 0 0 1-3.07-8.67A2 2 0 0 1 4.11 2h3a2 2 0 0 1 2 1.72 12.84 12.84 0 0 0 .7 2.81 2 2 0 0 1-.45 2.11L8.09 9.91a16 16 0 0 0 6 6l1.27-1.27a2 2 0 0 1 2.11-.45 12.84 12.84 0 0 0 2.81.7A2 2 0 0 1 22 16.92z"></path>
            </svg>
            <span style={{ marginLeft: '8px' }}>Audio Call</span>
          </button>
          
          <button
            onClick={handleStartVideoCall}
            disabled={isStartingCall}
            className="call-button primary"
          >
            <svg 
              width="20" 
              height="20" 
              viewBox="0 0 24 24" 
              fill="none" 
              stroke="currentColor" 
              strokeWidth="2" 
              strokeLinecap="round" 
              strokeLinejoin="round"
            >
              <polygon points="23 7 16 12 23 17 23 7"></polygon>
              <rect x="1" y="5" width="15" height="14" rx="2" ry="2"></rect>
            </svg>
            <span style={{ marginLeft: '8px' }}>Video Call</span>
          </button>
        </div>
      </div>
    );
  };
  
  // Render call participant grid
  const renderCallParticipants = () => {
    if (!activeCall) return null;
    
    const participants = Object.values(activeCall.participants);
    
    return (
      <div style={{ marginBottom: '20px' }}>
        <h4 style={{ margin: '0 0 10px 0' }}>
          {activeCall.has_video ? 'Video Call' : 'Audio Call'} 
          <span style={{ fontSize: '14px', fontWeight: 'normal', marginLeft: '5px' }}>
            ({participants.length} {participants.length === 1 ? 'participant' : 'participants'})
          </span>
        </h4>
        
        <div className="call-participants">
          {participants.map(participant => (
            <div
              key={participant.user_id}
              className="participant-card"
            >
              {/* Video placeholder or avatar */}
              <div className="video-container">
                {activeCall.has_video && participant.video_enabled ? (
                  <div className="video-placeholder">
                    <svg 
                      width="48" 
                      height="48" 
                      viewBox="0 0 24 24" 
                      fill="none" 
                      stroke="#9E9E9E" 
                      strokeWidth="1" 
                      strokeLinecap="round" 
                      strokeLinejoin="round"
                    >
                      <rect x="2" y="2" width="20" height="20" rx="2" ry="2"></rect>
                      <path d="M12 16a4 4 0 1 0 0-8 4 4 0 0 0 0 8z"></path>
                    </svg>
                  </div>
                ) : (
                  <div className="video-placeholder">
                    {participant.name.charAt(0).toUpperCase()}
                  </div>
                )}
                
                {/* Microphone status indicator */}
                <div className="media-indicator">
                  <svg 
                    width="16" 
                    height="16" 
                    viewBox="0 0 24 24" 
                    fill="none" 
                    stroke="white" 
                    strokeWidth="2" 
                    strokeLinecap="round" 
                    strokeLinejoin="round"
                  >
                    {participant.audio_enabled ? (
                      <>
                        <path d="M12 1a3 3 0 0 0-3 3v8a3 3 0 0 0 6 0V4a3 3 0 0 0-3-3z"></path>
                        <path d="M19 10v2a7 7 0 0 1-14 0v-2"></path>
                        <line x1="12" y1="19" x2="12" y2="23"></line>
                        <line x1="8" y1="23" x2="16" y2="23"></line>
                      </>
                    ) : (
                      <>
                        <line x1="1" y1="1" x2="23" y2="23"></line>
                        <path d="M9 9v3a3 3 0 0 0 5.12 2.12M15 9.34V4a3 3 0 0 0-5.94-.6"></path>
                        <path d="M17 16.95A7 7 0 0 1 5 12v-2m14 0v2c0 .74-.16 1.44-.43 2.08"></path>
                        <line x1="12" y1="19" x2="12" y2="23"></line>
                        <line x1="8" y1="23" x2="16" y2="23"></line>
                      </>
                    )}
                  </svg>
                </div>
              </div>
              
              {/* Participant name */}
              <div className="participant-info">
                <div className="participant-name">
                  {participant.name}
                  {state.currentUser && participant.user_id === state.currentUser.id && ' (You)'}
                </div>
                
                {/* Speaking indicator */}
                {participant.is_speaking && (
                  <div className="participant-status" style={{ color: '#4CAF50' }}>
                    Speaking...
                  </div>
                )}
              </div>
            </div>
          ))}
        </div>
      </div>
    );
  };
  
  // Render in-call controls
  const renderInCallControls = () => {
    if (!activeCall) return null;
    
    const currentParticipant = getCurrentParticipant();
    
    return (
      <div className="call-controls">
        {/* Mute/Unmute button */}
        <button
          onClick={handleToggleMute}
          className={`call-button ${currentParticipant?.audio_enabled ? 'secondary' : 'inactive'}`}
        >
          <svg 
            width="24" 
            height="24" 
            viewBox="0 0 24 24" 
            fill="none" 
            stroke="currentColor" 
            strokeWidth="2" 
            strokeLinecap="round" 
            strokeLinejoin="round"
          >
            {currentParticipant?.audio_enabled ? (
              <>
                <path d="M12 1a3 3 0 0 0-3 3v8a3 3 0 0 0 6 0V4a3 3 0 0 0-3-3z"></path>
                <path d="M19 10v2a7 7 0 0 1-14 0v-2"></path>
                <line x1="12" y1="19" x2="12" y2="23"></line>
                <line x1="8" y1="23" x2="16" y2="23"></line>
              </>
            ) : (
              <>
                <line x1="1" y1="1" x2="23" y2="23"></line>
                <path d="M9 9v3a3 3 0 0 0 5.12 2.12M15 9.34V4a3 3 0 0 0-5.94-.6"></path>
                <path d="M17 16.95A7 7 0 0 1 5 12v-2m14 0v2c0 .74-.16 1.44-.43 2.08"></path>
                <line x1="12" y1="19" x2="12" y2="23"></line>
                <line x1="8" y1="23" x2="16" y2="23"></line>
              </>
            )}
          </svg>
        </button>
        
        {/* Video Toggle button (if this is a video call) */}
        {activeCall.has_video && (
          <button
            onClick={handleToggleVideo}
            className={`call-button ${currentParticipant?.video_enabled ? 'secondary' : 'inactive'}`}
          >
            <svg 
              width="24" 
              height="24" 
              viewBox="0 0 24 24" 
              fill="none" 
              stroke="currentColor" 
              strokeWidth="2" 
              strokeLinecap="round" 
              strokeLinejoin="round"
            >
              {currentParticipant?.video_enabled ? (
                <>
                  <polygon points="23 7 16 12 23 17 23 7"></polygon>
                  <rect x="1" y="5" width="15" height="14" rx="2" ry="2"></rect>
                </>
              ) : (
                <>
                  <path d="M16 16v1a2 2 0 0 1-2 2H3a2 2 0 0 1-2-2V7a2 2 0 0 1 2-2h2m5.66 0H14a2 2 0 0 1 2 2v3.34l1 1L23 7v10"></path>
                  <line x1="1" y1="1" x2="23" y2="23"></line>
                </>
              )}
            </svg>
          </button>
        )}
        
        {/* End Call button */}
        <button
          onClick={handleEndCall}
          className="call-button danger"
        >
          <svg 
            width="24" 
            height="24" 
            viewBox="0 0 24 24" 
            fill="none" 
            stroke="currentColor" 
            strokeWidth="2" 
            strokeLinecap="round" 
            strokeLinejoin="round"
          >
            <path d="M10.68 13.31a16 16 0 0 0 3.41 2.6l1.27-1.27a2 2 0 0 1 2.11-.45 12.84 12.84 0 0 0 2.81.7 2 2 0 0 1 1.72 2v3a2 2 0 0 1-2.18 2 19.79 19.79 0 0 1-8.63-3.07 19.42 19.42 0 0 1-3.33-2.67m-2.67-3.34a19.79 19.79 0 0 1-3.07-8.63A2 2 0 0 1 4.11 2h3a2 2 0 0 1 2 1.72 12.84 12.84 0 0 0 .7 2.81 2 2 0 0 1-.45 2.11L8.09 9.91"></path>
            <line x1="23" y1="1" x2="1" y2="23"></line>
          </svg>
        </button>
      </div>
    );
  };
  
  // If audio/video features are not enabled
  if (!config.enable_av) {
    return (
      <div style={{ padding: '20px', textAlign: 'center' }}>
        <p>Audio/video features are disabled.</p>
        <p>Enable them in the collaboration settings.</p>
      </div>
    );
  }
  
  return (
    <div>
      {/* Error message */}
      {error && (
        <div className="collaboration-error">
          {error}
        </div>
      )}
      
      {/* Device selection */}
      {renderDeviceSelection()}
      
      {/* Call participants (if in a call) */}
      {activeCall ? renderCallParticipants() : renderStartCallControls()}
      
      {/* Call controls (if in a call) */}
      {activeCall && renderInCallControls()}
    </div>
  );
};

export default CallControls;
</file>

<file path="src-frontend/src/components/collaboration/CollaborationPanel.tsx">
// CollaborationPanel.tsx
//
// Main panel for collaboration features including:
// - Session management
// - User list
// - Call controls
// - Whiteboard
// - Collaboration settings

import React, { useState, useEffect } from 'react';
import { useCollaboration } from '../../hooks/useCollaboration';
import UserList from './UserList';
import { ConnectionStatus, UserRole } from './context/CollaborationContext';
import CallControls from './call/CallControls';
import CollaborationSettings from './settings/CollaborationSettings';
import CollaborativeWhiteboard from './whiteboard/CollaborativeWhiteboard';

// Import whiteboard styles
import '../../styles/whiteboard.css';

// Tab options
enum Tab {
  Users,
  Call,
  Whiteboard,
  Settings,
}

interface CollaborationPanelProps {
  conversationId: string;
}

const CollaborationPanel: React.FC<CollaborationPanelProps> = ({ conversationId }) => {
  const { state, createSession, joinSession, leaveSession } = useCollaboration();
  const { currentSession, connectionStatus, config } = state;
  
  const [activeTab, setActiveTab] = useState<Tab>(Tab.Users);
  const [sessionName, setSessionName] = useState<string>('');
  const [sessionIdToJoin, setSessionIdToJoin] = useState<string>('');
  const [showCreateDialog, setShowCreateDialog] = useState<boolean>(false);
  const [showJoinDialog, setShowJoinDialog] = useState<boolean>(false);
  const [isLoading, setIsLoading] = useState<boolean>(false);
  const [error, setError] = useState<string | null>(null);
  
  useEffect(() => {
    // Reset the form when dialog visibility changes
    if (!showCreateDialog) {
      setSessionName('');
    }
    if (!showJoinDialog) {
      setSessionIdToJoin('');
    }
  }, [showCreateDialog, showJoinDialog]);
  
  // Handle creating a new session
  const handleCreateSession = async () => {
    if (!sessionName.trim()) {
      setError('Please enter a session name');
      return;
    }
    
    setIsLoading(true);
    setError(null);
    
    try {
      await createSession(sessionName, conversationId);
      setShowCreateDialog(false);
    } catch (err) {
      setError(`Failed to create session: ${err}`);
    } finally {
      setIsLoading(false);
    }
  };
  
  // Handle joining an existing session
  const handleJoinSession = async () => {
    if (!sessionIdToJoin.trim()) {
      setError('Please enter a session ID');
      return;
    }
    
    setIsLoading(true);
    setError(null);
    
    try {
      await joinSession(sessionIdToJoin);
      setShowJoinDialog(false);
    } catch (err) {
      setError(`Failed to join session: ${err}`);
    } finally {
      setIsLoading(false);
    }
  };
  
  // Handle leaving the current session
  const handleLeaveSession = async () => {
    setIsLoading(true);
    
    try {
      await leaveSession();
    } catch (err) {
      console.error('Failed to leave session:', err);
    } finally {
      setIsLoading(false);
    }
  };
  
  // Render connection status indicator
  const renderConnectionStatus = () => {
    let color = '#9E9E9E';
    let label = 'Disconnected';
    
    switch (connectionStatus) {
      case ConnectionStatus.Connected:
        color = '#4CAF50';
        label = 'Connected';
        break;
      case ConnectionStatus.Connecting:
        color = '#FFC107';
        label = 'Connecting';
        break;
      case ConnectionStatus.Limited:
        color = '#FF9800';
        label = 'Limited';
        break;
      case ConnectionStatus.Error:
        color = '#F44336';
        label = 'Error';
        break;
      default:
        break;
    }
    
    return (
      <div style={{ display: 'flex', alignItems: 'center' }}>
        <div
          style={{
            width: '10px',
            height: '10px',
            borderRadius: '50%',
            backgroundColor: color,
            marginRight: '6px',
          }}
        />
        <span>{label}</span>
      </div>
    );
  };
  
  // Render session details
  const renderSessionDetails = () => {
    if (!currentSession) {
      return (
        <div style={{ padding: '20px', textAlign: 'center' }}>
          <p>You are not in a collaborative session.</p>
          <div style={{ display: 'flex', justifyContent: 'center', gap: '10px', marginTop: '15px' }}>
            <button
              onClick={() => setShowCreateDialog(true)}
              disabled={isLoading}
              className="collaboration-button primary"
            >
              Create Session
            </button>
            <button
              onClick={() => setShowJoinDialog(true)}
              disabled={isLoading}
              className="collaboration-button primary"
            >
              Join Session
            </button>
          </div>
        </div>
      );
    }
    
    return (
      <div style={{ padding: '10px' }}>
        <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', marginBottom: '15px' }}>
          <div>
            <h3 style={{ margin: '0 0 5px 0' }}>{currentSession.name}</h3>
            <div style={{ fontSize: '12px', color: '#757575' }}>
              Session ID: {currentSession.id}
            </div>
          </div>
          <button
            onClick={handleLeaveSession}
            disabled={isLoading}
            className="collaboration-button danger"
          >
            Leave
          </button>
        </div>
        
        {/* Tabs navigation */}
        <div className="collaboration-tabs">
          <button
            onClick={() => setActiveTab(Tab.Users)}
            className={`collaboration-tab ${activeTab === Tab.Users ? 'active' : ''}`}
          >
            Users
          </button>
          
          {config.enable_av && (
            <button
              onClick={() => setActiveTab(Tab.Call)}
              className={`collaboration-tab ${activeTab === Tab.Call ? 'active' : ''}`}
            >
              Call
            </button>
          )}
          
          <button
            onClick={() => setActiveTab(Tab.Whiteboard)}
            className={`collaboration-tab ${activeTab === Tab.Whiteboard ? 'active' : ''}`}
          >
            Whiteboard
          </button>
          
          <button
            onClick={() => setActiveTab(Tab.Settings)}
            className={`collaboration-tab ${activeTab === Tab.Settings ? 'active' : ''}`}
          >
            Settings
          </button>
        </div>
        
        {/* Tab content */}
        <div style={{ padding: '10px' }}>
          {activeTab === Tab.Users && <UserList />}
          {activeTab === Tab.Call && <CallControls />}
          {activeTab === Tab.Whiteboard && (
            <CollaborativeWhiteboard 
              sessionId={currentSession.id}
              width={280}
              height={400}
            />
          )}
          {activeTab === Tab.Settings && <CollaborationSettings />}
        </div>
      </div>
    );
  };
  
  // Create session dialog
  const renderCreateSessionDialog = () => {
    if (!showCreateDialog) return null;
    
    return (
      <div className="collaboration-dialog-overlay">
        <div className="collaboration-dialog">
          <h3 className="collaboration-dialog-title">Create New Collaboration Session</h3>
          
          {error && (
            <div className="collaboration-error">
              {error}
            </div>
          )}
          
          <div className="collaboration-form-group">
            <label 
              htmlFor="sessionName" 
              className="collaboration-label"
            >
              Session Name
            </label>
            <input
              id="sessionName"
              type="text"
              value={sessionName}
              onChange={(e) => setSessionName(e.target.value)}
              placeholder="Enter a name for your session"
              className="collaboration-input"
            />
          </div>
          
          <div className="collaboration-button-group">
            <button
              onClick={() => setShowCreateDialog(false)}
              disabled={isLoading}
              className="collaboration-button secondary"
            >
              Cancel
            </button>
            <button
              onClick={handleCreateSession}
              disabled={isLoading || !sessionName.trim()}
              className="collaboration-button primary"
              style={{
                opacity: isLoading || !sessionName.trim() ? 0.7 : 1,
              }}
            >
              {isLoading ? 'Creating...' : 'Create Session'}
            </button>
          </div>
        </div>
      </div>
    );
  };
  
  // Join session dialog
  const renderJoinSessionDialog = () => {
    if (!showJoinDialog) return null;
    
    return (
      <div className="collaboration-dialog-overlay">
        <div className="collaboration-dialog">
          <h3 className="collaboration-dialog-title">Join Collaboration Session</h3>
          
          {error && (
            <div className="collaboration-error">
              {error}
            </div>
          )}
          
          <div className="collaboration-form-group">
            <label 
              htmlFor="sessionId" 
              className="collaboration-label"
            >
              Session ID
            </label>
            <input
              id="sessionId"
              type="text"
              value={sessionIdToJoin}
              onChange={(e) => setSessionIdToJoin(e.target.value)}
              placeholder="Enter the session ID to join"
              className="collaboration-input"
            />
          </div>
          
          <div className="collaboration-button-group">
            <button
              onClick={() => setShowJoinDialog(false)}
              disabled={isLoading}
              className="collaboration-button secondary"
            >
              Cancel
            </button>
            <button
              onClick={handleJoinSession}
              disabled={isLoading || !sessionIdToJoin.trim()}
              className="collaboration-button primary"
              style={{
                opacity: isLoading || !sessionIdToJoin.trim() ? 0.7 : 1,
              }}
            >
              {isLoading ? 'Joining...' : 'Join Session'}
            </button>
          </div>
        </div>
      </div>
    );
  };
  
  return (
    <div className="collaboration-panel">
      {/* Header with status */}
      <div className="collaboration-panel-header">
        <h3 className="collaboration-panel-title">Collaboration</h3>
        {renderConnectionStatus()}
      </div>
      
      {/* Main content */}
      <div className="collaboration-panel-content">
        {renderSessionDetails()}
      </div>
      
      {/* Dialogs */}
      {renderCreateSessionDialog()}
      {renderJoinSessionDialog()}
    </div>
  );
};

export default CollaborationPanel;
</file>

<file path="src-frontend/src/components/collaboration/context/CollaborationContext.tsx">
// CollaborationContext.tsx
//
// This file provides a React context for collaboration features including:
// - Session management
// - User presence
// - Real-time synchronization
// - Audio/video calls

import React, { createContext, useReducer, useContext, useEffect, useState } from 'react';
import { invoke } from '@tauri-apps/api/tauri';

// Types from the backend
export interface User {
  id: string;
  name: string;
  role: UserRole;
  avatar?: string;
  color: string;
  online: boolean;
  last_active: string;
  device_id: string;
  metadata: Record<string, string>;
}

export enum UserRole {
  Owner = 'Owner',
  CoOwner = 'CoOwner',
  Editor = 'Editor',
  Commentator = 'Commentator',
  Viewer = 'Viewer',
}

export interface Session {
  id: string;
  name: string;
  active: boolean;
  created_at: string;
  updated_at: string;
  conversation_id: string;
  users: Record<string, User>;
  metadata: Record<string, string>;
}

export enum ConnectionStatus {
  Disconnected = 'Disconnected',
  Connecting = 'Connecting',
  Connected = 'Connected',
  Limited = 'Limited',
  Error = 'Error',
}

export interface CursorPosition {
  user_id: string;
  device_id: string;
  x: number;
  y: number;
  element_id?: string;
  timestamp: string;
}

export interface Selection {
  user_id: string;
  device_id: string;
  start_id: string;
  end_id: string;
  start_offset: number;
  end_offset: number;
  timestamp: string;
}

export interface MediaDevice {
  id: string;
  name: string;
  kind: string;
}

export interface Participant {
  user_id: string;
  name: string;
  device_id: string;
  audio_enabled: boolean;
  video_enabled: boolean;
  is_speaking: boolean;
  audio_level: number;
  network_quality: number;
  joined_at: string;
}

export interface Call {
  id: string;
  session_id: string;
  has_audio: boolean;
  has_video: boolean;
  start_time: string;
  participants: Record<string, Participant>;
}

export interface CollaborationConfig {
  enabled: boolean;
  max_users_per_session: number;
  auto_discover: boolean;
  show_presence: boolean;
  enable_av: boolean;
  sync_interval_ms: number;
  p2p_enabled: boolean;
  server_urls: string[];
  username?: string;
  user_avatar?: string;
}

export interface CollaborationStatistics {
  session_count: number;
  total_users: number;
  active_sessions: number;
  cursor_updates: number;
  selection_updates: number;
  messages_sent: number;
  messages_received: number;
  sync_operations: number;
  conflicts_resolved: number;
  calls_initiated: number;
  call_duration_seconds: number;
  current_session_id?: string;
  connection_status: ConnectionStatus;
}

// State and actions
interface CollaborationState {
  initialized: boolean;
  config: CollaborationConfig;
  connectionStatus: ConnectionStatus;
  currentUser?: User;
  currentSession?: Session;
  sessions: Session[];
  users: User[];
  cursors: Record<string, CursorPosition>;
  selections: Record<string, Selection>;
  mediaDevices: MediaDevice[];
  activeCall?: Call;
  statistics: CollaborationStatistics;
  error?: string;
}

type ActionType =
  | { type: 'INITIALIZE_SUCCESS'; payload: { config: CollaborationConfig } }
  | { type: 'UPDATE_CONFIG'; payload: { config: CollaborationConfig } }
  | { type: 'UPDATE_CONNECTION_STATUS'; payload: { status: ConnectionStatus } }
  | { type: 'SET_CURRENT_USER'; payload: { user: User } }
  | { type: 'CREATE_SESSION_SUCCESS'; payload: { session: Session } }
  | { type: 'JOIN_SESSION_SUCCESS'; payload: { session: Session } }
  | { type: 'LEAVE_SESSION_SUCCESS' }
  | { type: 'UPDATE_USERS'; payload: { users: User[] } }
  | { type: 'UPDATE_CURSORS'; payload: { cursors: Record<string, CursorPosition> } }
  | { type: 'UPDATE_SELECTIONS'; payload: { selections: Record<string, Selection> } }
  | { type: 'UPDATE_CALL'; payload: { call?: Call } }
  | { type: 'UPDATE_MEDIA_DEVICES'; payload: { devices: MediaDevice[] } }
  | { type: 'UPDATE_STATISTICS'; payload: { statistics: CollaborationStatistics } }
  | { type: 'SET_ERROR'; payload: { error: string } }
  | { type: 'CLEAR_ERROR' };

// Default state
const initialState: CollaborationState = {
  initialized: false,
  config: {
    enabled: true,
    max_users_per_session: 10,
    auto_discover: true,
    show_presence: true,
    enable_av: false,
    sync_interval_ms: 1000,
    p2p_enabled: true,
    server_urls: [
      'https://signaling.mcp-client.com',
      'stun:stun.mcp-client.com:19302',
      'turn:turn.mcp-client.com:3478',
    ],
  },
  connectionStatus: ConnectionStatus.Disconnected,
  sessions: [],
  users: [],
  cursors: {},
  selections: {},
  mediaDevices: [],
  statistics: {
    session_count: 0,
    total_users: 0,
    active_sessions: 0,
    cursor_updates: 0,
    selection_updates: 0,
    messages_sent: 0,
    messages_received: 0,
    sync_operations: 0,
    conflicts_resolved: 0,
    calls_initiated: 0,
    call_duration_seconds: 0,
    connection_status: ConnectionStatus.Disconnected,
  },
};

// Reducer function
function collaborationReducer(state: CollaborationState, action: ActionType): CollaborationState {
  switch (action.type) {
    case 'INITIALIZE_SUCCESS':
      return {
        ...state,
        initialized: true,
        config: action.payload.config,
      };
    case 'UPDATE_CONFIG':
      return {
        ...state,
        config: action.payload.config,
      };
    case 'UPDATE_CONNECTION_STATUS':
      return {
        ...state,
        connectionStatus: action.payload.status,
      };
    case 'SET_CURRENT_USER':
      return {
        ...state,
        currentUser: action.payload.user,
      };
    case 'CREATE_SESSION_SUCCESS':
      return {
        ...state,
        currentSession: action.payload.session,
        sessions: [...state.sessions, action.payload.session],
      };
    case 'JOIN_SESSION_SUCCESS':
      return {
        ...state,
        currentSession: action.payload.session,
        sessions: [...state.sessions.filter(s => s.id !== action.payload.session.id), action.payload.session],
      };
    case 'LEAVE_SESSION_SUCCESS':
      return {
        ...state,
        currentSession: undefined,
        users: [],
        cursors: {},
        selections: {},
        activeCall: undefined,
      };
    case 'UPDATE_USERS':
      return {
        ...state,
        users: action.payload.users,
      };
    case 'UPDATE_CURSORS':
      return {
        ...state,
        cursors: action.payload.cursors,
      };
    case 'UPDATE_SELECTIONS':
      return {
        ...state,
        selections: action.payload.selections,
      };
    case 'UPDATE_CALL':
      return {
        ...state,
        activeCall: action.payload.call,
      };
    case 'UPDATE_MEDIA_DEVICES':
      return {
        ...state,
        mediaDevices: action.payload.devices,
      };
    case 'UPDATE_STATISTICS':
      return {
        ...state,
        statistics: action.payload.statistics,
      };
    case 'SET_ERROR':
      return {
        ...state,
        error: action.payload.error,
      };
    case 'CLEAR_ERROR':
      return {
        ...state,
        error: undefined,
      };
    default:
      return state;
  }
}

// Context interface
interface CollaborationContextType {
  state: CollaborationState;
  initializeCollaboration: (config?: CollaborationConfig) => Promise<void>;
  updateConfig: (config: CollaborationConfig) => Promise<void>;
  createSession: (name: string, conversationId: string) => Promise<Session>;
  joinSession: (sessionId: string) => Promise<Session>;
  leaveSession: () => Promise<void>;
  inviteUser: (email: string, role: UserRole) => Promise<void>;
  removeUser: (userId: string) => Promise<void>;
  changeUserRole: (userId: string, role: UserRole) => Promise<void>;
  updateCursorPosition: (x: number, y: number, elementId?: string) => Promise<void>;
  updateSelection: (startId: string, endId: string, startOffset: number, endOffset: number) => Promise<void>;
  startAudioCall: () => Promise<void>;
  startVideoCall: () => Promise<void>;
  endCall: () => Promise<void>;
  toggleMute: () => Promise<boolean>;
  toggleVideo: () => Promise<boolean>;
  updateUsername: (name: string) => Promise<void>;
  updateAvatar: (avatar?: string) => Promise<void>;
  refreshUsers: () => Promise<void>;
  refreshCursors: () => Promise<void>;
  refreshSelections: () => Promise<void>;
  refreshCall: () => Promise<void>;
  refreshMediaDevices: () => Promise<void>;
  refreshStatistics: () => Promise<void>;
  refreshAll: () => Promise<void>;
}

// Create the context
export const CollaborationContext = createContext<CollaborationContextType | undefined>(undefined);

// Provider component
export const CollaborationProvider: React.FC<{ children: React.ReactNode }> = ({ children }) => {
  const [state, dispatch] = useReducer(collaborationReducer, initialState);
  const [refreshInterval, setRefreshInterval] = useState<number | null>(null);

  // Initialize collaboration system
  const initializeCollaboration = async (config?: CollaborationConfig) => {
    try {
      await invoke('init_collaboration_system', { config });
      const loadedConfig = await invoke<CollaborationConfig>('get_collaboration_config');
      dispatch({ type: 'INITIALIZE_SUCCESS', payload: { config: loadedConfig } });
      
      // Start refresh interval if enabled
      if (loadedConfig.enabled) {
        setRefreshInterval(window.setInterval(() => {
          refreshAll();
        }, 2000));
      }
    } catch (error) {
      dispatch({ type: 'SET_ERROR', payload: { error: `Failed to initialize collaboration: ${error}` } });
    }
  };

  // Clean up interval on unmount
  useEffect(() => {
    return () => {
      if (refreshInterval) {
        clearInterval(refreshInterval);
      }
    };
  }, [refreshInterval]);

  // Update configuration
  const updateConfig = async (config: CollaborationConfig) => {
    try {
      await invoke('update_collaboration_config', { config });
      dispatch({ type: 'UPDATE_CONFIG', payload: { config } });
      
      // Update refresh interval if needed
      if (config.enabled && !refreshInterval) {
        setRefreshInterval(window.setInterval(() => {
          refreshAll();
        }, 2000));
      } else if (!config.enabled && refreshInterval) {
        clearInterval(refreshInterval);
        setRefreshInterval(null);
      }
    } catch (error) {
      dispatch({ type: 'SET_ERROR', payload: { error: `Failed to update config: ${error}` } });
    }
  };

  // Create a new session
  const createSession = async (name: string, conversationId: string) => {
    try {
      const session = await invoke<Session>('create_session', { name, conversationId });
      dispatch({ type: 'CREATE_SESSION_SUCCESS', payload: { session } });
      await refreshUsers();
      return session;
    } catch (error) {
      dispatch({ type: 'SET_ERROR', payload: { error: `Failed to create session: ${error}` } });
      throw error;
    }
  };

  // Join an existing session
  const joinSession = async (sessionId: string) => {
    try {
      const session = await invoke<Session>('join_session', { sessionId });
      dispatch({ type: 'JOIN_SESSION_SUCCESS', payload: { session } });
      await refreshUsers();
      return session;
    } catch (error) {
      dispatch({ type: 'SET_ERROR', payload: { error: `Failed to join session: ${error}` } });
      throw error;
    }
  };

  // Leave the current session
  const leaveSession = async () => {
    try {
      await invoke('leave_session');
      dispatch({ type: 'LEAVE_SESSION_SUCCESS' });
    } catch (error) {
      dispatch({ type: 'SET_ERROR', payload: { error: `Failed to leave session: ${error}` } });
      throw error;
    }
  };

  // Invite a user to the current session
  const inviteUser = async (email: string, role: UserRole) => {
    try {
      await invoke('invite_user', { email, role });
      await refreshUsers();
    } catch (error) {
      dispatch({ type: 'SET_ERROR', payload: { error: `Failed to invite user: ${error}` } });
      throw error;
    }
  };

  // Remove a user from the current session
  const removeUser = async (userId: string) => {
    try {
      await invoke('remove_user', { userId });
      await refreshUsers();
    } catch (error) {
      dispatch({ type: 'SET_ERROR', payload: { error: `Failed to remove user: ${error}` } });
      throw error;
    }
  };

  // Change a user's role
  const changeUserRole = async (userId: string, role: UserRole) => {
    try {
      await invoke('change_user_role', { userId, role });
      await refreshUsers();
    } catch (error) {
      dispatch({ type: 'SET_ERROR', payload: { error: `Failed to change user role: ${error}` } });
      throw error;
    }
  };

  // Update cursor position
  const updateCursorPosition = async (x: number, y: number, elementId?: string) => {
    try {
      await invoke('update_cursor_position', { x, y, elementId });
    } catch (error) {
      console.error(`Failed to update cursor position: ${error}`);
      // Don't show an error dialog for cursor updates
    }
  };

  // Update text selection
  const updateSelection = async (
    startId: string,
    endId: string,
    startOffset: number,
    endOffset: number
  ) => {
    try {
      await invoke('update_selection', { startId, endId, startOffset, endOffset });
    } catch (error) {
      console.error(`Failed to update selection: ${error}`);
      // Don't show an error dialog for selection updates
    }
  };

  // Start an audio call
  const startAudioCall = async () => {
    try {
      await invoke('start_audio_call');
      await refreshCall();
    } catch (error) {
      dispatch({ type: 'SET_ERROR', payload: { error: `Failed to start audio call: ${error}` } });
      throw error;
    }
  };

  // Start a video call
  const startVideoCall = async () => {
    try {
      await invoke('start_video_call');
      await refreshCall();
    } catch (error) {
      dispatch({ type: 'SET_ERROR', payload: { error: `Failed to start video call: ${error}` } });
      throw error;
    }
  };

  // End the current call
  const endCall = async () => {
    try {
      await invoke('end_call');
      dispatch({ type: 'UPDATE_CALL', payload: { call: undefined } });
    } catch (error) {
      dispatch({ type: 'SET_ERROR', payload: { error: `Failed to end call: ${error}` } });
      throw error;
    }
  };

  // Toggle mute status
  const toggleMute = async () => {
    try {
      const audioEnabled = await invoke<boolean>('toggle_mute');
      await refreshCall();
      return audioEnabled;
    } catch (error) {
      dispatch({ type: 'SET_ERROR', payload: { error: `Failed to toggle mute: ${error}` } });
      throw error;
    }
  };

  // Toggle video status
  const toggleVideo = async () => {
    try {
      const videoEnabled = await invoke<boolean>('toggle_video');
      await refreshCall();
      return videoEnabled;
    } catch (error) {
      dispatch({ type: 'SET_ERROR', payload: { error: `Failed to toggle video: ${error}` } });
      throw error;
    }
  };

  // Update username
  const updateUsername = async (name: string) => {
    try {
      await invoke('update_username', { name });
      await refreshUsers();
    } catch (error) {
      dispatch({ type: 'SET_ERROR', payload: { error: `Failed to update username: ${error}` } });
      throw error;
    }
  };

  // Update avatar
  const updateAvatar = async (avatar?: string) => {
    try {
      await invoke('update_avatar', { avatar });
      await refreshUsers();
    } catch (error) {
      dispatch({ type: 'SET_ERROR', payload: { error: `Failed to update avatar: ${error}` } });
      throw error;
    }
  };

  // Refresh users
  const refreshUsers = async () => {
    try {
      const users = await invoke<User[]>('get_session_users');
      dispatch({ type: 'UPDATE_USERS', payload: { users } });
    } catch (error) {
      console.error(`Failed to refresh users: ${error}`);
    }
  };

  // Refresh cursors
  const refreshCursors = async () => {
    try {
      const cursors = await invoke<Record<string, CursorPosition>>('get_cursors');
      dispatch({ type: 'UPDATE_CURSORS', payload: { cursors } });
    } catch (error) {
      console.error(`Failed to refresh cursors: ${error}`);
    }
  };

  // Refresh selections
  const refreshSelections = async () => {
    try {
      const selections = await invoke<Record<string, Selection>>('get_selections');
      dispatch({ type: 'UPDATE_SELECTIONS', payload: { selections } });
    } catch (error) {
      console.error(`Failed to refresh selections: ${error}`);
    }
  };

  // Refresh call status
  const refreshCall = async () => {
    try {
      const call = await invoke<Call | null>('get_active_call');
      dispatch({ type: 'UPDATE_CALL', payload: { call: call || undefined } });
    } catch (error) {
      console.error(`Failed to refresh call: ${error}`);
    }
  };

  // Refresh media devices
  const refreshMediaDevices = async () => {
    try {
      const devices = await invoke<MediaDevice[]>('get_media_devices');
      dispatch({ type: 'UPDATE_MEDIA_DEVICES', payload: { devices } });
    } catch (error) {
      console.error(`Failed to refresh media devices: ${error}`);
    }
  };

  // Refresh statistics
  const refreshStatistics = async () => {
    try {
      const statistics = await invoke<CollaborationStatistics>('get_collaboration_statistics');
      dispatch({ type: 'UPDATE_STATISTICS', payload: { statistics } });
      
      // Also update connection status
      const status = await invoke<ConnectionStatus>('get_connection_status');
      dispatch({ type: 'UPDATE_CONNECTION_STATUS', payload: { status } });
    } catch (error) {
      console.error(`Failed to refresh statistics: ${error}`);
    }
  };

  // Refresh all data
  const refreshAll = async () => {
    if (!state.initialized || !state.config.enabled) return;
    
    await Promise.all([
      refreshStatistics(),
      state.currentSession ? refreshUsers() : Promise.resolve(),
      state.config.show_presence ? refreshCursors() : Promise.resolve(),
      state.config.show_presence ? refreshSelections() : Promise.resolve(),
      state.config.enable_av ? refreshCall() : Promise.resolve(),
      state.config.enable_av ? refreshMediaDevices() : Promise.resolve(),
    ]);
  };

  const contextValue: CollaborationContextType = {
    state,
    initializeCollaboration,
    updateConfig,
    createSession,
    joinSession,
    leaveSession,
    inviteUser,
    removeUser,
    changeUserRole,
    updateCursorPosition,
    updateSelection,
    startAudioCall,
    startVideoCall,
    endCall,
    toggleMute,
    toggleVideo,
    updateUsername,
    updateAvatar,
    refreshUsers,
    refreshCursors,
    refreshSelections,
    refreshCall,
    refreshMediaDevices,
    refreshStatistics,
    refreshAll,
  };

  return (
    <CollaborationContext.Provider value={contextValue}>
      {children}
    </CollaborationContext.Provider>
  );
};

// Custom hook for using the collaboration context
export const useCollaboration = () => {
  const context = useContext(CollaborationContext);
  if (context === undefined) {
    throw new Error('useCollaboration must be used within a CollaborationProvider');
  }
  return context;
};
</file>

<file path="src-frontend/src/components/collaboration/index.ts">
// index.ts for the collaboration module
//
// This file exports all components, hooks, and types for the collaboration system.

// Context and hooks
export { 
  CollaborationContext, 
  CollaborationProvider,
  useCollaboration,
  ConnectionStatus,
  UserRole,
  type User,
  type Session,
  type CursorPosition,
  type Selection,
  type MediaDevice,
  type Participant,
  type Call,
  type CollaborationConfig,
  type CollaborationStatistics
} from './context/CollaborationContext';

// Main components
export { default as CollaborationPanel } from './CollaborationPanel';
export { default as UserList } from './UserList';

// Presence components
export { default as CursorOverlay } from './presence/CursorOverlay';
export { default as SelectionOverlay } from './presence/SelectionOverlay';
export { default as UserBadge } from './presence/UserBadge';

// Settings components
export { default as CollaborationSettings } from './settings/CollaborationSettings';

// Call components
export { default as CallControls } from './call/CallControls';

// Whiteboard components
export { default as CollaborativeWhiteboard } from './whiteboard/CollaborativeWhiteboard';
</file>

<file path="src-frontend/src/components/collaboration/presence/CursorOverlay.tsx">
// CursorOverlay.tsx
//
// This component renders remote user cursors on top of the application.
// It displays a cursor icon and name badge for each connected user.

import React, { useEffect, useRef, useState, useCallback } from 'react';
import { useCollaboration } from '../../../hooks/useCollaboration';
import { CursorPosition, User } from '../context/CollaborationContext';
import UserBadge from './UserBadge';
import { throttle } from '../../../utils/throttle';

interface CursorOverlayProps {
  containerRef: React.RefObject<HTMLElement>;
  zIndex?: number;
  // Set to customize the throttle time for cursor updates (in ms)
  throttleTime?: number;
}

const CursorOverlay: React.FC<CursorOverlayProps> = ({ 
  containerRef, 
  zIndex = 1000,
  throttleTime = 50 // 50ms default throttle time
}) => {
  const { state, updateCursorPosition } = useCollaboration();
  const { cursors, users, config } = state;
  const overlayRef = useRef<HTMLDivElement>(null);
  const [userMap, setUserMap] = useState<Record<string, User>>({});
  
  // Create a map of users by ID for easy lookup
  useEffect(() => {
    const map: Record<string, User> = {};
    users.forEach(user => {
      map[user.id] = user;
    });
    setUserMap(map);
  }, [users]);

  // Create a throttled version of the updateCursorPosition function
  const throttledUpdateCursor = useCallback(
    throttle((x: number, y: number, elementId?: string) => {
      updateCursorPosition(x, y, elementId);
    }, throttleTime),
    [updateCursorPosition, throttleTime]
  );

  // Track mouse movement in the container to update the user's cursor position
  useEffect(() => {
    if (!containerRef.current || !config.show_presence) return;

    const container = containerRef.current;
    
    const handleMouseMove = (e: MouseEvent) => {
      const rect = container.getBoundingClientRect();
      // Normalize position as 0-1 relative to container
      const x = (e.clientX - rect.left) / rect.width;
      const y = (e.clientY - rect.top) / rect.height;
      
      // Get element under cursor if available
      const element = document.elementFromPoint(e.clientX, e.clientY);
      const elementId = element?.id || undefined;
      
      // Send cursor position update (throttled)
      throttledUpdateCursor(x, y, elementId);
    };

    container.addEventListener('mousemove', handleMouseMove);
    
    return () => {
      container.removeEventListener('mousemove', handleMouseMove);
    };
  }, [containerRef, config.show_presence, throttledUpdateCursor]);

  // Don't render if presence is disabled
  if (!config.show_presence) {
    return null;
  }

  // Calculate absolute position from normalized coordinates
  const getAbsolutePosition = (cursor: CursorPosition) => {
    if (!containerRef.current) {
      return { x: 0, y: 0 };
    }

    const rect = containerRef.current.getBoundingClientRect();
    return {
      x: rect.left + cursor.x * rect.width,
      y: rect.top + cursor.y * rect.height,
    };
  };

  return (
    <div 
      ref={overlayRef}
      style={{
        position: 'absolute',
        top: 0,
        left: 0,
        width: '100%',
        height: '100%',
        pointerEvents: 'none',
        zIndex,
      }}
      className="cursor-overlay"
    >
      {Object.entries(cursors).map(([userId, cursor]) => {
        const user = userMap[userId];
        if (!user) return null;

        const { x, y } = getAbsolutePosition(cursor);
        
        // Cursor was updated in the last 10 seconds (not stale)
        const isFresh = new Date(cursor.timestamp).getTime() > (Date.now() - 10000);
        
        // Don't render stale cursors
        if (!isFresh) return null;

        return (
          <div
            key={`cursor-${userId}`}
            style={{
              position: 'absolute',
              left: `${x}px`,
              top: `${y}px`,
              transform: 'translate(-50%, -50%)',
              pointerEvents: 'none',
              transition: 'left 0.1s ease, top 0.1s ease',
            }}
            className="cursor-wrapper"
            data-user-id={userId}
          >
            {/* Cursor icon */}
            <svg
              width="24"
              height="24"
              viewBox="0 0 24 24"
              fill="none"
              stroke={user.color}
              strokeWidth="2"
              strokeLinecap="round"
              strokeLinejoin="round"
              className="cursor-icon"
            >
              <path d="M3 3l7.07 16.97 2.51-7.39 7.39-2.51L3 3z" />
            </svg>
            
            {/* User badge */}
            <UserBadge user={user} />
          </div>
        );
      })}
    </div>
  );
};

export default CursorOverlay;
</file>

<file path="src-frontend/src/components/collaboration/presence/SelectionOverlay.tsx">
// SelectionOverlay.tsx
//
// This component renders remote user selections in the text editor.
// It displays a colored highlight over text that other users have selected.

import React, { useEffect, useState } from 'react';
import { useCollaboration } from '../../../hooks/useCollaboration';
import { Selection, User } from '../context/CollaborationContext';

interface SelectionOverlayProps {
  editorRef: React.RefObject<HTMLElement>;
}

// Helper to get DOM range from selection data
const getSelectionRange = (
  selection: Selection, 
  container: HTMLElement
): Range | null => {
  try {
    // Find start and end elements by ID
    const startElement = container.querySelector(`#${selection.start_id}`);
    const endElement = container.querySelector(`#${selection.end_id}`);
    
    if (!startElement || !endElement) {
      return null;
    }
    
    const range = document.createRange();
    
    // Set start position
    if (startElement.childNodes.length > 0) {
      range.setStart(startElement.childNodes[0], selection.start_offset);
    } else {
      range.setStart(startElement, selection.start_offset);
    }
    
    // Set end position
    if (endElement.childNodes.length > 0) {
      range.setEnd(endElement.childNodes[0], selection.end_offset);
    } else {
      range.setEnd(endElement, selection.end_offset);
    }
    
    return range;
  } catch (error) {
    console.error('Error creating selection range:', error);
    return null;
  }
};

const SelectionOverlay: React.FC<SelectionOverlayProps> = ({ editorRef }) => {
  const { state, updateSelection } = useCollaboration();
  const { selections, users, config } = state;
  const [userMap, setUserMap] = useState<Record<string, User>>({});
  const [selectionElements, setSelectionElements] = useState<HTMLElement[]>([]);
  
  // Create a map of users by ID for easy lookup
  useEffect(() => {
    const map: Record<string, User> = {};
    users.forEach(user => {
      map[user.id] = user;
    });
    setUserMap(map);
  }, [users]);
  
  // Track local text selection to send updates
  useEffect(() => {
    if (!editorRef.current || !config.show_presence) return;
    
    const container = editorRef.current;
    
    const handleSelectionChange = () => {
      const selection = document.getSelection();
      if (!selection || selection.isCollapsed || !selection.rangeCount) return;
      
      const range = selection.getRangeAt(0);
      const startContainer = range.startContainer.parentElement;
      const endContainer = range.endContainer.parentElement;
      
      if (!startContainer || !endContainer) return;
      
      // Get element IDs for the selection endpoints
      const startId = startContainer.id || startContainer.parentElement?.id;
      const endId = endContainer.id || endContainer.parentElement?.id;
      
      if (!startId || !endId) return;
      
      // Send selection update
      updateSelection(
        startId,
        endId,
        range.startOffset,
        range.endOffset
      );
    };
    
    document.addEventListener('selectionchange', handleSelectionChange);
    
    return () => {
      document.removeEventListener('selectionchange', handleSelectionChange);
    };
  }, [editorRef, config.show_presence, updateSelection]);
  
  // Render remote selections when they change
  useEffect(() => {
    if (!editorRef.current || !config.show_presence) {
      // Clean up any existing highlights
      selectionElements.forEach(el => el.remove());
      setSelectionElements([]);
      return;
    }
    
    const container = editorRef.current;
    
    // Clean up existing highlights
    selectionElements.forEach(el => el.remove());
    
    // Create new highlights for each selection
    const newElements: HTMLElement[] = [];
    
    Object.entries(selections).forEach(([userId, selection]) => {
      const user = userMap[userId];
      if (!user) return;
      
      // Selection was updated in the last 30 seconds (not stale)
      const isFresh = new Date(selection.timestamp).getTime() > (Date.now() - 30000);
      if (!isFresh) return;
      
      const range = getSelectionRange(selection, container);
      if (!range) return;
      
      // Create highlight elements
      const rects = range.getClientRects();
      for (let i = 0; i < rects.length; i++) {
        const rect = rects[i];
        const highlight = document.createElement('div');
        
        // Position the highlight
        highlight.style.position = 'absolute';
        highlight.style.left = `${rect.left}px`;
        highlight.style.top = `${rect.top}px`;
        highlight.style.width = `${rect.width}px`;
        highlight.style.height = `${rect.height}px`;
        highlight.style.backgroundColor = `${user.color}33`; // 20% opacity
        highlight.style.pointerEvents = 'none';
        highlight.style.zIndex = '999';
        highlight.dataset.userId = userId;
        highlight.className = 'selection-highlight';
        
        document.body.appendChild(highlight);
        newElements.push(highlight);
      }
    });
    
    setSelectionElements(newElements);
    
    // Clean up on unmount
    return () => {
      newElements.forEach(el => el.remove());
    };
  }, [editorRef, selections, userMap, config.show_presence, selectionElements]);
  
  // This is a DOM manipulation component that doesn't render anything directly
  return null;
};

export default SelectionOverlay;
</file>

<file path="src-frontend/src/components/collaboration/presence/UserBadge.tsx">
// UserBadge.tsx
//
// This component renders a badge with user information.
// It's used to display user names next to their cursors and selections.

import React, { useState } from 'react';
import { User, UserRole } from '../context/CollaborationContext';

interface UserBadgeProps {
  user: User;
  showRole?: boolean;
  alwaysShow?: boolean;
}

// Helper to get role display name
const getRoleDisplayName = (role: UserRole): string => {
  switch (role) {
    case UserRole.Owner:
      return 'Owner';
    case UserRole.CoOwner:
      return 'Co-owner';
    case UserRole.Editor:
      return 'Editor';
    case UserRole.Commentator:
      return 'Commentator';
    case UserRole.Viewer:
      return 'Viewer';
    default:
      return 'User';
  }
};

// Helper to get role badge color
const getRoleBadgeColor = (role: UserRole): string => {
  switch (role) {
    case UserRole.Owner:
      return '#FF5722';
    case UserRole.CoOwner:
      return '#FF9800';
    case UserRole.Editor:
      return '#2196F3';
    case UserRole.Commentator:
      return '#4CAF50';
    case UserRole.Viewer:
      return '#9E9E9E';
    default:
      return '#9E9E9E';
  }
};

const UserBadge: React.FC<UserBadgeProps> = ({ 
  user, 
  showRole = true,
  alwaysShow = false
}) => {
  const [isHovered, setIsHovered] = useState(false);
  
  // Only show badge on hover unless alwaysShow is true
  const isVisible = alwaysShow || isHovered;
  
  // Create user initials for avatar
  const getInitials = (name: string): string => {
    const parts = name.split(' ');
    if (parts.length === 1) {
      return name.substring(0, 2).toUpperCase();
    }
    return (parts[0][0] + parts[parts.length - 1][0]).toUpperCase();
  };
  
  return (
    <div
      onMouseEnter={() => setIsHovered(true)}
      onMouseLeave={() => setIsHovered(false)}
      style={{
        position: 'absolute',
        left: '20px',
        top: '-10px',
        display: 'flex',
        alignItems: 'center',
        transform: isVisible ? 'translateY(-100%)' : 'translateY(-100%) scale(0.8)',
        opacity: isVisible ? 1 : 0,
        transition: 'transform 0.2s ease, opacity 0.2s ease',
        pointerEvents: 'none',
        zIndex: 5,
      }}
      className="cursor-badge"
    >
      {/* User badge with name and optional role */}
      <div
        style={{
          backgroundColor: user.color,
          color: '#fff',
          padding: '4px 8px',
          borderRadius: '4px',
          fontWeight: 500,
          fontSize: '12px',
          whiteSpace: 'nowrap',
          display: 'flex',
          alignItems: 'center',
          boxShadow: '0 2px 5px rgba(0, 0, 0, 0.2)',
          maxWidth: '200px',
        }}
      >
        {/* Avatar/Initials */}
        <div
          style={{
            width: '18px',
            height: '18px',
            borderRadius: '50%',
            backgroundColor: '#ffffff44',
            display: 'flex',
            alignItems: 'center',
            justifyContent: 'center',
            marginRight: '6px',
            fontSize: '10px',
            fontWeight: 600,
            overflow: 'hidden',
          }}
        >
          {user.avatar ? (
            <img
              src={user.avatar}
              alt={user.name}
              style={{ width: '100%', height: '100%', objectFit: 'cover' }}
            />
          ) : (
            getInitials(user.name)
          )}
        </div>

        {/* Name */}
        <span
          style={{
            overflow: 'hidden',
            textOverflow: 'ellipsis',
          }}
        >
          {user.name}
        </span>
        
        {/* Role badge (if enabled) */}
        {showRole && (
          <div
            style={{
              backgroundColor: getRoleBadgeColor(user.role),
              fontSize: '9px',
              padding: '2px 4px',
              borderRadius: '3px',
              marginLeft: '5px',
              lineHeight: 1,
            }}
          >
            {getRoleDisplayName(user.role)}
          </div>
        )}
      </div>
      
      {/* Pointer */}
      <div
        style={{
          width: 0,
          height: 0,
          borderLeft: '5px solid transparent',
          borderRight: '5px solid transparent',
          borderTop: `5px solid ${user.color}`,
          position: 'absolute',
          bottom: '-5px',
          left: '20px',
        }}
      />
    </div>
  );
};

export default UserBadge;
</file>

<file path="src-frontend/src/components/collaboration/settings/CollaborationSettings.tsx">
// CollaborationSettings.tsx
//
// This component provides a UI for configuring collaboration settings.

import React, { useState } from 'react';
import { useCollaboration } from '../../../hooks/useCollaboration';
import { CollaborationConfig } from '../context/CollaborationContext';

interface CollaborationSettingsProps {
  // Optional props
}

const CollaborationSettings: React.FC<CollaborationSettingsProps> = () => {
  const { state, updateConfig } = useCollaboration();
  const { config } = state;
  
  const [localConfig, setLocalConfig] = useState<CollaborationConfig>(config);
  const [isSaving, setIsSaving] = useState<boolean>(false);
  const [error, setError] = useState<string | null>(null);
  const [saveSuccess, setSaveSuccess] = useState<boolean>(false);
  
  // Handle input changes
  const handleChange = (e: React.ChangeEvent<HTMLInputElement | HTMLSelectElement>) => {
    const { name, value, type } = e.target;
    
    setLocalConfig(prev => {
      // Handle different input types
      if (type === 'checkbox') {
        const checked = (e.target as HTMLInputElement).checked;
        return { ...prev, [name]: checked };
      } else if (type === 'number') {
        return { ...prev, [name]: parseInt(value, 10) };
      } else {
        return { ...prev, [name]: value };
      }
    });
    
    // Clear success message on changes
    setSaveSuccess(false);
  };
  
  // Handle saving the configuration
  const handleSave = async () => {
    setIsSaving(true);
    setError(null);
    setSaveSuccess(false);
    
    try {
      await updateConfig(localConfig);
      setSaveSuccess(true);
    } catch (err) {
      setError(`Failed to save settings: ${err}`);
    } finally {
      setIsSaving(false);
    }
  };
  
  return (
    <div className="collaboration-settings">
      <h3 style={{ marginTop: 0 }}>Collaboration Settings</h3>
      
      {error && (
        <div className="collaboration-error">
          {error}
        </div>
      )}
      
      {saveSuccess && (
        <div style={{
          padding: '10px',
          backgroundColor: '#E8F5E9',
          color: '#2E7D32',
          borderRadius: '4px',
          marginBottom: '15px',
        }}>
          Settings saved successfully!
        </div>
      )}
      
      <div className="collaboration-form-group">
        <label className="collaboration-label">
          <input
            type="checkbox"
            name="enabled"
            checked={localConfig.enabled}
            onChange={handleChange}
            style={{ marginRight: '8px' }}
          />
          Enable collaboration features
        </label>
      </div>
      
      <div className="collaboration-form-group">
        <label className="collaboration-label">
          <input
            type="checkbox"
            name="show_presence"
            checked={localConfig.show_presence}
            onChange={handleChange}
            style={{ marginRight: '8px' }}
          />
          Show user presence (cursors, selections)
        </label>
      </div>
      
      <div className="collaboration-form-group">
        <label className="collaboration-label">
          <input
            type="checkbox"
            name="enable_av"
            checked={localConfig.enable_av}
            onChange={handleChange}
            style={{ marginRight: '8px' }}
          />
          Enable audio/video calls
        </label>
      </div>
      
      <div className="collaboration-form-group">
        <label className="collaboration-label">
          <input
            type="checkbox"
            name="auto_discover"
            checked={localConfig.auto_discover}
            onChange={handleChange}
            style={{ marginRight: '8px' }}
          />
          Auto-discover other devices
        </label>
      </div>
      
      <div className="collaboration-form-group">
        <label className="collaboration-label">
          <input
            type="checkbox"
            name="p2p_enabled"
            checked={localConfig.p2p_enabled}
            onChange={handleChange}
            style={{ marginRight: '8px' }}
          />
          Enable peer-to-peer mode
        </label>
      </div>
      
      <div className="collaboration-form-group">
        <label className="collaboration-label" htmlFor="max_users">
          Maximum users per session
        </label>
        <input
          id="max_users"
          type="number"
          name="max_users_per_session"
          value={localConfig.max_users_per_session}
          onChange={handleChange}
          min={1}
          max={50}
          className="collaboration-input"
        />
      </div>
      
      <div className="collaboration-form-group">
        <label className="collaboration-label" htmlFor="sync_interval">
          Sync interval (milliseconds)
        </label>
        <input
          id="sync_interval"
          type="number"
          name="sync_interval_ms"
          value={localConfig.sync_interval_ms}
          onChange={handleChange}
          min={100}
          max={10000}
          step={100}
          className="collaboration-input"
        />
      </div>
      
      <div className="collaboration-form-group">
        <label className="collaboration-label" htmlFor="username">
          Display name (optional)
        </label>
        <input
          id="username"
          type="text"
          name="username"
          value={localConfig.username || ''}
          onChange={handleChange}
          placeholder="Your name in collaborative sessions"
          className="collaboration-input"
        />
      </div>
      
      <div className="collaboration-button-group" style={{ marginTop: '20px' }}>
        <button
          onClick={handleSave}
          disabled={isSaving}
          className="collaboration-button primary"
        >
          {isSaving ? 'Saving...' : 'Save Settings'}
        </button>
      </div>
    </div>
  );
};

export default CollaborationSettings;
</file>

<file path="src-frontend/src/components/collaboration/UserList.tsx">
// UserList.tsx
//
// This component displays a list of users in the current collaboration session,
// allowing management of users and their roles.

import React, { useState } from 'react';
import { useCollaboration } from '../../hooks/useCollaboration';
import { User, UserRole } from './context/CollaborationContext';

interface UserListProps {
  // Optional props can be added here
}

const UserList: React.FC<UserListProps> = () => {
  const { state, inviteUser, removeUser, changeUserRole } = useCollaboration();
  const { users, currentUser } = state;
  
  const [email, setEmail] = useState<string>('');
  const [selectedRole, setSelectedRole] = useState<UserRole>(UserRole.Editor);
  const [showInviteForm, setShowInviteForm] = useState<boolean>(false);
  const [isLoading, setIsLoading] = useState<boolean>(false);
  const [error, setError] = useState<string | null>(null);
  
  // Handle inviting a new user
  const handleInviteUser = async () => {
    if (!email.trim()) {
      setError('Please enter an email address');
      return;
    }
    
    setIsLoading(true);
    setError(null);
    
    try {
      await inviteUser(email, selectedRole);
      setEmail('');
      setShowInviteForm(false);
    } catch (err) {
      setError(`Failed to invite user: ${err}`);
    } finally {
      setIsLoading(false);
    }
  };
  
  // Handle removing a user
  const handleRemoveUser = async (userId: string) => {
    if (!confirm('Are you sure you want to remove this user?')) {
      return;
    }
    
    try {
      await removeUser(userId);
    } catch (err) {
      console.error('Failed to remove user:', err);
      alert(`Failed to remove user: ${err}`);
    }
  };
  
  // Handle changing a user's role
  const handleChangeRole = async (userId: string, newRole: UserRole) => {
    try {
      await changeUserRole(userId, newRole);
    } catch (err) {
      console.error('Failed to change user role:', err);
      alert(`Failed to change user role: ${err}`);
    }
  };
  
  // Check if current user has permission to manage users
  const canManageUsers = () => {
    if (!currentUser) return false;
    return (
      currentUser.role === UserRole.Owner || 
      currentUser.role === UserRole.CoOwner
    );
  };
  
  // Get time since user was last active
  const getLastActiveTime = (user: User) => {
    const lastActive = new Date(user.last_active);
    const now = new Date();
    const diffMs = now.getTime() - lastActive.getTime();
    const diffSec = Math.floor(diffMs / 1000);
    
    if (diffSec < 60) {
      return 'Just now';
    } else if (diffSec < 3600) {
      const mins = Math.floor(diffSec / 60);
      return `${mins} ${mins === 1 ? 'minute' : 'minutes'} ago`;
    } else if (diffSec < 86400) {
      const hours = Math.floor(diffSec / 3600);
      return `${hours} ${hours === 1 ? 'hour' : 'hours'} ago`;
    } else {
      const days = Math.floor(diffSec / 86400);
      return `${days} ${days === 1 ? 'day' : 'days'} ago`;
    }
  };
  
  // Render invite form
  const renderInviteForm = () => {
    if (!showInviteForm) {
      return (
        <button
          onClick={() => setShowInviteForm(true)}
          className="collaboration-button primary"
          style={{
            marginBottom: '15px',
            width: '100%',
          }}
        >
          Invite User
        </button>
      );
    }
    
    return (
      <div style={{
        padding: '15px',
        backgroundColor: '#F5F5F5',
        borderRadius: '6px',
        marginBottom: '15px',
      }}>
        <h4 style={{ margin: '0 0 10px 0' }}>Invite a User</h4>
        
        {error && (
          <div className="collaboration-error" style={{ fontSize: '14px' }}>
            {error}
          </div>
        )}
        
        <div className="collaboration-form-group" style={{ marginBottom: '10px' }}>
          <label 
            htmlFor="email" 
            className="collaboration-label"
            style={{ fontSize: '14px' }}
          >
            Email Address
          </label>
          <input
            id="email"
            type="email"
            value={email}
            onChange={(e) => setEmail(e.target.value)}
            placeholder="Enter email address"
            className="collaboration-input"
          />
        </div>
        
        <div className="collaboration-form-group" style={{ marginBottom: '15px' }}>
          <label 
            htmlFor="role" 
            className="collaboration-label"
            style={{ fontSize: '14px' }}
          >
            Role
          </label>
          <select
            id="role"
            value={selectedRole}
            onChange={(e) => setSelectedRole(e.target.value as UserRole)}
            className="collaboration-select"
          >
            <option value={UserRole.Editor}>Editor</option>
            <option value={UserRole.CoOwner}>Co-Owner</option>
            <option value={UserRole.Commentator}>Commentator</option>
            <option value={UserRole.Viewer}>Viewer</option>
          </select>
        </div>
        
        <div className="collaboration-button-group" style={{ marginTop: '0' }}>
          <button
            onClick={() => setShowInviteForm(false)}
            className="collaboration-button secondary"
            style={{ fontSize: '14px' }}
          >
            Cancel
          </button>
          <button
            onClick={handleInviteUser}
            disabled={isLoading || !email.trim()}
            className="collaboration-button primary"
            style={{
              fontSize: '14px',
              opacity: isLoading || !email.trim() ? 0.7 : 1,
            }}
          >
            {isLoading ? 'Inviting...' : 'Send Invite'}
          </button>
        </div>
      </div>
    );
  };
  
  // If no users in session
  if (users.length === 0) {
    return (
      <div style={{ padding: '15px', textAlign: 'center' }}>
        <p>No users in this session yet.</p>
        
        {canManageUsers() && renderInviteForm()}
      </div>
    );
  }
  
  return (
    <div className="user-list">
      {canManageUsers() && renderInviteForm()}
      
      {users.map(user => (
        <div
          key={user.id}
          className="user-item"
          style={{
            backgroundColor: user.id === currentUser?.id ? '#E3F2FD' : 'white',
          }}
        >
          <div className="user-item-info">
            {/* User color indicator */}
            <div style={{
              width: '12px',
              height: '12px',
              borderRadius: '50%',
              backgroundColor: user.color,
              marginRight: '10px',
            }}/>
            
            {/* User avatar/name */}
            <div className="user-details">
              <div className="user-name">
                {user.name}
                {user.id === currentUser?.id && (
                  <span style={{ fontSize: '12px', marginLeft: '5px', color: '#757575' }}>
                    (You)
                  </span>
                )}
              </div>
              <div style={{ display: 'flex', alignItems: 'center', fontSize: '12px', color: '#757575' }}>
                <div style={{
                  width: '8px',
                  height: '8px',
                  borderRadius: '50%',
                  backgroundColor: user.online ? '#4CAF50' : '#9E9E9E',
                  marginRight: '5px',
                }}/>
                {user.online ? 'Online' : `Last active ${getLastActiveTime(user)}`}
              </div>
            </div>
          </div>
          
          {/* User role and actions */}
          <div className="user-actions">
            {/* Role indicator */}
            <div style={{
              padding: '4px 8px',
              borderRadius: '4px',
              backgroundColor: '#EEEEEE',
              fontSize: '12px',
              marginRight: '10px',
            }}>
              {user.role}
            </div>
            
            {/* Actions for owners/co-owners */}
            {canManageUsers() && user.id !== currentUser?.id && (
              <div style={{ display: 'flex', gap: '5px' }}>
                {/* Role change dropdown */}
                <select
                  value={user.role}
                  onChange={(e) => handleChangeRole(user.id, e.target.value as UserRole)}
                  style={{
                    padding: '4px',
                    fontSize: '12px',
                    borderRadius: '4px',
                    border: '1px solid #BDBDBD',
                  }}
                >
                  <option value={UserRole.CoOwner}>Co-Owner</option>
                  <option value={UserRole.Editor}>Editor</option>
                  <option value={UserRole.Commentator}>Commentator</option>
                  <option value={UserRole.Viewer}>Viewer</option>
                </select>
                
                {/* Remove button */}
                <button
                  onClick={() => handleRemoveUser(user.id)}
                  className="collaboration-button danger"
                  style={{
                    padding: '4px 8px',
                    fontSize: '12px',
                  }}
                >
                  Remove
                </button>
              </div>
            )}
          </div>
        </div>
      ))}
    </div>
  );
};

export default UserList;
</file>

<file path="src-frontend/src/components/collaboration/whiteboard/CollaborativeWhiteboard.tsx">
// CollaborativeWhiteboard.tsx
//
// This component provides a shared whiteboard that multiple users can draw on simultaneously.
// It synchronizes drawing operations between all connected users.

import React, { useRef, useEffect, useState, useCallback } from 'react';
import { useCollaboration } from '../../../hooks/useCollaboration';
import { throttle } from '../../../utils/throttle';
import { invoke } from '@tauri-apps/api/tauri';

// Types for whiteboard operations
interface Point {
  x: number;
  y: number;
}

interface DrawOperation {
  type: 'pencil' | 'line' | 'rectangle' | 'circle' | 'text' | 'eraser';
  points: Point[];
  color: string;
  size: number;
  text?: string;
  userId: string;
  timestamp: number;
}

interface WhiteboardState {
  operations: DrawOperation[];
  version: number;
}

interface CollaborativeWhiteboardProps {
  width?: number;
  height?: number;
  sessionId: string;
}

// Custom hook for synchronized whiteboard operations
const useSynchronizedWhiteboard = (sessionId: string) => {
  const { state } = useCollaboration();
  const [whiteboardState, setWhiteboardState] = useState<WhiteboardState>({
    operations: [],
    version: 0,
  });
  const [isDrawing, setIsDrawing] = useState(false);
  const [currentOperation, setCurrentOperation] = useState<DrawOperation | null>(null);
  const [tool, setTool] = useState<'pencil' | 'line' | 'rectangle' | 'circle' | 'text' | 'eraser'>('pencil');
  const [color, setColor] = useState('#000000');
  const [size, setSize] = useState(2);
  
  // Send an operation to other users
  const sendOperation = useCallback(
    throttle((operation: DrawOperation) => {
      // In a real implementation, we would use a Tauri command to send this to other users
      // For now, we'll just update our local state
      invoke('send_whiteboard_operation', { 
        sessionId, 
        operation: JSON.stringify(operation) 
      }).catch(error => {
        console.error('Failed to send whiteboard operation:', error);
      });
      
      // Optimistically update our local state
      setWhiteboardState(prev => ({
        operations: [...prev.operations, operation],
        version: prev.version + 1,
      }));
    }, 50), // throttle to 50ms
    [sessionId]
  );
  
  // Start a new drawing operation
  const startOperation = useCallback((x: number, y: number) => {
    if (!state.currentUser) return;
    
    const newOperation: DrawOperation = {
      type: tool,
      points: [{ x, y }],
      color,
      size,
      userId: state.currentUser.id,
      timestamp: Date.now(),
    };
    
    setCurrentOperation(newOperation);
    setIsDrawing(true);
  }, [tool, color, size, state.currentUser]);
  
  // Continue a drawing operation
  const continueOperation = useCallback((x: number, y: number) => {
    if (!isDrawing || !currentOperation) return;
    
    const updatedOperation = {
      ...currentOperation,
      points: [...currentOperation.points, { x, y }],
    };
    
    setCurrentOperation(updatedOperation);
  }, [isDrawing, currentOperation]);
  
  // End a drawing operation
  const endOperation = useCallback(() => {
    if (!isDrawing || !currentOperation) return;
    
    // Send the completed operation to other users
    sendOperation(currentOperation);
    
    setIsDrawing(false);
    setCurrentOperation(null);
  }, [isDrawing, currentOperation, sendOperation]);
  
  // Receive operations from other users
  useEffect(() => {
    // In a real implementation, we would listen for whiteboard operations from other users
    // For now, we'll simulate this with a mock implementation
    
    const handleIncomingOperation = (event: CustomEvent) => {
      const operation = event.detail as DrawOperation;
      
      // Don't process our own operations (we already added them)
      if (state.currentUser && operation.userId === state.currentUser.id) {
        return;
      }
      
      // Add the operation to our state
      setWhiteboardState(prev => ({
        operations: [...prev.operations, operation],
        version: prev.version + 1,
      }));
    };
    
    // Create a custom event type for whiteboard operations
    window.addEventListener('whiteboard-operation' as any, handleIncomingOperation as EventListener);
    
    return () => {
      window.removeEventListener('whiteboard-operation' as any, handleIncomingOperation as EventListener);
    };
  }, [state.currentUser]);
  
  return {
    whiteboardState,
    isDrawing,
    currentOperation,
    tool,
    color,
    size,
    setTool,
    setColor,
    setSize,
    startOperation,
    continueOperation,
    endOperation,
  };
};

const CollaborativeWhiteboard: React.FC<CollaborativeWhiteboardProps> = ({
  width = 800,
  height = 600,
  sessionId,
}) => {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const {
    whiteboardState,
    isDrawing,
    currentOperation,
    tool,
    color,
    size,
    setTool,
    setColor,
    setSize,
    startOperation,
    continueOperation,
    endOperation,
  } = useSynchronizedWhiteboard(sessionId);
  
  // Convert from screen coordinates to canvas coordinates
  const getCanvasCoordinates = useCallback((e: React.MouseEvent<HTMLCanvasElement>) => {
    const canvas = canvasRef.current;
    if (!canvas) return { x: 0, y: 0 };
    
    const rect = canvas.getBoundingClientRect();
    return {
      x: ((e.clientX - rect.left) / rect.width) * canvas.width,
      y: ((e.clientY - rect.top) / rect.height) * canvas.height,
    };
  }, []);
  
  // Mouse event handlers
  const handleMouseDown = useCallback((e: React.MouseEvent<HTMLCanvasElement>) => {
    const { x, y } = getCanvasCoordinates(e);
    startOperation(x, y);
  }, [getCanvasCoordinates, startOperation]);
  
  const handleMouseMove = useCallback((e: React.MouseEvent<HTMLCanvasElement>) => {
    const { x, y } = getCanvasCoordinates(e);
    continueOperation(x, y);
  }, [getCanvasCoordinates, continueOperation]);
  
  const handleMouseUp = useCallback(() => {
    endOperation();
  }, [endOperation]);
  
  const handleMouseLeave = useCallback(() => {
    if (isDrawing) {
      endOperation();
    }
  }, [isDrawing, endOperation]);
  
  // Render all operations to the canvas
  useEffect(() => {
    const canvas = canvasRef.current;
    if (!canvas) return;
    
    const ctx = canvas.getContext('2d');
    if (!ctx) return;
    
    // Clear the canvas
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    
    // Draw all completed operations
    whiteboardState.operations.forEach(op => {
      drawOperation(ctx, op);
    });
    
    // Draw the current operation (if any)
    if (currentOperation) {
      drawOperation(ctx, currentOperation);
    }
  }, [whiteboardState, currentOperation]);
  
  // Draw a single operation to the canvas
  const drawOperation = (ctx: CanvasRenderingContext2D, op: DrawOperation) => {
    const { type, points, color, size } = op;
    
    ctx.strokeStyle = color;
    ctx.lineWidth = size;
    ctx.lineJoin = 'round';
    ctx.lineCap = 'round';
    
    switch (type) {
      case 'pencil':
        if (points.length < 2) return;
        
        ctx.beginPath();
        ctx.moveTo(points[0].x, points[0].y);
        
        for (let i = 1; i < points.length; i++) {
          ctx.lineTo(points[i].x, points[i].y);
        }
        
        ctx.stroke();
        break;
        
      case 'line':
        if (points.length < 2) return;
        
        ctx.beginPath();
        ctx.moveTo(points[0].x, points[0].y);
        ctx.lineTo(points[points.length - 1].x, points[points.length - 1].y);
        ctx.stroke();
        break;
        
      case 'rectangle':
        if (points.length < 2) return;
        
        const startPoint = points[0];
        const endPoint = points[points.length - 1];
        
        ctx.beginPath();
        ctx.rect(
          startPoint.x,
          startPoint.y,
          endPoint.x - startPoint.x,
          endPoint.y - startPoint.y
        );
        ctx.stroke();
        break;
        
      case 'circle':
        if (points.length < 2) return;
        
        const center = points[0];
        const edge = points[points.length - 1];
        const radius = Math.sqrt(
          Math.pow(edge.x - center.x, 2) + Math.pow(edge.y - center.y, 2)
        );
        
        ctx.beginPath();
        ctx.arc(center.x, center.y, radius, 0, 2 * Math.PI);
        ctx.stroke();
        break;
        
      case 'eraser':
        if (points.length < 2) return;
        
        // For the eraser, we draw in white with a larger size
        const originalStrokeStyle = ctx.strokeStyle;
        const originalLineWidth = ctx.lineWidth;
        
        ctx.strokeStyle = '#ffffff';
        ctx.lineWidth = size * 2;
        
        ctx.beginPath();
        ctx.moveTo(points[0].x, points[0].y);
        
        for (let i = 1; i < points.length; i++) {
          ctx.lineTo(points[i].x, points[i].y);
        }
        
        ctx.stroke();
        
        // Restore original styles
        ctx.strokeStyle = originalStrokeStyle;
        ctx.lineWidth = originalLineWidth;
        break;
        
      case 'text':
        if (points.length < 1 || !op.text) return;
        
        ctx.font = `${size * 5}px Arial`;
        ctx.fillStyle = color;
        ctx.fillText(op.text, points[0].x, points[0].y);
        break;
    }
  };
  
  return (
    <div className="collaborative-whiteboard">
      <div className="whiteboard-toolbar">
        <div className="tool-group">
          <button
            className={`tool-button ${tool === 'pencil' ? 'active' : ''}`}
            onClick={() => setTool('pencil')}
            title="Pencil"
          >
            <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2">
              <path d="M17 3a2.85 2.83 0 1 1 4 4L7.5 20.5 2 22l1.5-5.5L17 3z"></path>
            </svg>
          </button>
          <button
            className={`tool-button ${tool === 'line' ? 'active' : ''}`}
            onClick={() => setTool('line')}
            title="Line"
          >
            <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2">
              <line x1="5" y1="12" x2="19" y2="12"></line>
            </svg>
          </button>
          <button
            className={`tool-button ${tool === 'rectangle' ? 'active' : ''}`}
            onClick={() => setTool('rectangle')}
            title="Rectangle"
          >
            <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2">
              <rect x="3" y="3" width="18" height="18" rx="2" ry="2"></rect>
            </svg>
          </button>
          <button
            className={`tool-button ${tool === 'circle' ? 'active' : ''}`}
            onClick={() => setTool('circle')}
            title="Circle"
          >
            <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2">
              <circle cx="12" cy="12" r="10"></circle>
            </svg>
          </button>
          <button
            className={`tool-button ${tool === 'eraser' ? 'active' : ''}`}
            onClick={() => setTool('eraser')}
            title="Eraser"
          >
            <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2">
              <path d="M20.24 12.24a6 6 0 0 0-8.49-8.49L5 10.5V19h8.5z"></path>
              <line x1="16" y1="8" x2="2" y2="22"></line>
            </svg>
          </button>
        </div>
        
        <div className="color-group">
          <input
            type="color"
            value={color}
            onChange={(e) => setColor(e.target.value)}
            title="Color"
          />
        </div>
        
        <div className="size-group">
          <input
            type="range"
            min="1"
            max="10"
            value={size}
            onChange={(e) => setSize(parseInt(e.target.value))}
            title="Size"
          />
        </div>
      </div>
      
      <canvas
        ref={canvasRef}
        width={width}
        height={height}
        onMouseDown={handleMouseDown}
        onMouseMove={handleMouseMove}
        onMouseUp={handleMouseUp}
        onMouseLeave={handleMouseLeave}
        className="whiteboard-canvas"
      />
    </div>
  );
};

export default CollaborativeWhiteboard;
</file>

<file path="src-frontend/src/components/CommandPalette.css">
.command-palette-overlay {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.4);
  display: flex;
  align-items: flex-start;
  justify-content: center;
  padding-top: 10vh;
  z-index: var(--z-modal);
  animation: fadeIn 0.2s ease;
}

.command-palette {
  width: 35rem;
  max-width: 90vw;
  background-color: var(--color-surface);
  border-radius: var(--radius-lg);
  box-shadow: var(--shadow-lg);
  overflow: hidden;
  animation: slideDown 0.2s ease;
}

.command-palette-header {
  padding: var(--spacing-md);
  border-bottom: 1px solid var(--color-border);
}

.command-palette-search {
  position: relative;
  display: flex;
  align-items: center;
}

.command-palette-search-icon {
  position: absolute;
  left: var(--spacing-md);
  width: 1.25rem;
  height: 1.25rem;
  color: var(--color-on-surface-variant);
}

.command-palette-search-input {
  width: 100%;
  height: 2.75rem;
  padding: 0 var(--spacing-md) 0 3rem;
  background-color: var(--color-surface-variant);
  color: var(--color-on-surface);
  border: 1px solid var(--color-border);
  border-radius: var(--radius-md);
  font-size: var(--font-size-md);
  transition: all var(--transition-fast);
}

.command-palette-search-input:focus {
  outline: none;
  border-color: var(--color-primary);
  box-shadow: 0 0 0 2px var(--color-primary-light);
}

.command-palette-content {
  max-height: 50vh;
  overflow-y: auto;
}

.command-palette-list {
  list-style: none;
  margin: 0;
  padding: 0;
}

.command-palette-item {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: var(--spacing-md);
  cursor: pointer;
  transition: background-color var(--transition-fast);
}

.command-palette-item:hover,
.command-palette-item.selected {
  background-color: var(--color-surface-variant);
}

.command-palette-item-content {
  display: flex;
  align-items: center;
  gap: var(--spacing-md);
}

.command-palette-item-icon {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 1.5rem;
  height: 1.5rem;
  color: var(--color-on-surface-variant);
}

.command-palette-item-text {
  display: flex;
  flex-direction: column;
}

.command-palette-item-name {
  font-weight: 500;
  color: var(--color-on-surface);
}

.command-palette-item-description {
  font-size: var(--font-size-sm);
  color: var(--color-on-surface-variant);
}

.command-palette-item-shortcut {
  display: flex;
  gap: var(--spacing-xxs);
}

.command-palette-kbd {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  min-width: 1.25rem;
  height: 1.25rem;
  padding: 0 var(--spacing-xxs);
  font-size: var(--font-size-xs);
  font-family: var(--font-family);
  background-color: var(--color-surface-variant);
  color: var(--color-on-surface-variant);
  border-radius: var(--radius-sm);
  border: 1px solid var(--color-border);
  box-shadow: 0 1px 0 var(--color-border);
}

.command-palette-empty {
  padding: var(--spacing-xl);
  text-align: center;
  color: var(--color-on-surface-variant);
}

@keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}

@keyframes slideDown {
  from {
    transform: translateY(-20px);
    opacity: 0;
  }
  to {
    transform: translateY(0);
    opacity: 1;
  }
}
</file>

<file path="src-frontend/src/components/CommandPalette.tsx">
import React, { useState, useEffect, useRef } from 'react';
import './CommandPalette.css';

export interface CommandItem {
  id: string;
  name: string;
  description?: string;
  shortcut?: string;
  category?: string;
  icon?: React.ReactNode;
  action: () => void;
}

interface CommandPaletteProps {
  isOpen: boolean;
  onClose: () => void;
  commands: CommandItem[];
}

export const CommandPalette: React.FC<CommandPaletteProps> = ({
  isOpen,
  onClose,
  commands,
}) => {
  const [searchQuery, setSearchQuery] = useState('');
  const [selectedIndex, setSelectedIndex] = useState(0);
  const searchInputRef = useRef<HTMLInputElement>(null);
  const commandListRef = useRef<HTMLUListElement>(null);
  
  // Filter commands based on search query
  const filteredCommands = commands.filter((command) => {
    const query = searchQuery.toLowerCase();
    return (
      command.name.toLowerCase().includes(query) ||
      (command.description?.toLowerCase().includes(query)) ||
      (command.category?.toLowerCase().includes(query))
    );
  });
  
  // Focus the search input when the palette opens
  useEffect(() => {
    if (isOpen && searchInputRef.current) {
      setTimeout(() => {
        searchInputRef.current?.focus();
      }, 10);
    } else {
      // Reset search and selection when closing
      setSearchQuery('');
      setSelectedIndex(0);
    }
  }, [isOpen]);
  
  // Handle keyboard navigation
  const handleKeyDown = (e: React.KeyboardEvent) => {
    switch (e.key) {
      case 'ArrowDown':
        e.preventDefault();
        setSelectedIndex((prevIndex) => 
          prevIndex < filteredCommands.length - 1 ? prevIndex + 1 : prevIndex
        );
        break;
      case 'ArrowUp':
        e.preventDefault();
        setSelectedIndex((prevIndex) => 
          prevIndex > 0 ? prevIndex - 1 : prevIndex
        );
        break;
      case 'Enter':
        e.preventDefault();
        if (filteredCommands[selectedIndex]) {
          executeCommand(filteredCommands[selectedIndex]);
        }
        break;
      case 'Escape':
        e.preventDefault();
        onClose();
        break;
    }
  };
  
  // Execute a command and close the palette
  const executeCommand = (command: CommandItem) => {
    command.action();
    onClose();
  };
  
  // Scroll selected item into view
  useEffect(() => {
    if (commandListRef.current && filteredCommands.length > 0) {
      const selectedElement = commandListRef.current.children[selectedIndex] as HTMLElement;
      if (selectedElement) {
        selectedElement.scrollIntoView({
          block: 'nearest',
        });
      }
    }
  }, [selectedIndex, filteredCommands.length]);
  
  if (!isOpen) return null;
  
  return (
    <div className="command-palette-overlay" onClick={onClose}>
      <div className="command-palette" onClick={(e) => e.stopPropagation()}>
        <div className="command-palette-header">
          <div className="command-palette-search">
            <svg 
              className="command-palette-search-icon" 
              xmlns="http://www.w3.org/2000/svg" 
              viewBox="0 0 24 24" 
              fill="none" 
              stroke="currentColor" 
              strokeWidth="2" 
              strokeLinecap="round" 
              strokeLinejoin="round"
            >
              <circle cx="11" cy="11" r="8" />
              <line x1="21" y1="21" x2="16.65" y2="16.65" />
            </svg>
            <input
              ref={searchInputRef}
              type="text"
              className="command-palette-search-input"
              placeholder="Type a command or search..."
              value={searchQuery}
              onChange={(e) => {
                setSearchQuery(e.target.value);
                setSelectedIndex(0); // Reset selection when search changes
              }}
              onKeyDown={handleKeyDown}
            />
          </div>
        </div>
        
        <div className="command-palette-content">
          {filteredCommands.length > 0 ? (
            <ul className="command-palette-list" ref={commandListRef}>
              {filteredCommands.map((command, index) => (
                <li
                  key={command.id}
                  className={`command-palette-item ${index === selectedIndex ? 'selected' : ''}`}
                  onClick={() => executeCommand(command)}
                  onMouseEnter={() => setSelectedIndex(index)}
                >
                  <div className="command-palette-item-content">
                    {command.icon && (
                      <div className="command-palette-item-icon">
                        {command.icon}
                      </div>
                    )}
                    <div className="command-palette-item-text">
                      <div className="command-palette-item-name">
                        {command.name}
                      </div>
                      {command.description && (
                        <div className="command-palette-item-description">
                          {command.description}
                        </div>
                      )}
                    </div>
                  </div>
                  {command.shortcut && (
                    <div className="command-palette-item-shortcut">
                      {command.shortcut.split('+').map((key, i) => (
                        <kbd key={i} className="command-palette-kbd">
                          {key.trim()}
                        </kbd>
                      ))}
                    </div>
                  )}
                </li>
              ))}
            </ul>
          ) : (
            <div className="command-palette-empty">
              No commands found
            </div>
          )}
        </div>
      </div>
    </div>
  );
};

export default CommandPalette;
</file>

<file path="src-frontend/src/components/dashboard/llm/LLMPerformanceDashboard.css">
/* LLM Performance Dashboard Styles */
.metric-card {
  background-color: #ffffff;
  border-radius: 8px;
  padding: 16px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 16px;
}

.metric-value {
  font-size: 1.8rem;
  font-weight: 600;
  margin: 8px 0;
  color: #333;
}

.metric-label {
  font-size: 0.9rem;
  color: #666;
  margin-bottom: 4px;
}

.success-text {
  color: #4caf50;
}

.error-text {
  color: #f44336;
}

.warning-text {
  color: #ff9800;
}

.status-badge {
  display: inline-block;
  padding: 4px 8px;
  border-radius: 4px;
  font-size: 0.8rem;
  font-weight: 500;
  margin-left: 8px;
}

.status-badge.success {
  background-color: rgba(76, 175, 80, 0.1);
  color: #4caf50;
  border: 1px solid rgba(76, 175, 80, 0.5);
}

.status-badge.error {
  background-color: rgba(244, 67, 54, 0.1);
  color: #f44336;
  border: 1px solid rgba(244, 67, 54, 0.5);
}

.status-badge.warning {
  background-color: rgba(255, 152, 0, 0.1);
  color: #ff9800;
  border: 1px solid rgba(255, 152, 0, 0.5);
}

.status-badge.info {
  background-color: rgba(33, 150, 243, 0.1);
  color: #2196f3;
  border: 1px solid rgba(33, 150, 243, 0.5);
}

.chart-container {
  height: 300px;
  width: 100%;
  margin-top: 16px;
}

.filter-container {
  display: flex;
  gap: 16px;
  margin-bottom: 16px;
  flex-wrap: wrap;
}

.metric-table {
  width: 100%;
  border-collapse: collapse;
}

.metric-table th {
  text-align: left;
  padding: 12px;
  background-color: #f5f5f5;
  font-weight: 600;
}

.metric-table td {
  padding: 12px;
  border-bottom: 1px solid #eee;
}

.metric-table tr:hover {
  background-color: #f9f9f9;
}

.highlight-row {
  background-color: rgba(33, 150, 243, 0.05);
}

.refresh-control {
  display: flex;
  align-items: center;
  gap: 8px;
}

@media (max-width: 768px) {
  .filter-container {
    flex-direction: column;
  }
}
</file>

<file path="src-frontend/src/components/dashboard/llm/LLMPerformanceDashboard.tsx">
import React, { useState, useEffect } from 'react';
import { invoke } from '@tauri-apps/api/tauri';
import { 
  LineChart, Line, BarChart, Bar, XAxis, YAxis, CartesianGrid, 
  Tooltip, Legend, ResponsiveContainer, PieChart, Pie, Cell
} from 'recharts';
import { 
  Box, Typography, Paper, Divider, CircularProgress, 
  Alert, Tabs, Tab, FormControlLabel, Switch, Select, 
  MenuItem, InputLabel, FormControl, Grid, Card, CardContent,
  Table, TableBody, TableCell, TableContainer, TableHead, TableRow
} from '@mui/material';

// Types for our metrics data
interface ProviderPerformanceMetrics {
  provider_type: string;
  generation_count: number;
  successful_generations: number;
  failed_generations: number;
  avg_tokens_per_second: number;
  avg_latency_ms: number;
  p90_latency_ms: number;
  p99_latency_ms: number;
  avg_cpu_usage: number;
  avg_memory_usage: number;
  last_updated: string;
}

interface ModelPerformanceMetrics {
  model_id: string;
  provider_type: string;
  generation_count: number;
  tokens_generated: number;
  successful_generations: number;
  failed_generations: number;
  avg_tokens_per_second: number;
  avg_latency_ms: number;
  p90_latency_ms: number;
  p99_latency_ms: number;
  avg_time_to_first_token_ms: number;
  avg_tokens_per_request: number;
  last_updated: string;
}

interface LLMMetricsData {
  provider_metrics: Record<string, ProviderPerformanceMetrics>;
  model_metrics: Record<string, ModelPerformanceMetrics>;
  active_provider?: string;
  default_model?: string;
  enabled: boolean;
}

// Component to format time in ms to a human-readable format
const formatTime = (ms: number): string => {
  if (ms < 1) {
    return `${(ms * 1000).toFixed(2)} μs`;
  } else if (ms < 1000) {
    return `${ms.toFixed(2)} ms`;
  } else {
    return `${(ms / 1000).toFixed(2)} s`;
  }
};

// Color palette for charts
const COLORS = ['#8884d8', '#82ca9d', '#ffc658', '#ff8042', '#0088FE', '#00C49F', '#FFBB28', '#FF8042'];

// Main dashboard component
const LLMPerformanceDashboard: React.FC = () => {
  const [activeTab, setActiveTab] = useState<number>(0);
  const [metrics, setMetrics] = useState<LLMMetricsData | null>(null);
  const [loading, setLoading] = useState<boolean>(true);
  const [error, setError] = useState<string | null>(null);
  const [autoRefresh, setAutoRefresh] = useState<boolean>(true);
  const [refreshInterval, setRefreshInterval] = useState<number>(5000);
  const [selectedProvider, setSelectedProvider] = useState<string>('all');
  const [selectedModel, setSelectedModel] = useState<string>('all');

  // Fetch metrics data
  const fetchMetrics = async () => {
    try {
      setLoading(true);
      
      // Invoke Tauri commands to get metrics
      const [
        providerMetrics,
        modelMetrics,
        activeProvider,
        defaultModel,
        metricsEnabled
      ] = await Promise.all([
        invoke('get_llm_provider_metrics'),
        invoke('get_llm_model_metrics'),
        invoke('get_active_llm_provider'),
        invoke('get_default_llm_model'),
        invoke('get_llm_metrics_enabled')
      ]);
      
      setMetrics({
        provider_metrics: providerMetrics as Record<string, ProviderPerformanceMetrics>,
        model_metrics: modelMetrics as Record<string, ModelPerformanceMetrics>,
        active_provider: activeProvider as string,
        default_model: defaultModel as string,
        enabled: metricsEnabled as boolean
      });
      
      setError(null);
    } catch (err) {
      console.error('Error fetching LLM metrics:', err);
      setError(`Failed to fetch LLM metrics: ${err}`);
    } finally {
      setLoading(false);
    }
  };

  // Set up auto-refresh
  useEffect(() => {
    fetchMetrics();
    
    let intervalId: number;
    
    if (autoRefresh) {
      intervalId = window.setInterval(fetchMetrics, refreshInterval);
    }
    
    return () => {
      if (intervalId) {
        clearInterval(intervalId);
      }
    };
  }, [autoRefresh, refreshInterval]);

  // Handle tab change
  const handleTabChange = (_: React.SyntheticEvent, newValue: number) => {
    setActiveTab(newValue);
  };

  // Filter provider metrics based on selection
  const filteredProviderMetrics = React.useMemo(() => {
    if (!metrics) return {};
    
    if (selectedProvider === 'all') {
      return metrics.provider_metrics;
    }
    
    return Object.entries(metrics.provider_metrics)
      .filter(([key]) => key === selectedProvider)
      .reduce((obj, [key, value]) => {
        obj[key] = value;
        return obj;
      }, {} as Record<string, ProviderPerformanceMetrics>);
  }, [metrics, selectedProvider]);

  // Filter model metrics based on selection
  const filteredModelMetrics = React.useMemo(() => {
    if (!metrics) return {};
    
    let filtered = metrics.model_metrics;
    
    // Filter by provider if needed
    if (selectedProvider !== 'all') {
      filtered = Object.entries(filtered)
        .filter(([_, value]) => value.provider_type === selectedProvider)
        .reduce((obj, [key, value]) => {
          obj[key] = value;
          return obj;
        }, {} as Record<string, ModelPerformanceMetrics>);
    }
    
    // Filter by model if needed
    if (selectedModel !== 'all') {
      filtered = Object.entries(filtered)
        .filter(([_, value]) => value.model_id === selectedModel)
        .reduce((obj, [key, value]) => {
          obj[key] = value;
          return obj;
        }, {} as Record<string, ModelPerformanceMetrics>);
    }
    
    return filtered;
  }, [metrics, selectedProvider, selectedModel]);

  // Get all available providers for filter dropdown
  const availableProviders = React.useMemo(() => {
    if (!metrics) return [];
    return [...new Set(Object.values(metrics.provider_metrics).map(m => m.provider_type))];
  }, [metrics]);

  // Get all available models for filter dropdown
  const availableModels = React.useMemo(() => {
    if (!metrics) return [];
    return [...new Set(Object.values(metrics.model_metrics).map(m => m.model_id))];
  }, [metrics]);

  // Prepare data for provider comparison chart
  const providerComparisonData = React.useMemo(() => {
    if (!metrics) return [];
    
    return Object.values(filteredProviderMetrics).map(provider => ({
      name: provider.provider_type,
      'Avg Tokens/sec': provider.avg_tokens_per_second,
      'Avg Latency (ms)': provider.avg_latency_ms,
      'Success Rate': provider.generation_count > 0 
        ? (provider.successful_generations / provider.generation_count) * 100 
        : 0
    }));
  }, [filteredProviderMetrics]);

  // Prepare data for model comparison chart
  const modelComparisonData = React.useMemo(() => {
    if (!metrics) return [];
    
    return Object.values(filteredModelMetrics).map(model => ({
      name: model.model_id,
      'Avg Tokens/sec': model.avg_tokens_per_second,
      'Avg Latency (ms)': model.avg_latency_ms,
      'Avg Time to First Token (ms)': model.avg_time_to_first_token_ms,
      'Success Rate': model.generation_count > 0 
        ? (model.successful_generations / model.generation_count) * 100 
        : 0
    }));
  }, [filteredModelMetrics]);

  // Prepare data for success/failure pie chart
  const successFailureData = React.useMemo(() => {
    if (!metrics) return [];
    
    const totalSuccess = Object.values(filteredModelMetrics).reduce((sum, model) => 
      sum + model.successful_generations, 0);
    
    const totalFailed = Object.values(filteredModelMetrics).reduce((sum, model) => 
      sum + model.failed_generations, 0);
    
    return [
      { name: 'Successful', value: totalSuccess },
      { name: 'Failed', value: totalFailed }
    ];
  }, [filteredModelMetrics]);

  // Render overview tab
  const renderOverview = () => {
    if (!metrics) return null;
    
    // Get total metrics across all providers and models
    const totalGenerations = Object.values(metrics.provider_metrics).reduce((sum, p) => 
      sum + p.generation_count, 0);
    
    const totalSuccess = Object.values(metrics.provider_metrics).reduce((sum, p) => 
      sum + p.successful_generations, 0);
    
    const totalFailed = Object.values(metrics.provider_metrics).reduce((sum, p) => 
      sum + p.failed_generations, 0);
    
    const totalTokens = Object.values(metrics.model_metrics).reduce((sum, m) => 
      sum + m.tokens_generated, 0);
    
    // Calculate averages
    const avgTokensPerSecond = Object.values(metrics.model_metrics).reduce((sum, m) => 
      sum + m.avg_tokens_per_second, 0) / Math.max(1, Object.values(metrics.model_metrics).length);
    
    const avgLatency = Object.values(metrics.model_metrics).reduce((sum, m) => 
      sum + m.avg_latency_ms, 0) / Math.max(1, Object.values(metrics.model_metrics).length);

    return (
      <Box sx={{ mt: 3 }}>
        <Grid container spacing={3}>
          {/* Summary Cards */}
          <Grid item xs={12} md={6} lg={3}>
            <Card sx={{ height: '100%' }}>
              <CardContent>
                <Typography variant="h6" color="text.secondary" gutterBottom>
                  Total Generations
                </Typography>
                <Typography variant="h3">
                  {totalGenerations.toLocaleString()}
                </Typography>
                <Box sx={{ display: 'flex', mt: 2, justifyContent: 'space-between' }}>
                  <Typography variant="body2" color="success.main">
                    Success: {totalSuccess.toLocaleString()}
                  </Typography>
                  <Typography variant="body2" color="error.main">
                    Failed: {totalFailed.toLocaleString()}
                  </Typography>
                </Box>
              </CardContent>
            </Card>
          </Grid>
          
          <Grid item xs={12} md={6} lg={3}>
            <Card sx={{ height: '100%' }}>
              <CardContent>
                <Typography variant="h6" color="text.secondary" gutterBottom>
                  Tokens Generated
                </Typography>
                <Typography variant="h3">
                  {totalTokens.toLocaleString()}
                </Typography>
                <Typography variant="body2" color="text.secondary" sx={{ mt: 2 }}>
                  Across all models and providers
                </Typography>
              </CardContent>
            </Card>
          </Grid>
          
          <Grid item xs={12} md={6} lg={3}>
            <Card sx={{ height: '100%' }}>
              <CardContent>
                <Typography variant="h6" color="text.secondary" gutterBottom>
                  Avg. Throughput
                </Typography>
                <Typography variant="h3">
                  {avgTokensPerSecond.toFixed(2)}
                </Typography>
                <Typography variant="body2" color="text.secondary" sx={{ mt: 2 }}>
                  Tokens per second
                </Typography>
              </CardContent>
            </Card>
          </Grid>
          
          <Grid item xs={12} md={6} lg={3}>
            <Card sx={{ height: '100%' }}>
              <CardContent>
                <Typography variant="h6" color="text.secondary" gutterBottom>
                  Avg. Latency
                </Typography>
                <Typography variant="h3">
                  {formatTime(avgLatency)}
                </Typography>
                <Typography variant="body2" color="text.secondary" sx={{ mt: 2 }}>
                  Response time
                </Typography>
              </CardContent>
            </Card>
          </Grid>
          
          {/* Success/Failure Chart */}
          <Grid item xs={12} md={6}>
            <Card>
              <CardContent>
                <Typography variant="h6" gutterBottom>
                  Generation Success Rate
                </Typography>
                {successFailureData.length > 0 ? (
                  <ResponsiveContainer width="100%" height={300}>
                    <PieChart>
                      <Pie
                        data={successFailureData}
                        cx="50%"
                        cy="50%"
                        labelLine={false}
                        outerRadius={80}
                        fill="#8884d8"
                        dataKey="value"
                        label={({ name, percent }) => `${name}: ${(percent * 100).toFixed(0)}%`}
                      >
                        {successFailureData.map((_, index) => (
                          <Cell key={`cell-${index}`} fill={index === 0 ? '#4caf50' : '#f44336'} />
                        ))}
                      </Pie>
                      <Tooltip formatter={(value: number) => [value.toLocaleString(), 'Generations']} />
                      <Legend />
                    </PieChart>
                  </ResponsiveContainer>
                ) : (
                  <Box sx={{ display: 'flex', justifyContent: 'center', alignItems: 'center', height: 300 }}>
                    <Typography variant="body1" color="text.secondary">
                      No generation data available
                    </Typography>
                  </Box>
                )}
              </CardContent>
            </Card>
          </Grid>
          
          {/* Provider Comparison */}
          <Grid item xs={12} md={6}>
            <Card>
              <CardContent>
                <Typography variant="h6" gutterBottom>
                  Provider Performance Comparison
                </Typography>
                {providerComparisonData.length > 0 ? (
                  <ResponsiveContainer width="100%" height={300}>
                    <BarChart data={providerComparisonData}>
                      <CartesianGrid strokeDasharray="3 3" />
                      <XAxis dataKey="name" />
                      <YAxis yAxisId="left" orientation="left" stroke="#8884d8" />
                      <YAxis yAxisId="right" orientation="right" stroke="#82ca9d" />
                      <Tooltip />
                      <Legend />
                      <Bar yAxisId="left" dataKey="Avg Tokens/sec" fill="#8884d8" />
                      <Bar yAxisId="right" dataKey="Avg Latency (ms)" fill="#82ca9d" />
                    </BarChart>
                  </ResponsiveContainer>
                ) : (
                  <Box sx={{ display: 'flex', justifyContent: 'center', alignItems: 'center', height: 300 }}>
                    <Typography variant="body1" color="text.secondary">
                      No provider comparison data available
                    </Typography>
                  </Box>
                )}
              </CardContent>
            </Card>
          </Grid>
          
          {/* Active Provider Info */}
          <Grid item xs={12}>
            <Card>
              <CardContent>
                <Typography variant="h6" gutterBottom>
                  Current Configuration
                </Typography>
                <Box sx={{ display: 'flex', flexWrap: 'wrap', gap: 4, mt: 2 }}>
                  <Box>
                    <Typography variant="body2" color="text.secondary">
                      Active Provider
                    </Typography>
                    <Typography variant="body1" fontWeight="bold">
                      {metrics.active_provider || 'None'}
                    </Typography>
                  </Box>
                  <Box>
                    <Typography variant="body2" color="text.secondary">
                      Default Model
                    </Typography>
                    <Typography variant="body1" fontWeight="bold">
                      {metrics.default_model || 'None'}
                    </Typography>
                  </Box>
                  <Box>
                    <Typography variant="body2" color="text.secondary">
                      Metrics Enabled
                    </Typography>
                    <Typography variant="body1" fontWeight="bold" color={metrics.enabled ? 'success.main' : 'error.main'}>
                      {metrics.enabled ? 'Yes' : 'No'}
                    </Typography>
                  </Box>
                </Box>
              </CardContent>
            </Card>
          </Grid>
        </Grid>
      </Box>
    );
  };

  // Render providers tab
  const renderProvidersTab = () => {
    if (!metrics) return null;
    
    return (
      <Box sx={{ mt: 3 }}>
        <Grid container spacing={3}>
          {/* Provider Filter */}
          <Grid item xs={12}>
            <FormControl fullWidth variant="outlined" size="small">
              <InputLabel id="provider-filter-label">Provider</InputLabel>
              <Select
                labelId="provider-filter-label"
                value={selectedProvider}
                onChange={(e) => setSelectedProvider(e.target.value)}
                label="Provider"
              >
                <MenuItem value="all">All Providers</MenuItem>
                {availableProviders.map((provider) => (
                  <MenuItem key={provider} value={provider}>{provider}</MenuItem>
                ))}
              </Select>
            </FormControl>
          </Grid>
          
          {/* Provider Metrics Table */}
          <Grid item xs={12}>
            <TableContainer component={Paper}>
              <Table>
                <TableHead>
                  <TableRow>
                    <TableCell>Provider</TableCell>
                    <TableCell align="right">Generations</TableCell>
                    <TableCell align="right">Success Rate</TableCell>
                    <TableCell align="right">Avg. Tokens/sec</TableCell>
                    <TableCell align="right">Avg. Latency</TableCell>
                    <TableCell align="right">P90 Latency</TableCell>
                    <TableCell align="right">P99 Latency</TableCell>
                    <TableCell align="right">CPU Usage</TableCell>
                    <TableCell align="right">Memory Usage</TableCell>
                    <TableCell align="right">Last Updated</TableCell>
                  </TableRow>
                </TableHead>
                <TableBody>
                  {Object.values(filteredProviderMetrics).length > 0 ? (
                    Object.values(filteredProviderMetrics).map((provider) => (
                      <TableRow key={provider.provider_type}>
                        <TableCell component="th" scope="row">
                          {provider.provider_type}
                          {metrics.active_provider === provider.provider_type && (
                            <Typography component="span" color="primary" sx={{ ml: 1, fontSize: '0.75rem' }}>
                              (Active)
                            </Typography>
                          )}
                        </TableCell>
                        <TableCell align="right">{provider.generation_count.toLocaleString()}</TableCell>
                        <TableCell align="right">
                          {provider.generation_count > 0 
                            ? `${((provider.successful_generations / provider.generation_count) * 100).toFixed(1)}%` 
                            : 'N/A'}
                        </TableCell>
                        <TableCell align="right">{provider.avg_tokens_per_second.toFixed(2)}</TableCell>
                        <TableCell align="right">{formatTime(provider.avg_latency_ms)}</TableCell>
                        <TableCell align="right">{formatTime(provider.p90_latency_ms)}</TableCell>
                        <TableCell align="right">{formatTime(provider.p99_latency_ms)}</TableCell>
                        <TableCell align="right">{provider.avg_cpu_usage > 0 ? `${provider.avg_cpu_usage.toFixed(1)}%` : 'N/A'}</TableCell>
                        <TableCell align="right">{provider.avg_memory_usage > 0 ? `${(provider.avg_memory_usage / (1024 * 1024)).toFixed(1)} MB` : 'N/A'}</TableCell>
                        <TableCell align="right">
                          {provider.last_updated ? new Date(provider.last_updated).toLocaleTimeString() : 'N/A'}
                        </TableCell>
                      </TableRow>
                    ))
                  ) : (
                    <TableRow>
                      <TableCell colSpan={10} align="center">
                        No provider metrics available
                      </TableCell>
                    </TableRow>
                  )}
                </TableBody>
              </Table>
            </TableContainer>
          </Grid>
          
          {/* Provider Performance Charts */}
          <Grid item xs={12} md={6}>
            <Card>
              <CardContent>
                <Typography variant="h6" gutterBottom>
                  Provider Throughput
                </Typography>
                {providerComparisonData.length > 0 ? (
                  <ResponsiveContainer width="100%" height={300}>
                    <BarChart data={providerComparisonData}>
                      <CartesianGrid strokeDasharray="3 3" />
                      <XAxis dataKey="name" />
                      <YAxis />
                      <Tooltip formatter={(value: number) => [`${value.toFixed(2)} tokens/sec`, 'Throughput']} />
                      <Legend />
                      <Bar dataKey="Avg Tokens/sec" fill="#8884d8" />
                    </BarChart>
                  </ResponsiveContainer>
                ) : (
                  <Box sx={{ display: 'flex', justifyContent: 'center', alignItems: 'center', height: 300 }}>
                    <Typography variant="body1" color="text.secondary">
                      No throughput data available
                    </Typography>
                  </Box>
                )}
              </CardContent>
            </Card>
          </Grid>
          
          <Grid item xs={12} md={6}>
            <Card>
              <CardContent>
                <Typography variant="h6" gutterBottom>
                  Provider Latency
                </Typography>
                {providerComparisonData.length > 0 ? (
                  <ResponsiveContainer width="100%" height={300}>
                    <BarChart data={providerComparisonData}>
                      <CartesianGrid strokeDasharray="3 3" />
                      <XAxis dataKey="name" />
                      <YAxis />
                      <Tooltip formatter={(value: number) => [formatTime(value), 'Latency']} />
                      <Legend />
                      <Bar dataKey="Avg Latency (ms)" fill="#82ca9d" />
                    </BarChart>
                  </ResponsiveContainer>
                ) : (
                  <Box sx={{ display: 'flex', justifyContent: 'center', alignItems: 'center', height: 300 }}>
                    <Typography variant="body1" color="text.secondary">
                      No latency data available
                    </Typography>
                  </Box>
                )}
              </CardContent>
            </Card>
          </Grid>
        </Grid>
      </Box>
    );
  };

  // Render models tab
  const renderModelsTab = () => {
    if (!metrics) return null;
    
    return (
      <Box sx={{ mt: 3 }}>
        <Grid container spacing={3}>
          {/* Filters */}
          <Grid item xs={12} md={6}>
            <FormControl fullWidth variant="outlined" size="small">
              <InputLabel id="provider-filter-label">Provider</InputLabel>
              <Select
                labelId="provider-filter-label"
                value={selectedProvider}
                onChange={(e) => setSelectedProvider(e.target.value)}
                label="Provider"
              >
                <MenuItem value="all">All Providers</MenuItem>
                {availableProviders.map((provider) => (
                  <MenuItem key={provider} value={provider}>{provider}</MenuItem>
                ))}
              </Select>
            </FormControl>
          </Grid>
          <Grid item xs={12} md={6}>
            <FormControl fullWidth variant="outlined" size="small">
              <InputLabel id="model-filter-label">Model</InputLabel>
              <Select
                labelId="model-filter-label"
                value={selectedModel}
                onChange={(e) => setSelectedModel(e.target.value)}
                label="Model"
              >
                <MenuItem value="all">All Models</MenuItem>
                {availableModels.map((model) => (
                  <MenuItem key={model} value={model}>{model}</MenuItem>
                ))}
              </Select>
            </FormControl>
          </Grid>
          
          {/* Model Metrics Table */}
          <Grid item xs={12}>
            <TableContainer component={Paper}>
              <Table>
                <TableHead>
                  <TableRow>
                    <TableCell>Model</TableCell>
                    <TableCell>Provider</TableCell>
                    <TableCell align="right">Generations</TableCell>
                    <TableCell align="right">Tokens</TableCell>
                    <TableCell align="right">Success Rate</TableCell>
                    <TableCell align="right">Avg. Tokens/sec</TableCell>
                    <TableCell align="right">Avg. Latency</TableCell>
                    <TableCell align="right">Time to First Token</TableCell>
                    <TableCell align="right">Tokens/Request</TableCell>
                    <TableCell align="right">Last Updated</TableCell>
                  </TableRow>
                </TableHead>
                <TableBody>
                  {Object.values(filteredModelMetrics).length > 0 ? (
                    Object.values(filteredModelMetrics).map((model) => (
                      <TableRow key={`${model.provider_type}:${model.model_id}`}>
                        <TableCell component="th" scope="row">
                          {model.model_id}
                          {metrics.default_model === model.model_id && (
                            <Typography component="span" color="primary" sx={{ ml: 1, fontSize: '0.75rem' }}>
                              (Default)
                            </Typography>
                          )}
                        </TableCell>
                        <TableCell>{model.provider_type}</TableCell>
                        <TableCell align="right">{model.generation_count.toLocaleString()}</TableCell>
                        <TableCell align="right">{model.tokens_generated.toLocaleString()}</TableCell>
                        <TableCell align="right">
                          {model.generation_count > 0 
                            ? `${((model.successful_generations / model.generation_count) * 100).toFixed(1)}%` 
                            : 'N/A'}
                        </TableCell>
                        <TableCell align="right">{model.avg_tokens_per_second.toFixed(2)}</TableCell>
                        <TableCell align="right">{formatTime(model.avg_latency_ms)}</TableCell>
                        <TableCell align="right">{formatTime(model.avg_time_to_first_token_ms)}</TableCell>
                        <TableCell align="right">{model.avg_tokens_per_request.toFixed(1)}</TableCell>
                        <TableCell align="right">
                          {model.last_updated ? new Date(model.last_updated).toLocaleTimeString() : 'N/A'}
                        </TableCell>
                      </TableRow>
                    ))
                  ) : (
                    <TableRow>
                      <TableCell colSpan={10} align="center">
                        No model metrics available
                      </TableCell>
                    </TableRow>
                  )}
                </TableBody>
              </Table>
            </TableContainer>
          </Grid>
          
          {/* Model Performance Charts */}
          <Grid item xs={12} md={6}>
            <Card>
              <CardContent>
                <Typography variant="h6" gutterBottom>
                  Model Throughput
                </Typography>
                {modelComparisonData.length > 0 ? (
                  <ResponsiveContainer width="100%" height={300}>
                    <BarChart data={modelComparisonData}>
                      <CartesianGrid strokeDasharray="3 3" />
                      <XAxis dataKey="name" />
                      <YAxis />
                      <Tooltip formatter={(value: number) => [`${value.toFixed(2)} tokens/sec`, 'Throughput']} />
                      <Legend />
                      <Bar dataKey="Avg Tokens/sec" fill="#8884d8" />
                    </BarChart>
                  </ResponsiveContainer>
                ) : (
                  <Box sx={{ display: 'flex', justifyContent: 'center', alignItems: 'center', height: 300 }}>
                    <Typography variant="body1" color="text.secondary">
                      No throughput data available
                    </Typography>
                  </Box>
                )}
              </CardContent>
            </Card>
          </Grid>
          
          <Grid item xs={12} md={6}>
            <Card>
              <CardContent>
                <Typography variant="h6" gutterBottom>
                  Model Response Time
                </Typography>
                {modelComparisonData.length > 0 ? (
                  <ResponsiveContainer width="100%" height={300}>
                    <BarChart data={modelComparisonData}>
                      <CartesianGrid strokeDasharray="3 3" />
                      <XAxis dataKey="name" />
                      <YAxis />
                      <Tooltip formatter={(value: number) => [formatTime(value), 'Time']} />
                      <Legend />
                      <Bar dataKey="Avg Latency (ms)" fill="#82ca9d" />
                      <Bar dataKey="Avg Time to First Token (ms)" fill="#ffc658" />
                    </BarChart>
                  </ResponsiveContainer>
                ) : (
                  <Box sx={{ display: 'flex', justifyContent: 'center', alignItems: 'center', height: 300 }}>
                    <Typography variant="body1" color="text.secondary">
                      No response time data available
                    </Typography>
                  </Box>
                )}
              </CardContent>
            </Card>
          </Grid>
        </Grid>
      </Box>
    );
  };

  return (
    <Box sx={{ mt: 2 }}>
      <Paper sx={{ p: 3 }}>
        <Box sx={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', mb: 2 }}>
          <Typography variant="h5">
            Local LLM Performance
          </Typography>
          
          <Box sx={{ display: 'flex', alignItems: 'center' }}>
            <FormControlLabel
              control={
                <Switch
                  checked={autoRefresh}
                  onChange={(e) => setAutoRefresh(e.target.checked)}
                  color="primary"
                />
              }
              label="Auto-refresh"
              sx={{ mr: 2 }}
            />
            
            {autoRefresh && (
              <FormControl variant="outlined" size="small" sx={{ minWidth: 120, mr: 2 }}>
                <Select
                  value={refreshInterval}
                  onChange={(e) => setRefreshInterval(Number(e.target.value))}
                >
                  <MenuItem value={1000}>Every 1s</MenuItem>
                  <MenuItem value={5000}>Every 5s</MenuItem>
                  <MenuItem value={10000}>Every 10s</MenuItem>
                  <MenuItem value={30000}>Every 30s</MenuItem>
                  <MenuItem value={60000}>Every 1m</MenuItem>
                </Select>
              </FormControl>
            )}
            
            <Button 
              variant="outlined" 
              color="primary" 
              onClick={fetchMetrics}
              disabled={loading}
            >
              Refresh
            </Button>
          </Box>
        </Box>
        
        {!metrics?.enabled && (
          <Alert severity="info" sx={{ mb: 3 }}>
            LLM performance metrics collection is currently disabled. Enable it in the offline settings to see performance data.
          </Alert>
        )}
        
        {error && (
          <Alert severity="error" sx={{ mb: 3 }}>
            {error}
          </Alert>
        )}
        
        {loading && !metrics && (
          <Box sx={{ display: 'flex', justifyContent: 'center', p: 4 }}>
            <CircularProgress />
          </Box>
        )}
        
        {metrics && (
          <>
            <Tabs 
              value={activeTab} 
              onChange={handleTabChange} 
              sx={{ borderBottom: 1, borderColor: 'divider' }}
            >
              <Tab label="Overview" />
              <Tab label="Providers" />
              <Tab label="Models" />
            </Tabs>
            
            {activeTab === 0 && renderOverview()}
            {activeTab === 1 && renderProvidersTab()}
            {activeTab === 2 && renderModelsTab()}
          </>
        )}
      </Paper>
    </Box>
  );
};

export default LLMPerformanceDashboard;
</file>

<file path="src-frontend/src/components/dashboard/ResourceDashboard.css">
.resource-dashboard {
  display: flex;
  flex-direction: column;
  height: 100%;
  color: var(--text-color);
  background-color: var(--bg-color-secondary);
  padding: 1rem;
  border-radius: 0.5rem;
  box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
}

.dashboard-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 1rem;
  padding-bottom: 1rem;
  border-bottom: 1px solid var(--border-color);
}

.dashboard-header h2 {
  margin: 0;
  font-size: 1.5rem;
  font-weight: 600;
}

.dashboard-controls {
  display: flex;
  align-items: center;
  gap: 1rem;
}

.refresh-control {
  display: flex;
  align-items: center;
  gap: 0.5rem;
  cursor: pointer;
}

.interval-select {
  padding: 0.25rem 0.5rem;
  border-radius: 0.25rem;
  border: 1px solid var(--border-color);
  background-color: var(--bg-color);
  color: var(--text-color);
  font-size: 0.9rem;
}

.refresh-button {
  padding: 0.25rem 0.75rem;
  border-radius: 0.25rem;
  border: none;
  background-color: var(--primary-color);
  color: white;
  font-size: 0.9rem;
  cursor: pointer;
  transition: background-color 0.2s ease;
}

.refresh-button:hover {
  background-color: var(--primary-color-hover);
}

.refresh-button:disabled {
  background-color: var(--disabled-color);
  cursor: not-allowed;
}

.dashboard-tabs {
  display: flex;
  gap: 0.25rem;
  margin-bottom: 1rem;
  overflow-x: auto;
  scrollbar-width: thin;
}

.dashboard-tabs button {
  padding: 0.5rem 1rem;
  background-color: transparent;
  border: none;
  border-bottom: 2px solid transparent;
  color: var(--text-color-secondary);
  font-size: 1rem;
  cursor: pointer;
  transition: all 0.2s ease;
  white-space: nowrap;
}

.dashboard-tabs button:hover {
  color: var(--text-color);
}

.dashboard-tabs button.active {
  color: var(--primary-color);
  border-bottom-color: var(--primary-color);
}

.dashboard-content {
  flex-grow: 1;
  overflow-y: auto;
  padding: 0.5rem;
}

.error-message {
  padding: 0.75rem;
  background-color: var(--error-bg-color);
  color: var(--error-text-color);
  border-radius: 0.25rem;
  margin-bottom: 1rem;
}

.loading-indicator {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  height: 100%;
}

.spinner {
  width: 2rem;
  height: 2rem;
  border: 3px solid rgba(0, 0, 0, 0.1);
  border-top-color: var(--primary-color);
  border-radius: 50%;
  animation: spin 1s linear infinite;
  margin-bottom: 1rem;
}

@keyframes spin {
  to {
    transform: rotate(360deg);
  }
}

.overview-grid {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(280px, 1fr));
  gap: 1rem;
}

.metric-card {
  background-color: var(--bg-color);
  border-radius: 0.5rem;
  padding: 1rem;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);
  transition: transform 0.2s ease;
}

.metric-card:hover {
  transform: translateY(-2px);
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
}

.metric-card h3 {
  margin-top: 0;
  margin-bottom: 0.5rem;
  font-size: 1rem;
  font-weight: 600;
  color: var(--text-color-secondary);
}

.metric-value {
  font-size: 1.5rem;
  font-weight: 700;
  margin-bottom: 0.75rem;
}

.meter {
  height: 8px;
  background-color: var(--meter-bg-color, #f0f0f0);
  border-radius: 4px;
  overflow: hidden;
}

.meter-fill {
  height: 100%;
  background-color: var(--primary-color);
  border-radius: 4px;
  transition: width 0.5s ease;
}

.detailed-metrics {
  display: flex;
  flex-direction: column;
  gap: 1.5rem;
}

.metric-detail-card {
  background-color: var(--bg-color);
  border-radius: 0.5rem;
  padding: 1.5rem;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);
}

.metric-detail-card h3 {
  margin-top: 0;
  margin-bottom: 1rem;
  font-size: 1.1rem;
  font-weight: 600;
}

.stats-grid {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
  gap: 1rem;
}

.stat-item {
  padding: 0.75rem;
  background-color: var(--bg-color-secondary);
  border-radius: 0.25rem;
}

.stat-label {
  font-size: 0.9rem;
  color: var(--text-color-secondary);
  margin-bottom: 0.25rem;
}

.stat-value {
  font-size: 1.1rem;
  font-weight: 600;
}

.stats-detail {
  display: flex;
  flex-direction: column;
  gap: 0.75rem;
}

.stat-row {
  display: flex;
  justify-content: space-between;
  padding: 0.75rem;
  background-color: var(--bg-color-secondary);
  border-radius: 0.25rem;
}

.metrics-table {
  width: 100%;
  border-collapse: collapse;
}

.metrics-table th,
.metrics-table td {
  padding: 0.75rem 1rem;
  text-align: left;
  border-bottom: 1px solid var(--border-color);
}

.metrics-table th {
  font-weight: 600;
  color: var(--text-color-secondary);
  background-color: var(--bg-color-secondary);
}

.metrics-table tr:last-child td {
  border-bottom: none;
}

.metrics-table tr:hover td {
  background-color: var(--bg-color-hover);
}

.stat-warning {
  color: var(--warning-color);
}

.status-indicator {
  display: inline-block;
  width: 10px;
  height: 10px;
  border-radius: 50%;
  margin-right: 0.5rem;
}

.status-indicator.connected {
  background-color: var(--success-color);
}

.status-indicator.disconnected {
  background-color: var(--error-color);
}

/* Dark mode specific styles */
@media (prefers-color-scheme: dark) {
  .meter {
    --meter-bg-color: #2a2a2a;
  }
}

/* Responsive adjustments */
@media (max-width: 768px) {
  .dashboard-header {
    flex-direction: column;
    align-items: flex-start;
    gap: 1rem;
  }
  
  .dashboard-controls {
    width: 100%;
    justify-content: space-between;
  }
  
  .overview-grid {
    grid-template-columns: 1fr;
  }
  
  .stats-grid {
    grid-template-columns: 1fr;
  }
}
</file>

<file path="src-frontend/src/components/help/ExtendedGuide.css">
.extended-guide {
  display: flex;
  flex-direction: column;
  height: 100%;
  background-color: #f5f7fa;
  color: #333;
  overflow: hidden;
  font-family: 'Inter', sans-serif;
}

.guide-header {
  background-color: #2196f3;
  color: white;
  padding: 24px;
  text-align: center;
  position: relative;
}

.guide-header h1 {
  font-size: 28px;
  margin: 0;
  font-weight: 600;
}

.guide-subtitle {
  font-size: 16px;
  opacity: 0.8;
  margin-top: 6px;
}

.close-guide {
  position: absolute;
  top: 20px;
  right: 20px;
  background: rgba(255, 255, 255, 0.2);
  border: none;
  border-radius: 50%;
  width: 36px;
  height: 36px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  transition: background-color 0.2s ease;
}

.close-guide:hover {
  background: rgba(255, 255, 255, 0.3);
}

.close-guide svg {
  fill: white;
}

.guide-navigation {
  background-color: #fff;
  border-bottom: 1px solid #e0e4e8;
  padding: 12px 24px;
}

.chapter-navigation {
  display: flex;
  align-items: center;
  gap: 16px;
}

.chapter-nav-button {
  display: flex;
  align-items: center;
  gap: 8px;
  padding: 8px 16px;
  background-color: #f0f4f8;
  border: 1px solid #e0e4e8;
  border-radius: 4px;
  font-size: 14px;
  cursor: pointer;
  transition: all 0.2s ease;
}

.chapter-nav-button:hover:not(:disabled) {
  background-color: #e3f2fd;
  border-color: #90caf9;
}

.chapter-nav-button:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.chapter-nav-button svg {
  fill: #2196f3;
}

.chapter-select {
  flex: 1;
  min-width: 200px;
}

.chapter-select select {
  width: 100%;
  padding: 8px 12px;
  border: 1px solid #e0e4e8;
  border-radius: 4px;
  background-color: white;
  font-size: 14px;
}

.guide-content {
  display: flex;
  flex: 1;
  overflow: hidden;
}

.section-sidebar {
  width: 250px;
  background-color: #fff;
  border-right: 1px solid #e0e4e8;
  padding: 20px 0;
  overflow-y: auto;
}

.section-sidebar h2 {
  font-size: 18px;
  margin: 0 0 16px;
  padding: 0 20px 16px;
  border-bottom: 1px solid #e0e4e8;
}

.section-list {
  list-style: none;
  padding: 0;
  margin: 0;
}

.section-list li {
  padding: 12px 20px;
  cursor: pointer;
  transition: all 0.2s ease;
  border-left: 3px solid transparent;
}

.section-list li:hover {
  background-color: #f0f4f8;
  border-left-color: #2196f3;
}

.section-list li.active {
  background-color: #e3f2fd;
  border-left-color: #2196f3;
  font-weight: 500;
}

.section-content {
  flex: 1;
  padding: 30px;
  overflow-y: auto;
  line-height: 1.6;
}

.section-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 24px;
  padding-bottom: 16px;
  border-bottom: 1px solid #e0e4e8;
}

.section-header h2 {
  font-size: 24px;
  margin: 0;
  color: #1976d2;
}

.progress-indicator {
  color: #666;
  font-size: 14px;
}

.section-body {
  margin-bottom: 30px;
}

.section-navigation {
  display: flex;
  justify-content: space-between;
  margin-top: 40px;
  padding-top: 20px;
  border-top: 1px solid #e0e4e8;
}

.section-nav-button {
  display: flex;
  align-items: center;
  gap: 8px;
  padding: 10px 18px;
  background-color: #2196f3;
  color: white;
  border: none;
  border-radius: 4px;
  font-size: 15px;
  cursor: pointer;
  transition: all 0.2s ease;
}

.section-nav-button:hover:not(:disabled) {
  background-color: #1976d2;
}

.section-nav-button:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.section-nav-button svg {
  fill: white;
}

/* Content Styling */

.guide-objectives {
  background-color: #e8f5e9;
  border-left: 4px solid #4caf50;
  padding: 16px 20px;
  margin: 0 0 24px;
  border-radius: 0 4px 4px 0;
}

.guide-objectives li {
  margin-bottom: 8px;
}

.guide-objectives li:last-child {
  margin-bottom: 0;
}

.info-bubble {
  background-color: #e3f2fd;
  border-radius: 6px;
  padding: 16px;
  margin: 16px 0;
}

.mini-knowledge-drop {
  background-color: #fff8e1;
  border-radius: 6px;
  padding: 16px;
  margin: 16px 0;
  border-left: 4px solid #ffc107;
}

.mini-knowledge-drop h4 {
  margin-top: 0;
  color: #f57c00;
}

.tip-box {
  background-color: #e8f5e9;
  border-radius: 6px;
  padding: 16px;
  margin: 16px 0;
}

.metaphor-key {
  background-color: #f3e5f5;
  border-radius: 6px;
  padding: 16px;
  margin: 24px 0;
  border-left: 4px solid #9c27b0;
}

.metaphor-key h4 {
  margin-top: 0;
  color: #7b1fa2;
  margin-bottom: 12px;
}

.feature-grid {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(240px, 1fr));
  gap: 16px;
  margin: 24px 0;
}

.feature-card {
  background-color: white;
  border-radius: 6px;
  padding: 16px;
  box-shadow: 0 2px 6px rgba(0, 0, 0, 0.05);
  transition: transform 0.2s ease, box-shadow 0.2s ease;
}

.feature-card:hover {
  transform: translateY(-3px);
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
}

.feature-card h4 {
  margin-top: 0;
  color: #1976d2;
  margin-bottom: 8px;
}

.feature-card p {
  margin-bottom: 0;
}

.use-case {
  background-color: #f5f5f5;
  border-radius: 6px;
  padding: 20px;
  margin: 24px 0;
}

.use-case h4 {
  margin-top: 0;
  margin-bottom: 12px;
  color: #333;
}

.learning-path {
  display: flex;
  flex-direction: column;
  gap: 8px;
  margin: 24px 0;
}

.path-stage {
  display: flex;
  gap: 16px;
  padding: 16px;
  background-color: white;
  border-radius: 6px;
  box-shadow: 0 2px 6px rgba(0, 0, 0, 0.05);
}

.stage-number {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 36px;
  height: 36px;
  border-radius: 50%;
  background-color: #2196f3;
  color: white;
  font-weight: 600;
  flex-shrink: 0;
}

.stage-content {
  flex: 1;
}

.stage-content h4 {
  margin: 0 0 4px;
  color: #1976d2;
}

.stage-content p {
  margin: 0;
}

.concept-checkpoint {
  background-color: #fff3e0;
  border-radius: 6px;
  padding: 20px;
  margin: 30px 0;
}

.concept-checkpoint h4 {
  margin-top: 0;
  color: #e65100;
  margin-bottom: 16px;
}

.question {
  margin-bottom: 16px;
}

.options {
  display: flex;
  flex-direction: column;
  gap: 8px;
  margin: 12px 0;
}

.option {
  display: flex;
  align-items: center;
  gap: 8px;
  cursor: pointer;
  padding: 8px;
  border-radius: 4px;
  transition: background-color 0.2s ease;
}

.option:hover {
  background-color: rgba(0, 0, 0, 0.05);
}

.feedback {
  background-color: #e8f5e9;
  border-radius: 4px;
  padding: 12px;
  margin-top: 12px;
  display: none;
}

.feedback-correct {
  display: block;
}

.practice-exercise {
  background-color: #f3f6f9;
  border-radius: 6px;
  padding: 24px;
  margin: 30px 0;
  border-left: 4px solid #1976d2;
}

.practice-exercise h4 {
  margin-top: 0;
  color: #1976d2;
  margin-bottom: 16px;
}

.exercise-template {
  background-color: white;
  border-radius: 4px;
  padding: 16px;
  margin: 16px 0;
}

.exercise-template h5 {
  margin-top: 0;
  margin-bottom: 8px;
}

.exercise-template pre {
  background-color: #f7f9fc;
  padding: 12px;
  overflow-x: auto;
  border-radius: 4px;
  margin: 0;
}

.exercise-note {
  font-style: italic;
  margin-top: 16px;
  margin-bottom: 0;
}

.key-takeaways {
  background-color: #e8eaf6;
  border-radius: 6px;
  padding: 20px 20px 20px 40px;
  margin: 24px 0;
}

.key-takeaways li {
  margin-bottom: 8px;
}

.growth-mindset {
  background-color: #f3e5f5;
  border-radius: 6px;
  padding: 20px;
  margin: 24px 0;
}

.growth-mindset h4 {
  margin-top: 0;
  color: #7b1fa2;
  margin-bottom: 12px;
}

.apply-to-life {
  background-color: #e0f7fa;
  border-radius: 6px;
  padding: 20px;
  margin: 24px 0;
}

.apply-to-life h4 {
  margin-top: 0;
  color: #00838f;
  margin-bottom: 12px;
}

.peer-reflection {
  background-color: #e3f2fd;
  border-radius: 6px;
  padding: 20px;
  margin: 24px 0;
}

.peer-reflection h4 {
  margin-top: 0;
  color: #1565c0;
  margin-bottom: 12px;
}

.tabbed-content {
  margin: 24px 0;
  border: 1px solid #e0e4e8;
  border-radius: 6px;
  overflow: hidden;
}

.tab-headers {
  display: flex;
  background-color: #f5f7fa;
  border-bottom: 1px solid #e0e4e8;
}

.tab-header {
  padding: 12px 20px;
  cursor: pointer;
  transition: all 0.2s ease;
  font-weight: 500;
  border-bottom: 2px solid transparent;
}

.tab-header:hover {
  background-color: #e3f2fd;
}

.tab-header.active {
  background-color: white;
  border-bottom-color: #2196f3;
  color: #1976d2;
}

.tab-content {
  padding: 20px;
  display: none;
  background-color: white;
}

.tab-content.active {
  display: block;
}

.setup-steps {
  display: flex;
  flex-direction: column;
  gap: 16px;
  margin: 24px 0;
}

.setup-step {
  display: flex;
  gap: 16px;
  padding: 16px;
  background-color: white;
  border-radius: 6px;
  box-shadow: 0 2px 6px rgba(0, 0, 0, 0.05);
}

.interface-overview {
  margin: 24px 0;
}

.interface-section {
  margin-bottom: 24px;
}

.interface-section h4 {
  margin-top: 0;
  color: #1976d2;
  margin-bottom: 12px;
}

.interface-image {
  background-color: #f5f5f5;
  border: 1px solid #e0e0e0;
  border-radius: 4px;
  padding: 16px;
  text-align: center;
  margin-bottom: 16px;
  color: #666;
}

.keyboard-shortcuts {
  margin: 24px 0;
}

.keyboard-shortcuts h4 {
  margin-top: 0;
  margin-bottom: 12px;
}

.keyboard-shortcuts table {
  width: 100%;
  border-collapse: collapse;
  border: 1px solid #e0e4e8;
  border-radius: 4px;
  overflow: hidden;
}

.keyboard-shortcuts th {
  background-color: #f5f7fa;
  padding: 12px 16px;
  text-align: left;
  font-weight: 600;
  border-bottom: 1px solid #e0e4e8;
}

.keyboard-shortcuts td {
  padding: 10px 16px;
  border-bottom: 1px solid #e0e4e8;
}

.keyboard-shortcuts tr:last-child td {
  border-bottom: none;
}

.first-conversation-steps {
  counter-reset: step-counter;
  list-style-type: none;
  padding: 0;
  margin: 24px 0;
}

.first-conversation-steps li {
  counter-increment: step-counter;
  position: relative;
  padding-left: 50px;
  margin-bottom: 24px;
}

.first-conversation-steps li::before {
  content: counter(step-counter);
  position: absolute;
  left: 0;
  top: 0;
  width: 36px;
  height: 36px;
  background-color: #2196f3;
  color: white;
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  font-weight: 600;
}

.first-conversation-steps li strong {
  display: block;
  font-size: 18px;
  margin-bottom: 8px;
  color: #333;
}

.example-message {
  background-color: #f5f5f5;
  border-radius: 4px;
  padding: 12px;
  margin-top: 8px;
}

.conversation-example {
  background-color: white;
  border-radius: 6px;
  border: 1px solid #e0e4e8;
  overflow: hidden;
  margin-top: 16px;
}

.message {
  padding: 16px;
}

.message:not(:last-child) {
  border-bottom: 1px solid #e0e4e8;
}

.user-message {
  background-color: #f5f7fa;
}

.ai-message {
  background-color: white;
}

.message strong {
  display: block;
  margin-bottom: 8px;
}

.message p {
  margin-top: 0;
  margin-bottom: 12px;
}

.message p:last-child {
  margin-bottom: 0;
}

code {
  background-color: #f0f4f8;
  padding: 2px 6px;
  border-radius: 4px;
  font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
  font-size: 0.9em;
}

pre {
  background-color: #f7f9fc;
  padding: 12px;
  overflow-x: auto;
  border-radius: 4px;
  margin: 16px 0;
  font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
  font-size: 0.9em;
}

@media (max-width: 768px) {
  .guide-content {
    flex-direction: column;
  }
  
  .section-sidebar {
    width: 100%;
    border-right: none;
    border-bottom: 1px solid #e0e4e8;
    padding: 16px;
  }
  
  .feature-grid {
    grid-template-columns: 1fr;
  }
  
  .section-content {
    padding: 16px;
  }
  
  .chapter-navigation {
    flex-direction: column;
  }
  
  .keyboard-shortcuts {
    overflow-x: auto;
  }
}
</file>

<file path="src-frontend/src/components/help/ExtendedGuide.tsx">
import React, { useState } from 'react';
import './ExtendedGuide.css';

interface ExtendedGuideProps {
  onClose: () => void;
}

const ExtendedGuide: React.FC<ExtendedGuideProps> = ({ onClose }) => {
  const [currentChapter, setCurrentChapter] = useState(0);
  const [currentSection, setCurrentSection] = useState(0);
  
  const chapters = [
    {
      title: "Chapter 1: Introduction to Papin",
      sections: [
        {
          title: "Learning Objectives",
          content: (
            <ul className="guide-objectives">
              <li>Understand what Papin is and how it enhances AI interactions</li>
              <li>Recognize the key benefits of using Papin as your MCP client</li>
              <li>Identify the core features that set Papin apart</li>
              <li>Prepare for successful installation and configuration</li>
            </ul>
          )
        },
        {
          title: "What is Papin?",
          content: (
            <div>
              <p>Papin is a sophisticated desktop application that serves as a Model Communication Protocol (MCP) client. Think of it as your personal interface for interacting with artificial intelligence models, designed to provide seamless access whether you're online or offline.</p>
              
              <div className="info-bubble">
                <strong>Did You Know?</strong> Papin is named after Denis Papin, a 17th-century physicist and inventor known for his work on steam engines. Just as Papin helped harness steam power for practical use, our application helps make AI power more accessible and practical for everyday tasks.
              </div>
              
              <p>At its core, Papin bridges the gap between sophisticated AI technology and practical, everyday use by:</p>
              
              <ul>
                <li><strong>Enabling offline capabilities</strong> through local model integration</li>
                <li><strong>Enhancing privacy</strong> with local-first processing options</li>
                <li><strong>Providing robust performance monitoring</strong> to optimize your experience</li>
                <li><strong>Ensuring reliable synchronization</strong> across your devices</li>
              </ul>
              
              <div className="metaphor-key">
                <h4>Metaphor Key: Understanding Papin</h4>
                <p><strong>Papin as a Universal Translator:</strong> Imagine you're in a room with brilliant minds who speak different languages. Papin acts as your interpreter, helping you communicate with AI models regardless of your technical knowledge or network connectivity.</p>
                <p><strong>Papin as a Smart Workspace:</strong> Think of Papin as your dedicated office that intelligently adapts to your needs. It organizes your AI conversations, manages resources efficiently, and ensures your work is backed up and available wherever you go.</p>
              </div>
            </div>
          )
        },
        {
          title: "Core Features and Benefits",
          content: (
            <div>
              <div className="feature-grid">
                <div className="feature-card">
                  <h4>Offline Capabilities</h4>
                  <p>Work with AI models even without internet connectivity, ensuring productivity regardless of where you are.</p>
                </div>
                <div className="feature-card">
                  <h4>Local-First Architecture</h4>
                  <p>Keep your data on your device first, enhancing privacy and reducing latency for faster interactions.</p>
                </div>
                <div className="feature-card">
                  <h4>Performance Monitoring</h4>
                  <p>Gain insights into system resources, model performance, and application metrics to optimize your experience.</p>
                </div>
                <div className="feature-card">
                  <h4>Seamless Synchronization</h4>
                  <p>Maintain consistency across multiple devices with intelligent conflict resolution and efficient sync protocols.</p>
                </div>
              </div>
              
              <div className="use-case">
                <h4>Real-World Scenario: Mobile Researcher</h4>
                <p>Dr. Chen is a field researcher who often works in remote locations with limited connectivity. Before discovering Papin, she had to prepare all her research questions in advance and wait until returning to areas with internet access to get AI assistance.</p>
                <p>With Papin, Dr. Chen downloads specialized research models before her expeditions. While in the field, she can continue asking complex questions about her observations, generating hypotheses, and analyzing preliminary data—all without internet connectivity. When she returns to camp with WiFi, Papin automatically synchronizes her conversations to the cloud, making them available on her lab computer and ensuring no insights are lost.</p>
              </div>
            </div>
          )
        },
        {
          title: "Your Learning Path",
          content: (
            <div>
              <p>This comprehensive guide will take you from complete beginner to advanced user through a carefully structured learning path:</p>
              
              <div className="learning-path">
                <div className="path-stage">
                  <div className="stage-number">1</div>
                  <div className="stage-content">
                    <h4>Foundation</h4>
                    <p>Installation, basic setup, and your first AI conversation</p>
                  </div>
                </div>
                <div className="path-stage">
                  <div className="stage-number">2</div>
                  <div className="stage-content">
                    <h4>Core Features</h4>
                    <p>Mastering conversations, organization, and offline capabilities</p>
                  </div>
                </div>
                <div className="path-stage">
                  <div className="stage-number">3</div>
                  <div className="stage-content">
                    <h4>Advanced Techniques</h4>
                    <p>Performance tuning, customization, and specialized workflows</p>
                  </div>
                </div>
                <div className="path-stage">
                  <div className="stage-number">4</div>
                  <div className="stage-content">
                    <h4>Expert Integration</h4>
                    <p>API usage, programmatic control, and enterprise workflows</p>
                  </div>
                </div>
              </div>
              
              <div className="concept-checkpoint">
                <h4>Quick Check: Understanding Papin's Purpose</h4>
                <div className="question">
                  <p>What is the primary advantage of Papin's local-first architecture?</p>
                  <div className="options">
                    <label className="option">
                      <input type="radio" name="q1" value="a" />
                      <span>Reduces installation size</span>
                    </label>
                    <label className="option">
                      <input type="radio" name="q1" value="b" />
                      <span>Enhances privacy and enables offline use</span>
                    </label>
                    <label className="option">
                      <input type="radio" name="q1" value="c" />
                      <span>Eliminates the need for synchronization</span>
                    </label>
                  </div>
                  <div className="feedback feedback-correct">
                    <p>Correct! Papin's local-first approach prioritizes keeping your data on your device, enhancing privacy and enabling offline functionality.</p>
                  </div>
                </div>
              </div>
            </div>
          )
        },
        {
          title: "Practice Exercise",
          content: (
            <div className="practice-exercise">
              <h4>Envisioning Your Papin Usage</h4>
              
              <p>Consider your unique needs and workflow to prepare for an effective Papin setup:</p>
              
              <ol>
                <li>Identify 2-3 scenarios in your work or personal life where you'd benefit from AI assistance</li>
                <li>For each scenario, note whether internet connectivity is consistently available</li>
                <li>Consider what types of questions or tasks you'd typically need help with</li>
                <li>Think about your device constraints (storage space, memory, processing power)</li>
              </ol>
              
              <div className="exercise-template">
                <h5>Sample Template:</h5>
                <pre>{`Scenario 1: Writing technical documentation
Connectivity: Usually available, occasional travel
Typical tasks: Structuring content, improving clarity, generating examples
Device constraints: 16GB RAM, 256GB SSD (75GB free)

Scenario 2: Research data analysis
Connectivity: Limited (fieldwork)
Typical tasks: Data interpretation, methodology suggestions, literature connections
Device constraints: Same laptop as above`}</pre>
              </div>
              
              <p className="exercise-note">This exercise will help you make better decisions about which local models to install, how to configure offline settings, and which features to prioritize in your setup.</p>
            </div>
          )
        },
        {
          title: "Key Takeaways",
          content: (
            <div>
              <ul className="key-takeaways">
                <li>Papin is a sophisticated MCP client that enables AI interaction with enhanced offline capabilities.</li>
                <li>The local-first architecture provides benefits for privacy, performance, and accessibility.</li>
                <li>Core features include offline capabilities, performance monitoring, and seamless synchronization.</li>
                <li>Understanding your specific use cases helps optimize your Papin configuration.</li>
              </ul>
              
              <div className="growth-mindset">
                <h4>Growth Mindset Moment</h4>
                <p>Learning a new tool like Papin might seem daunting at first, especially if you're new to AI technologies. Remember that mastery comes through exploration and practice. Each feature you learn builds upon the last, creating a foundation for advanced usage. Don't worry about understanding everything immediately—this guide is structured to introduce concepts gradually, building your confidence as you progress.</p>
              </div>
              
              <div className="apply-to-life">
                <h4>Apply It To Your Life</h4>
                <p>Consider areas in your life where consistent AI assistance could enhance your productivity or creativity. Perhaps you're a student needing research help in places with unreliable internet, a professional who travels frequently, or someone who values privacy in AI interactions. Identifying these scenarios now will help you recognize the value of specific Papin features as we explore them.</p>
              </div>
            </div>
          )
        }
      ]
    },
    {
      title: "Chapter 2: Getting Started with Papin",
      sections: [
        {
          title: "Learning Objectives",
          content: (
            <ul className="guide-objectives">
              <li>Successfully install Papin on your preferred operating system</li>
              <li>Create and configure your Papin account</li>
              <li>Navigate the Papin interface</li>
              <li>Initiate and complete your first AI conversation</li>
            </ul>
          )
        },
        {
          title: "Installation Process",
          content: (
            <div>
              <p>Installing Papin is straightforward across all major operating systems. Choose the instructions for your platform:</p>
              
              <div className="tabbed-content">
                <div className="tab-headers">
                  <div className="tab-header active">Windows</div>
                  <div className="tab-header">macOS</div>
                  <div className="tab-header">Linux</div>
                </div>
                <div className="tab-content active">
                  <ol>
                    <li>Download the installer (.msi file) from the <a href="#">official Papin website</a>.</li>
                    <li>Run the downloaded .msi file.</li>
                    <li>If prompted by User Account Control, click "Yes" to allow the installer to run.</li>
                    <li>Follow the on-screen instructions in the installation wizard.</li>
                    <li>Choose your installation location or accept the default.</li>
                    <li>Select whether to create a desktop shortcut and Start menu entry.</li>
                    <li>Click "Install" to begin the installation process.</li>
                    <li>Once complete, click "Finish" to exit the installer.</li>
                  </ol>
                  <div className="info-bubble">
                    <strong>Tip:</strong> If you're installing in an enterprise environment, you can use the silent installation option with <code>papin-installer.exe /S</code> for automated deployment.
                  </div>
                </div>
                <div className="tab-content">
                  <ol>
                    <li>Download the .dmg file from the <a href="#">official Papin website</a>.</li>
                    <li>Open the downloaded .dmg file.</li>
                    <li>In the window that appears, drag the Papin icon to the Applications folder.</li>
                    <li>Eject the disk image by dragging it to the Trash, which becomes an Eject button when a disk image is selected.</li>
                    <li>Open your Applications folder and double-click on Papin to launch it.</li>
                    <li>If you see a security warning, go to System Preferences > Security & Privacy, and click "Open Anyway."</li>
                  </ol>
                  <div className="info-bubble">
                    <strong>Note:</strong> On newer versions of macOS, you may need to right-click (or Control+click) on the app and select "Open" the first time to bypass Gatekeeper.
                  </div>
                </div>
                <div className="tab-content">
                  <ol>
                    <li>Download the appropriate package for your distribution (.deb, .rpm, or .AppImage).</li>
                    <li>For .deb packages (Ubuntu, Debian, etc.):
                      <pre>sudo dpkg -i papin_1.0.0.deb</pre>
                      <pre>sudo apt-get install -f # To resolve any dependencies</pre>
                    </li>
                    <li>For .rpm packages (Fedora, RHEL, etc.):
                      <pre>sudo rpm -i papin_1.0.0.rpm</pre>
                      <pre>sudo dnf install -f # To resolve any dependencies</pre>
                    </li>
                    <li>For .AppImage:
                      <pre>chmod +x Papin-1.0.0.AppImage</pre>
                      <pre>./Papin-1.0.0.AppImage</pre>
                    </li>
                  </ol>
                  <div className="info-bubble">
                    <strong>Tip:</strong> You can integrate AppImage files with your desktop environment using tools like AppImageLauncher.
                  </div>
                </div>
              </div>
              
              <div className="mini-knowledge-drop">
                <h4>Installation Verification</h4>
                <p>After installation, Papin automatically runs a verification process to ensure all components are properly installed. If you encounter any issues, you can manually trigger this verification by running <code>papin --verify</code> from the command line.</p>
              </div>
            </div>
          )
        },
        {
          title: "Initial Setup and Configuration",
          content: (
            <div>
              <p>When you first launch Papin, you'll be guided through a setup wizard to create your account and configure basic settings:</p>
              
              <div className="setup-steps">
                <div className="setup-step">
                  <div className="step-number">1</div>
                  <div className="step-content">
                    <h4>Welcome Screen</h4>
                    <p>Papin will present a welcome screen with options to create a new account or sign in with an existing one. Click "Create Account" to get started.</p>
                  </div>
                </div>
                <div className="setup-step">
                  <div className="step-number">2</div>
                  <div className="step-content">
                    <h4>Account Creation</h4>
                    <p>Enter your email address and create a secure password. This account will be used to synchronize your conversations and settings across devices.</p>
                    <div className="info-bubble">
                      <strong>Privacy Note:</strong> All user data is encrypted both in transit and at rest. Papin's local-first architecture means your data stays on your device by default.
                    </div>
                  </div>
                </div>
                <div className="setup-step">
                  <div className="step-number">3</div>
                  <div className="step-content">
                    <h4>Model Selection</h4>
                    <p>Choose which AI models you want to use. You can select from cloud-based models (which require internet connectivity) and local models (which run directly on your device).</p>
                    <div className="tip-box">
                      <strong>Recommendation:</strong> For a balanced experience, select at least one general-purpose local model for offline use and several specialized cloud models for when you have connectivity.
                    </div>
                  </div>
                </div>
                <div className="setup-step">
                  <div className="step-number">4</div>
                  <div className="step-content">
                    <h4>Offline Capabilities</h4>
                    <p>Configure your offline settings, including whether to download models for offline use, how much disk space to allocate, and synchronization preferences.</p>
                  </div>
                </div>
                <div className="setup-step">
                  <div className="step-number">5</div>
                  <div className="step-content">
                    <h4>Performance Settings</h4>
                    <p>Papin will automatically detect your system capabilities and suggest optimal performance settings. You can adjust these settings based on your preferences for speed vs. resource usage.</p>
                  </div>
                </div>
                <div className="setup-step">
                  <div className="step-number">6</div>
                  <div className="step-content">
                    <h4>Setup Complete</h4>
                    <p>Once configuration is complete, Papin will finish downloading any selected local models (if you opted for offline capabilities) and present you with the main interface.</p>
                  </div>
                </div>
              </div>
              
              <div className="concept-checkpoint">
                <h4>Quick Check: Initial Setup</h4>
                <div className="question">
                  <p>Which of the following is NOT part of the initial Papin setup process?</p>
                  <div className="options">
                    <label className="option">
                      <input type="radio" name="q2" value="a" />
                      <span>Account creation</span>
                    </label>
                    <label className="option">
                      <input type="radio" name="q2" value="b" />
                      <span>Model selection</span>
                    </label>
                    <label className="option">
                      <input type="radio" name="q2" value="c" />
                      <span>Creating your first conversation template</span>
                    </label>
                  </div>
                  <div className="feedback feedback-correct">
                    <p>Correct! Creating conversation templates is an advanced feature you can explore after the initial setup, not part of the onboarding process.</p>
                  </div>
                </div>
              </div>
            </div>
          )
        },
        {
          title: "Navigating the Interface",
          content: (
            <div>
              <p>The Papin interface is designed to be intuitive and focused on your conversations. Let's explore the main elements:</p>
              
              <div className="interface-overview">
                <div className="interface-section">
                  <h4>Main Layout</h4>
                  <div className="interface-image">
                    [Interface Diagram: Main Layout]
                  </div>
                  <ul>
                    <li><strong>Sidebar:</strong> Contains your conversation history, folders, and navigation options</li>
                    <li><strong>Main Content Area:</strong> Displays the current conversation or other active content</li>
                    <li><strong>Toolbar:</strong> Provides access to settings, help, and additional features</li>
                    <li><strong>Status Bar:</strong> Shows connection status, sync progress, and system metrics</li>
                  </ul>
                </div>
                
                <div className="interface-section">
                  <h4>Sidebar Navigation</h4>
                  <ul>
                    <li><strong>New Conversation:</strong> Button to start a new AI conversation</li>
                    <li><strong>Search Bar:</strong> Search across all your conversations</li>
                    <li><strong>Recent Conversations:</strong> Quick access to recently active conversations</li>
                    <li><strong>Folders:</strong> Organizational structure for your conversations</li>
                    <li><strong>Favorites:</strong> Starred or pinned conversations for easy access</li>
                  </ul>
                </div>
                
                <div className="interface-section">
                  <h4>Conversation Interface</h4>
                  <ul>
                    <li><strong>Conversation Title:</strong> Shows the name of your current conversation (editable)</li>
                    <li><strong>Message History:</strong> Scrollable area showing the conversation between you and the AI</li>
                    <li><strong>Input Area:</strong> Where you type your messages to the AI</li>
                    <li><strong>Model Information:</strong> Shows which AI model is currently active</li>
                    <li><strong>Settings:</strong> Access conversation-specific settings and model parameters</li>
                  </ul>
                </div>
              </div>
              
              <div className="keyboard-shortcuts">
                <h4>Essential Keyboard Shortcuts</h4>
                <table>
                  <thead>
                    <tr>
                      <th>Action</th>
                      <th>Windows/Linux</th>
                      <th>macOS</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>New Conversation</td>
                      <td>Ctrl+N</td>
                      <td>⌘+N</td>
                    </tr>
                    <tr>
                      <td>Find in Conversation</td>
                      <td>Ctrl+F</td>
                      <td>⌘+F</td>
                    </tr>
                    <tr>
                      <td>Search All Conversations</td>
                      <td>Ctrl+Shift+F</td>
                      <td>⌘+Shift+F</td>
                    </tr>
                    <tr>
                      <td>Settings</td>
                      <td>Ctrl+,</td>
                      <td>⌘+,</td>
                    </tr>
                    <tr>
                      <td>Help</td>
                      <td>F1</td>
                      <td>F1</td>
                    </tr>
                    <tr>
                      <td>Manual Sync</td>
                      <td>Ctrl+Shift+S</td>
                      <td>⌘+Shift+S</td>
                    </tr>
                  </tbody>
                </table>
              </div>
              
              <div className="mini-knowledge-drop">
                <h4>Interface Customization</h4>
                <p>You can customize the Papin interface by going to Settings > Appearance. Options include dark/light theme, sidebar width, font size, and density settings for the conversation display.</p>
              </div>
            </div>
          )
        },
        {
          title: "Your First Conversation",
          content: (
            <div>
              <p>Now that you're familiar with the interface, let's start your first conversation with an AI model:</p>
              
              <ol className="first-conversation-steps">
                <li>
                  <strong>Start a new conversation</strong>
                  <p>Click the "New Conversation" button in the sidebar or use the Ctrl+N (⌘+N on Mac) keyboard shortcut.</p>
                </li>
                <li>
                  <strong>Select an AI model</strong>
                  <p>You'll be prompted to choose which AI model to use. For your first conversation, select a general-purpose model that best matches your needs.</p>
                  <div className="info-bubble">
                    <strong>Model Selection Tip:</strong> Models with a "Local" badge run directly on your device and work offline. Models with a "Cloud" badge require internet connectivity but may have more advanced capabilities.
                  </div>
                </li>
                <li>
                  <strong>Type your first message</strong>
                  <p>In the input area at the bottom of the conversation, type your question or message. Be clear and specific for the best results.</p>
                  <div className="example-message">
                    <p><strong>Example Message:</strong> "Hello! I'd like to learn more about quantum computing. Can you explain the basic principles in simple terms and provide some examples of potential applications?"</p>
                  </div>
                </li>
                <li>
                  <strong>Send your message</strong>
                  <p>Press Enter or click the Send button (arrow icon) to send your message to the AI model.</p>
                </li>
                <li>
                  <strong>Review the response</strong>
                  <p>The AI will process your message and generate a response. This may take a few moments depending on the model and your hardware.</p>
                </li>
                <li>
                  <strong>Continue the conversation</strong>
                  <p>You can follow up with additional questions, ask for clarification, or steer the conversation in new directions. The AI maintains context from previous messages.</p>
                </li>
              </ol>
              
              <div className="use-case">
                <h4>Example Conversation Flow</h4>
                <div className="conversation-example">
                  <div className="message user-message">
                    <strong>You:</strong> Hello! I'd like to learn more about quantum computing. Can you explain the basic principles in simple terms and provide some examples of potential applications?
                  </div>
                  <div className="message ai-message">
                    <strong>AI:</strong> Certainly! Let me explain quantum computing in simple terms.
                    
                    <p><strong>Basic Principles:</strong></p>
                    <p>Classical computers use bits (0s and 1s) as their basic unit of information. Quantum computers use quantum bits or "qubits" which can exist in multiple states simultaneously thanks to two quantum phenomena:</p>
                    <ul>
                      <li><strong>Superposition:</strong> Unlike a classical bit which must be either 0 OR 1, a qubit can be in a state that is effectively both 0 AND 1 at the same time.</li>
                      <li><strong>Entanglement:</strong> Qubits can be linked together so that the state of one instantly affects the state of another, regardless of distance.</li>
                    </ul>
                    
                    <p><strong>Think of it like this:</strong> If you flip a coin, it lands on either heads or tails. But a "quantum coin" could be spinning in a state that represents both heads and tails simultaneously until you measure it.</p>
                    
                    <p><strong>Potential Applications:</strong></p>
                    <ul>
                      <li><strong>Cryptography:</strong> Breaking (and creating) more complex encryption systems</li>
                      <li><strong>Drug Discovery:</strong> Modeling complex molecular interactions to develop new medications faster</li>
                      <li><strong>Optimization Problems:</strong> Solving complex logistics challenges like traffic flow or supply chain management</li>
                      <li><strong>Materials Science:</strong> Designing new materials with specific properties by modeling quantum interactions</li>
                    </ul>
                    
                    <p>Would you like me to explain any of these principles or applications in more detail?</p>
                  </div>
                  <div className="message user-message">
                    <strong>You:</strong> Thanks for the explanation! Could you tell me more about how quantum computing could impact drug discovery specifically?
                  </div>
                  <div className="message ai-message">
                    <strong>AI:</strong> [Response about quantum computing in drug discovery would appear here]
                  </div>
                </div>
              </div>
              
              <div className="practice-exercise">
                <h4>Your Turn: First Conversation Practice</h4>
                <p>Now it's your turn to start your first conversation in Papin. Choose a topic you're genuinely interested in learning about, as this will make the experience more engaging and useful. Here are some starter ideas:</p>
                <ul>
                  <li>Ask for an explanation of a complex concept in your field</li>
                  <li>Request help brainstorming ideas for a project</li>
                  <li>Ask for a summary of a recent development in your area of interest</li>
                  <li>Seek guidance on learning a new skill or topic</li>
                </ul>
                <p>After your conversation, consider:</p>
                <ul>
                  <li>How clear was the AI's response?</li>
                  <li>Did the AI maintain context when you asked follow-up questions?</li>
                  <li>How might you phrase your questions differently next time?</li>
                </ul>
              </div>
            </div>
          )
        },
        {
          title: "Key Takeaways",
          content: (
            <div>
              <ul className="key-takeaways">
                <li>Installing Papin is straightforward across Windows, macOS, and Linux platforms.</li>
                <li>The initial setup process configures your account, model selection, and offline capabilities.</li>
                <li>The Papin interface is organized around a sidebar for navigation and a main area for conversations.</li>
                <li>Starting a new conversation involves selecting a model and typing your message in the input area.</li>
                <li>The AI maintains context throughout the conversation, allowing for natural follow-up questions.</li>
              </ul>
              
              <div className="growth-mindset">
                <h4>Growth Mindset Moment</h4>
                <p>Your first conversations with AI models might not be perfect. Learning to effectively communicate with AI is a skill that develops over time. Don't be discouraged if you don't get the exact responses you're looking for at first. Experiment with different ways of phrasing your questions, providing context, and guiding the conversation. With practice, you'll develop an intuition for how to get the most out of these interactions.</p>
              </div>
              
              <div className="peer-reflection">
                <h4>Reflection Prompt</h4>
                <p>Consider sharing your experience starting with Papin with a colleague or friend. Compare notes on:</p>
                <ul>
                  <li>Which models did you choose and why?</li>
                  <li>What types of questions yielded the most useful responses?</li>
                  <li>How might you integrate Papin into your daily workflow?</li>
                </ul>
              </div>
              
              <div className="apply-to-life">
                <h4>Apply It To Your Life</h4>
                <p>Identify one specific task in your workflow that could benefit from AI assistance through Papin. It could be drafting emails, researching topics, brainstorming ideas, or analyzing data. Set up a dedicated folder for this task and experiment with using Papin to enhance your productivity in this area over the next week.</p>
              </div>
            </div>
          )
        }
      ]
    }
  ];
  
  const nextChapter = () => {
    if (currentChapter < chapters.length - 1) {
      setCurrentChapter(currentChapter + 1);
      setCurrentSection(0);
      window.scrollTo(0, 0);
    }
  };
  
  const prevChapter = () => {
    if (currentChapter > 0) {
      setCurrentChapter(currentChapter - 1);
      setCurrentSection(0);
      window.scrollTo(0, 0);
    }
  };
  
  const nextSection = () => {
    const currentChapterObj = chapters[currentChapter];
    if (currentSection < currentChapterObj.sections.length - 1) {
      setCurrentSection(currentSection + 1);
      window.scrollTo(0, 0);
    } else if (currentChapter < chapters.length - 1) {
      nextChapter();
    }
  };
  
  const prevSection = () => {
    if (currentSection > 0) {
      setCurrentSection(currentSection - 1);
      window.scrollTo(0, 0);
    } else if (currentChapter > 0) {
      setCurrentChapter(currentChapter - 1);
      setCurrentSection(chapters[currentChapter - 1].sections.length - 1);
      window.scrollTo(0, 0);
    }
  };
  
  const goToSection = (sectionIndex: number) => {
    setCurrentSection(sectionIndex);
    window.scrollTo(0, 0);
  };

  const currentChapterObj = chapters[currentChapter];
  const currentSectionObj = currentChapterObj.sections[currentSection];
  
  return (
    <div className="extended-guide">
      <div className="guide-header">
        <button className="close-guide" onClick={onClose}>
          <svg viewBox="0 0 24 24" width="24" height="24">
            <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z" />
          </svg>
        </button>
        <h1>Papin: The Complete Guide</h1>
        <div className="guide-subtitle">From Beginner to Expert</div>
      </div>
      
      <div className="guide-navigation">
        <div className="chapter-navigation">
          <button 
            className="chapter-nav-button prev-chapter" 
            onClick={prevChapter}
            disabled={currentChapter === 0}
          >
            <svg viewBox="0 0 24 24" width="24" height="24">
              <path d="M15.41 7.41L14 6l-6 6 6 6 1.41-1.41L10.83 12z" />
            </svg>
            Previous Chapter
          </button>
          <div className="chapter-select">
            <select 
              value={currentChapter}
              onChange={(e) => {
                setCurrentChapter(parseInt(e.target.value));
                setCurrentSection(0);
                window.scrollTo(0, 0);
              }}
            >
              {chapters.map((chapter, index) => (
                <option key={index} value={index}>
                  {chapter.title}
                </option>
              ))}
            </select>
          </div>
          <button 
            className="chapter-nav-button next-chapter" 
            onClick={nextChapter}
            disabled={currentChapter === chapters.length - 1}
          >
            Next Chapter
            <svg viewBox="0 0 24 24" width="24" height="24">
              <path d="M10 6L8.59 7.41 13.17 12l-4.58 4.59L10 18l6-6z" />
            </svg>
          </button>
        </div>
      </div>
      
      <div className="guide-content">
        <div className="section-sidebar">
          <h2>{currentChapterObj.title}</h2>
          <ul className="section-list">
            {currentChapterObj.sections.map((section, index) => (
              <li 
                key={index} 
                className={index === currentSection ? 'active' : ''}
                onClick={() => goToSection(index)}
              >
                {section.title}
              </li>
            ))}
          </ul>
        </div>
        
        <div className="section-content">
          <div className="section-header">
            <h2>{currentSectionObj.title}</h2>
            <div className="progress-indicator">
              Section {currentSection + 1} of {currentChapterObj.sections.length}
            </div>
          </div>
          
          <div className="section-body">
            {currentSectionObj.content}
          </div>
          
          <div className="section-navigation">
            <button 
              className="section-nav-button prev-section" 
              onClick={prevSection}
              disabled={currentChapter === 0 && currentSection === 0}
            >
              <svg viewBox="0 0 24 24" width="20" height="20">
                <path d="M15.41 7.41L14 6l-6 6 6 6 1.41-1.41L10.83 12z" />
              </svg>
              Previous
            </button>
            <button 
              className="section-nav-button next-section" 
              onClick={nextSection}
              disabled={currentChapter === chapters.length - 1 && currentSection === currentChapterObj.sections.length - 1}
            >
              Next
              <svg viewBox="0 0 24 24" width="20" height="20">
                <path d="M10 6L8.59 7.41 13.17 12l-4.58 4.59L10 18l6-6z" />
              </svg>
            </button>
          </div>
        </div>
      </div>
    </div>
  );
};

export default ExtendedGuide;
</file>

<file path="src-frontend/src/components/help/HelpButton.css">
.help-button {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 36px;
  height: 36px;
  border-radius: 50%;
  background-color: #f5f7fa;
  border: 1px solid #e0e4e8;
  cursor: pointer;
  transition: all 0.2s ease;
  padding: 0;
  margin: 0;
}

.help-button:hover {
  background-color: #e3f2fd;
  border-color: #90caf9;
}

.help-button:focus {
  outline: none;
  box-shadow: 0 0 0 2px rgba(33, 150, 243, 0.4);
}

.help-icon {
  fill: #2196f3;
}

.help-modal-overlay {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.5);
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: 1000;
  backdrop-filter: blur(2px);
}

.help-modal {
  width: 90%;
  max-width: 1200px;
  height: 90%;
  max-height: 900px;
  background-color: #fff;
  border-radius: 8px;
  box-shadow: 0 8px 24px rgba(0, 0, 0, 0.15);
  position: relative;
  overflow: hidden;
  display: flex;
  flex-direction: column;
}

.help-close-button {
  position: absolute;
  top: 12px;
  right: 12px;
  width: 36px;
  height: 36px;
  border-radius: 50%;
  background-color: transparent;
  border: none;
  cursor: pointer;
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: 10;
  transition: background-color 0.2s ease;
}

.help-close-button:hover {
  background-color: rgba(0, 0, 0, 0.05);
}

.help-close-button svg {
  fill: #666;
}

@media (max-width: 768px) {
  .help-modal {
    width: 100%;
    height: 100%;
    max-width: none;
    max-height: none;
    border-radius: 0;
  }
}
</file>

<file path="src-frontend/src/components/help/HelpButton.tsx">
import React, { useState, useEffect } from 'react';
import './HelpButton.css';
import HelpCenter from './HelpCenter';
import { createPortal } from 'react-dom';

interface HelpButtonProps {
  className?: string;
  isOpen?: boolean;
  onOpenChange?: (isOpen: boolean) => void;
}

const HelpButton: React.FC<HelpButtonProps> = ({ 
  className, 
  isOpen: propIsOpen, 
  onOpenChange 
}) => {
  const [isOpen, setIsOpen] = useState(false);

  useEffect(() => {
    if (propIsOpen !== undefined) {
      setIsOpen(propIsOpen);
    }
  }, [propIsOpen]);

  const toggleHelp = () => {
    const newState = !isOpen;
    setIsOpen(newState);
    if (onOpenChange) {
      onOpenChange(newState);
    }
  };

  const closeHelp = () => {
    setIsOpen(false);
    if (onOpenChange) {
      onOpenChange(false);
    }
  };

  return (
    <>
      <button 
        className={`help-button ${className || ''}`} 
        onClick={toggleHelp}
        aria-label="Help"
        title="Open Help Center"
      >
        <svg className="help-icon" viewBox="0 0 24 24" width="20" height="20">
          <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm1 17h-2v-2h2v2zm2.07-7.75l-.9.92C13.45 12.9 13 13.5 13 15h-2v-.5c0-1.1.45-2.1 1.17-2.83l1.24-1.26c.37-.36.59-.86.59-1.41 0-1.1-.9-2-2-2s-2 .9-2 2H8c0-2.21 1.79-4 4-4s4 1.79 4 4c0 .88-.36 1.68-.93 2.25z" />
        </svg>
      </button>
      
      {isOpen && createPortal(
        <div className="help-modal-overlay" onClick={closeHelp}>
          <div className="help-modal" onClick={e => e.stopPropagation()}>
            <button className="help-close-button" onClick={closeHelp} aria-label="Close Help">
              <svg viewBox="0 0 24 24" width="24" height="24">
                <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z" />
              </svg>
            </button>
            <HelpCenter />
          </div>
        </div>,
        document.body
      )}
    </>
  );
};

export default HelpButton;
</file>

<file path="src-frontend/src/components/help/HelpCenter.css">
.help-center {
  display: flex;
  flex-direction: column;
  height: 100%;
  width: 100%;
  background-color: #f5f7fa;
  color: #333;
  overflow: hidden;
}

.help-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 16px 24px;
  background-color: #fff;
  border-bottom: 1px solid #e0e4e8;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
}

.help-header h1 {
  font-size: 24px;
  font-weight: 600;
  margin: 0;
  color: #2196f3;
}

.user-level-selector {
  display: flex;
  align-items: center;
  gap: 12px;
}

.user-level-selector span {
  font-size: 14px;
  color: #666;
}

.level-buttons {
  display: flex;
  gap: 8px;
  border-radius: 6px;
  overflow: hidden;
  border: 1px solid #e0e4e8;
}

.level-buttons button {
  border: none;
  background-color: #f5f7fa;
  padding: 8px 12px;
  font-size: 14px;
  cursor: pointer;
  transition: all 0.2s ease;
}

.level-buttons button.active {
  background-color: #2196f3;
  color: white;
}

.level-buttons button:hover:not(.active) {
  background-color: #e0e4e8;
}

.help-content {
  display: flex;
  flex: 1;
  overflow: hidden;
}

.help-sidebar {
  width: 280px;
  background-color: #fff;
  border-right: 1px solid #e0e4e8;
  display: flex;
  flex-direction: column;
  overflow: hidden;
}

.topic-list {
  flex: 1;
  overflow-y: auto;
  padding: 16px;
}

.topic-list h2 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 16px;
  padding-bottom: 8px;
  border-bottom: 1px solid #e0e4e8;
}

.topic-item {
  padding: 10px 12px;
  margin-bottom: 4px;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.2s ease;
}

.topic-item:hover {
  background-color: #f0f4f8;
}

.topic-item.active {
  background-color: #e3f2fd;
  color: #1976d2;
  font-weight: 500;
}

.help-main {
  flex: 1;
  padding: 24px;
  overflow-y: auto;
}

.search-results h2 {
  margin-top: 0;
  margin-bottom: 24px;
  font-size: 20px;
}

.search-result-item {
  margin-bottom: 24px;
  padding-bottom: 24px;
  border-bottom: 1px solid #e0e4e8;
}

.search-result-item h3 {
  color: #1976d2;
  cursor: pointer;
  margin-top: 0;
  margin-bottom: 8px;
}

.search-result-item h3:hover {
  text-decoration: underline;
}

.search-result-item p {
  margin: 0;
  color: #666;
}

/* Custom scrollbar */
::-webkit-scrollbar {
  width: 8px;
}

::-webkit-scrollbar-track {
  background: #f5f7fa;
}

::-webkit-scrollbar-thumb {
  background-color: #c1c9d6;
  border-radius: 20px;
}

::-webkit-scrollbar-thumb:hover {
  background-color: #a0aec0;
}
</file>

<file path="src-frontend/src/components/help/HelpCenter.tsx">
import React, { useState, useEffect } from 'react';
import './HelpCenter.css';
import HelpTopic from './HelpTopic';
import HelpSearch from './HelpSearch';
import HelpGuide from './HelpGuide';
import { helpTopics } from './helpContent';

export type UserLevel = 'beginner' | 'intermediate' | 'advanced';

const HelpCenter: React.FC = () => {
  const [selectedTopic, setSelectedTopic] = useState<string | null>(null);
  const [searchQuery, setSearchQuery] = useState<string>('');
  const [userLevel, setUserLevel] = useState<UserLevel>('beginner');
  const [filteredTopics, setFilteredTopics] = useState(helpTopics);

  useEffect(() => {
    if (searchQuery) {
      const filtered = helpTopics.filter(topic => 
        topic.title.toLowerCase().includes(searchQuery.toLowerCase()) ||
        topic.content.some(item => 
          item.title.toLowerCase().includes(searchQuery.toLowerCase()) ||
          item.content.toLowerCase().includes(searchQuery.toLowerCase())
        )
      );
      setFilteredTopics(filtered);
    } else {
      setFilteredTopics(helpTopics);
    }
  }, [searchQuery]);

  const handleTopicSelect = (topicId: string) => {
    setSelectedTopic(topicId);
  };

  const handleSearch = (query: string) => {
    setSearchQuery(query);
    setSelectedTopic(null);
  };

  const handleUserLevelChange = (level: UserLevel) => {
    setUserLevel(level);
  };

  return (
    <div className="help-center">
      <div className="help-header">
        <h1>Papin Help Center</h1>
        <div className="user-level-selector">
          <span>Expertise Level:</span>
          <div className="level-buttons">
            <button 
              className={userLevel === 'beginner' ? 'active' : ''} 
              onClick={() => handleUserLevelChange('beginner')}
            >
              Beginner
            </button>
            <button 
              className={userLevel === 'intermediate' ? 'active' : ''} 
              onClick={() => handleUserLevelChange('intermediate')}
            >
              Intermediate
            </button>
            <button 
              className={userLevel === 'advanced' ? 'active' : ''} 
              onClick={() => handleUserLevelChange('advanced')}
            >
              Advanced
            </button>
          </div>
        </div>
      </div>
      
      <div className="help-content">
        <div className="help-sidebar">
          <HelpSearch onSearch={handleSearch} />
          <div className="topic-list">
            <h2>Topics</h2>
            {filteredTopics.map(topic => (
              <div 
                key={topic.id}
                className={`topic-item ${selectedTopic === topic.id ? 'active' : ''}`}
                onClick={() => handleTopicSelect(topic.id)}
              >
                {topic.title}
              </div>
            ))}
          </div>
        </div>
        
        <div className="help-main">
          {selectedTopic ? (
            <HelpTopic 
              topicId={selectedTopic} 
              userLevel={userLevel} 
            />
          ) : searchQuery ? (
            <div className="search-results">
              <h2>Search Results for "{searchQuery}"</h2>
              {filteredTopics.length === 0 ? (
                <p>No results found. Try a different search term.</p>
              ) : (
                filteredTopics.map(topic => (
                  <div key={topic.id} className="search-result-item">
                    <h3 onClick={() => handleTopicSelect(topic.id)}>{topic.title}</h3>
                    <p>{topic.summary}</p>
                  </div>
                ))
              )}
            </div>
          ) : (
            <HelpGuide userLevel={userLevel} onTopicSelect={handleTopicSelect} />
          )}
        </div>
      </div>
    </div>
  );
};

export default HelpCenter;
</file>

<file path="src-frontend/src/components/help/HelpGuide.css">
.help-guide {
  max-width: 900px;
  margin: 0 auto;
}

.guide-welcome {
  background-color: #fff;
  border-radius: 8px;
  padding: 24px;
  margin-bottom: 32px;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.06);
}

.guide-welcome h2 {
  margin-top: 0;
  margin-bottom: 16px;
  font-size: 28px;
  color: #2c3e50;
}

.guide-welcome p {
  font-size: 16px;
  color: #505a66;
  margin-bottom: 12px;
  line-height: 1.6;
}

.guide-tip {
  background-color: #e3f2fd;
  border-left: 4px solid #2196f3;
  padding: 12px 16px;
  border-radius: 4px;
  margin-top: 20px;
}

.extended-guide-promo {
  background-color: #fce4ec;
  background-image: linear-gradient(135deg, #fce4ec 0%, #f3e5f5 100%);
  border-radius: 8px;
  padding: 24px;
  margin-bottom: 32px;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.06);
  text-align: center;
}

.extended-guide-promo h3 {
  margin-top: 0;
  margin-bottom: 12px;
  font-size: 22px;
  color: #ad1457;
}

.extended-guide-promo p {
  font-size: 16px;
  color: #555;
  max-width: 600px;
  margin: 0 auto 20px;
  line-height: 1.6;
}

.extended-guide-button {
  background-color: #e91e63;
  color: white;
  border: none;
  padding: 10px 20px;
  border-radius: 4px;
  font-size: 16px;
  font-weight: 500;
  cursor: pointer;
  transition: background-color 0.2s ease;
}

.extended-guide-button:hover {
  background-color: #d81b60;
}

.quick-start-guide {
  background-color: #fff;
  border-radius: 8px;
  padding: 24px;
  margin-bottom: 32px;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.06);
}

.quick-start-guide h3 {
  margin-top: 0;
  margin-bottom: 24px;
  font-size: 22px;
  color: #2c3e50;
}

.steps {
  display: flex;
  flex-direction: column;
  gap: 20px;
}

.step {
  display: flex;
  gap: 16px;
}

.step-number {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 36px;
  height: 36px;
  border-radius: 50%;
  background-color: #2196f3;
  color: white;
  font-weight: 600;
  font-size: 18px;
  flex-shrink: 0;
}

.step-content {
  flex: 1;
}

.step-content h4 {
  margin-top: 0;
  margin-bottom: 8px;
  font-size: 18px;
  color: #2c3e50;
}

.step-content p {
  margin-top: 0;
  margin-bottom: 12px;
  color: #505a66;
}

.step-button {
  background-color: #e3f2fd;
  color: #1976d2;
  border: none;
  padding: 8px 16px;
  border-radius: 4px;
  font-weight: 500;
  cursor: pointer;
  transition: background-color 0.2s ease;
}

.step-button:hover {
  background-color: #bbdefb;
}

.featured-topics {
  margin-bottom: 32px;
}

.featured-topics h3 {
  margin-top: 0;
  margin-bottom: 24px;
  font-size: 22px;
  color: #2c3e50;
}

.topic-cards {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
  gap: 24px;
}

.topic-card {
  background-color: #fff;
  border-radius: 8px;
  padding: 20px;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.06);
  cursor: pointer;
  transition: transform 0.2s ease, box-shadow 0.2s ease;
}

.topic-card:hover {
  transform: translateY(-4px);
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
}

.topic-card h4 {
  margin-top: 0;
  margin-bottom: 12px;
  font-size: 18px;
  color: #2c3e50;
}

.topic-card p {
  margin-top: 0;
  margin-bottom: 16px;
  color: #505a66;
  font-size: 14px;
  line-height: 1.5;
}

.learn-more {
  color: #1976d2;
  font-weight: 500;
  font-size: 14px;
}

.help-categories {
  display: flex;
  flex-direction: column;
  gap: 32px;
}

.help-category h3 {
  margin-top: 0;
  margin-bottom: 24px;
  font-size: 22px;
  color: #2c3e50;
  border-bottom: 1px solid #e0e4e8;
  padding-bottom: 8px;
}

.category-topics {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
  gap: 20px;
}

.category-topic {
  background-color: #fff;
  border-radius: 8px;
  padding: 16px;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.06);
  cursor: pointer;
  transition: transform 0.2s ease, box-shadow 0.2s ease;
}

.category-topic:hover {
  transform: translateY(-2px);
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
}

.category-topic h4 {
  margin-top: 0;
  margin-bottom: 8px;
  font-size: 16px;
  color: #2c3e50;
}

.category-topic p {
  margin-top: 0;
  margin-bottom: 0;
  color: #505a66;
  font-size: 14px;
  line-height: 1.5;
  display: -webkit-box;
  -webkit-line-clamp: 2;
  -webkit-box-orient: vertical;
  overflow: hidden;
}

@media (max-width: 768px) {
  .topic-cards, .category-topics {
    grid-template-columns: 1fr;
  }
}
</file>

<file path="src-frontend/src/components/help/HelpGuide.tsx">
import React from 'react';
import './HelpGuide.css';
import { helpTopics } from './helpContent';
import { UserLevel } from './HelpCenter';

interface HelpGuideProps {
  userLevel: UserLevel;
  onTopicSelect: (topicId: string) => void;
}

const HelpGuide: React.FC<HelpGuideProps> = ({ userLevel, onTopicSelect }) => {
  // Group topics by category
  const groupedTopics: Record<string, any[]> = {};
  
  helpTopics.forEach(topic => {
    if (!groupedTopics[topic.category]) {
      groupedTopics[topic.category] = [];
    }
    groupedTopics[topic.category].push(topic);
  });

  // Content tailored to user level
  const getWelcomeContent = () => {
    switch (userLevel) {
      case 'beginner':
        return {
          title: "Welcome to Papin Help Center",
          description: "We're here to help you get started with Papin, an MCP Client. Explore our guides and tutorials to learn the basics.",
          guidance: "If you're new to Papin, we recommend starting with 'Getting Started' and 'Basic Features'."
        };
      case 'intermediate':
        return {
          title: "Welcome to Papin Help Center",
          description: "Find detailed explanations and guides for Papin's features and capabilities.",
          guidance: "Explore our advanced topics to get the most out of Papin's powerful capabilities."
        };
      case 'advanced':
        return {
          title: "Welcome to Papin Technical Documentation",
          description: "Access comprehensive technical documentation for Papin's architecture, APIs, and advanced features.",
          guidance: "Our developer resources include detailed architecture overviews, configuration references, and performance optimization guides."
        };
      default:
        return {
          title: "Welcome to Papin Help Center",
          description: "Find answers, guides, and resources for Papin, an MCP Client.",
          guidance: "Select a topic from the sidebar or explore the featured topics below."
        };
    }
  };

  const welcomeContent = getWelcomeContent();

  return (
    <div className="help-guide">
      <div className="guide-welcome">
        <h2>{welcomeContent.title}</h2>
        <p>{welcomeContent.description}</p>
        <p className="guide-tip">{welcomeContent.guidance}</p>
      </div>

      {userLevel === 'beginner' && (
        <div className="quick-start-guide">
          <h3>Quick Start Guide</h3>
          <div className="steps">
            <div className="step">
              <div className="step-number">1</div>
              <div className="step-content">
                <h4>Installation</h4>
                <p>Download and install Papin on your computer.</p>
                <button className="step-button" onClick={() => onTopicSelect('installation')}>
                  Installation Guide
                </button>
              </div>
            </div>
            <div className="step">
              <div className="step-number">2</div>
              <div className="step-content">
                <h4>Account Setup</h4>
                <p>Create your account and configure basic settings.</p>
                <button className="step-button" onClick={() => onTopicSelect('account-setup')}>
                  Account Guide
                </button>
              </div>
            </div>
            <div className="step">
              <div className="step-number">3</div>
              <div className="step-content">
                <h4>First Conversation</h4>
                <p>Learn how to start your first AI conversation.</p>
                <button className="step-button" onClick={() => onTopicSelect('conversations')}>
                  Conversation Guide
                </button>
              </div>
            </div>
          </div>
        </div>
      )}

      <div className="featured-topics">
        <h3>Featured Topics</h3>
        <div className="topic-cards">
          {['offline-capabilities', 'performance-monitoring', 'local-llm'].map(topicId => {
            const topic = helpTopics.find(t => t.id === topicId);
            if (!topic) return null;
            
            return (
              <div 
                key={topic.id} 
                className="topic-card"
                onClick={() => onTopicSelect(topic.id)}
              >
                <h4>{topic.title}</h4>
                <p>{topic.summary}</p>
                <span className="learn-more">Learn more →</span>
              </div>
            );
          })}
        </div>
      </div>

      <div className="help-categories">
        {Object.entries(groupedTopics).map(([category, topics]) => (
          <div key={category} className="help-category">
            <h3>{category}</h3>
            <div className="category-topics">
              {topics.map(topic => (
                <div 
                  key={topic.id} 
                  className="category-topic"
                  onClick={() => onTopicSelect(topic.id)}
                >
                  <h4>{topic.title}</h4>
                  <p>{topic.summary}</p>
                </div>
              ))}
            </div>
          </div>
        ))}
      </div>
    </div>
  );
};

export default HelpGuide;
</file>

<file path="src-frontend/src/components/help/HelpSearch.css">
.help-search {
  padding: 16px;
  border-bottom: 1px solid #e0e4e8;
}

.help-search form {
  display: flex;
  gap: 8px;
}

.search-input-container {
  position: relative;
  flex: 1;
}

.search-icon {
  position: absolute;
  left: 12px;
  top: 50%;
  transform: translateY(-50%);
  fill: #666;
}

.search-input {
  width: 100%;
  padding: 10px 12px 10px 36px;
  border: 1px solid #e0e4e8;
  border-radius: 6px;
  font-size: 14px;
  outline: none;
  transition: border-color 0.2s ease;
}

.search-input:focus {
  border-color: #2196f3;
}

.clear-button {
  position: absolute;
  right: 12px;
  top: 50%;
  transform: translateY(-50%);
  border: none;
  background: none;
  font-size: 18px;
  color: #666;
  cursor: pointer;
  padding: 0;
  display: flex;
  align-items: center;
  justify-content: center;
  width: 16px;
  height: 16px;
}

.search-button {
  border: none;
  background-color: #2196f3;
  color: white;
  padding: 10px 16px;
  border-radius: 6px;
  font-size: 14px;
  cursor: pointer;
  transition: background-color 0.2s ease;
}

.search-button:hover {
  background-color: #1976d2;
}
</file>

<file path="src-frontend/src/components/help/HelpSearch.tsx">
import React, { useState } from 'react';
import './HelpSearch.css';

interface HelpSearchProps {
  onSearch: (query: string) => void;
}

const HelpSearch: React.FC<HelpSearchProps> = ({ onSearch }) => {
  const [searchQuery, setSearchQuery] = useState('');

  const handleSubmit = (e: React.FormEvent) => {
    e.preventDefault();
    onSearch(searchQuery);
  };

  return (
    <div className="help-search">
      <form onSubmit={handleSubmit}>
        <div className="search-input-container">
          <svg className="search-icon" viewBox="0 0 24 24" width="16" height="16">
            <path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z" />
          </svg>
          <input
            type="text"
            placeholder="Search help topics..."
            value={searchQuery}
            onChange={(e) => setSearchQuery(e.target.value)}
            className="search-input"
          />
          {searchQuery && (
            <button 
              type="button" 
              className="clear-button"
              onClick={() => {
                setSearchQuery('');
                onSearch('');
              }}
            >
              ×
            </button>
          )}
        </div>
        <button type="submit" className="search-button">
          Search
        </button>
      </form>
    </div>
  );
};

export default HelpSearch;
</file>

<file path="src-frontend/src/components/help/HelpTopic.css">
.help-topic {
  max-width: 800px;
  margin: 0 auto;
}

.help-topic h2 {
  font-size: 28px;
  margin-top: 0;
  margin-bottom: 24px;
  color: #2c3e50;
  padding-bottom: 16px;
  border-bottom: 1px solid #e0e4e8;
}

.topic-intro {
  margin-bottom: 32px;
  font-size: 18px;
  line-height: 1.6;
  color: #505a66;
}

.topic-section {
  margin-bottom: 36px;
}

.topic-section h3 {
  font-size: 22px;
  margin-top: 0;
  margin-bottom: 16px;
  color: #1976d2;
}

.section-content {
  font-size: 16px;
  line-height: 1.6;
  color: #333;
}

.content-item {
  margin-bottom: 24px;
}

.content-item h4 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 12px;
  color: #2c3e50;
}

pre {
  background-color: #f7fafc;
  border-radius: 6px;
  padding: 16px;
  overflow-x: auto;
  margin: 16px 0;
  border: 1px solid #e0e4e8;
}

code {
  font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
  font-size: 14px;
  color: #333;
}

.image-container {
  margin: 16px 0;
  text-align: center;
}

.image-container img {
  max-width: 100%;
  border-radius: 6px;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
}

.note-box, .warning-box, .tip-box {
  padding: 16px;
  margin: 16px 0;
  border-radius: 6px;
  position: relative;
}

.note-box {
  background-color: #e3f2fd;
  border-left: 4px solid #2196f3;
}

.warning-box {
  background-color: #fff3e0;
  border-left: 4px solid #ff9800;
}

.tip-box {
  background-color: #e8f5e9;
  border-left: 4px solid #4caf50;
}

.topic-examples {
  margin-top: 40px;
  margin-bottom: 40px;
}

.example {
  background-color: #f7fafc;
  border-radius: 8px;
  padding: 20px;
  margin-bottom: 24px;
  border: 1px solid #e0e4e8;
}

.example h4 {
  margin-top: 0;
  color: #2c3e50;
}

.steps {
  margin-left: 20px;
  padding-left: 0;
}

.steps li {
  margin-bottom: 12px;
  line-height: 1.6;
}

.topic-faq {
  margin-top: 40px;
  margin-bottom: 40px;
}

.faq-item {
  margin-bottom: 24px;
  padding-bottom: 24px;
  border-bottom: 1px solid #e0e4e8;
}

.faq-item:last-child {
  border-bottom: none;
}

.faq-item h4 {
  margin-top: 0;
  color: #2c3e50;
  font-size: 18px;
}

.related-topics {
  margin-top: 40px;
  padding-top: 24px;
  border-top: 1px solid #e0e4e8;
}

.related-topics h3 {
  font-size: 20px;
  margin-top: 0;
  margin-bottom: 16px;
  color: #2c3e50;
}

.related-topics ul {
  padding-left: 20px;
  margin: 0;
}

.related-topics li {
  margin-bottom: 8px;
}

.related-topics a {
  color: #1976d2;
  text-decoration: none;
}

.related-topics a:hover {
  text-decoration: underline;
}

.help-topic-loading {
  display: flex;
  align-items: center;
  justify-content: center;
  height: 300px;
  font-size: 18px;
  color: #666;
}
</file>

<file path="src-frontend/src/components/help/HelpTopic.tsx">
import React, { useEffect, useState } from 'react';
import './HelpTopic.css';
import { helpTopics } from './helpContent';
import { UserLevel } from './HelpCenter';

interface HelpTopicProps {
  topicId: string;
  userLevel: UserLevel;
}

const HelpTopic: React.FC<HelpTopicProps> = ({ topicId, userLevel }) => {
  const [topic, setTopic] = useState<any | null>(null);
  
  useEffect(() => {
    const selectedTopic = helpTopics.find(t => t.id === topicId);
    if (selectedTopic) {
      setTopic(selectedTopic);
    }
  }, [topicId]);

  if (!topic) {
    return <div className="help-topic-loading">Loading topic...</div>;
  }

  // Determine which content to show based on user level
  const getContentForLevel = (content: any) => {
    if (userLevel === 'beginner') {
      return content.beginner || content.content;
    } else if (userLevel === 'intermediate') {
      return content.intermediate || content.content;
    } else {
      return content.advanced || content.content;
    }
  };

  return (
    <div className="help-topic">
      <h2>{topic.title}</h2>
      
      {topic.intro && (
        <div className="topic-intro">
          <p>{getContentForLevel(topic.intro)}</p>
        </div>
      )}
      
      {topic.content.map((section: any, index: number) => (
        <div key={index} className="topic-section">
          <h3>{section.title}</h3>
          <div className="section-content">
            {typeof getContentForLevel(section) === 'string' ? (
              <p>{getContentForLevel(section)}</p>
            ) : (
              Array.isArray(getContentForLevel(section)) ? 
                getContentForLevel(section).map((item: any, i: number) => (
                  <div key={i} className="content-item">
                    {item.title && <h4>{item.title}</h4>}
                    <p>{item.content}</p>
                    {item.code && (
                      <pre>
                        <code>{item.code}</code>
                      </pre>
                    )}
                    {item.image && (
                      <div className="image-container">
                        <img src={item.image} alt={item.title || 'Help illustration'} />
                      </div>
                    )}
                    {item.note && (
                      <div className="note-box">
                        <strong>Note:</strong> {item.note}
                      </div>
                    )}
                    {item.warning && (
                      <div className="warning-box">
                        <strong>Warning:</strong> {item.warning}
                      </div>
                    )}
                    {item.tip && (
                      <div className="tip-box">
                        <strong>Tip:</strong> {item.tip}
                      </div>
                    )}
                  </div>
                ))
              : <p>{section.content}</p>
            )}
          </div>
        </div>
      ))}
      
      {topic.examples && (
        <div className="topic-examples">
          <h3>Examples</h3>
          {topic.examples.map((example: any, index: number) => (
            <div key={index} className="example">
              <h4>{example.title}</h4>
              <p>{example.description}</p>
              {example.code && (
                <pre>
                  <code>{example.code}</code>
                </pre>
              )}
              {example.steps && (
                <ol className="steps">
                  {example.steps.map((step: string, i: number) => (
                    <li key={i}>{step}</li>
                  ))}
                </ol>
              )}
            </div>
          ))}
        </div>
      )}
      
      {topic.faq && (
        <div className="topic-faq">
          <h3>Frequently Asked Questions</h3>
          {topic.faq.map((item: any, index: number) => (
            <div key={index} className="faq-item">
              <h4>{item.question}</h4>
              <p>{getContentForLevel(item)}</p>
            </div>
          ))}
        </div>
      )}
      
      {topic.relatedTopics && topic.relatedTopics.length > 0 && (
        <div className="related-topics">
          <h3>Related Topics</h3>
          <ul>
            {topic.relatedTopics.map((relatedId: string) => {
              const relatedTopic = helpTopics.find(t => t.id === relatedId);
              return relatedTopic ? (
                <li key={relatedId}>
                  <a href={`#${relatedId}`} onClick={(e) => {
                    e.preventDefault();
                    // You would typically use a navigation function here
                    // For now we'll just log
                    console.log(`Navigate to topic: ${relatedId}`);
                  }}>
                    {relatedTopic.title}
                  </a>
                </li>
              ) : null;
            })}
          </ul>
        </div>
      )}
    </div>
  );
};

export default HelpTopic;
</file>

<file path="src-frontend/src/components/menu/HelpMenu.tsx">
import React from 'react';
import HelpButton from '../help/HelpButton';

interface HelpMenuProps {
  showHelpCenter: () => void;
}

const HelpMenu: React.FC<HelpMenuProps> = ({ showHelpCenter }) => {
  return (
    <div className="menu-dropdown">
      <ul className="menu-list">
        <li className="menu-item" onClick={showHelpCenter}>
          <div className="menu-item-icon">
            <svg viewBox="0 0 24 24" width="16" height="16">
              <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm1 17h-2v-2h2v2zm2.07-7.75l-.9.92C13.45 12.9 13 13.5 13 15h-2v-.5c0-1.1.45-2.1 1.17-2.83l1.24-1.26c.37-.36.59-.86.59-1.41 0-1.1-.9-2-2-2s-2 .9-2 2H8c0-2.21 1.79-4 4-4s4 1.79 4 4c0 .88-.36 1.68-.93 2.25z" />
            </svg>
          </div>
          <span>Help Center</span>
          <div className="menu-item-shortcut">F1</div>
        </li>
        <li className="menu-item">
          <div className="menu-item-icon">
            <svg viewBox="0 0 24 24" width="16" height="16">
              <path d="M11 18h2v-2h-2v2zm1-16C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8zm0-14c-2.21 0-4 1.79-4 4h2c0-1.1.9-2 2-2s2 .9 2 2c0 2-3 1.75-3 5h2c0-2.25 3-2.5 3-5 0-2.21-1.79-4-4-4z" />
            </svg>
          </div>
          <span>View Documentation</span>
        </li>
        <li className="menu-divider"></li>
        <li className="menu-item">
          <div className="menu-item-icon">
            <svg viewBox="0 0 24 24" width="16" height="16">
              <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z" />
            </svg>
          </div>
          <span>Check for Updates</span>
        </li>
        <li className="menu-item">
          <div className="menu-item-icon">
            <svg viewBox="0 0 24 24" width="16" height="16">
              <path d="M11 9h2V7h-2v2zm0 8h2v-6h-2v6zm1-15C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8z" />
            </svg>
          </div>
          <span>About Papin</span>
        </li>
      </ul>
    </div>
  );
};

export default HelpMenu;
</file>

<file path="src-frontend/src/components/menu/Menu.css">
.menu-dropdown {
  position: absolute;
  top: 100%;
  right: 0;
  background-color: #fff;
  border-radius: 6px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
  min-width: 240px;
  z-index: 1000;
  overflow: hidden;
  animation: menu-fade-in 0.15s ease;
}

@keyframes menu-fade-in {
  from {
    opacity: 0;
    transform: translateY(-8px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

.menu-list {
  list-style: none;
  padding: 8px 0;
  margin: 0;
}

.menu-item {
  padding: 10px 16px;
  display: flex;
  align-items: center;
  cursor: pointer;
  transition: background-color 0.2s ease;
  font-size: 14px;
  color: #333;
}

.menu-item:hover {
  background-color: #f0f4f8;
}

.menu-item-icon {
  margin-right: 12px;
  display: flex;
  align-items: center;
  justify-content: center;
  width: 20px;
  height: 20px;
}

.menu-item-icon svg {
  fill: #666;
}

.menu-item-shortcut {
  margin-left: auto;
  font-size: 12px;
  color: #888;
  padding-left: 16px;
}

.menu-divider {
  height: 1px;
  background-color: #e0e4e8;
  margin: 8px 0;
}

.menu-header {
  padding: 10px 16px;
  font-size: 12px;
  font-weight: 600;
  color: #666;
  text-transform: uppercase;
  letter-spacing: 0.5px;
}
</file>

<file path="src-frontend/src/components/offline/LLMMetricsPrivacyNotice.tsx">
import React, { useState, useEffect } from 'react';
import { invoke } from '@tauri-apps/api/tauri';
import { 
  Typography, Paper, Box, Button, Stack, Checkbox, FormControlLabel, 
  Dialog, DialogTitle, DialogContent, DialogActions, Alert, Divider,
  IconButton, List, ListItem, ListItemText, ListItemIcon
} from '@mui/material';
import InfoIcon from '@mui/icons-material/Info';
import SecurityIcon from '@mui/icons-material/Security';
import PrivacyTipIcon from '@mui/icons-material/PrivacyTip';
import CheckCircleIcon from '@mui/icons-material/CheckCircle';
import DoDisturbIcon from '@mui/icons-material/DoDisturb';
import AutoGraphIcon from '@mui/icons-material/AutoGraph';

interface LLMMetricsConfig {
  enabled: boolean;
  collect_performance_metrics: boolean;
  collect_usage_metrics: boolean;
  collect_error_metrics: boolean;
  anonymization_level: string;
  performance_sampling_rate: number;
  track_provider_changes: boolean;
  track_model_events: boolean;
  privacy_notice_version: string;
  privacy_notice_accepted: boolean;
}

interface PrivacyNoticeProps {
  onAccept?: () => void;
  onDecline?: () => void;
}

const CURRENT_PRIVACY_NOTICE_VERSION = "1.0.0";

const LLMMetricsPrivacyNotice: React.FC<PrivacyNoticeProps> = ({ 
  onAccept, 
  onDecline 
}) => {
  const [open, setOpen] = useState<boolean>(false);
  const [loading, setLoading] = useState<boolean>(true);
  const [config, setConfig] = useState<LLMMetricsConfig | null>(null);
  const [agreed, setAgreed] = useState<boolean>(false);
  const [error, setError] = useState<string | null>(null);

  // Check if privacy notice has been accepted
  useEffect(() => {
    const checkPrivacyNotice = async () => {
      try {
        setLoading(true);
        const result = await invoke('get_llm_metrics_config');
        setConfig(result as LLMMetricsConfig);
        
        // If privacy notice hasn't been accepted or version is different, show dialog
        if (!(result as LLMMetricsConfig).privacy_notice_accepted || 
            (result as LLMMetricsConfig).privacy_notice_version !== CURRENT_PRIVACY_NOTICE_VERSION) {
          setOpen(true);
        }
      } catch (err) {
        console.error('Failed to check privacy notice status:', err);
        setError(`Failed to check privacy notice status: ${err}`);
      } finally {
        setLoading(false);
      }
    };
    
    checkPrivacyNotice();
  }, []);

  // Handle privacy notice acceptance
  const handleAccept = async () => {
    if (!agreed) {
      return;
    }
    
    try {
      setLoading(true);
      await invoke('accept_llm_metrics_privacy_notice', { 
        version: CURRENT_PRIVACY_NOTICE_VERSION 
      });
      
      if (config) {
        // Update local state
        setConfig({
          ...config,
          privacy_notice_accepted: true,
          privacy_notice_version: CURRENT_PRIVACY_NOTICE_VERSION,
          enabled: true
        });
      }
      
      setOpen(false);
      
      // Call parent callback if provided
      if (onAccept) {
        onAccept();
      }
    } catch (err) {
      console.error('Failed to accept privacy notice:', err);
      setError(`Failed to accept privacy notice: ${err}`);
    } finally {
      setLoading(false);
    }
  };

  // Handle privacy notice decline
  const handleDecline = () => {
    setOpen(false);
    
    // Call parent callback if provided
    if (onDecline) {
      onDecline();
    }
  };

  return (
    <>
      <Button 
        startIcon={<PrivacyTipIcon />}
        onClick={() => setOpen(true)}
        variant="outlined"
        color="primary"
        sx={{ mt: 2 }}
      >
        View LLM Metrics Privacy Notice
      </Button>
      
      <Dialog
        open={open}
        onClose={() => {}}
        fullWidth
        maxWidth="md"
        PaperProps={{
          sx: {
            borderRadius: 2,
            boxShadow: 5,
          }
        }}
      >
        <DialogTitle sx={{ 
          bgcolor: 'primary.main', 
          color: 'white',
          display: 'flex',
          alignItems: 'center',
          gap: 1
        }}>
          <PrivacyTipIcon />
          LLM Metrics Collection Privacy Notice
        </DialogTitle>
        
        <DialogContent dividers>
          {error && (
            <Alert severity="error" sx={{ mb: 2 }}>
              {error}
            </Alert>
          )}
          
          <Box sx={{ mb: 3 }}>
            <Typography variant="h6" gutterBottom>
              About LLM Performance Metrics Collection
            </Typography>
            
            <Typography paragraph>
              To improve the performance and user experience of local LLM providers, 
              we collect anonymous metrics about how the LLM providers and models perform. 
              This data helps us optimize the application, prioritize features, and understand 
              which models and providers work best.
            </Typography>
            
            <Alert severity="info" sx={{ mb: 2 }}>
              <Typography variant="subtitle2">
                This metrics collection is completely optional and disabled by default.
              </Typography>
            </Alert>
          </Box>
          
          <Box sx={{ mb: 3 }}>
            <Typography variant="h6" gutterBottom>
              What We Collect
            </Typography>
            
            <List>
              <ListItem>
                <ListItemIcon>
                  <AutoGraphIcon color="primary" />
                </ListItemIcon>
                <ListItemText 
                  primary="Performance Metrics" 
                  secondary="Response times, tokens per second, time to first token, and resource usage"
                />
              </ListItem>
              
              <ListItem>
                <ListItemIcon>
                  <InfoIcon color="primary" />
                </ListItemIcon>
                <ListItemText 
                  primary="Usage Statistics" 
                  secondary="Which providers and models are used, successful/failed generations"
                />
              </ListItem>
              
              <ListItem>
                <ListItemIcon>
                  <DoDisturbIcon color="primary" />
                </ListItemIcon>
                <ListItemText 
                  primary="Error Information" 
                  secondary="Types of errors that occur during model loading or generation"
                />
              </ListItem>
            </List>
            
            <Typography variant="subtitle2" color="text.secondary" sx={{ mt: 1 }}>
              We never collect any of your prompts, generated content, or personal information.
            </Typography>
          </Box>
          
          <Box sx={{ mb: 3 }}>
            <Typography variant="h6" gutterBottom>
              Privacy Protection
            </Typography>
            
            <List>
              <ListItem>
                <ListItemIcon>
                  <SecurityIcon color="primary" />
                </ListItemIcon>
                <ListItemText 
                  primary="Full Anonymization" 
                  secondary="All metrics are anonymized and cannot be traced back to you"
                />
              </ListItem>
              
              <ListItem>
                <ListItemIcon>
                  <CheckCircleIcon color="primary" />
                </ListItemIcon>
                <ListItemText 
                  primary="Local Processing" 
                  secondary="Most metrics are processed locally and never leave your device"
                />
              </ListItem>
              
              <ListItem>
                <ListItemIcon>
                  <CheckCircleIcon color="primary" />
                </ListItemIcon>
                <ListItemText 
                  primary="Data Sampling" 
                  secondary="We only collect a small sample of metrics to minimize impact"
                />
              </ListItem>
            </List>
          </Box>
          
          <Box sx={{ mb: 3 }}>
            <Typography variant="h6" gutterBottom>
              Your Control
            </Typography>
            
            <Typography paragraph>
              You have complete control over what is collected:
            </Typography>
            
            <Typography component="ul">
              <li>Enable or disable metrics collection at any time</li>
              <li>Choose which specific types of metrics are collected</li>
              <li>Adjust anonymization level to your comfort</li>
              <li>View all collected metrics in the dashboard</li>
            </Typography>
            
            <Typography variant="subtitle2" color="text.secondary" sx={{ mt: 1 }}>
              You can change these settings at any time in the Offline Settings.
            </Typography>
          </Box>
          
          <Divider sx={{ my: 2 }} />
          
          <FormControlLabel
            control={
              <Checkbox 
                checked={agreed}
                onChange={(e) => setAgreed(e.target.checked)}
                color="primary"
              />
            }
            label="I understand and agree to the collection of anonymous metrics data"
          />
        </DialogContent>
        
        <DialogActions sx={{ px: 3, py: 2 }}>
          <Button 
            onClick={handleDecline} 
            color="inherit"
            disabled={loading}
          >
            Decline
          </Button>
          
          <Button 
            onClick={handleAccept}
            variant="contained" 
            color="primary"
            disabled={loading || !agreed}
          >
            Accept and Enable Metrics
          </Button>
        </DialogActions>
      </Dialog>
    </>
  );
};

export default LLMMetricsPrivacyNotice;
</file>

<file path="src-frontend/src/components/offline/OfflineSettings.css">
.offline-settings {
  max-width: 1200px;
  margin: 0 auto;
  padding: 1.5rem;
  color: var(--text-color);
}

.offline-settings h2 {
  margin-top: 0;
  margin-bottom: 1.5rem;
  font-size: 1.8rem;
  font-weight: 600;
}

.offline-settings h3 {
  margin-top: 0;
  margin-bottom: 1rem;
  font-size: 1.3rem;
  font-weight: 600;
  color: var(--text-color-secondary);
}

.notification {
  padding: 0.75rem 1rem;
  margin-bottom: 1.5rem;
  border-radius: 0.375rem;
  font-weight: 500;
  animation: fadein 0.3s ease-in-out;
}

.notification.success {
  background-color: var(--success-bg-color, #d4edda);
  color: var(--success-text-color, #155724);
  border: 1px solid var(--success-border-color, #c3e6cb);
}

.notification.error {
  background-color: var(--error-bg-color, #f8d7da);
  color: var(--error-text-color, #721c24);
  border: 1px solid var(--error-border-color, #f5c6cb);
}

@keyframes fadein {
  from { opacity: 0; transform: translateY(-10px); }
  to { opacity: 1; transform: translateY(0); }
}

.settings-grid {
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 2rem;
  margin-bottom: 2rem;
}

.settings-section {
  background-color: var(--bg-color);
  border-radius: 0.5rem;
  padding: 1.5rem;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);
}

.loading {
  display: flex;
  align-items: center;
  justify-content: center;
  padding: 2rem;
  color: var(--text-color-secondary);
  font-style: italic;
}

.form-group {
  margin-bottom: 1.25rem;
}

.form-group label {
  display: flex;
  align-items: center;
  margin-bottom: 0.5rem;
  font-weight: 500;
}

.form-group label input[type="checkbox"] {
  margin-right: 0.5rem;
}

.form-group input[type="text"],
.form-group input[type="number"],
.form-group select {
  display: block;
  width: 100%;
  padding: 0.5rem;
  border: 1px solid var(--border-color);
  border-radius: 0.25rem;
  background-color: var(--input-bg-color);
  color: var(--text-color);
  margin-top: 0.25rem;
  font-size: 0.9rem;
}

.form-group input[type="text"]:focus,
.form-group input[type="number"]:focus,
.form-group select:focus {
  outline: none;
  border-color: var(--primary-color);
  box-shadow: 0 0 0 2px rgba(var(--primary-color-rgb), 0.2);
}

.form-actions {
  margin-top: 1.5rem;
  display: flex;
  justify-content: flex-end;
}

.primary-button {
  padding: 0.5rem 1rem;
  background-color: var(--primary-color);
  color: white;
  border: none;
  border-radius: 0.25rem;
  font-weight: 500;
  cursor: pointer;
  transition: background-color 0.2s ease;
}

.primary-button:hover {
  background-color: var(--primary-color-hover);
}

.primary-button:disabled {
  background-color: var(--disabled-color);
  cursor: not-allowed;
}

.status-info {
  display: flex;
  flex-direction: column;
  gap: 0.75rem;
}

.status-item {
  display: flex;
  justify-content: space-between;
  padding: 0.75rem;
  border-radius: 0.25rem;
  background-color: var(--bg-color-secondary);
}

.status-label {
  font-weight: 500;
  color: var(--text-color-secondary);
}

.status-value {
  font-weight: 600;
}

.status-online {
  color: var(--success-color, #28a745);
}

.status-limited {
  color: var(--warning-color, #ffc107);
}

.status-offline {
  color: var(--error-color, #dc3545);
}

.status-active {
  color: var(--success-color, #28a745);
}

.status-inactive {
  color: var(--text-color-secondary);
}

.status-actions {
  margin-top: 1.5rem;
  display: flex;
  flex-wrap: wrap;
  gap: 0.75rem;
}

.action-button {
  padding: 0.5rem 1rem;
  background-color: var(--button-bg-color, #f0f0f0);
  color: var(--text-color);
  border: 1px solid var(--border-color);
  border-radius: 0.25rem;
  font-weight: 500;
  cursor: pointer;
  transition: all 0.2s ease;
}

.action-button:hover {
  background-color: var(--button-hover-bg-color, #e0e0e0);
}

.action-button:disabled {
  background-color: var(--disabled-color);
  color: var(--disabled-text-color);
  cursor: not-allowed;
}

.sync-button {
  background-color: var(--primary-color);
  color: white;
  border: none;
}

.sync-button:hover {
  background-color: var(--primary-color-hover);
}

.offline-help {
  background-color: var(--bg-color);
  border-radius: 0.5rem;
  padding: 1.5rem;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);
}

.offline-help p {
  margin-bottom: 1rem;
  line-height: 1.5;
}

.offline-help p:last-child {
  margin-bottom: 0;
}

.error-message {
  color: var(--error-color);
  padding: 1rem;
  text-align: center;
}

/* Responsive adjustments */
@media (max-width: 992px) {
  .settings-grid {
    grid-template-columns: 1fr;
    gap: 1.5rem;
  }
}

@media (max-width: 768px) {
  .offline-settings {
    padding: 1rem;
  }
  
  .settings-section {
    padding: 1.25rem;
  }
}

/* Dark mode specific styles */
@media (prefers-color-scheme: dark) {
  .action-button {
    background-color: var(--bg-color-secondary, #2a2a2a);
  }
  
  .action-button:hover {
    background-color: var(--button-hover-bg-color, #333333);
  }
}
</file>

<file path="src-frontend/src/components/security/DataFlowVisualization.css">
.data-flow-visualization {
  display: flex;
  flex-direction: column;
  height: 100%;
  padding: 20px;
  overflow: hidden;
  background-color: #f5f5f7;
  border-radius: 8px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

.visualization-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 20px;
  padding-bottom: 15px;
  border-bottom: 1px solid #e0e0e0;
}

.visualization-header h2 {
  margin: 0;
  font-size: 24px;
  color: #333;
}

.view-controls {
  display: flex;
  border-radius: 8px;
  overflow: hidden;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
}

.view-controls button {
  padding: 8px 16px;
  background-color: #fff;
  border: none;
  cursor: pointer;
  font-weight: 500;
  transition: all 0.2s;
}

.view-controls button:hover {
  background-color: #f0f0f0;
}

.view-controls button.active {
  background-color: #5c6bc0;
  color: white;
}

.visualization-controls {
  display: flex;
  align-items: center;
  gap: 16px;
}

.filter-control {
  display: flex;
  align-items: center;
  gap: 8px;
}

.filter-control label {
  font-size: 14px;
  font-weight: 500;
}

.filter-control select {
  padding: 6px 10px;
  border-radius: 4px;
  border: 1px solid #ccc;
  background-color: white;
}

.refresh-control {
  display: flex;
  align-items: center;
  gap: 8px;
}

.auto-refresh-label {
  display: flex;
  align-items: center;
  gap: 6px;
  font-size: 14px;
  cursor: pointer;
}

.refresh-button {
  padding: 6px 16px;
  background-color: #4caf50;
  color: white;
  border: none;
  border-radius: 4px;
  cursor: pointer;
  transition: background-color 0.2s;
}

.refresh-button:hover {
  background-color: #388e3c;
}

.refresh-button:disabled {
  background-color: #a5d6a7;
  cursor: not-allowed;
}

.error-message {
  margin-bottom: 16px;
  padding: 12px;
  background-color: #ffebee;
  border-left: 4px solid #f44336;
  color: #b71c1c;
  border-radius: 4px;
}

.visualization-content {
  flex: 1;
  min-height: 0;
  display: flex;
  overflow: hidden;
}

/* Graph View */
.graph-view {
  display: flex;
  width: 100%;
  height: 100%;
  gap: 20px;
}

.graph-container {
  flex: 3;
  position: relative;
  height: 100%;
  background-color: white;
  border-radius: 8px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  overflow: hidden;
}

.graph-svg {
  width: 100%;
  height: 100%;
}

.loading-overlay {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  display: flex;
  flex-direction: column;
  justify-content: center;
  align-items: center;
  background-color: rgba(255, 255, 255, 0.8);
}

.loading-spinner {
  width: 40px;
  height: 40px;
  border: 4px solid #f3f3f3;
  border-top: 4px solid #5c6bc0;
  border-radius: 50%;
  animation: spin 1s linear infinite;
  margin-bottom: 16px;
}

@keyframes spin {
  0% { transform: rotate(0deg); }
  100% { transform: rotate(360deg); }
}

.loading-text {
  font-size: 16px;
  color: #333;
}

.no-data-overlay {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  display: flex;
  justify-content: center;
  align-items: center;
}

.no-data-message {
  max-width: 300px;
  padding: 20px;
  text-align: center;
  background-color: #f5f5f7;
  border-radius: 8px;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
  color: #666;
  line-height: 1.5;
}

.details-container {
  flex: 1;
  min-width: 300px;
  max-width: 400px;
  height: 100%;
  display: flex;
  flex-direction: column;
  background-color: white;
  border-radius: 8px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  overflow: hidden;
}

.empty-details {
  height: 100%;
  display: flex;
  justify-content: center;
  align-items: center;
  color: #999;
  font-style: italic;
}

.details-panel {
  display: flex;
  flex-direction: column;
  height: 100%;
}

.details-panel h3 {
  margin: 0;
  padding: 16px;
  background-color: #5c6bc0;
  color: white;
  font-size: 18px;
  font-weight: 500;
}

.details-content {
  flex: 1;
  padding: 16px;
  overflow-y: auto;
}

.detail-item {
  margin-bottom: 12px;
  display: flex;
  flex-wrap: wrap;
}

.detail-label {
  flex: 0 0 120px;
  font-weight: 500;
  color: #666;
}

.detail-value {
  flex: 1;
}

.detail-section {
  margin: 20px 0 12px;
  padding-bottom: 6px;
  font-weight: 600;
  color: #333;
  border-bottom: 1px solid #eee;
}

.detail-subtitle {
  margin: 12px 0 8px;
  font-weight: 500;
  color: #5c6bc0;
}

.detail-empty {
  color: #999;
  font-style: italic;
  margin: 8px 0 16px;
}

.detail-classifications {
  display: flex;
  flex-wrap: wrap;
  gap: 6px;
  margin-top: 6px;
}

.classification-tag {
  padding: 2px 8px;
  border-radius: 12px;
  font-size: 12px;
  color: white;
  font-weight: 500;
}

.events-list {
  display: flex;
  flex-direction: column;
  gap: 10px;
}

.event-item {
  background-color: #f5f5f7;
  border-radius: 4px;
  overflow: hidden;
}

.event-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 6px 10px;
  font-size: 12px;
}

.event-classification {
  padding: 2px 8px;
  border-radius: 12px;
  color: white;
  font-weight: 500;
}

.event-timestamp {
  color: #666;
}

.event-details {
  padding: 10px;
  background-color: white;
  border-top: 1px solid #eee;
}

.event-operation {
  font-weight: 500;
  margin-bottom: 4px;
}

.event-data-item {
  margin-bottom: 4px;
  color: #666;
  font-size: 14px;
}

.event-encryption {
  font-size: 12px;
}

.encryption-enabled {
  color: #4caf50;
}

.encryption-disabled {
  color: #f44336;
}

.more-events {
  padding: 8px;
  text-align: center;
  background-color: #eee;
  color: #666;
  font-size: 12px;
  border-radius: 0 0 4px 4px;
}

.legend {
  position: absolute;
  bottom: 20px;
  left: 20px;
  display: flex;
  flex-direction: column;
  gap: 16px;
  background-color: rgba(255, 255, 255, 0.9);
  padding: 10px;
  border-radius: 8px;
  box-shadow: 0 1px 4px rgba(0, 0, 0, 0.2);
}

.legend-title {
  font-weight: 600;
  font-size: 12px;
  margin-bottom: 6px;
  color: #333;
}

.legend-item {
  display: flex;
  align-items: center;
  gap: 8px;
  margin-bottom: 4px;
}

.legend-color {
  width: 12px;
  height: 12px;
  border-radius: 3px;
}

.legend-label {
  font-size: 12px;
  color: #666;
}

/* Node and Link Styles for D3 */
.node {
  cursor: pointer;
  transition: stroke-width 0.2s;
}

.node:hover {
  stroke-width: 3px;
}

.link {
  fill: none;
  stroke-opacity: 0.6;
  transition: stroke-opacity 0.2s;
}

.link:hover {
  stroke-opacity: 1;
  cursor: pointer;
}

.node-label {
  font-size: 12px;
  pointer-events: none;
}

/* Events List View */
.events-view {
  flex: 1;
  display: flex;
  flex-direction: column;
  overflow: hidden;
}

.events-list-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 16px;
}

.events-list-header h3 {
  margin: 0;
  font-size: 18px;
  color: #333;
}

.clear-events-button {
  padding: 6px 16px;
  background-color: #f44336;
  color: white;
  border: none;
  border-radius: 4px;
  cursor: pointer;
  transition: background-color 0.2s;
}

.clear-events-button:hover {
  background-color: #d32f2f;
}

.clear-events-button:disabled {
  background-color: #ffcdd2;
  cursor: not-allowed;
}

.no-events {
  flex: 1;
  display: flex;
  justify-content: center;
  align-items: center;
  color: #999;
  font-style: italic;
}

.events-table {
  flex: 1;
  display: flex;
  flex-direction: column;
  overflow: hidden;
  background-color: white;
  border-radius: 8px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
}

.events-table-header {
  display: flex;
  padding: 12px;
  background-color: #5c6bc0;
  color: white;
  font-weight: 500;
  font-size: 14px;
}

.events-table-body {
  flex: 1;
  overflow-y: auto;
}

.events-table-row {
  display: flex;
  padding: 10px 12px;
  border-bottom: 1px solid #eee;
}

.events-table-row:hover {
  background-color: #f5f5f7;
}

.event-col {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
  font-size: 14px;
}

.event-col.timestamp {
  flex: 1.5;
  min-width: 150px;
}

.event-col.operation {
  flex: 1;
  min-width: 100px;
}

.event-col.data-item {
  flex: 1.5;
  min-width: 150px;
}

.event-col.classification {
  flex: 0.8;
  min-width: 100px;
}

.event-col.source {
  flex: 1;
  min-width: 100px;
}

.event-col.destination {
  flex: 1;
  min-width: 100px;
}

.event-col.encrypted {
  flex: 0.3;
  min-width: 30px;
  text-align: center;
}

/* Statistics View */
.statistics-view {
  flex: 1;
  display: flex;
  flex-direction: column;
  gap: 24px;
  overflow-y: auto;
  padding: 0 12px;
}

.stats-row {
  display: flex;
  justify-content: space-between;
  gap: 16px;
}

.stat-card {
  flex: 1;
  background-color: white;
  padding: 16px;
  border-radius: 8px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  text-align: center;
}

.stat-value {
  font-size: 32px;
  font-weight: 600;
  color: #5c6bc0;
  margin-bottom: 4px;
}

.stat-label {
  color: #666;
  font-size: 14px;
}

.stats-grid {
  display: grid;
  grid-template-columns: repeat(2, 1fr);
  gap: 24px;
}

.stat-chart-card {
  background-color: white;
  padding: 16px;
  border-radius: 8px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
}

.stat-chart-card h3 {
  margin: 0 0 16px;
  font-size: 16px;
  color: #333;
}

.classification-chart,
.destination-chart,
.operation-chart {
  display: flex;
  flex-direction: column;
  gap: 12px;
}

.chart-bar-container {
  display: flex;
  align-items: center;
  gap: 10px;
}

.chart-label {
  flex: 0 0 120px;
  font-size: 14px;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

.chart-bar {
  flex: 1;
  height: 16px;
  background-color: #f0f0f0;
  border-radius: 4px;
  overflow: hidden;
}

.chart-bar-fill {
  height: 100%;
  border-radius: 4px;
}

.chart-value {
  flex: 0 0 40px;
  text-align: right;
  font-size: 14px;
  font-weight: 500;
}

.no-external {
  margin-top: 20px;
  color: #999;
  text-align: center;
  font-style: italic;
}

.external-destinations {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
  gap: 12px;
  margin-top: 10px;
}

.external-destination {
  padding: 12px;
  background-color: #f5f5f7;
  border-radius: 6px;
}

.external-name {
  font-weight: 500;
  margin-bottom: 4px;
}

.external-type {
  font-size: 12px;
  color: #5c6bc0;
  margin-bottom: 2px;
}

.external-location {
  font-size: 12px;
  color: #666;
}

/* Responsive Adjustments */
@media (max-width: 1200px) {
  .stats-grid {
    grid-template-columns: 1fr;
  }
}

@media (max-width: 900px) {
  .graph-view {
    flex-direction: column;
  }
  
  .details-container {
    max-width: none;
    height: 300px;
  }
  
  .stats-row {
    flex-wrap: wrap;
  }
  
  .stat-card {
    min-width: 200px;
  }
}

@media (max-width: 700px) {
  .visualization-header {
    flex-direction: column;
    align-items: flex-start;
    gap: 12px;
  }
  
  .visualization-controls {
    width: 100%;
    flex-wrap: wrap;
  }
}
</file>

<file path="src-frontend/src/components/security/DataFlowVisualization.tsx">
import React, { useState, useEffect, useRef } from 'react';
import { invoke } from '@tauri-apps/api/tauri';
import * as d3 from 'd3';
import './DataFlowVisualization.css';

// Types
interface DataFlowNode {
  id: string;
  name: string;
  node_type: string;
  internal: boolean;
  location: string;
  metadata: Record<string, string>;
}

interface DataFlowGraph {
  nodes: DataFlowNode[];
  edges: [string, string, string[]][];
  data_items: Record<string, string>;
}

interface DataFlowEvent {
  id: string;
  operation: string;
  data_item: string;
  classification: string;
  source: string;
  destination: string;
  consent_obtained: boolean;
  timestamp: string;
  encrypted: boolean;
  metadata: Record<string, string>;
}

interface DataFlowStatistics {
  total_events: number;
  node_count: number;
  edge_count: number;
  data_item_count: number;
  classification_counts: Record<string, number>;
  destination_counts: Record<string, number>;
  operation_counts: Record<string, number>;
  external_destinations: string[];
}

// Classification colors
const classificationColors: Record<string, string> = {
  Public: '#4caf50',
  Personal: '#2196f3',
  Sensitive: '#ff9800',
  Confidential: '#f44336',
};

// Graph node types and colors
const nodeTypeColors: Record<string, string> = {
  application: '#8e24aa',
  storage: '#0288d1',
  memory: '#00796b',
  secure_storage: '#c62828',
  api: '#fb8c00',
  model: '#9c27b0',
  unknown: '#757575',
};

// Simulation node for d3
interface SimulationNode extends d3.SimulationNodeDatum {
  id: string;
  name: string;
  type: string;
  internal: boolean;
  location: string;
  radius: number;
  color: string;
}

// Simulation link for d3
interface SimulationLink extends d3.SimulationLinkDatum<SimulationNode> {
  source: string | SimulationNode;
  target: string | SimulationNode;
  classifications: string[];
  value: number;
}

const DataFlowVisualization: React.FC = () => {
  const [graph, setGraph] = useState<DataFlowGraph | null>(null);
  const [events, setEvents] = useState<DataFlowEvent[]>([]);
  const [statistics, setStatistics] = useState<DataFlowStatistics | null>(null);
  const [loading, setLoading] = useState<boolean>(true);
  const [error, setError] = useState<string | null>(null);
  const [selectedNode, setSelectedNode] = useState<string | null>(null);
  const [selectedEdge, setSelectedEdge] = useState<[string, string] | null>(null);
  const [filterClassification, setFilterClassification] = useState<string | null>(null);
  const [autoRefresh, setAutoRefresh] = useState<boolean>(false);
  const [viewType, setViewType] = useState<'graph' | 'list' | 'stats'>('graph');
  
  const svgRef = useRef<SVGSVGElement>(null);

  // Load data on mount
  useEffect(() => {
    loadData();
    
    // Set up auto-refresh if enabled
    let intervalId: number;
    if (autoRefresh) {
      intervalId = window.setInterval(loadData, 5000);
    }
    
    return () => {
      if (intervalId) {
        clearInterval(intervalId);
      }
    };
  }, [autoRefresh, filterClassification]);

  // Function to load data from backend
  const loadData = async () => {
    try {
      setLoading(true);
      
      // Fetch data flow graph
      const graphData = await invoke<DataFlowGraph>('get_data_flow_graph');
      setGraph(graphData);
      
      // Fetch recent events
      const eventsData = await invoke<DataFlowEvent[]>('get_recent_data_flow_events', { limit: 100 });
      setEvents(eventsData);
      
      // Fetch statistics
      const statsData = await invoke<DataFlowStatistics>('get_data_flow_statistics');
      setStatistics(statsData);
      
      // Draw graph if in graph view
      if (viewType === 'graph' && graphData) {
        drawGraph(graphData);
      }
      
      setError(null);
    } catch (err) {
      console.error('Error loading data flow data:', err);
      setError(`Failed to load data flow information: ${err}`);
    } finally {
      setLoading(false);
    }
  };

  // Clear data flow events
  const clearEvents = async () => {
    try {
      await invoke('clear_data_flow_events');
      
      // Reload data
      loadData();
    } catch (err) {
      console.error('Error clearing data flow events:', err);
      setError(`Failed to clear data flow events: ${err}`);
    }
  };

  // Draw the force-directed graph using D3
  const drawGraph = (graphData: DataFlowGraph) => {
    if (!svgRef.current) return;
    
    // Clear previous graph
    d3.select(svgRef.current).selectAll('*').remove();
    
    const svg = d3.select(svgRef.current);
    const width = svgRef.current.clientWidth;
    const height = svgRef.current.clientHeight;
    
    // Create simulation nodes from graph data
    const nodes: SimulationNode[] = graphData.nodes.map(node => ({
      id: node.id,
      name: node.name,
      type: node.node_type,
      internal: node.internal,
      location: node.location,
      radius: node.internal ? 20 : 15,
      color: nodeTypeColors[node.node_type] || nodeTypeColors.unknown,
    }));
    
    // Create links from edges
    const links: SimulationLink[] = graphData.edges.map(edge => ({
      source: edge[0],
      target: edge[1],
      classifications: edge[2],
      value: edge[2].length,
    }));
    
    // Filter by classification if needed
    const filteredLinks = filterClassification 
      ? links.filter(link => link.classifications.includes(filterClassification))
      : links;
      
    // Create force simulation
    const simulation = d3.forceSimulation<SimulationNode, SimulationLink>(nodes)
      .force('link', d3.forceLink<SimulationNode, SimulationLink>(filteredLinks)
        .id(d => d.id)
        .distance(100))
      .force('charge', d3.forceManyBody().strength(-300))
      .force('center', d3.forceCenter(width / 2, height / 2))
      .force('collision', d3.forceCollide().radius(d => d.radius + 10));
    
    // Create arrow marker for links
    svg.append('defs').selectAll('marker')
      .data(['end'])
      .enter().append('marker')
      .attr('id', 'arrow')
      .attr('viewBox', '0 -5 10 10')
      .attr('refX', 25)
      .attr('refY', 0)
      .attr('markerWidth', 6)
      .attr('markerHeight', 6)
      .attr('orient', 'auto')
      .append('path')
      .attr('d', 'M0,-5L10,0L0,5')
      .attr('fill', '#999');
    
    // Create links
    const link = svg.append('g')
      .selectAll('path')
      .data(filteredLinks)
      .enter().append('path')
      .attr('class', 'link')
      .attr('marker-end', 'url(#arrow)')
      .attr('stroke-width', d => Math.sqrt(d.value) * 2)
      .attr('stroke', d => {
        if (d.classifications.length === 1) {
          return classificationColors[d.classifications[0]] || '#999';
        } else {
          return '#999';
        }
      })
      .on('click', (event, d) => {
        setSelectedEdge([d.source.id || d.source as string, d.target.id || d.target as string]);
        setSelectedNode(null);
      });
    
    // Create nodes
    const node = svg.append('g')
      .selectAll('circle')
      .data(nodes)
      .enter().append('circle')
      .attr('class', 'node')
      .attr('r', d => d.radius)
      .attr('fill', d => d.color)
      .attr('stroke', d => d.internal ? '#fff' : '#333')
      .attr('stroke-width', 2)
      .on('click', (event, d) => {
        setSelectedNode(d.id);
        setSelectedEdge(null);
      })
      .call(d3.drag<SVGCircleElement, SimulationNode>()
        .on('start', (event, d) => {
          if (!event.active) simulation.alphaTarget(0.3).restart();
          d.fx = d.x;
          d.fy = d.y;
        })
        .on('drag', (event, d) => {
          d.fx = event.x;
          d.fy = event.y;
        })
        .on('end', (event, d) => {
          if (!event.active) simulation.alphaTarget(0);
          d.fx = null;
          d.fy = null;
        }) as any); // Type cast to any due to d3 typing issues
    
    // Create node labels
    const labels = svg.append('g')
      .selectAll('text')
      .data(nodes)
      .enter().append('text')
      .attr('class', 'node-label')
      .text(d => d.name)
      .attr('text-anchor', 'middle')
      .attr('dy', '0.35em')
      .attr('fill', d => d.internal ? '#fff' : '#333')
      .attr('pointer-events', 'none');
    
    // Update positions on simulation tick
    simulation.on('tick', () => {
      link.attr('d', d => {
        const source = d.source as SimulationNode;
        const target = d.target as SimulationNode;
        
        return `M${source.x},${source.y}L${target.x},${target.y}`;
      });
      
      node.attr('cx', d => Math.max(d.radius, Math.min(width - d.radius, d.x || 0)))
          .attr('cy', d => Math.max(d.radius, Math.min(height - d.radius, d.y || 0)));
          
      labels.attr('x', d => Math.max(d.radius, Math.min(width - d.radius, d.x || 0)))
            .attr('y', d => Math.max(d.radius + 20, Math.min(height - d.radius, (d.y || 0) + 30)));
    });
  };

  // Format timestamp for display
  const formatTimestamp = (timestamp: string) => {
    return new Date(timestamp).toLocaleString();
  };

  // Render selected node details
  const renderNodeDetails = () => {
    if (!selectedNode || !graph) return null;
    
    const node = graph.nodes.find(n => n.id === selectedNode);
    if (!node) return null;
    
    // Find all edges connected to this node
    const connectedEdges = graph.edges.filter(
      edge => edge[0] === selectedNode || edge[1] === selectedNode
    );
    
    return (
      <div className="details-panel">
        <h3>Node Details: {node.name}</h3>
        
        <div className="details-content">
          <div className="detail-item">
            <span className="detail-label">ID:</span>
            <span className="detail-value">{node.id}</span>
          </div>
          
          <div className="detail-item">
            <span className="detail-label">Type:</span>
            <span className="detail-value">{node.node_type}</span>
          </div>
          
          <div className="detail-item">
            <span className="detail-label">Location:</span>
            <span className="detail-value">{node.location}</span>
          </div>
          
          <div className="detail-item">
            <span className="detail-label">Internal:</span>
            <span className="detail-value">{node.internal ? 'Yes' : 'No'}</span>
          </div>
          
          {Object.keys(node.metadata).length > 0 && (
            <>
              <div className="detail-section">Metadata</div>
              {Object.entries(node.metadata).map(([key, value]) => (
                <div key={key} className="detail-item">
                  <span className="detail-label">{key}:</span>
                  <span className="detail-value">{value}</span>
                </div>
              ))}
            </>
          )}
          
          <div className="detail-section">Connections</div>
          {connectedEdges.length === 0 ? (
            <div className="detail-empty">No connections</div>
          ) : (
            <>
              <div className="detail-subtitle">Incoming:</div>
              {connectedEdges.filter(edge => edge[1] === selectedNode).length === 0 ? (
                <div className="detail-empty">None</div>
              ) : (
                connectedEdges
                  .filter(edge => edge[1] === selectedNode)
                  .map(edge => (
                    <div key={`${edge[0]}-${edge[1]}`} className="detail-item">
                      <span className="detail-label">From:</span>
                      <span className="detail-value">{graph.nodes.find(n => n.id === edge[0])?.name || edge[0]}</span>
                      <div className="detail-classifications">
                        {edge[2].map(cls => (
                          <span 
                            key={cls} 
                            className="classification-tag" 
                            style={{ backgroundColor: classificationColors[cls] || '#999' }}
                          >
                            {cls}
                          </span>
                        ))}
                      </div>
                    </div>
                  ))
              )}
              
              <div className="detail-subtitle">Outgoing:</div>
              {connectedEdges.filter(edge => edge[0] === selectedNode).length === 0 ? (
                <div className="detail-empty">None</div>
              ) : (
                connectedEdges
                  .filter(edge => edge[0] === selectedNode)
                  .map(edge => (
                    <div key={`${edge[0]}-${edge[1]}`} className="detail-item">
                      <span className="detail-label">To:</span>
                      <span className="detail-value">{graph.nodes.find(n => n.id === edge[1])?.name || edge[1]}</span>
                      <div className="detail-classifications">
                        {edge[2].map(cls => (
                          <span 
                            key={cls} 
                            className="classification-tag" 
                            style={{ backgroundColor: classificationColors[cls] || '#999' }}
                          >
                            {cls}
                          </span>
                        ))}
                      </div>
                    </div>
                  ))
              )}
            </>
          )}
        </div>
      </div>
    );
  };

  // Render selected edge details
  const renderEdgeDetails = () => {
    if (!selectedEdge || !graph) return null;
    
    const [sourceId, targetId] = selectedEdge;
    
    // Find the edge in the graph
    const edge = graph.edges.find(e => e[0] === sourceId && e[1] === targetId);
    if (!edge) return null;
    
    const sourceNode = graph.nodes.find(n => n.id === sourceId);
    const targetNode = graph.nodes.find(n => n.id === targetId);
    
    // Find events related to this edge
    const relatedEvents = events.filter(
      event => event.source === sourceId && event.destination === targetId
    );
    
    return (
      <div className="details-panel">
        <h3>Data Flow Details</h3>
        
        <div className="details-content">
          <div className="detail-item">
            <span className="detail-label">From:</span>
            <span className="detail-value">{sourceNode?.name || sourceId}</span>
          </div>
          
          <div className="detail-item">
            <span className="detail-label">To:</span>
            <span className="detail-value">{targetNode?.name || targetId}</span>
          </div>
          
          <div className="detail-item">
            <span className="detail-label">Data Classifications:</span>
            <div className="detail-classifications">
              {edge[2].map(cls => (
                <span 
                  key={cls} 
                  className="classification-tag" 
                  style={{ backgroundColor: classificationColors[cls] || '#999' }}
                >
                  {cls}
                </span>
              ))}
            </div>
          </div>
          
          <div className="detail-section">Recent Events</div>
          {relatedEvents.length === 0 ? (
            <div className="detail-empty">No events found</div>
          ) : (
            <div className="events-list">
              {relatedEvents.slice(0, 5).map(event => (
                <div key={event.id} className="event-item">
                  <div className="event-header">
                    <span 
                      className="event-classification"
                      style={{ backgroundColor: classificationColors[event.classification] || '#999' }}
                    >
                      {event.classification}
                    </span>
                    <span className="event-timestamp">{formatTimestamp(event.timestamp)}</span>
                  </div>
                  <div className="event-details">
                    <div className="event-operation">{event.operation}</div>
                    <div className="event-data-item">{event.data_item}</div>
                    <div className="event-encryption">
                      {event.encrypted ? (
                        <span className="encryption-enabled">🔒 Encrypted</span>
                      ) : (
                        <span className="encryption-disabled">🔓 Unencrypted</span>
                      )}
                    </div>
                  </div>
                </div>
              ))}
              
              {relatedEvents.length > 5 && (
                <div className="more-events">
                  + {relatedEvents.length - 5} more events
                </div>
              )}
            </div>
          )}
        </div>
      </div>
    );
  };

  // Render statistics view
  const renderStatistics = () => {
    if (!statistics) return null;
    
    return (
      <div className="statistics-view">
        <div className="stats-row">
          <div className="stat-card">
            <div className="stat-value">{statistics.total_events}</div>
            <div className="stat-label">Total Events</div>
          </div>
          
          <div className="stat-card">
            <div className="stat-value">{statistics.node_count}</div>
            <div className="stat-label">System Components</div>
          </div>
          
          <div className="stat-card">
            <div className="stat-value">{statistics.edge_count}</div>
            <div className="stat-label">Data Flows</div>
          </div>
          
          <div className="stat-card">
            <div className="stat-value">{statistics.data_item_count}</div>
            <div className="stat-label">Data Items</div>
          </div>
        </div>
        
        <div className="stats-grid">
          <div className="stat-chart-card">
            <h3>Data Classification</h3>
            <div className="classification-chart">
              {Object.entries(statistics.classification_counts).map(([classification, count]) => (
                <div key={classification} className="chart-bar-container">
                  <div className="chart-label">{classification}</div>
                  <div className="chart-bar">
                    <div 
                      className="chart-bar-fill" 
                      style={{
                        width: `${(count / statistics.total_events) * 100}%`,
                        backgroundColor: classificationColors[classification] || '#999'
                      }}
                    ></div>
                  </div>
                  <div className="chart-value">{count}</div>
                </div>
              ))}
            </div>
          </div>
          
          <div className="stat-chart-card">
            <h3>Top Destinations</h3>
            <div className="destination-chart">
              {Object.entries(statistics.destination_counts)
                .sort((a, b) => b[1] - a[1])
                .slice(0, 5)
                .map(([destination, count]) => (
                  <div key={destination} className="chart-bar-container">
                    <div className="chart-label">
                      {graph?.nodes.find(n => n.id === destination)?.name || destination}
                    </div>
                    <div className="chart-bar">
                      <div 
                        className="chart-bar-fill" 
                        style={{
                          width: `${(count / statistics.total_events) * 100}%`,
                          backgroundColor: '#2196f3'
                        }}
                      ></div>
                    </div>
                    <div className="chart-value">{count}</div>
                  </div>
                ))}
            </div>
          </div>
          
          <div className="stat-chart-card">
            <h3>Top Operations</h3>
            <div className="operation-chart">
              {Object.entries(statistics.operation_counts)
                .sort((a, b) => b[1] - a[1])
                .slice(0, 5)
                .map(([operation, count]) => (
                  <div key={operation} className="chart-bar-container">
                    <div className="chart-label">{operation}</div>
                    <div className="chart-bar">
                      <div 
                        className="chart-bar-fill" 
                        style={{
                          width: `${(count / statistics.total_events) * 100}%`,
                          backgroundColor: '#ff9800'
                        }}
                      ></div>
                    </div>
                    <div className="chart-value">{count}</div>
                  </div>
                ))}
            </div>
          </div>
          
          <div className="stat-chart-card">
            <h3>External Data Destinations</h3>
            {statistics.external_destinations.length === 0 ? (
              <div className="no-external">No external data destinations found</div>
            ) : (
              <div className="external-destinations">
                {statistics.external_destinations.map(destination => {
                  const node = graph?.nodes.find(n => n.id === destination);
                  return (
                    <div key={destination} className="external-destination">
                      <div className="external-name">{node?.name || destination}</div>
                      <div className="external-type">{node?.node_type || 'unknown'}</div>
                      <div className="external-location">{node?.location || 'unknown'}</div>
                    </div>
                  );
                })}
              </div>
            )}
          </div>
        </div>
      </div>
    );
  };

  // Render events list view
  const renderEventsList = () => {
    return (
      <div className="events-view">
        <div className="events-list-header">
          <h3>Recent Data Flow Events</h3>
          <div className="events-actions">
            <button 
              className="clear-events-button" 
              onClick={clearEvents}
              disabled={events.length === 0}
            >
              Clear Events
            </button>
          </div>
        </div>
        
        {events.length === 0 ? (
          <div className="no-events">No data flow events recorded</div>
        ) : (
          <div className="events-table">
            <div className="events-table-header">
              <div className="event-col timestamp">Timestamp</div>
              <div className="event-col operation">Operation</div>
              <div className="event-col data-item">Data Item</div>
              <div className="event-col classification">Classification</div>
              <div className="event-col source">Source</div>
              <div className="event-col destination">Destination</div>
              <div className="event-col encrypted">Encrypted</div>
            </div>
            
            <div className="events-table-body">
              {events.map(event => (
                <div key={event.id} className="events-table-row">
                  <div className="event-col timestamp">{formatTimestamp(event.timestamp)}</div>
                  <div className="event-col operation">{event.operation}</div>
                  <div className="event-col data-item">{event.data_item}</div>
                  <div className="event-col classification">
                    <span 
                      className="classification-tag" 
                      style={{ backgroundColor: classificationColors[event.classification] || '#999' }}
                    >
                      {event.classification}
                    </span>
                  </div>
                  <div className="event-col source">
                    {graph?.nodes.find(n => n.id === event.source)?.name || event.source}
                  </div>
                  <div className="event-col destination">
                    {graph?.nodes.find(n => n.id === event.destination)?.name || event.destination}
                  </div>
                  <div className="event-col encrypted">
                    {event.encrypted ? (
                      <span className="encryption-enabled">🔒</span>
                    ) : (
                      <span className="encryption-disabled">🔓</span>
                    )}
                  </div>
                </div>
              ))}
            </div>
          </div>
        )}
      </div>
    );
  };

  return (
    <div className="data-flow-visualization">
      <div className="visualization-header">
        <h2>Data Flow Visualization</h2>
        
        <div className="view-controls">
          <button 
            className={viewType === 'graph' ? 'active' : ''}
            onClick={() => setViewType('graph')}
          >
            Graph View
          </button>
          <button 
            className={viewType === 'list' ? 'active' : ''}
            onClick={() => setViewType('list')}
          >
            Events List
          </button>
          <button 
            className={viewType === 'stats' ? 'active' : ''}
            onClick={() => setViewType('stats')}
          >
            Statistics
          </button>
        </div>
        
        <div className="visualization-controls">
          {viewType === 'graph' && (
            <div className="filter-control">
              <label>Filter by Classification:</label>
              <select 
                value={filterClassification || ''} 
                onChange={e => setFilterClassification(e.target.value || null)}
              >
                <option value="">All Classifications</option>
                <option value="Public">Public</option>
                <option value="Personal">Personal</option>
                <option value="Sensitive">Sensitive</option>
                <option value="Confidential">Confidential</option>
              </select>
            </div>
          )}
          
          <div className="refresh-control">
            <label className="auto-refresh-label">
              <input 
                type="checkbox" 
                checked={autoRefresh} 
                onChange={e => setAutoRefresh(e.target.checked)} 
              />
              Auto-refresh
            </label>
            <button 
              className="refresh-button" 
              onClick={loadData}
              disabled={loading}
            >
              {loading ? 'Loading...' : 'Refresh'}
            </button>
          </div>
        </div>
      </div>
      
      {error && (
        <div className="error-message">
          {error}
        </div>
      )}
      
      <div className="visualization-content">
        {viewType === 'graph' && (
          <div className="graph-view">
            <div className="graph-container">
              <svg ref={svgRef} className="graph-svg"></svg>
              
              {loading && (
                <div className="loading-overlay">
                  <div className="loading-spinner"></div>
                  <div className="loading-text">Loading data flow graph...</div>
                </div>
              )}
              
              {!loading && graph && graph.nodes.length === 0 && (
                <div className="no-data-overlay">
                  <div className="no-data-message">
                    No data flow information available.
                    <br />
                    Start using the application to see data flows.
                  </div>
                </div>
              )}
            </div>
            
            <div className="details-container">
              {selectedNode && renderNodeDetails()}
              {selectedEdge && renderEdgeDetails()}
              {!selectedNode && !selectedEdge && (
                <div className="empty-details">
                  <p>Select a node or edge to see details</p>
                </div>
              )}
            </div>
            
            <div className="legend">
              <div className="legend-section">
                <div className="legend-title">Node Types</div>
                {Object.entries(nodeTypeColors).map(([type, color]) => (
                  <div key={type} className="legend-item">
                    <div 
                      className="legend-color" 
                      style={{ backgroundColor: color }}
                    ></div>
                    <div className="legend-label">{type}</div>
                  </div>
                ))}
              </div>
              
              <div className="legend-section">
                <div className="legend-title">Data Classifications</div>
                {Object.entries(classificationColors).map(([type, color]) => (
                  <div key={type} className="legend-item">
                    <div 
                      className="legend-color" 
                      style={{ backgroundColor: color }}
                    ></div>
                    <div className="legend-label">{type}</div>
                  </div>
                ))}
              </div>
            </div>
          </div>
        )}
        
        {viewType === 'list' && renderEventsList()}
        
        {viewType === 'stats' && renderStatistics()}
      </div>
    </div>
  );
};

export default DataFlowVisualization;
</file>

<file path="src-frontend/src/components/security/PermissionsManager.css">
.permissions-manager {
  display: flex;
  flex-direction: column;
  height: 100%;
  background-color: #f5f5f7;
  border-radius: 8px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
  overflow: hidden;
}

.permissions-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 20px;
  background-color: white;
  border-bottom: 1px solid #e0e0e0;
}

.permissions-header h2 {
  margin: 0;
  font-size: 24px;
  color: #333;
}

.header-controls {
  display: flex;
  gap: 16px;
  align-items: center;
}

.search-box {
  position: relative;
  width: 280px;
}

.search-box input {
  width: 100%;
  padding: 8px 12px;
  border: 1px solid #ddd;
  border-radius: 4px;
  font-size: 14px;
  transition: border-color 0.2s;
}

.search-box input:focus {
  border-color: #5c6bc0;
  outline: none;
}

.clear-search {
  position: absolute;
  right: 8px;
  top: 50%;
  transform: translateY(-50%);
  background: none;
  border: none;
  color: #999;
  font-size: 18px;
  cursor: pointer;
  padding: 0;
  line-height: 1;
}

.clear-search:hover {
  color: #f44336;
}

.reset-all-button {
  padding: 8px 16px;
  background-color: #f44336;
  color: white;
  border: none;
  border-radius: 4px;
  font-size: 14px;
  cursor: pointer;
  transition: background-color 0.2s;
}

.reset-all-button:hover {
  background-color: #d32f2f;
}

.error-message {
  margin: 0 20px 20px;
  padding: 12px;
  background-color: #ffebee;
  border-left: 4px solid #f44336;
  color: #b71c1c;
  border-radius: 4px;
}

.permissions-content {
  display: flex;
  flex: 1;
  overflow: hidden;
}

.permissions-sidebar {
  flex: 0 0 280px;
  background-color: white;
  border-right: 1px solid #e0e0e0;
  display: flex;
  flex-direction: column;
  overflow-y: auto;
}

.categories-nav {
  display: flex;
  flex-direction: column;
  padding: 16px 0;
}

.categories-nav button {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 12px 20px;
  text-align: left;
  background: none;
  border: none;
  font-size: 14px;
  color: #555;
  cursor: pointer;
  transition: background-color 0.2s;
}

.categories-nav button:hover {
  background-color: #f5f5f7;
}

.categories-nav button.active {
  background-color: #ede7f6;
  color: #5c6bc0;
  font-weight: 500;
}

.count {
  background-color: #e0e0e0;
  color: #666;
  font-size: 12px;
  font-weight: 500;
  padding: 2px 6px;
  border-radius: 10px;
  min-width: 24px;
  text-align: center;
}

.categories-nav button.active .count {
  background-color: #5c6bc0;
  color: white;
}

.statistics-panel {
  padding: 20px;
  border-top: 1px solid #e0e0e0;
}

.statistics-panel h3 {
  margin: 0 0 16px;
  font-size: 16px;
  color: #333;
}

.statistics-panel h4 {
  margin: 20px 0 12px;
  font-size: 14px;
  color: #666;
}

.stat-item {
  display: flex;
  justify-content: space-between;
  margin-bottom: 8px;
}

.stat-label {
  color: #666;
  font-size: 14px;
}

.stat-value {
  font-weight: 500;
  color: #333;
}

.stat-value.granted {
  color: #4caf50;
}

.stat-value.denied {
  color: #f44336;
}

.levels-chart {
  margin-top: 12px;
}

.level-bar-container {
  display: flex;
  align-items: center;
  margin-bottom: 8px;
}

.level-label {
  flex: 0 0 110px;
  font-size: 12px;
  color: #666;
}

.level-bar {
  flex: 1;
  height: 8px;
  background-color: #eee;
  border-radius: 4px;
  overflow: hidden;
}

.level-bar-fill {
  height: 100%;
}

.level-bar-fill.level-alwaysallow {
  background-color: #4caf50;
}

.level-bar-fill.level-askfirsttime {
  background-color: #2196f3;
}

.level-bar-fill.level-askeverytime {
  background-color: #ff9800;
}

.level-bar-fill.level-neverallow {
  background-color: #f44336;
}

.level-count {
  flex: 0 0 30px;
  text-align: right;
  font-size: 12px;
  font-weight: 500;
  color: #666;
}

.most-used-list {
  margin-top: 12px;
}

.most-used-item {
  display: flex;
  justify-content: space-between;
  margin-bottom: 8px;
  padding: 8px;
  background-color: #f5f5f7;
  border-radius: 4px;
}

.most-used-name {
  font-size: 13px;
}

.most-used-count {
  font-size: 12px;
  color: #666;
}

.no-data {
  margin-top: 12px;
  color: #999;
  font-style: italic;
  font-size: 14px;
}

.permissions-list {
  flex: 1;
  padding: 20px;
  overflow-y: auto;
}

.loading-message {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  padding: 40px;
  color: #666;
}

.loading-spinner {
  width: 40px;
  height: 40px;
  border: 4px solid #f3f3f3;
  border-top: 4px solid #5c6bc0;
  border-radius: 50%;
  animation: spin 1s linear infinite;
  margin-bottom: 16px;
}

@keyframes spin {
  0% { transform: rotate(0deg); }
  100% { transform: rotate(360deg); }
}

.no-permissions {
  padding: 40px;
  text-align: center;
  color: #999;
  font-style: italic;
}

.permissions-count {
  margin-bottom: 16px;
  color: #666;
  font-size: 14px;
}

.permission-card {
  margin-bottom: 16px;
  background-color: white;
  border-radius: 8px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  overflow: hidden;
}

.permission-header {
  display: flex;
  justify-content: space-between;
  align-items: flex-start;
  padding: 16px;
  border-bottom: 1px solid #f0f0f0;
}

.permission-name {
  margin: 0 0 8px;
  font-size: 18px;
  color: #333;
}

.permission-meta {
  display: flex;
  flex-wrap: wrap;
  gap: 8px;
}

.permission-id {
  font-size: 12px;
  color: #999;
  background-color: #f5f5f7;
  padding: 2px 6px;
  border-radius: 4px;
}

.permission-category {
  font-size: 12px;
  color: #5c6bc0;
  background-color: #ede7f6;
  padding: 2px 6px;
  border-radius: 4px;
}

.permission-required {
  font-size: 12px;
  color: #f57c00;
  background-color: #fff3e0;
  padding: 2px 6px;
  border-radius: 4px;
}

.permission-actions {
  display: flex;
  gap: 8px;
}

.permission-level-select {
  padding: 6px 12px;
  border-radius: 4px;
  border: 1px solid #ddd;
  font-size: 14px;
  font-weight: 500;
  cursor: pointer;
  transition: border-color 0.2s;
}

.permission-level-select:hover {
  border-color: #5c6bc0;
}

.permission-level-select:focus {
  outline: none;
  border-color: #5c6bc0;
}

.permission-level-select.level-alwaysallow {
  background-color: #e8f5e9;
  color: #2e7d32;
  border-color: #a5d6a7;
}

.permission-level-select.level-askfirsttime {
  background-color: #e3f2fd;
  color: #1565c0;
  border-color: #90caf9;
}

.permission-level-select.level-askeverytime {
  background-color: #fff3e0;
  color: #e65100;
  border-color: #ffcc80;
}

.permission-level-select.level-neverallow {
  background-color: #ffebee;
  color: #c62828;
  border-color: #ef9a9a;
}

.reset-button {
  padding: 6px 12px;
  background-color: #f5f5f7;
  color: #666;
  border: 1px solid #ddd;
  border-radius: 4px;
  font-size: 14px;
  cursor: pointer;
  transition: all 0.2s;
}

.reset-button:hover {
  background-color: #e0e0e0;
}

.reset-button:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.permission-description {
  padding: 16px;
  color: #666;
  font-size: 14px;
  line-height: 1.5;
}

.permission-footer {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 12px 16px;
  background-color: #f9f9f9;
  border-top: 1px solid #f0f0f0;
  font-size: 12px;
  color: #999;
}

.permission-info {
  display: flex;
  gap: 16px;
}

.permission-status {
  font-weight: 500;
  padding: 3px 8px;
  border-radius: 4px;
}

.permission-status.level-alwaysallow {
  background-color: #e8f5e9;
  color: #2e7d32;
}

.permission-status.level-askfirsttime {
  background-color: #e3f2fd;
  color: #1565c0;
}

.permission-status.level-askeverytime {
  background-color: #fff3e0;
  color: #e65100;
}

.permission-status.level-neverallow {
  background-color: #ffebee;
  color: #c62828;
}

.modal-overlay {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.5);
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 1000;
}

.confirm-modal {
  width: 400px;
  background-color: white;
  border-radius: 8px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
  padding: 24px;
}

.confirm-modal h3 {
  margin: 0 0 16px;
  color: #f44336;
  font-size: 20px;
}

.confirm-modal p {
  margin: 0 0 24px;
  color: #666;
  line-height: 1.5;
}

.modal-actions {
  display: flex;
  justify-content: flex-end;
  gap: 16px;
}

.cancel-button {
  padding: 8px 16px;
  background-color: #f5f5f7;
  color: #666;
  border: 1px solid #ddd;
  border-radius: 4px;
  font-size: 14px;
  cursor: pointer;
  transition: background-color 0.2s;
}

.cancel-button:hover {
  background-color: #e0e0e0;
}

.confirm-button {
  padding: 8px 16px;
  background-color: #f44336;
  color: white;
  border: none;
  border-radius: 4px;
  font-size: 14px;
  cursor: pointer;
  transition: background-color 0.2s;
}

.confirm-button:hover {
  background-color: #d32f2f;
}

/* Responsive Adjustments */
@media (max-width: 900px) {
  .permissions-content {
    flex-direction: column;
  }
  
  .permissions-sidebar {
    flex: 0 0 auto;
    border-right: none;
    border-bottom: 1px solid #e0e0e0;
  }
  
  .categories-nav {
    flex-direction: row;
    overflow-x: auto;
    padding: 12px;
  }
  
  .categories-nav button {
    padding: 8px 12px;
    white-space: nowrap;
  }
  
  .statistics-panel {
    display: none;
  }
}

@media (max-width: 700px) {
  .permissions-header {
    flex-direction: column;
    align-items: flex-start;
    gap: 16px;
  }
  
  .header-controls {
    width: 100%;
    flex-wrap: wrap;
  }
  
  .search-box {
    width: 100%;
  }
  
  .permission-header {
    flex-direction: column;
    gap: 12px;
  }
  
  .permission-actions {
    width: 100%;
  }
  
  .permission-level-select {
    flex: 1;
  }
}
</file>

<file path="src-frontend/src/components/security/PermissionsManager.tsx">
import React, { useState, useEffect } from 'react';
import { invoke } from '@tauri-apps/api/tauri';
import './PermissionsManager.css';

// Types
interface Permission {
  id: string;
  name: string;
  description: string;
  level: PermissionLevel;
  category: string;
  last_modified: string;
  usage_count: number;
  required: boolean;
}

interface PermissionStatistics {
  total_permissions: number;
  count_by_level: Record<string, number>;
  count_by_category: Record<string, number>;
  total_requests: number;
  granted_count: number;
  denied_count: number;
  most_used_permissions: [string, number][];
}

enum PermissionLevel {
  AlwaysAllow = "AlwaysAllow",
  AskFirstTime = "AskFirstTime",
  AskEveryTime = "AskEveryTime",
  NeverAllow = "NeverAllow",
}

const PermissionsManager: React.FC = () => {
  const [permissions, setPermissions] = useState<Permission[]>([]);
  const [statistics, setStatistics] = useState<PermissionStatistics | null>(null);
  const [loading, setLoading] = useState<boolean>(true);
  const [error, setError] = useState<string | null>(null);
  const [activeTab, setActiveTab] = useState<'all' | 'privacy' | 'network' | 'system' | 'data' | 'security'>('all');
  const [searchQuery, setSearchQuery] = useState<string>('');
  const [showResetConfirm, setShowResetConfirm] = useState<boolean>(false);
  
  // Load permissions on mount
  useEffect(() => {
    loadData();
  }, []);
  
  // Function to load data from backend
  const loadData = async () => {
    try {
      setLoading(true);
      
      // Load permissions
      const permissionsData = await invoke<any[]>('get_all_permissions');
      
      // Convert from JSON values to typed objects
      const typedPermissions: Permission[] = permissionsData.map(item => ({
        id: item.id,
        name: item.name,
        description: item.description,
        level: item.level as PermissionLevel,
        category: item.category,
        last_modified: item.last_modified,
        usage_count: item.usage_count,
        required: item.required,
      }));
      
      setPermissions(typedPermissions);
      
      // Load statistics
      const statsData = await invoke<PermissionStatistics>('get_permission_statistics');
      setStatistics(statsData);
      
      setError(null);
    } catch (err) {
      console.error('Error loading permissions data:', err);
      setError(`Failed to load permissions: ${err}`);
    } finally {
      setLoading(false);
    }
  };
  
  // Change permission level
  const changePermissionLevel = async (id: string, level: PermissionLevel) => {
    try {
      await invoke('set_permission_level', { id, level });
      
      // Update local state
      setPermissions(permissions.map(permission => 
        permission.id === id 
          ? { ...permission, level, last_modified: new Date().toISOString() } 
          : permission
      ));
      
      // Reload statistics
      const statsData = await invoke<PermissionStatistics>('get_permission_statistics');
      setStatistics(statsData);
    } catch (err) {
      console.error('Error changing permission level:', err);
      setError(`Failed to change permission level: ${err}`);
    }
  };
  
  // Reset permission to default
  const resetPermission = async (id: string) => {
    try {
      await invoke('reset_permission', { id });
      
      // Reload data to get updated state
      loadData();
    } catch (err) {
      console.error('Error resetting permission:', err);
      setError(`Failed to reset permission: ${err}`);
    }
  };
  
  // Reset all permissions to default
  const resetAllPermissions = async () => {
    try {
      await invoke('reset_all_permissions');
      
      // Reload data to get updated state
      loadData();
      
      // Close confirm dialog
      setShowResetConfirm(false);
    } catch (err) {
      console.error('Error resetting all permissions:', err);
      setError(`Failed to reset all permissions: ${err}`);
    }
  };
  
  // Format timestamp
  const formatTimestamp = (timestamp: string) => {
    return new Date(timestamp).toLocaleString();
  };
  
  // Format level name for display
  const formatLevelName = (level: string): string => {
    switch (level) {
      case PermissionLevel.AlwaysAllow:
        return "Always Allow";
      case PermissionLevel.AskFirstTime:
        return "Ask First Time";
      case PermissionLevel.AskEveryTime:
        return "Ask Every Time";
      case PermissionLevel.NeverAllow:
        return "Never Allow";
      default:
        return level;
    }
  };
  
  // Filter permissions by tab and search query
  const filteredPermissions = (() => {
    let filtered = [...permissions];
    
    // Filter by category
    if (activeTab !== 'all') {
      filtered = filtered.filter(p => 
        p.category.toLowerCase() === activeTab.toLowerCase()
      );
    }
    
    // Filter by search query
    if (searchQuery.trim()) {
      const query = searchQuery.toLowerCase();
      filtered = filtered.filter(p =>
        p.name.toLowerCase().includes(query) ||
        p.description.toLowerCase().includes(query) ||
        p.id.toLowerCase().includes(query)
      );
    }
    
    return filtered;
  })();
  
  // Render permission level selector with appropriate styling
  const renderLevelSelector = (permission: Permission) => {
    const isDisabled = permission.required && permission.level === PermissionLevel.AlwaysAllow;
    
    return (
      <select
        value={permission.level}
        onChange={(e) => changePermissionLevel(permission.id, e.target.value as PermissionLevel)}
        className={`permission-level-select level-${permission.level.toLowerCase()}`}
        disabled={isDisabled}
        title={isDisabled ? "This permission is required for core functionality" : undefined}
      >
        <option value={PermissionLevel.AlwaysAllow}>Always Allow</option>
        <option value={PermissionLevel.AskFirstTime}>Ask First Time</option>
        <option value={PermissionLevel.AskEveryTime}>Ask Every Time</option>
        <option value={PermissionLevel.NeverAllow}>Never Allow</option>
      </select>
    );
  };
  
  return (
    <div className="permissions-manager">
      <div className="permissions-header">
        <h2>Permissions Manager</h2>
        
        <div className="header-controls">
          <div className="search-box">
            <input
              type="text"
              placeholder="Search permissions..."
              value={searchQuery}
              onChange={(e) => setSearchQuery(e.target.value)}
            />
            {searchQuery && (
              <button 
                className="clear-search"
                onClick={() => setSearchQuery('')}
              >
                ×
              </button>
            )}
          </div>
          
          <button 
            className="reset-all-button"
            onClick={() => setShowResetConfirm(true)}
          >
            Reset All Permissions
          </button>
        </div>
      </div>
      
      {error && (
        <div className="error-message">
          {error}
        </div>
      )}
      
      <div className="permissions-content">
        <div className="permissions-sidebar">
          <nav className="categories-nav">
            <button
              className={activeTab === 'all' ? 'active' : ''}
              onClick={() => setActiveTab('all')}
            >
              All Categories
              <span className="count">{permissions.length}</span>
            </button>
            
            <button
              className={activeTab === 'privacy' ? 'active' : ''}
              onClick={() => setActiveTab('privacy')}
            >
              Privacy
              <span className="count">
                {permissions.filter(p => p.category === 'Privacy').length}
              </span>
            </button>
            
            <button
              className={activeTab === 'network' ? 'active' : ''}
              onClick={() => setActiveTab('network')}
            >
              Network
              <span className="count">
                {permissions.filter(p => p.category === 'Network').length}
              </span>
            </button>
            
            <button
              className={activeTab === 'system' ? 'active' : ''}
              onClick={() => setActiveTab('system')}
            >
              System
              <span className="count">
                {permissions.filter(p => p.category === 'System').length}
              </span>
            </button>
            
            <button
              className={activeTab === 'data' ? 'active' : ''}
              onClick={() => setActiveTab('data')}
            >
              Data
              <span className="count">
                {permissions.filter(p => p.category === 'Data').length}
              </span>
            </button>
            
            <button
              className={activeTab === 'security' ? 'active' : ''}
              onClick={() => setActiveTab('security')}
            >
              Security
              <span className="count">
                {permissions.filter(p => p.category === 'Security').length}
              </span>
            </button>
          </nav>
          
          {statistics && (
            <div className="statistics-panel">
              <h3>Statistics</h3>
              
              <div className="stat-item">
                <div className="stat-label">Total Permissions</div>
                <div className="stat-value">{statistics.total_permissions}</div>
              </div>
              
              <div className="stat-item">
                <div className="stat-label">Permission Requests</div>
                <div className="stat-value">{statistics.total_requests}</div>
              </div>
              
              <div className="stat-item">
                <div className="stat-label">Granted Requests</div>
                <div className="stat-value granted">{statistics.granted_count}</div>
              </div>
              
              <div className="stat-item">
                <div className="stat-label">Denied Requests</div>
                <div className="stat-value denied">{statistics.denied_count}</div>
              </div>
              
              <h4>Permission Levels</h4>
              <div className="levels-chart">
                {Object.entries(statistics.count_by_level).map(([level, count]) => (
                  <div key={level} className="level-bar-container">
                    <div className="level-label">{formatLevelName(level)}</div>
                    <div className="level-bar">
                      <div 
                        className={`level-bar-fill level-${level.toLowerCase()}`}
                        style={{ width: `${(count / statistics.total_permissions) * 100}%` }}
                      ></div>
                    </div>
                    <div className="level-count">{count}</div>
                  </div>
                ))}
              </div>
              
              <h4>Most Used Permissions</h4>
              {statistics.most_used_permissions.length === 0 ? (
                <div className="no-data">No usage data yet</div>
              ) : (
                <div className="most-used-list">
                  {statistics.most_used_permissions.map(([id, count]) => {
                    const permission = permissions.find(p => p.id === id);
                    return permission ? (
                      <div key={id} className="most-used-item">
                        <div className="most-used-name">{permission.name}</div>
                        <div className="most-used-count">{count} uses</div>
                      </div>
                    ) : null;
                  })}
                </div>
              )}
            </div>
          )}
        </div>
        
        <div className="permissions-list">
          {loading ? (
            <div className="loading-message">
              <div className="loading-spinner"></div>
              <div>Loading permissions...</div>
            </div>
          ) : filteredPermissions.length === 0 ? (
            <div className="no-permissions">
              {searchQuery 
                ? `No permissions matching "${searchQuery}"` 
                : activeTab !== 'all' 
                  ? `No permissions in the ${activeTab} category` 
                  : 'No permissions found'
              }
            </div>
          ) : (
            <>
              <div className="permissions-count">
                {filteredPermissions.length} permission{filteredPermissions.length !== 1 ? 's' : ''}
                {activeTab !== 'all' && ` in ${activeTab}`}
                {searchQuery && ` matching "${searchQuery}"`}
              </div>
              
              {filteredPermissions.map(permission => (
                <div key={permission.id} className="permission-card">
                  <div className="permission-header">
                    <div>
                      <h3 className="permission-name">{permission.name}</h3>
                      <div className="permission-meta">
                        <span className="permission-id">{permission.id}</span>
                        <span className="permission-category">{permission.category}</span>
                        {permission.required && (
                          <span className="permission-required">Required</span>
                        )}
                      </div>
                    </div>
                    
                    <div className="permission-actions">
                      {renderLevelSelector(permission)}
                      <button 
                        className="reset-button" 
                        onClick={() => resetPermission(permission.id)}
                        disabled={permission.required && permission.level === PermissionLevel.AlwaysAllow}
                        title="Reset to default"
                      >
                        Reset
                      </button>
                    </div>
                  </div>
                  
                  <div className="permission-description">
                    {permission.description}
                  </div>
                  
                  <div className="permission-footer">
                    <div className="permission-info">
                      <span className="permission-usage">
                        Used {permission.usage_count} time{permission.usage_count !== 1 ? 's' : ''}
                      </span>
                      <span className="permission-last-modified">
                        Last changed: {formatTimestamp(permission.last_modified)}
                      </span>
                    </div>
                    
                    <div className={`permission-status level-${permission.level.toLowerCase()}`}>
                      {formatLevelName(permission.level)}
                    </div>
                  </div>
                </div>
              ))}
            </>
          )}
        </div>
      </div>
      
      {showResetConfirm && (
        <div className="modal-overlay">
          <div className="confirm-modal">
            <h3>Reset All Permissions?</h3>
            <p>
              This will reset all permissions to their default values. 
              This action cannot be undone.
            </p>
            <div className="modal-actions">
              <button 
                className="cancel-button"
                onClick={() => setShowResetConfirm(false)}
              >
                Cancel
              </button>
              <button 
                className="confirm-button"
                onClick={resetAllPermissions}
              >
                Reset All
              </button>
            </div>
          </div>
        </div>
      )}
    </div>
  );
};

export default PermissionsManager;
</file>

<file path="src-frontend/src/components/security/SecuritySettings.css">
.security-settings {
  display: flex;
  flex-direction: column;
  height: 100%;
  overflow: hidden;
}

.settings-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 20px;
  position: relative;
}

.settings-header h2 {
  margin: 0;
  font-size: 28px;
  color: #333;
}

.saved-message {
  position: absolute;
  right: 20px;
  top: 20px;
  padding: 8px 16px;
  background-color: #e8f5e9;
  color: #2e7d32;
  border-radius: 4px;
  font-size: 14px;
  animation: fadeOut 3s ease-in-out forwards;
}

@keyframes fadeOut {
  0% { opacity: 1; }
  70% { opacity: 1; }
  100% { opacity: 0; }
}

.error-message {
  margin: 0 20px 20px;
  padding: 12px;
  background-color: #ffebee;
  border-left: 4px solid #f44336;
  color: #b71c1c;
  border-radius: 4px;
}

.tabs-container {
  display: flex;
  flex-direction: column;
  flex: 1;
  overflow: hidden;
}

.tabs-navigation {
  display: flex;
  background-color: #f5f5f7;
  border-bottom: 1px solid #e0e0e0;
  padding: 0 20px;
}

.tabs-navigation button {
  padding: 16px 24px;
  background: none;
  border: none;
  border-bottom: 2px solid transparent;
  font-size: 16px;
  color: #666;
  cursor: pointer;
  transition: all 0.2s;
}

.tabs-navigation button:hover {
  color: #5c6bc0;
}

.tabs-navigation button.active {
  color: #5c6bc0;
  border-bottom-color: #5c6bc0;
  font-weight: 500;
}

.tab-content {
  flex: 1;
  overflow: auto;
  padding: 0;
  position: relative;
}

.loading-message {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  height: 100%;
  padding: 40px;
  color: #666;
}

.loading-spinner {
  width: 40px;
  height: 40px;
  border: 4px solid #f3f3f3;
  border-top: 4px solid #5c6bc0;
  border-radius: 50%;
  animation: spin 1s linear infinite;
  margin-bottom: 16px;
}

@keyframes spin {
  0% { transform: rotate(0deg); }
  100% { transform: rotate(360deg); }
}

/* General Settings Tab */

.settings-tab {
  padding: 24px;
}

.settings-tab h3 {
  margin: 0 0 24px;
  font-size: 20px;
  color: #333;
}

.settings-section {
  margin-bottom: 32px;
  padding-bottom: 24px;
  border-bottom: 1px solid #eee;
}

.settings-section:last-child {
  margin-bottom: 0;
  padding-bottom: 0;
  border-bottom: none;
}

.settings-section h4 {
  margin: 0 0 16px;
  font-size: 16px;
  color: #555;
}

.setting-group {
  margin-bottom: 24px;
}

.setting-group:last-child {
  margin-bottom: 0;
}

.toggle-label {
  display: flex;
  align-items: center;
  cursor: pointer;
}

.toggle-label input[type="checkbox"] {
  margin-right: 10px;
  width: 18px;
  height: 18px;
  cursor: pointer;
}

.toggle-text {
  font-weight: 500;
  color: #333;
}

.setting-description {
  margin: 6px 0 0 28px;
  font-size: 14px;
  color: #777;
  line-height: 1.5;
}

.select-input, .number-input {
  display: block;
  width: 100%;
  max-width: 300px;
  margin-top: 8px;
  padding: 8px 12px;
  border: 1px solid #ddd;
  border-radius: 4px;
  font-size: 14px;
  transition: border-color 0.2s;
}

.select-input:focus, .number-input:focus {
  border-color: #5c6bc0;
  outline: none;
}

.action-button {
  padding: 8px 16px;
  background-color: #5c6bc0;
  color: white;
  border: none;
  border-radius: 4px;
  font-size: 14px;
  cursor: pointer;
  transition: background-color 0.2s;
}

.action-button:hover {
  background-color: #4a59ac;
}

.settings-actions {
  margin-top: 32px;
  display: flex;
  justify-content: flex-end;
}

.save-button {
  padding: 10px 20px;
  background-color: #4caf50;
  color: white;
  border: none;
  border-radius: 4px;
  font-size: 16px;
  font-weight: 500;
  cursor: pointer;
  transition: background-color 0.2s;
}

.save-button:hover {
  background-color: #388e3c;
}

.save-button:disabled {
  background-color: #a5d6a7;
  cursor: not-allowed;
}

/* Credentials Tab */

.credentials-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 24px;
}

.credentials-description {
  max-width: 600px;
  margin: 0;
  font-size: 14px;
  color: #666;
  line-height: 1.5;
}

.add-button {
  padding: 8px 16px;
  background-color: #5c6bc0;
  color: white;
  border: none;
  border-radius: 4px;
  font-size: 14px;
  cursor: pointer;
  transition: background-color 0.2s;
}

.add-button:hover {
  background-color: #4a59ac;
}

.credential-error {
  margin-bottom: 16px;
  padding: 12px;
  background-color: #ffebee;
  border-left: 4px solid #f44336;
  color: #b71c1c;
  border-radius: 4px;
}

.credential-form {
  margin-bottom: 24px;
  padding: 16px;
  background-color: #f5f5f7;
  border-radius: 8px;
}

.form-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 16px;
}

.form-header h4 {
  margin: 0;
  font-size: 16px;
  color: #333;
}

.close-button {
  background: none;
  border: none;
  font-size: 24px;
  color: #999;
  cursor: pointer;
  padding: 0;
  line-height: 1;
}

.close-button:hover {
  color: #f44336;
}

.form-group {
  margin-bottom: 16px;
}

.form-group label {
  display: block;
  font-size: 14px;
  font-weight: 500;
  color: #555;
  margin-bottom: 8px;
}

.form-group input {
  width: 100%;
  padding: 10px 12px;
  border: 1px solid #ddd;
  border-radius: 4px;
  font-size: 14px;
  transition: border-color 0.2s;
}

.form-group input:focus {
  border-color: #5c6bc0;
  outline: none;
}

.form-group input[readonly] {
  background-color: #f1f1f1;
  cursor: not-allowed;
}

.form-actions {
  display: flex;
  justify-content: flex-end;
  gap: 16px;
  margin-top: 24px;
}

.cancel-button {
  padding: 8px 16px;
  background-color: #f5f5f5;
  color: #666;
  border: 1px solid #ddd;
  border-radius: 4px;
  font-size: 14px;
  cursor: pointer;
  transition: background-color 0.2s;
}

.cancel-button:hover {
  background-color: #e0e0e0;
}

.store-button {
  padding: 8px 16px;
  background-color: #4caf50;
  color: white;
  border: none;
  border-radius: 4px;
  font-size: 14px;
  cursor: pointer;
  transition: background-color 0.2s;
}

.store-button:hover {
  background-color: #388e3c;
}

.store-button:disabled {
  background-color: #a5d6a7;
  cursor: not-allowed;
}

.credentials-list h4 {
  margin: 0 0 16px;
  font-size: 16px;
  color: #333;
}

.no-credentials {
  padding: 24px;
  text-align: center;
  color: #999;
  font-style: italic;
  background-color: #f9f9f9;
  border-radius: 8px;
}

.credentials-table {
  border: 1px solid #e0e0e0;
  border-radius: 8px;
  overflow: hidden;
}

.credentials-table-header {
  display: flex;
  padding: 12px 16px;
  background-color: #f5f5f7;
  font-weight: 500;
  color: #333;
  border-bottom: 1px solid #e0e0e0;
}

.credential-row {
  display: flex;
  padding: 12px 16px;
  border-bottom: 1px solid #e0e0e0;
}

.credential-row:last-child {
  border-bottom: none;
}

.credential-row:hover {
  background-color: #f9f9f9;
}

.credential-key {
  flex: 1;
}

.credential-actions {
  display: flex;
  gap: 8px;
}

.view-button, .delete-button {
  padding: 4px 8px;
  border-radius: 4px;
  font-size: 12px;
  cursor: pointer;
  transition: all 0.2s;
}

.view-button {
  background-color: #e3f2fd;
  color: #1565c0;
  border: 1px solid #90caf9;
}

.view-button:hover {
  background-color: #bbdefb;
}

.delete-button {
  background-color: #ffebee;
  color: #c62828;
  border: 1px solid #ef9a9a;
}

.delete-button:hover {
  background-color: #ffcdd2;
}

/* Responsive Adjustments */
@media (max-width: 900px) {
  .tabs-navigation {
    overflow-x: auto;
    padding: 0 12px;
  }
  
  .tabs-navigation button {
    padding: 12px 16px;
    font-size: 14px;
    white-space: nowrap;
  }
  
  .settings-tab {
    padding: 16px;
  }
  
  .credentials-header {
    flex-direction: column;
    align-items: flex-start;
    gap: 16px;
  }
  
  .credentials-description {
    max-width: none;
  }
}

@media (max-width: 600px) {
  .form-actions {
    flex-direction: column;
    gap: 8px;
  }
  
  .form-actions button {
    width: 100%;
  }
  
  .credential-actions {
    flex-direction: column;
    gap: 4px;
  }
  
  .credential-key, .credential-actions {
    font-size: 14px;
  }
  
  .setting-group {
    margin-bottom: 20px;
  }
  
  .setting-description {
    margin-left: 0;
    margin-top: 8px;
  }
}
</file>

<file path="src-frontend/src/components/security/SecuritySettings.tsx">
import React, { useState, useEffect } from 'react';
import { invoke } from '@tauri-apps/api/tauri';
import PermissionsManager from './PermissionsManager';
import DataFlowVisualization from './DataFlowVisualization';
import './SecuritySettings.css';

// Types
interface SecurityConfig {
  e2ee_enabled: boolean;
  use_secure_enclave: boolean;
  data_flow_tracking_enabled: boolean;
  default_permission_level: string;
  interactive_permissions: boolean;
  anonymize_telemetry: boolean;
  encrypt_local_storage: boolean;
  credential_cache_duration: number;
  clipboard_security_enabled: boolean;
}

const SecuritySettings: React.FC = () => {
  const [activeTab, setActiveTab] = useState<'general' | 'permissions' | 'data-flow' | 'credentials'>('general');
  const [config, setConfig] = useState<SecurityConfig | null>(null);
  const [loading, setLoading] = useState<boolean>(true);
  const [error, setError] = useState<string | null>(null);
  const [savingConfig, setSavingConfig] = useState<boolean>(false);
  const [showCredential, setShowCredential] = useState<boolean>(false);
  const [credentialKey, setCredentialKey] = useState<string>('');
  const [credentialValue, setCredentialValue] = useState<string>('');
  const [credentials, setCredentials] = useState<string[]>([]);
  const [credentialError, setCredentialError] = useState<string | null>(null);
  const [savedMessage, setSavedMessage] = useState<string | null>(null);
  
  // Load security config on mount
  useEffect(() => {
    loadData();
  }, []);
  
  // Function to load data from backend
  const loadData = async () => {
    try {
      setLoading(true);
      
      // Load security config
      const configData = await invoke<SecurityConfig>('get_security_config');
      setConfig(configData);
      
      // Load credential keys
      await loadCredentials();
      
      setError(null);
    } catch (err) {
      console.error('Error loading security settings:', err);
      setError(`Failed to load security settings: ${err}`);
    } finally {
      setLoading(false);
    }
  };
  
  // Load credentials list
  const loadCredentials = async () => {
    try {
      const credentialsList = await invoke<string[]>('list_secure_credentials');
      setCredentials(credentialsList);
    } catch (err) {
      console.error('Error loading credentials:', err);
      setCredentialError(`Failed to load credentials: ${err}`);
    }
  };
  
  // Save security config
  const saveConfig = async () => {
    if (!config) return;
    
    try {
      setSavingConfig(true);
      
      await invoke('update_security_config', { config });
      
      // Show saved message
      setSavedMessage('Settings saved successfully');
      setTimeout(() => setSavedMessage(null), 3000);
    } catch (err) {
      console.error('Error saving security settings:', err);
      setError(`Failed to save security settings: ${err}`);
    } finally {
      setSavingConfig(false);
    }
  };
  
  // Handle input change
  const handleInputChange = (e: React.ChangeEvent<HTMLInputElement | HTMLSelectElement>) => {
    if (!config) return;
    
    const { name, value, type } = e.target;
    
    if (type === 'checkbox') {
      const checked = (e.target as HTMLInputElement).checked;
      setConfig({ ...config, [name]: checked });
    } else if (type === 'number') {
      setConfig({ ...config, [name]: parseInt(value, 10) });
    } else {
      setConfig({ ...config, [name]: value });
    }
  };
  
  // Store a credential
  const storeCredential = async () => {
    if (!credentialKey || !credentialValue) {
      setCredentialError('Both key and value are required');
      return;
    }
    
    try {
      await invoke('store_secure_credential', { key: credentialKey, value: credentialValue });
      
      // Reset form
      setCredentialKey('');
      setCredentialValue('');
      setShowCredential(false);
      setCredentialError(null);
      
      // Reload credentials
      await loadCredentials();
      
      // Show message
      setSavedMessage('Credential stored successfully');
      setTimeout(() => setSavedMessage(null), 3000);
    } catch (err) {
      console.error('Error storing credential:', err);
      setCredentialError(`Failed to store credential: ${err}`);
    }
  };
  
  // Delete a credential
  const deleteCredential = async (key: string) => {
    try {
      await invoke('delete_secure_credential', { key });
      
      // Reload credentials
      await loadCredentials();
      
      // Show message
      setSavedMessage('Credential deleted successfully');
      setTimeout(() => setSavedMessage(null), 3000);
    } catch (err) {
      console.error('Error deleting credential:', err);
      setCredentialError(`Failed to delete credential: ${err}`);
    }
  };
  
  // Get a credential
  const getCredential = async (key: string) => {
    try {
      const value = await invoke<string>('get_secure_credential', { key });
      
      // Set value in form
      setCredentialKey(key);
      setCredentialValue(value);
      setShowCredential(true);
      setCredentialError(null);
    } catch (err) {
      console.error('Error getting credential:', err);
      setCredentialError(`Failed to get credential: ${err}`);
    }
  };
  
  // Rotate encryption keys
  const rotateEncryptionKeys = async () => {
    try {
      await invoke('rotate_encryption_keys');
      
      // Show message
      setSavedMessage('Encryption keys rotated successfully');
      setTimeout(() => setSavedMessage(null), 3000);
    } catch (err) {
      console.error('Error rotating encryption keys:', err);
      setError(`Failed to rotate encryption keys: ${err}`);
    }
  };
  
  // Render General tab
  const renderGeneralTab = () => {
    if (!config) return null;
    
    return (
      <div className="settings-tab">
        <h3>General Security Settings</h3>
        
        <div className="settings-section">
          <div className="setting-group">
            <label htmlFor="e2ee_enabled" className="toggle-label">
              <input
                type="checkbox"
                id="e2ee_enabled"
                name="e2ee_enabled"
                checked={config.e2ee_enabled}
                onChange={handleInputChange}
              />
              <span className="toggle-text">Enable End-to-End Encryption</span>
            </label>
            <p className="setting-description">
              Encrypts your data before sending it to the cloud, ensuring that only you can read it.
            </p>
          </div>
          
          <div className="setting-group">
            <label htmlFor="use_secure_enclave" className="toggle-label">
              <input
                type="checkbox"
                id="use_secure_enclave"
                name="use_secure_enclave"
                checked={config.use_secure_enclave}
                onChange={handleInputChange}
              />
              <span className="toggle-text">Use Secure Enclave for Credentials</span>
            </label>
            <p className="setting-description">
              Stores credentials in your device's secure hardware storage for maximum protection.
            </p>
          </div>
          
          <div className="setting-group">
            <label htmlFor="encrypt_local_storage" className="toggle-label">
              <input
                type="checkbox"
                id="encrypt_local_storage"
                name="encrypt_local_storage"
                checked={config.encrypt_local_storage}
                onChange={handleInputChange}
              />
              <span className="toggle-text">Encrypt Local Storage</span>
            </label>
            <p className="setting-description">
              Encrypts all locally stored data to protect it if your device is lost or stolen.
            </p>
          </div>
          
          <div className="setting-group">
            <label htmlFor="clipboard_security_enabled" className="toggle-label">
              <input
                type="checkbox"
                id="clipboard_security_enabled"
                name="clipboard_security_enabled"
                checked={config.clipboard_security_enabled}
                onChange={handleInputChange}
              />
              <span className="toggle-text">Enable Clipboard Security</span>
            </label>
            <p className="setting-description">
              Automatically clears the clipboard after copying sensitive information.
            </p>
          </div>
        </div>
        
        <div className="settings-section">
          <h4>Privacy Settings</h4>
          
          <div className="setting-group">
            <label htmlFor="anonymize_telemetry" className="toggle-label">
              <input
                type="checkbox"
                id="anonymize_telemetry"
                name="anonymize_telemetry"
                checked={config.anonymize_telemetry}
                onChange={handleInputChange}
              />
              <span className="toggle-text">Anonymize Telemetry Data</span>
            </label>
            <p className="setting-description">
              Removes personally identifiable information from usage data before sending it.
            </p>
          </div>
          
          <div className="setting-group">
            <label htmlFor="data_flow_tracking_enabled" className="toggle-label">
              <input
                type="checkbox"
                id="data_flow_tracking_enabled"
                name="data_flow_tracking_enabled"
                checked={config.data_flow_tracking_enabled}
                onChange={handleInputChange}
              />
              <span className="toggle-text">Enable Data Flow Tracking</span>
            </label>
            <p className="setting-description">
              Tracks how your data moves through the application and shows you where it goes.
            </p>
          </div>
        </div>
        
        <div className="settings-section">
          <h4>Permission Settings</h4>
          
          <div className="setting-group">
            <label htmlFor="default_permission_level">
              Default Permission Level
              <select
                id="default_permission_level"
                name="default_permission_level"
                value={config.default_permission_level}
                onChange={handleInputChange}
                className="select-input"
              >
                <option value="AlwaysAllow">Always Allow</option>
                <option value="AskFirstTime">Ask First Time</option>
                <option value="AskEveryTime">Ask Every Time</option>
                <option value="NeverAllow">Never Allow</option>
              </select>
            </label>
            <p className="setting-description">
              The default level for new permissions that aren't explicitly set.
            </p>
          </div>
          
          <div className="setting-group">
            <label htmlFor="interactive_permissions" className="toggle-label">
              <input
                type="checkbox"
                id="interactive_permissions"
                name="interactive_permissions"
                checked={config.interactive_permissions}
                onChange={handleInputChange}
              />
              <span className="toggle-text">Enable Interactive Permissions</span>
            </label>
            <p className="setting-description">
              Prompts you for permissions when they're needed rather than using defaults.
            </p>
          </div>
          
          <div className="setting-group">
            <label htmlFor="credential_cache_duration">
              Credential Cache Duration (seconds)
              <input
                type="number"
                id="credential_cache_duration"
                name="credential_cache_duration"
                value={config.credential_cache_duration}
                onChange={handleInputChange}
                min="0"
                max="86400"
                className="number-input"
              />
            </label>
            <p className="setting-description">
              How long to keep credentials in memory before requiring re-authentication.
            </p>
          </div>
        </div>
        
        <div className="settings-section">
          <h4>Advanced Security</h4>
          
          <div className="setting-group">
            <button 
              className="action-button" 
              onClick={rotateEncryptionKeys}
            >
              Rotate Encryption Keys
            </button>
            <p className="setting-description">
              Generates new encryption keys and re-encrypts your data for increased security.
            </p>
          </div>
        </div>
        
        <div className="settings-actions">
          <button
            className="save-button"
            onClick={saveConfig}
            disabled={savingConfig}
          >
            {savingConfig ? 'Saving...' : 'Save Settings'}
          </button>
        </div>
      </div>
    );
  };
  
  // Render Credentials tab
  const renderCredentialsTab = () => {
    return (
      <div className="settings-tab">
        <h3>Secure Credentials Manager</h3>
        
        <div className="credentials-header">
          <p className="credentials-description">
            Securely store and manage sensitive information like API keys and passwords.
            {config?.use_secure_enclave 
              ? ' Credentials are stored in your device\'s secure enclave.' 
              : ' Credentials are stored encrypted on your device.'}
          </p>
          
          <button
            className="add-button"
            onClick={() => {
              setCredentialKey('');
              setCredentialValue('');
              setShowCredential(true);
              setCredentialError(null);
            }}
          >
            Add Credential
          </button>
        </div>
        
        {credentialError && (
          <div className="credential-error">
            {credentialError}
          </div>
        )}
        
        {showCredential && (
          <div className="credential-form">
            <div className="form-header">
              <h4>{credentialKey ? 'Edit Credential' : 'Add Credential'}</h4>
              <button
                className="close-button"
                onClick={() => setShowCredential(false)}
              >
                ×
              </button>
            </div>
            
            <div className="form-group">
              <label htmlFor="credential-key">
                Credential Key
                <input
                  type="text"
                  id="credential-key"
                  value={credentialKey}
                  onChange={(e) => setCredentialKey(e.target.value)}
                  placeholder="e.g., api_key, database_password"
                  readOnly={!!credentialKey && credentials.includes(credentialKey)}
                />
              </label>
            </div>
            
            <div className="form-group">
              <label htmlFor="credential-value">
                Credential Value
                <input
                  type="password"
                  id="credential-value"
                  value={credentialValue}
                  onChange={(e) => setCredentialValue(e.target.value)}
                  placeholder="Secure value to store"
                />
              </label>
            </div>
            
            <div className="form-actions">
              <button
                className="cancel-button"
                onClick={() => setShowCredential(false)}
              >
                Cancel
              </button>
              <button
                className="store-button"
                onClick={storeCredential}
                disabled={!credentialKey || !credentialValue}
              >
                Store Credential
              </button>
            </div>
          </div>
        )}
        
        <div className="credentials-list">
          <h4>Stored Credentials</h4>
          
          {credentials.length === 0 ? (
            <div className="no-credentials">
              No credentials stored yet. Click "Add Credential" to add one.
            </div>
          ) : (
            <div className="credentials-table">
              <div className="credentials-table-header">
                <div className="credential-key">Credential Key</div>
                <div className="credential-actions">Actions</div>
              </div>
              
              {credentials.map(key => (
                <div key={key} className="credential-row">
                  <div className="credential-key">{key}</div>
                  <div className="credential-actions">
                    <button
                      className="view-button"
                      onClick={() => getCredential(key)}
                      title="View/Edit"
                    >
                      View
                    </button>
                    <button
                      className="delete-button"
                      onClick={() => {
                        if (confirm(`Are you sure you want to delete the credential "${key}"?`)) {
                          deleteCredential(key);
                        }
                      }}
                      title="Delete"
                    >
                      Delete
                    </button>
                  </div>
                </div>
              ))}
            </div>
          )}
        </div>
      </div>
    );
  };
  
  return (
    <div className="security-settings">
      <div className="settings-header">
        <h2>Security & Privacy</h2>
        
        {savedMessage && (
          <div className="saved-message">
            {savedMessage}
          </div>
        )}
      </div>
      
      {error && (
        <div className="error-message">
          {error}
        </div>
      )}
      
      <div className="tabs-container">
        <div className="tabs-navigation">
          <button 
            className={activeTab === 'general' ? 'active' : ''}
            onClick={() => setActiveTab('general')}
          >
            General
          </button>
          <button 
            className={activeTab === 'permissions' ? 'active' : ''}
            onClick={() => setActiveTab('permissions')}
          >
            Permissions
          </button>
          <button 
            className={activeTab === 'data-flow' ? 'active' : ''}
            onClick={() => setActiveTab('data-flow')}
          >
            Data Flow
          </button>
          <button 
            className={activeTab === 'credentials' ? 'active' : ''}
            onClick={() => setActiveTab('credentials')}
          >
            Credentials
          </button>
        </div>
        
        <div className="tab-content">
          {loading ? (
            <div className="loading-message">
              <div className="loading-spinner"></div>
              <div>Loading security settings...</div>
            </div>
          ) : (
            <>
              {activeTab === 'general' && renderGeneralTab()}
              {activeTab === 'permissions' && <PermissionsManager />}
              {activeTab === 'data-flow' && <DataFlowVisualization />}
              {activeTab === 'credentials' && renderCredentialsTab()}
            </>
          )}
        </div>
      </div>
    </div>
  );
};

export default SecuritySettings;
</file>

<file path="src-frontend/src/components/settings/PrivacySettings.tsx">
import React, { useState, useEffect } from 'react';
import { invoke } from '@tauri-apps/api/tauri';
import { useFeatureFlags } from '../../contexts/FeatureFlagContext';

interface TelemetryConfig {
  enabled: boolean;
  client_id: string;
  collection_categories: Record<string, boolean>;
  batch_size: number;
  batch_interval_seconds: number;
  server_url: string;
}

const categoryDescriptions: Record<string, string> = {
  app_lifecycle: 'Application startup and shutdown events',
  feature_usage: 'Which features you use and how often',
  errors: 'Error reports to help improve stability',
  performance: 'Performance metrics to optimize the application',
  user_actions: 'User interface interactions and workflow patterns',
  system_info: 'Information about your system configuration',
  logs: 'Application logs for troubleshooting',
};

const PrivacySettings: React.FC = () => {
  const { isFeatureEnabled } = useFeatureFlags();
  const [config, setConfig] = useState<TelemetryConfig | null>(null);
  const [isLoading, setIsLoading] = useState(true);
  const [hasChanges, setHasChanges] = useState(false);
  const [deleteConfirm, setDeleteConfirm] = useState(false);
  
  // Fetch current configuration
  useEffect(() => {
    const fetchConfig = async () => {
      try {
        const currentConfig = await invoke<TelemetryConfig>('get_telemetry_config');
        setConfig(currentConfig);
        setIsLoading(false);
      } catch (error) {
        console.error('Failed to fetch telemetry config:', error);
        setIsLoading(false);
      }
    };
    
    fetchConfig();
  }, []);
  
  // Toggle master switch
  const handleToggleTelemetry = () => {
    if (!config) return;
    
    const newConfig = {
      ...config,
      enabled: !config.enabled,
    };
    
    setConfig(newConfig);
    setHasChanges(true);
  };
  
  // Toggle category
  const handleToggleCategory = (category: string) => {
    if (!config) return;
    
    const newCategories = {
      ...config.collection_categories,
      [category]: !config.collection_categories[category],
    };
    
    const newConfig = {
      ...config,
      collection_categories: newCategories,
    };
    
    setConfig(newConfig);
    setHasChanges(true);
  };
  
  // Reset client ID
  const handleResetClientId = () => {
    if (!config) return;
    
    // Generate new UUID for client ID
    const newClientId = crypto.randomUUID();
    
    const newConfig = {
      ...config,
      client_id: newClientId,
    };
    
    setConfig(newConfig);
    setHasChanges(true);
  };
  
  // Save changes
  const handleSaveChanges = async () => {
    if (!config) return;
    
    try {
      await invoke('update_telemetry_config', { config });
      setHasChanges(false);
      
      // Show saved notification
      alert('Privacy settings saved successfully!');
    } catch (error) {
      console.error('Failed to update telemetry config:', error);
      alert('Failed to save privacy settings. Please try again.');
    }
  };
  
  // Reset changes
  const handleResetChanges = async () => {
    try {
      const currentConfig = await invoke<TelemetryConfig>('get_telemetry_config');
      setConfig(currentConfig);
      setHasChanges(false);
    } catch (error) {
      console.error('Failed to fetch telemetry config:', error);
    }
  };
  
  // Delete telemetry data
  const handleDeleteData = async () => {
    try {
      await invoke('delete_telemetry_data');
      setDeleteConfirm(false);
      
      // Refresh config to get new client ID
      const currentConfig = await invoke<TelemetryConfig>('get_telemetry_config');
      setConfig(currentConfig);
      
      // Show success notification
      alert('Telemetry data deleted successfully. You have been assigned a new anonymous ID.');
    } catch (error) {
      console.error('Failed to delete telemetry data:', error);
      alert('Failed to delete telemetry data. Please try again.');
    }
  };
  
  if (isLoading) {
    return <div className="loading">Loading privacy settings...</div>;
  }
  
  if (!config) {
    return <div className="error">Failed to load privacy settings.</div>;
  }
  
  // Check if the advanced telemetry feature is enabled
  const telemetryEnabled = isFeatureEnabled('advanced_telemetry');
  
  if (!telemetryEnabled) {
    return (
      <div className="privacy-settings">
        <h2>Privacy & Telemetry Settings</h2>
        <div className="feature-disabled">
          <p>Advanced telemetry features are currently disabled. To access these settings, please enable the "Advanced Telemetry" feature.</p>
          <p>You can enable this feature in the Feature Flags section or by joining the Alpha canary group.</p>
        </div>
      </div>
    );
  }
  
  return (
    <div className="privacy-settings">
      <h2>Privacy & Telemetry Settings</h2>
      
      <div className="telemetry-main-card">
        <div className="telemetry-main-toggle">
          <h3>Enable Telemetry & Usage Data</h3>
          <label className="switch">
            <input
              type="checkbox"
              checked={config.enabled}
              onChange={handleToggleTelemetry}
            />
            <span className="slider round"></span>
          </label>
          <span>{config.enabled ? 'Enabled' : 'Disabled'}</span>
        </div>
        
        <p className="description">
          Telemetry data helps us improve the MCP Client by understanding how it's used and identifying issues. All data is anonymized and never contains personal information or message content.
        </p>
        
        <div className="client-id-section">
          <div>
            <h4>Your anonymous client ID:</h4>
            <code>{config.client_id}</code>
          </div>
          <button 
            className="secondary-button" 
            onClick={handleResetClientId}
          >
            Generate New ID
          </button>
        </div>
      </div>
      
      <h3>Data Collection Categories</h3>
      <p className="subcategory-desc">
        Configure which types of data you're comfortable sharing. Adjustments only apply if telemetry is enabled.
      </p>
      
      <div className="category-toggles">
        {Object.entries(config.collection_categories).map(([category, enabled]) => (
          <div key={category} className="category-card">
            <div className="category-header">
              <h4>{formatCategoryName(category)}</h4>
              <label className="switch">
                <input
                  type="checkbox"
                  checked={enabled}
                  onChange={() => handleToggleCategory(category)}
                  disabled={!config.enabled}
                />
                <span className="slider round"></span>
              </label>
            </div>
            
            <p className="category-description">
              {categoryDescriptions[category] || 'No description available'}
            </p>
            
            {category === 'performance' && enabled && (
              <div className="subcategory-list">
                <h5>Includes:</h5>
                <ul>
                  <li>CPU & memory usage</li>
                  <li>API latency & throughput</li>
                  <li>UI responsiveness metrics</li>
                  <li>Startup & operation times</li>
                </ul>
              </div>
            )}
          </div>
        ))}
      </div>
      
      {hasChanges && (
        <div className="settings-actions">
          <button className="secondary-button" onClick={handleResetChanges}>
            Cancel
          </button>
          <button className="primary-button" onClick={handleSaveChanges}>
            Save Changes
          </button>
        </div>
      )}
      
      <div className="data-control-card">
        <h3>Your Data Control</h3>
        <p>
          You can delete all collected telemetry data associated with your client ID at any time.
          This will also generate a new anonymous ID for your device.
        </p>
        
        {!deleteConfirm ? (
          <button 
            className="danger-button" 
            onClick={() => setDeleteConfirm(true)}
          >
            Delete My Data
          </button>
        ) : (
          <div className="confirm-delete">
            <p className="warning">Are you sure you want to delete all telemetry data? This action cannot be undone.</p>
            <div className="confirm-buttons">
              <button 
                className="secondary-button" 
                onClick={() => setDeleteConfirm(false)}
              >
                Cancel
              </button>
              <button 
                className="danger-button" 
                onClick={handleDeleteData}
              >
                Yes, Delete My Data
              </button>
            </div>
          </div>
        )}
      </div>
    </div>
  );
};

// Helper function to format category names
function formatCategoryName(category: string): string {
  return category
    .split('_')
    .map(word => word.charAt(0).toUpperCase() + word.slice(1))
    .join(' ');
}

export default PrivacySettings;
</file>

<file path="src-frontend/src/components/Shell.css">
/* 
 * Shell.css
 * Minimal styling for the initial shell component
 * Optimized for fast rendering (<500ms)
 */

.shell {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  height: 100vh;
  width: 100vw;
  background-color: #1e1e2e;
  color: #cdd6f4;
  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen,
    Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1000;
  overflow: hidden;
}

.shell-loading {
  transition: opacity 0.5s ease-out;
}

.shell-error {
  background-color: #2a1a22;
}

.logo-container {
  display: flex;
  flex-direction: column;
  align-items: center;
  margin-bottom: 2rem;
}

.app-icon {
  width: 80px;
  height: 80px;
  background-color: #7287fd;
  border-radius: 16px;
  margin-bottom: 1rem;
  display: flex;
  align-items: center;
  justify-content: center;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
}

.icon-inner {
  width: 50%;
  height: 50%;
  background-color: #1e1e2e;
  border-radius: 8px;
}

.logo-container h1 {
  font-size: 1.8rem;
  font-weight: 600;
  margin: 0;
  color: #cdd6f4;
}

.loading-indicator {
  width: 280px;
  margin-bottom: 2rem;
}

.progress-bar {
  width: 100%;
  height: 4px;
  background-color: #313244;
  border-radius: 2px;
  overflow: hidden;
  margin-bottom: 8px;
}

.progress-fill {
  height: 100%;
  background-color: #7287fd;
  border-radius: 2px;
  transition: width 0.3s ease-in-out;
}

.loading-text {
  text-align: center;
  font-size: 0.9rem;
  color: #a6adc8;
}

.version-info {
  position: absolute;
  bottom: 16px;
  right: 16px;
  font-size: 0.8rem;
  color: #6c7086;
}

.error-container {
  max-width: 500px;
  padding: 2rem;
  background-color: #45283c;
  border-radius: 8px;
  text-align: center;
}

.error-container h2 {
  color: #f38ba8;
  margin-top: 0;
}

.error-container p {
  margin: 1rem 0;
  color: #f2cdcd;
  white-space: pre-wrap;
  text-align: left;
  font-family: monospace;
  background-color: rgba(0, 0, 0, 0.2);
  padding: 1rem;
  border-radius: 4px;
  overflow: auto;
  max-height: 200px;
}

.error-container button {
  background-color: #f38ba8;
  color: #1e1e2e;
  border: none;
  padding: 0.7rem 1.5rem;
  border-radius: 4px;
  font-weight: 600;
  cursor: pointer;
  transition: background-color 0.2s;
}

.error-container button:hover {
  background-color: #f5a9b8;
}

/* System light theme support */
@media (prefers-color-scheme: light) {
  .shell {
    background-color: #f5f5fa;
    color: #11111b;
  }
  
  .shell-error {
    background-color: #f9e2e2;
  }
  
  .app-icon {
    background-color: #7287fd;
  }
  
  .icon-inner {
    background-color: #f5f5fa;
  }
  
  .logo-container h1 {
    color: #11111b;
  }
  
  .progress-bar {
    background-color: #dce0ee;
  }
  
  .loading-text {
    color: #45475a;
  }
  
  .version-info {
    color: #7c7f93;
  }
  
  .error-container {
    background-color: #f9dadc;
  }
  
  .error-container h2 {
    color: #d20f39;
  }
  
  .error-container p {
    color: #4c4f69;
    background-color: rgba(255, 255, 255, 0.5);
  }
}
</file>

<file path="src-frontend/src/components/Shell.tsx">
import React, { useEffect, useState } from 'react';
import { invoke } from '@tauri-apps/api/tauri';
import './Shell.css';

// Types for loading state from the backend
export enum LoadState {
  NotStarted = 'NotStarted',
  ShellLoading = 'ShellLoading',
  ShellReady = 'ShellReady',
  CoreServicesLoading = 'CoreServicesLoading',
  CoreServicesReady = 'CoreServicesReady',
  SecondaryLoading = 'SecondaryLoading',
  FullyLoaded = 'FullyLoaded',
  Error = 'Error',
}

// Type for app info
interface AppInfo {
  name: string;
  version: string;
  platform: string;
}

// Shell component - designed to load in <500ms
const Shell: React.FC = () => {
  const [loadState, setLoadState] = useState<LoadState>(LoadState.ShellLoading);
  const [appInfo, setAppInfo] = useState<AppInfo | null>(null);
  const [enabledFeatures, setEnabledFeatures] = useState<string[]>([]);
  const [error, setError] = useState<string | null>(null);
  const [loadProgress, setLoadProgress] = useState<number>(0);

  // Set up loading state listener
  useEffect(() => {
    const loadApp = async () => {
      try {
        // Load basic app info asap
        const info = await invoke<AppInfo>('get_app_info');
        setAppInfo(info);

        // Get enabled features
        const features = await invoke<string[]>('get_enabled_features');
        setEnabledFeatures(features);

        // Update progress based on loading state
        // In a real app, we would subscribe to backend state updates
        setLoadState(LoadState.ShellReady);
        setLoadProgress(20);

        setTimeout(() => {
          setLoadState(LoadState.CoreServicesLoading);
          setLoadProgress(40);

          setTimeout(() => {
            setLoadState(LoadState.CoreServicesReady);
            setLoadProgress(60);

            setTimeout(() => {
              setLoadState(LoadState.SecondaryLoading);
              setLoadProgress(80);

              setTimeout(() => {
                setLoadState(LoadState.FullyLoaded);
                setLoadProgress(100);
              }, 300);
            }, 200);
          }, 150);
        }, 100);
      } catch (err) {
        console.error('Failed to initialize app:', err);
        setError(String(err));
        setLoadState(LoadState.Error);
      }
    };

    loadApp();
  }, []);

  // Show error state
  if (loadState === LoadState.Error) {
    return (
      <div className="shell shell-error">
        <div className="error-container">
          <h2>Error Starting Application</h2>
          <p>{error}</p>
          <button onClick={() => window.location.reload()}>
            Restart Application
          </button>
        </div>
      </div>
    );
  }

  // Show loading state
  if (loadState !== LoadState.FullyLoaded) {
    return (
      <div className="shell shell-loading">
        <div className="logo-container">
          <div className="app-icon">
            {/* Simple placeholder icon */}
            <div className="icon-inner"></div>
          </div>
          <h1>{appInfo?.name || 'Claude MCP'}</h1>
        </div>
        <div className="loading-indicator">
          <div className="progress-bar">
            <div 
              className="progress-fill"
              style={{ width: `${loadProgress}%` }}
            ></div>
          </div>
          <div className="loading-text">
            {loadStateToMessage(loadState)}
          </div>
        </div>
        <div className="version-info">
          {appInfo?.version && `v${appInfo.version}`}
        </div>
      </div>
    );
  }

  // When fully loaded, the main app will be loaded dynamically in App.tsx
  // This component serves as a lightweight shell only
  return null;
};

// Convert load state to user-friendly message
function loadStateToMessage(state: LoadState): string {
  switch (state) {
    case LoadState.ShellLoading:
      return 'Starting up...';
    case LoadState.ShellReady:
      return 'Initializing...';
    case LoadState.CoreServicesLoading:
      return 'Loading core services...';
    case LoadState.CoreServicesReady:
      return 'Preparing workspace...';
    case LoadState.SecondaryLoading:
      return 'Almost ready...';
    case LoadState.FullyLoaded:
      return 'Ready!';
    case LoadState.Error:
      return 'Error starting application';
    default:
      return 'Loading...';
  }
}

export default Shell;
</file>

<file path="src-frontend/src/components/ui/Button.css">
.button {
  position: relative;
  display: inline-flex;
  align-items: center;
  justify-content: center;
  font-family: var(--font-family);
  font-weight: 500;
  border-radius: var(--radius-md);
  transition: all var(--transition-fast);
  white-space: nowrap;
  cursor: pointer;
  user-select: none;
}

/* Variants */
.button-primary {
  background-color: var(--color-primary);
  color: white;
  border: none;
}

.button-primary:hover:not(:disabled) {
  background-color: var(--color-primary-dark);
}

.button-secondary {
  background-color: var(--color-secondary);
  color: white;
  border: none;
}

.button-secondary:hover:not(:disabled) {
  background-color: var(--color-secondary-dark);
}

.button-outline {
  background-color: transparent;
  color: var(--color-primary);
  border: 1px solid var(--color-primary);
}

.button-outline:hover:not(:disabled) {
  background-color: rgba(var(--color-primary-rgb), 0.1);
}

.button-ghost {
  background-color: transparent;
  color: var(--color-on-surface);
  border: none;
}

.button-ghost:hover:not(:disabled) {
  background-color: var(--color-surface-variant);
}

.button-danger {
  background-color: var(--color-error);
  color: white;
  border: none;
}

.button-danger:hover:not(:disabled) {
  background-color: darkred;
}

/* Sizes */
.button-sm {
  height: 2rem;
  padding: 0 var(--spacing-sm);
  font-size: var(--font-size-sm);
  min-width: 4rem;
}

.button-md {
  height: 2.5rem;
  padding: 0 var(--spacing-md);
  font-size: var(--font-size-md);
  min-width: 5rem;
}

.button-lg {
  height: 3rem;
  padding: 0 var(--spacing-lg);
  font-size: var(--font-size-lg);
  min-width: 6rem;
}

/* States */
.button:disabled {
  opacity: 0.6;
  cursor: not-allowed;
}

.button:focus-visible {
  outline: 2px solid var(--color-primary);
  outline-offset: 2px;
}

/* Loading state */
.button-loading {
  cursor: wait;
}

.loading-spinner {
  position: absolute;
  width: 1rem;
  height: 1rem;
  border: 2px solid rgba(255, 255, 255, 0.3);
  border-radius: 50%;
  border-top-color: white;
  animation: spin 0.8s linear infinite;
}

@keyframes spin {
  to {
    transform: rotate(360deg);
  }
}

.button-loading .button-text {
  opacity: 0;
}

/* Icons */
.button-icon {
  display: flex;
  align-items: center;
  justify-content: center;
}

.button-icon-left {
  margin-right: var(--spacing-xs);
}

.button-icon-right {
  margin-left: var(--spacing-xs);
}

.button-sm .button-icon {
  font-size: 0.875rem;
}

.button-md .button-icon {
  font-size: 1rem;
}

.button-lg .button-icon {
  font-size: 1.125rem;
}
</file>

<file path="src-frontend/src/components/ui/Button.tsx">
import React, { ButtonHTMLAttributes } from 'react';
import './Button.css';

export type ButtonVariant = 'primary' | 'secondary' | 'outline' | 'ghost' | 'danger';
export type ButtonSize = 'sm' | 'md' | 'lg';

export interface ButtonProps extends ButtonHTMLAttributes<HTMLButtonElement> {
  variant?: ButtonVariant;
  size?: ButtonSize;
  isLoading?: boolean;
  leftIcon?: React.ReactNode;
  rightIcon?: React.ReactNode;
}

export const Button = React.forwardRef<HTMLButtonElement, ButtonProps>(
  (
    {
      children,
      variant = 'primary',
      size = 'md',
      isLoading = false,
      leftIcon,
      rightIcon,
      className = '',
      disabled,
      ...props
    },
    ref
  ) => {
    const buttonClass = `
      button 
      button-${variant} 
      button-${size}
      ${isLoading ? 'button-loading' : ''} 
      ${className}
    `.trim();

    return (
      <button
        ref={ref}
        className={buttonClass}
        disabled={disabled || isLoading}
        {...props}
      >
        {isLoading && <span className="loading-spinner"></span>}
        {!isLoading && leftIcon && <span className="button-icon button-icon-left">{leftIcon}</span>}
        <span className="button-text">{children}</span>
        {!isLoading && rightIcon && <span className="button-icon button-icon-right">{rightIcon}</span>}
      </button>
    );
  }
);

Button.displayName = 'Button';

export default Button;
</file>

<file path="src-frontend/src/components/ui/index.ts">
export * from './Button';
export * from './Input';
</file>

<file path="src-frontend/src/components/ui/Input.css">
.input-wrapper {
  display: flex;
  flex-direction: column;
  margin-bottom: var(--spacing-md);
}

.input-label {
  font-size: var(--font-size-sm);
  font-weight: 500;
  margin-bottom: var(--spacing-xxs);
  color: var(--color-on-surface-variant);
}

.input-container {
  position: relative;
  display: flex;
  align-items: center;
}

.input-field {
  width: 100%;
  height: 2.5rem;
  padding: 0 var(--spacing-md);
  font-size: var(--font-size-md);
  background-color: var(--color-surface);
  color: var(--color-on-surface);
  border: 1px solid var(--color-border);
  border-radius: var(--radius-md);
  transition: all var(--transition-fast);
}

.input-field:focus {
  outline: none;
  border-color: var(--color-primary);
  box-shadow: 0 0 0 2px var(--color-primary-light);
}

.input-field::placeholder {
  color: var(--color-on-surface-variant);
  opacity: 0.7;
}

.input-icon {
  position: absolute;
  display: flex;
  align-items: center;
  justify-content: center;
  color: var(--color-on-surface-variant);
  pointer-events: none;
}

.input-icon-left {
  left: var(--spacing-md);
}

.input-icon-right {
  right: var(--spacing-md);
}

.input-with-left-icon {
  padding-left: 2.5rem;
}

.input-with-right-icon {
  padding-right: 2.5rem;
}

.input-helper-text {
  font-size: var(--font-size-xs);
  margin-top: var(--spacing-xxs);
  color: var(--color-on-surface-variant);
}

.input-error .input-field {
  border-color: var(--color-error);
}

.input-error .input-field:focus {
  box-shadow: 0 0 0 2px rgba(255, 59, 48, 0.2);
}

.input-error-text {
  color: var(--color-error);
}

.input-full-width {
  width: 100%;
}
</file>

<file path="src-frontend/src/components/ui/Input.tsx">
import React, { InputHTMLAttributes, forwardRef } from 'react';
import './Input.css';

export interface InputProps extends InputHTMLAttributes<HTMLInputElement> {
  label?: string;
  helperText?: string;
  error?: boolean;
  errorText?: string;
  leftIcon?: React.ReactNode;
  rightIcon?: React.ReactNode;
  fullWidth?: boolean;
}

export const Input = forwardRef<HTMLInputElement, InputProps>(
  (
    {
      label,
      helperText,
      error = false,
      errorText,
      leftIcon,
      rightIcon,
      fullWidth = false,
      className = '',
      id,
      ...props
    },
    ref
  ) => {
    // Generate a unique ID if not provided
    const inputId = id || `input-${Math.random().toString(36).substring(2, 9)}`;
    
    const inputClass = `
      input-wrapper
      ${error ? 'input-error' : ''}
      ${fullWidth ? 'input-full-width' : ''}
      ${className}
    `.trim();
    
    return (
      <div className={inputClass}>
        {label && (
          <label htmlFor={inputId} className="input-label">
            {label}
          </label>
        )}
        
        <div className="input-container">
          {leftIcon && <span className="input-icon input-icon-left">{leftIcon}</span>}
          
          <input
            ref={ref}
            id={inputId}
            className={`
              input-field
              ${leftIcon ? 'input-with-left-icon' : ''}
              ${rightIcon ? 'input-with-right-icon' : ''}
            `}
            aria-invalid={error}
            {...props}
          />
          
          {rightIcon && <span className="input-icon input-icon-right">{rightIcon}</span>}
        </div>
        
        {(helperText || (error && errorText)) && (
          <div className={`input-helper-text ${error ? 'input-error-text' : ''}`}>
            {error ? errorText : helperText}
          </div>
        )}
      </div>
    );
  }
);

Input.displayName = 'Input';

export default Input;
</file>

<file path="src-frontend/src/contexts/FeatureFlagContext.tsx">
import React, { createContext, useContext, useState, useEffect } from 'react';
import { invoke } from '@tauri-apps/api/tauri';

interface FeatureFlag {
  id: string;
  name: string;
  description: string;
  enabled: boolean;
  rollout_strategy: any;
  dependencies: string[];
  created_at: number;
  updated_at: number;
  metadata: Record<string, string>;
}

interface FeatureFlagContextType {
  flags: FeatureFlag[];
  enabledFlags: FeatureFlag[];
  isFeatureEnabled: (flagId: string) => boolean;
  getFlag: (flagId: string) => FeatureFlag | undefined;
  refreshFlags: () => Promise<void>;
  toggleFlag: (flagId: string, enabled: boolean) => Promise<void>;
  isLoading: boolean;
  error: string | null;
}

const FeatureFlagContext = createContext<FeatureFlagContextType>({
  flags: [],
  enabledFlags: [],
  isFeatureEnabled: () => false,
  getFlag: () => undefined,
  refreshFlags: async () => {},
  toggleFlag: async () => {},
  isLoading: true,
  error: null,
});

export const useFeatureFlags = () => useContext(FeatureFlagContext);

// Feature flag IDs
export const FLAG_ADVANCED_TELEMETRY = 'advanced_telemetry';
export const FLAG_PERFORMANCE_DASHBOARD = 'performance_dashboard';
export const FLAG_DEBUG_LOGGING = 'debug_logging';
export const FLAG_RESOURCE_MONITORING = 'resource_monitoring';
export const FLAG_CRASH_REPORTING = 'crash_reporting';

export const FeatureFlagProvider: React.FC<{ children: React.ReactNode }> = ({ children }) => {
  const [flags, setFlags] = useState<FeatureFlag[]>([]);
  const [isLoading, setIsLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);
  
  const fetchFlags = async () => {
    try {
      setIsLoading(true);
      const featureFlags = await invoke<FeatureFlag[]>('get_feature_flags');
      setFlags(featureFlags);
      setError(null);
    } catch (error: any) {
      console.error('Failed to fetch feature flags:', error);
      setError('Failed to fetch feature flags: ' + error.toString());
    } finally {
      setIsLoading(false);
    }
  };
  
  // Fetch flags on initial render
  useEffect(() => {
    fetchFlags();
    
    // Set up polling to refresh flags every 30 seconds
    const interval = setInterval(() => {
      fetchFlags();
    }, 30000);
    
    return () => clearInterval(interval);
  }, []);
  
  // Get enabled flags
  const enabledFlags = flags.filter(flag => flag.enabled);
  
  // Check if a feature is enabled
  const isFeatureEnabled = (flagId: string): boolean => {
    const flag = flags.find(f => f.id === flagId);
    return flag ? flag.enabled : false;
  };
  
  // Get flag by ID
  const getFlag = (flagId: string): FeatureFlag | undefined => {
    return flags.find(f => f.id === flagId);
  };
  
  // Toggle a feature flag
  const toggleFlag = async (flagId: string, enabled: boolean): Promise<void> => {
    try {
      await invoke('toggle_feature_flag', { flagId, enabled });
      
      // Update local state
      setFlags(flags.map(flag => 
        flag.id === flagId ? { ...flag, enabled } : flag
      ));
    } catch (error: any) {
      console.error('Failed to toggle feature flag:', error);
      throw new Error('Failed to toggle feature flag: ' + error.toString());
    }
  };
  
  return (
    <FeatureFlagContext.Provider
      value={{
        flags,
        enabledFlags,
        isFeatureEnabled,
        getFlag,
        refreshFlags: fetchFlags,
        toggleFlag,
        isLoading,
        error,
      }}
    >
      {children}
    </FeatureFlagContext.Provider>
  );
};

export default FeatureFlagContext;
</file>

<file path="src-frontend/src/disclosure/disclosure.css">
/* Progressive disclosure styles */

/* Level progress indicator */
.level-progress {
  margin-bottom: var(--spacing-md);
}

.level-info {
  display: flex;
  justify-content: space-between;
  margin-bottom: var(--spacing-xxs);
  font-size: var(--font-size-sm);
}

.user-level {
  font-weight: 500;
  color: var(--color-on-surface);
}

.user-points {
  color: var(--color-on-surface-variant);
}

.progress-bar-container {
  position: relative;
  height: 8px;
  background-color: var(--color-surface-variant);
  border-radius: var(--radius-full);
  overflow: hidden;
}

.progress-bar-fill {
  position: absolute;
  top: 0;
  left: 0;
  height: 100%;
  background-color: var(--color-primary);
  transition: width 0.6s var(--animation-easing-spring);
  border-radius: var(--radius-full);
}

.progress-label {
  position: absolute;
  top: 100%;
  right: 0;
  font-size: var(--font-size-xs);
  color: var(--color-on-surface-variant);
  margin-top: var(--spacing-xxs);
}

/* Advanced features toggle */
.advanced-features-toggle {
  display: flex;
  align-items: center;
  margin-bottom: var(--spacing-md);
  cursor: pointer;
}

.advanced-features-toggle input {
  margin-right: var(--spacing-xs);
}

.toggle-label {
  font-size: var(--font-size-sm);
  color: var(--color-on-surface);
}

/* Level up notification */
.level-up-notification {
  position: fixed;
  top: var(--spacing-lg);
  right: var(--spacing-lg);
  padding: var(--spacing-md);
  background-color: var(--color-primary);
  color: white;
  border-radius: var(--radius-md);
  box-shadow: var(--shadow-lg);
  z-index: var(--z-tooltip);
  max-width: 300px;
  transition: all 0.3s var(--animation-easing-standard);
}

.level-up-notification h3 {
  margin: 0 0 var(--spacing-xs) 0;
  font-size: var(--font-size-lg);
}

.level-up-notification p {
  margin: 0;
  font-size: var(--font-size-md);
}

.level-up-notification-close {
  position: absolute;
  top: var(--spacing-xs);
  right: var(--spacing-xs);
  background: none;
  border: none;
  color: white;
  font-size: 1.25rem;
  line-height: 1;
  cursor: pointer;
  opacity: 0.8;
  transition: opacity 0.2s var(--animation-easing-standard);
}

.level-up-notification-close:hover {
  opacity: 1;
}

/* Animation classes */
.notification-enter {
  animation: slideInRight 0.3s var(--animation-easing-spring) forwards;
}

.notification-exit {
  animation: slideOutRight 0.3s var(--animation-easing-accelerate) forwards;
}

/* Feature level badges */
.feature-level-badge {
  display: inline-block;
  font-size: var(--font-size-xs);
  padding: 2px var(--spacing-xs);
  border-radius: var(--radius-sm);
  margin-left: var(--spacing-xs);
  font-weight: 500;
}

.feature-level-badge.basic {
  background-color: #88cc88;
  color: #006600;
}

.feature-level-badge.intermediate {
  background-color: #88aaff;
  color: #0033aa;
}

.feature-level-badge.advanced {
  background-color: #cc88cc;
  color: #660066;
}

.feature-level-badge.expert {
  background-color: #ffaa44;
  color: #884400;
}

/* Locked feature indicator */
.locked-feature {
  opacity: 0.6;
  position: relative;
  pointer-events: none;
}

.locked-feature::after {
  content: '🔒';
  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  font-size: 2rem;
  pointer-events: none;
}

.locked-feature-message {
  padding: var(--spacing-md);
  background-color: var(--color-surface-variant);
  border: 1px dashed var(--color-border);
  border-radius: var(--radius-md);
  text-align: center;
  color: var(--color-on-surface-variant);
  font-size: var(--font-size-sm);
}

.locked-feature-message strong {
  color: var(--color-primary);
}

/* Feature discovery tooltip */
.feature-discovery {
  position: relative;
  display: inline-block;
}

.feature-discovery-tooltip {
  position: absolute;
  bottom: 100%;
  left: 50%;
  transform: translateX(-50%);
  margin-bottom: var(--spacing-xs);
  background-color: var(--color-primary);
  color: white;
  padding: var(--spacing-xs) var(--spacing-sm);
  border-radius: var(--radius-md);
  font-size: var(--font-size-xs);
  box-shadow: var(--shadow-md);
  white-space: nowrap;
  pointer-events: none;
  animation: bounce 1s infinite;
  z-index: var(--z-tooltip);
}

.feature-discovery-tooltip::after {
  content: '';
  position: absolute;
  top: 100%;
  left: 50%;
  transform: translateX(-50%);
  border-width: 5px;
  border-style: solid;
  border-color: var(--color-primary) transparent transparent transparent;
}

/* Feature onboarding list */
.feature-onboarding-list {
  list-style: none;
  margin: 0 0 var(--spacing-lg) 0;
  padding: 0;
}

.feature-onboarding-item {
  display: flex;
  align-items: flex-start;
  padding: var(--spacing-sm) 0;
  border-bottom: 1px solid var(--color-divider);
}

.feature-onboarding-item:last-child {
  border-bottom: none;
}

.feature-onboarding-check {
  margin-right: var(--spacing-sm);
  flex-shrink: 0;
  width: 20px;
  height: 20px;
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  color: white;
  font-size: var(--font-size-xs);
}

.feature-onboarding-check.completed {
  background-color: var(--color-success);
}

.feature-onboarding-check.pending {
  background-color: var(--color-on-surface-variant);
  opacity: 0.5;
}

.feature-onboarding-content {
  flex: 1;
}

.feature-onboarding-title {
  font-weight: 500;
  margin-bottom: var(--spacing-xxs);
  color: var(--color-on-surface);
}

.feature-onboarding-description {
  font-size: var(--font-size-sm);
  color: var(--color-on-surface-variant);
  margin: 0;
}

.feature-onboarding-points {
  margin-left: var(--spacing-sm);
  font-size: var(--font-size-xs);
  color: var(--color-primary);
  font-weight: 500;
  display: flex;
  align-items: center;
}

.feature-onboarding-points::before {
  content: '+';
  margin-right: 2px;
}
</file>

<file path="src-frontend/src/disclosure/FeatureBadge.tsx">
import React from 'react';
import { FeatureLevel } from './ProgressiveDisclosure';

interface FeatureBadgeProps {
  level: FeatureLevel;
  className?: string;
}

const FeatureBadge: React.FC<FeatureBadgeProps> = ({ level, className = '' }) => {
  return (
    <span className={`feature-level-badge ${level} ${className}`}>
      {level.charAt(0).toUpperCase() + level.slice(1)}
    </span>
  );
};

export default FeatureBadge;
</file>

<file path="src-frontend/src/disclosure/FeatureDiscovery.tsx">
import React, { useState, useEffect } from 'react';

interface FeatureDiscoveryProps {
  message: string;
  showCount?: number;
  localStorageKey?: string;
  children: React.ReactNode;
}

const FeatureDiscovery: React.FC<FeatureDiscoveryProps> = ({
  message,
  showCount = 3,
  localStorageKey,
  children,
}) => {
  const [showTooltip, setShowTooltip] = useState(false);
  
  useEffect(() => {
    // If a localStorage key is provided, use it to check whether to show the tooltip
    if (localStorageKey) {
      const viewCount = parseInt(localStorage.getItem(localStorageKey) || '0', 10);
      
      if (viewCount < showCount) {
        setShowTooltip(true);
        localStorage.setItem(localStorageKey, String(viewCount + 1));
      }
    } else {
      // If no key is provided, always show the tooltip
      setShowTooltip(true);
    }
    
    // Hide the tooltip after 5 seconds
    const timer = setTimeout(() => {
      setShowTooltip(false);
    }, 5000);
    
    return () => {
      clearTimeout(timer);
    };
  }, [localStorageKey, showCount]);
  
  return (
    <div className="feature-discovery">
      {children}
      
      {showTooltip && (
        <div className="feature-discovery-tooltip">
          {message}
        </div>
      )}
    </div>
  );
};

export default FeatureDiscovery;
</file>

<file path="src-frontend/src/disclosure/index.ts">
export * from './ProgressiveDisclosure';
export { default as FeatureBadge } from './FeatureBadge';
export { default as LockedFeatureMessage } from './LockedFeatureMessage';
export { default as FeatureDiscovery } from './FeatureDiscovery';
</file>

<file path="src-frontend/src/disclosure/LockedFeatureMessage.tsx">
import React from 'react';
import { FeatureLevel } from './ProgressiveDisclosure';

interface LockedFeatureMessageProps {
  level: FeatureLevel;
  className?: string;
}

const LockedFeatureMessage: React.FC<LockedFeatureMessageProps> = ({ level, className = '' }) => {
  return (
    <div className={`locked-feature-message ${className}`}>
      This feature is available at the <strong>{level}</strong> level.
      <br />
      Continue using the app to unlock it!
    </div>
  );
};

export default LockedFeatureMessage;
</file>

<file path="src-frontend/src/disclosure/ProgressiveDisclosure.tsx">
import React, { createContext, useContext, useState, useEffect, ReactNode } from 'react';
import './disclosure.css';

// Feature level types
export type FeatureLevel = 'basic' | 'intermediate' | 'advanced' | 'expert';

// Interface for the context
interface DisclosureContextType {
  userLevel: FeatureLevel;
  setUserLevel: (level: FeatureLevel) => void;
  isFeatureVisible: (requiredLevel: FeatureLevel) => boolean;
  userPoints: number;
  addPoints: (points: number) => void;
  resetPoints: () => void;
  getNextLevelThreshold: () => number;
  getNextLevelProgress: () => number;
  showAdvancedFeatures: boolean;
  toggleAdvancedFeatures: () => void;
}

// Feature level thresholds (points needed to unlock)
const LEVEL_THRESHOLDS = {
  basic: 0,
  intermediate: 100,
  advanced: 500,
  expert: 1500,
};

// Context
const DisclosureContext = createContext<DisclosureContextType>({
  userLevel: 'basic',
  setUserLevel: () => {},
  isFeatureVisible: () => false,
  userPoints: 0,
  addPoints: () => {},
  resetPoints: () => {},
  getNextLevelThreshold: () => 0,
  getNextLevelProgress: () => 0,
  showAdvancedFeatures: false,
  toggleAdvancedFeatures: () => {},
});

export const useDisclosure = () => useContext(DisclosureContext);

interface ProgressiveDisclosureProviderProps {
  children: ReactNode;
}

export const ProgressiveDisclosureProvider: React.FC<ProgressiveDisclosureProviderProps> = ({ children }) => {
  // Load initial user level and points from localStorage
  const [userLevel, setUserLevel] = useState<FeatureLevel>(() => {
    const savedLevel = localStorage.getItem('mcp-user-level');
    return (savedLevel as FeatureLevel) || 'basic';
  });
  
  const [userPoints, setUserPoints] = useState(() => {
    const savedPoints = localStorage.getItem('mcp-user-points');
    return savedPoints ? parseInt(savedPoints, 10) : 0;
  });
  
  // Advanced feature override
  const [showAdvancedFeatures, setShowAdvancedFeatures] = useState(() => {
    const savedPref = localStorage.getItem('mcp-show-advanced');
    return savedPref === 'true';
  });
  
  // Save state to localStorage when it changes
  useEffect(() => {
    localStorage.setItem('mcp-user-level', userLevel);
  }, [userLevel]);
  
  useEffect(() => {
    localStorage.setItem('mcp-user-points', String(userPoints));
  }, [userPoints]);
  
  useEffect(() => {
    localStorage.setItem('mcp-show-advanced', String(showAdvancedFeatures));
  }, [showAdvancedFeatures]);
  
  // Check if a feature is visible based on user level
  const isFeatureVisible = (requiredLevel: FeatureLevel): boolean => {
    // If the advanced features toggle is on, show everything
    if (showAdvancedFeatures) return true;
    
    // Otherwise, check the user's level
    const levels: FeatureLevel[] = ['basic', 'intermediate', 'advanced', 'expert'];
    const userLevelIndex = levels.indexOf(userLevel);
    const requiredLevelIndex = levels.indexOf(requiredLevel);
    
    return userLevelIndex >= requiredLevelIndex;
  };
  
  // Add points to the user's score and potentially level up
  const addPoints = (points: number) => {
    setUserPoints(prev => {
      const newPoints = prev + points;
      
      // Check for level up
      if (
        userLevel === 'basic' && newPoints >= LEVEL_THRESHOLDS.intermediate ||
        userLevel === 'intermediate' && newPoints >= LEVEL_THRESHOLDS.advanced ||
        userLevel === 'advanced' && newPoints >= LEVEL_THRESHOLDS.expert
      ) {
        // Determine new level
        let newLevel: FeatureLevel = 'basic';
        if (newPoints >= LEVEL_THRESHOLDS.expert) {
          newLevel = 'expert';
        } else if (newPoints >= LEVEL_THRESHOLDS.advanced) {
          newLevel = 'advanced';
        } else if (newPoints >= LEVEL_THRESHOLDS.intermediate) {
          newLevel = 'intermediate';
        }
        
        // If level changed, show level up notification
        if (newLevel !== userLevel) {
          setUserLevel(newLevel);
          showLevelUpNotification(newLevel);
        }
      }
      
      return newPoints;
    });
  };
  
  // Reset user points
  const resetPoints = () => {
    setUserPoints(0);
    setUserLevel('basic');
  };
  
  // Get threshold for next level
  const getNextLevelThreshold = (): number => {
    switch (userLevel) {
      case 'basic':
        return LEVEL_THRESHOLDS.intermediate;
      case 'intermediate':
        return LEVEL_THRESHOLDS.advanced;
      case 'advanced':
        return LEVEL_THRESHOLDS.expert;
      case 'expert':
        return LEVEL_THRESHOLDS.expert; // Already at max level
    }
  };
  
  // Get progress to next level (0-100)
  const getNextLevelProgress = (): number => {
    if (userLevel === 'expert') return 100;
    
    const currentThreshold = LEVEL_THRESHOLDS[userLevel];
    const nextThreshold = getNextLevelThreshold();
    
    const pointsInLevel = userPoints - currentThreshold;
    const pointsNeededForNextLevel = nextThreshold - currentThreshold;
    
    return Math.min(100, Math.round((pointsInLevel / pointsNeededForNextLevel) * 100));
  };
  
  // Toggle showing advanced features
  const toggleAdvancedFeatures = () => {
    setShowAdvancedFeatures(prev => !prev);
  };
  
  // Show level up notification
  const showLevelUpNotification = (newLevel: FeatureLevel) => {
    // Create a level up notification element
    const notification = document.createElement('div');
    notification.className = 'level-up-notification';
    
    const title = document.createElement('h3');
    title.textContent = `Level Up!`;
    
    const message = document.createElement('p');
    message.textContent = `You've reached the ${newLevel} level. New features are now available!`;
    
    const closeButton = document.createElement('button');
    closeButton.textContent = '×';
    closeButton.className = 'level-up-notification-close';
    closeButton.addEventListener('click', () => {
      document.body.removeChild(notification);
    });
    
    notification.appendChild(closeButton);
    notification.appendChild(title);
    notification.appendChild(message);
    
    // Add animation class
    notification.classList.add('notification-enter');
    
    // Add to document
    document.body.appendChild(notification);
    
    // Auto-remove after 5 seconds
    setTimeout(() => {
      notification.classList.add('notification-exit');
      setTimeout(() => {
        if (document.body.contains(notification)) {
          document.body.removeChild(notification);
        }
      }, 300);
    }, 5000);
  };
  
  return (
    <DisclosureContext.Provider
      value={{
        userLevel,
        setUserLevel,
        isFeatureVisible,
        userPoints,
        addPoints,
        resetPoints,
        getNextLevelThreshold,
        getNextLevelProgress,
        showAdvancedFeatures,
        toggleAdvancedFeatures,
      }}
    >
      {children}
    </DisclosureContext.Provider>
  );
};

// Component to conditionally render based on feature level
interface ProgressiveFeatureProps {
  level: FeatureLevel;
  children: ReactNode;
  fallback?: ReactNode;
}

export const ProgressiveFeature: React.FC<ProgressiveFeatureProps> = ({
  level,
  children,
  fallback,
}) => {
  const { isFeatureVisible } = useDisclosure();
  
  const isVisible = isFeatureVisible(level);
  
  if (isVisible) {
    return <>{children}</>;
  }
  
  return fallback ? <>{fallback}</> : null;
};

// Progress indicator for profile
export const LevelProgressIndicator: React.FC = () => {
  const { userLevel, userPoints, getNextLevelProgress, getNextLevelThreshold } = useDisclosure();
  
  const progress = getNextLevelProgress();
  const nextThreshold = getNextLevelThreshold();
  const isMaxLevel = userLevel === 'expert';
  
  return (
    <div className="level-progress">
      <div className="level-info">
        <span className="user-level">
          Level: <strong>{userLevel}</strong>
        </span>
        <span className="user-points">
          {userPoints} points
        </span>
      </div>
      
      <div className="progress-bar-container">
        <div 
          className="progress-bar-fill"
          style={{ width: `${progress}%` }}
        />
        <span className="progress-label">
          {isMaxLevel 
            ? 'Max Level Reached!' 
            : `${progress}% to ${nextThreshold} points`}
        </span>
      </div>
    </div>
  );
};

// Advanced features toggle
export const AdvancedFeaturesToggle: React.FC = () => {
  const { showAdvancedFeatures, toggleAdvancedFeatures } = useDisclosure();
  
  return (
    <label className="advanced-features-toggle">
      <input
        type="checkbox"
        checked={showAdvancedFeatures}
        onChange={toggleAdvancedFeatures}
      />
      <span className="toggle-label">Show all advanced features</span>
    </label>
  );
};

export default ProgressiveDisclosureProvider;
</file>

<file path="src-frontend/src/help/help.css">
/* Help system styles */

/* Help Panel */
.help-panel-overlay {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.5);
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: var(--z-modal);
  animation: fadeIn var(--animation-duration-fast) var(--animation-easing-standard);
}

.help-panel {
  width: 90%;
  max-width: 1200px;
  height: 80vh;
  background-color: var(--color-surface);
  border-radius: var(--radius-lg);
  box-shadow: var(--shadow-lg);
  display: flex;
  overflow: hidden;
  animation: scaleIn var(--animation-duration-normal) var(--animation-easing-spring);
}

.help-panel-sidebar {
  width: 300px;
  border-right: 1px solid var(--color-border);
  display: flex;
  flex-direction: column;
  overflow: hidden;
  background-color: var(--color-surface-variant);
}

.help-panel-search {
  padding: var(--spacing-md);
  border-bottom: 1px solid var(--color-border);
}

.help-panel-search-input {
  width: 100%;
  height: 2.5rem;
  padding: 0 var(--spacing-md);
  font-size: var(--font-size-md);
  background-color: var(--color-background);
  color: var(--color-on-background);
  border: 1px solid var(--color-border);
  border-radius: var(--radius-md);
  transition: all var(--transition-fast);
}

.help-panel-search-input:focus {
  outline: none;
  border-color: var(--color-primary);
  box-shadow: 0 0 0 2px var(--color-primary-light);
}

.help-panel-nav {
  flex: 1;
  overflow-y: auto;
  padding: var(--spacing-md);
}

.help-panel-category {
  margin-bottom: var(--spacing-lg);
}

.help-panel-category:last-child {
  margin-bottom: 0;
}

.help-panel-category-title {
  font-size: var(--font-size-md);
  font-weight: 600;
  margin: 0 0 var(--spacing-sm) 0;
  color: var(--color-on-surface-variant);
}

.help-panel-topic-list {
  list-style: none;
  margin: 0;
  padding: 0;
}

.help-panel-topic-item {
  margin-bottom: var(--spacing-xxs);
}

.help-panel-topic-button {
  display: block;
  width: 100%;
  text-align: left;
  padding: var(--spacing-xs) var(--spacing-sm);
  background: none;
  border: none;
  border-radius: var(--radius-md);
  font-size: var(--font-size-sm);
  color: var(--color-on-surface);
  cursor: pointer;
  transition: all var(--transition-fast);
}

.help-panel-topic-button:hover {
  background-color: var(--color-surface);
}

.help-panel-topic-button.active {
  background-color: var(--color-primary);
  color: white;
  font-weight: 500;
}

.help-panel-search-results {
  margin-bottom: var(--spacing-lg);
}

.help-panel-no-results {
  font-size: var(--font-size-sm);
  color: var(--color-on-surface-variant);
  font-style: italic;
  padding: var(--spacing-xs) var(--spacing-sm);
}

.help-panel-content {
  flex: 1;
  display: flex;
  flex-direction: column;
  overflow: hidden;
}

.help-panel-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: var(--spacing-md) var(--spacing-lg);
  border-bottom: 1px solid var(--color-border);
}

.help-panel-title {
  margin: 0;
  font-size: var(--font-size-xl);
  color: var(--color-on-surface);
}

.help-panel-close {
  background: none;
  border: none;
  font-size: 1.5rem;
  line-height: 1;
  padding: var(--spacing-xs);
  cursor: pointer;
  color: var(--color-on-surface-variant);
  transition: color var(--transition-fast) var(--animation-easing-standard);
}

.help-panel-close:hover {
  color: var(--color-on-surface);
}

.help-panel-body {
  flex: 1;
  padding: var(--spacing-lg);
  overflow-y: auto;
}

.help-panel-welcome {
  text-align: center;
  max-width: 600px;
  margin: 0 auto;
  padding: var(--spacing-xl) 0;
}

.help-panel-welcome h3 {
  font-size: var(--font-size-xl);
  margin-bottom: var(--spacing-md);
  color: var(--color-on-surface);
}

.help-panel-welcome p {
  font-size: var(--font-size-md);
  color: var(--color-on-surface-variant);
  line-height: 1.6;
}

.help-topic-content {
  font-size: var(--font-size-md);
  line-height: 1.6;
  color: var(--color-on-surface);
}

.help-topic-content h1,
.help-topic-content h2,
.help-topic-content h3 {
  margin-top: var(--spacing-lg);
  margin-bottom: var(--spacing-sm);
  color: var(--color-on-surface);
}

.help-topic-content h1 {
  font-size: var(--font-size-2xl);
  border-bottom: 1px solid var(--color-border);
  padding-bottom: var(--spacing-sm);
}

.help-topic-content h2 {
  font-size: var(--font-size-xl);
}

.help-topic-content h3 {
  font-size: var(--font-size-lg);
}

.help-topic-content p {
  margin-bottom: var(--spacing-md);
}

.help-topic-content ul,
.help-topic-content ol {
  margin-bottom: var(--spacing-md);
  padding-left: var(--spacing-lg);
}

.help-topic-content li {
  margin-bottom: var(--spacing-xs);
}

.help-topic-content code {
  background-color: var(--color-surface-variant);
  padding: 2px var(--spacing-xxs);
  border-radius: var(--radius-sm);
  font-family: monospace;
}

.help-topic-content pre {
  background-color: var(--color-surface-variant);
  padding: var(--spacing-md);
  border-radius: var(--radius-md);
  overflow-x: auto;
  margin-bottom: var(--spacing-md);
}

.help-topic-content pre code {
  background-color: transparent;
  padding: 0;
}

.help-topic-content blockquote {
  border-left: 4px solid var(--color-primary);
  padding-left: var(--spacing-md);
  margin-left: 0;
  margin-right: 0;
  font-style: italic;
  color: var(--color-on-surface-variant);
}

.help-topic-content img {
  max-width: 100%;
  border-radius: var(--radius-md);
  margin-bottom: var(--spacing-md);
}

.help-topic-content table {
  width: 100%;
  border-collapse: collapse;
  margin-bottom: var(--spacing-md);
}

.help-topic-content th,
.help-topic-content td {
  padding: var(--spacing-sm);
  border: 1px solid var(--color-border);
  text-align: left;
}

.help-topic-content th {
  background-color: var(--color-surface-variant);
  font-weight: 600;
}

.help-topic-related {
  margin-top: var(--spacing-xl);
  padding-top: var(--spacing-md);
  border-top: 1px solid var(--color-border);
}

.help-topic-related h3 {
  font-size: var(--font-size-md);
  margin-bottom: var(--spacing-sm);
  color: var(--color-on-surface);
}

.help-topic-related-list {
  list-style: none;
  margin: 0;
  padding: 0;
  display: flex;
  flex-wrap: wrap;
  gap: var(--spacing-xs);
}

.help-topic-related-button {
  display: inline-block;
  padding: var(--spacing-xs) var(--spacing-sm);
  background-color: var(--color-surface-variant);
  border: 1px solid var(--color-border);
  border-radius: var(--radius-md);
  font-size: var(--font-size-sm);
  color: var(--color-on-surface);
  cursor: pointer;
  transition: all var(--transition-fast);
}

.help-topic-related-button:hover {
  background-color: var(--color-primary);
  color: white;
  border-color: var(--color-primary);
}

/* Explanation Tooltip */
.explanation-tooltip {
  position: absolute;
  z-index: var(--z-tooltip);
  width: 320px;
  transform: translate(-50%, 0);
  pointer-events: auto;
  background-color: var(--color-surface);
  border: 1px solid var(--color-border);
  border-radius: var(--radius-lg);
  box-shadow: var(--shadow-lg);
  animation: scaleIn var(--animation-duration-normal) var(--animation-easing-spring);
}

.explanation-tooltip::before {
  content: '';
  position: absolute;
  top: -8px;
  left: 50%;
  transform: translateX(-50%) rotate(45deg);
  width: 16px;
  height: 16px;
  background-color: var(--color-surface);
  border-left: 1px solid var(--color-border);
  border-top: 1px solid var(--color-border);
}

.explanation-tooltip-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: var(--spacing-sm) var(--spacing-md);
  border-bottom: 1px solid var(--color-border);
}

.explanation-tooltip-title {
  margin: 0;
  font-size: var(--font-size-md);
  color: var(--color-on-surface);
}

.explanation-tooltip-close {
  background: none;
  border: none;
  font-size: 1.25rem;
  line-height: 1;
  padding: 0;
  cursor: pointer;
  color: var(--color-on-surface-variant);
  transition: color var(--transition-fast) var(--animation-easing-standard);
}

.explanation-tooltip-close:hover {
  color: var(--color-on-surface);
}

.explanation-tooltip-content {
  padding: var(--spacing-md);
  font-size: var(--font-size-sm);
  line-height: 1.5;
  color: var(--color-on-surface);
}

/* Inline Help */
.inline-help-overlay {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  z-index: var(--z-dropdown);
  pointer-events: none;
}

.inline-help-icon {
  position: absolute;
  width: 20px;
  height: 20px;
  background-color: var(--color-primary);
  color: white;
  border: none;
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: var(--font-size-xs);
  font-weight: bold;
  cursor: pointer;
  pointer-events: auto;
  z-index: var(--z-dropdown);
  box-shadow: var(--shadow-sm);
  animation: pulse 2s infinite;
}

.inline-help-icon:hover {
  animation: none;
  transform: scale(1.2);
}

/* Help button */
.help-button {
  display: inline-flex;
  align-items: center;
  gap: var(--spacing-xs);
  padding: var(--spacing-xs) var(--spacing-sm);
  background-color: var(--color-surface-variant);
  color: var(--color-on-surface);
  border: 1px solid var(--color-border);
  border-radius: var(--radius-md);
  font-size: var(--font-size-sm);
  cursor: pointer;
  transition: all var(--transition-fast);
}

.help-button:hover {
  background-color: var(--color-surface);
  border-color: var(--color-primary);
}

.help-button svg {
  width: 16px;
  height: 16px;
}

/* Help trigger for individual elements */
.help-trigger {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  width: 16px;
  height: 16px;
  background-color: var(--color-surface-variant);
  color: var(--color-on-surface-variant);
  border: 1px solid var(--color-border);
  border-radius: 50%;
  font-size: var(--font-size-xs);
  cursor: pointer;
  margin-left: var(--spacing-xs);
  transition: all var(--transition-fast);
}

.help-trigger:hover {
  background-color: var(--color-primary);
  color: white;
  border-color: var(--color-primary);
}
</file>

<file path="src-frontend/src/help/HelpButton.tsx">
import React from 'react';
import { useHelp } from './HelpProvider';

interface HelpButtonProps {
  className?: string;
}

const HelpButton: React.FC<HelpButtonProps> = ({ className = '' }) => {
  const { openHelpPanel } = useHelp();
  
  return (
    <button
      className={`help-button ${className}`}
      onClick={openHelpPanel}
      aria-label="Open help center"
    >
      <svg
        xmlns="http://www.w3.org/2000/svg"
        viewBox="0 0 24 24"
        fill="none"
        stroke="currentColor"
        strokeWidth="2"
        strokeLinecap="round"
        strokeLinejoin="round"
      >
        <circle cx="12" cy="12" r="10" />
        <path d="M9.09 9a3 3 0 0 1 5.83 1c0 2-3 3-3 3" />
        <line x1="12" y1="17" x2="12.01" y2="17" />
      </svg>
      Help Center
    </button>
  );
};

export default HelpButton;
</file>

<file path="src-frontend/src/help/HelpProvider.tsx">
import React, { createContext, useContext, useState, ReactNode } from 'react';
import './help.css';

// Help topic structure
export interface HelpTopic {
  id: string;
  title: string;
  content: string;
  category: string;
  keywords: string[];
  related?: string[];
}

// Explanation structure for inline help
export interface Explanation {
  id: string;
  title: string;
  content: string;
  targetSelector?: string;
}

// Context interface
interface HelpContextType {
  // Topics management
  topics: HelpTopic[];
  explanations: Explanation[];
  registerTopic: (topic: HelpTopic) => void;
  registerExplanation: (explanation: Explanation) => void;
  
  // Help panel controls
  isHelpPanelOpen: boolean;
  openHelpPanel: () => void;
  closeHelpPanel: () => void;
  openTopicInPanel: (topicId: string) => void;
  
  // Active help panel state
  activeTopicId: string | null;
  searchQuery: string;
  setSearchQuery: (query: string) => void;
  searchResults: HelpTopic[];
  
  // Inline help tooltips
  enableInlineHelp: boolean;
  toggleInlineHelp: () => void;
  activeExplanation: Explanation | null;
  showExplanation: (id: string) => void;
  hideExplanation: () => void;
}

const HelpContext = createContext<HelpContextType>({
  topics: [],
  explanations: [],
  registerTopic: () => {},
  registerExplanation: () => {},
  
  isHelpPanelOpen: false,
  openHelpPanel: () => {},
  closeHelpPanel: () => {},
  openTopicInPanel: () => {},
  
  activeTopicId: null,
  searchQuery: '',
  setSearchQuery: () => {},
  searchResults: [],
  
  enableInlineHelp: false,
  toggleInlineHelp: () => {},
  activeExplanation: null,
  showExplanation: () => {},
  hideExplanation: () => {},
});

export const useHelp = () => useContext(HelpContext);

interface HelpProviderProps {
  children: ReactNode;
}

export const HelpProvider: React.FC<HelpProviderProps> = ({ children }) => {
  // Help content
  const [topics, setTopics] = useState<HelpTopic[]>([]);
  const [explanations, setExplanations] = useState<Explanation[]>([]);
  
  // Help panel state
  const [isHelpPanelOpen, setIsHelpPanelOpen] = useState(false);
  const [activeTopicId, setActiveTopicId] = useState<string | null>(null);
  
  // Search
  const [searchQuery, setSearchQuery] = useState('');
  
  // Inline help state
  const [enableInlineHelp, setEnableInlineHelp] = useState(() => {
    const savedPref = localStorage.getItem('mcp-enable-inline-help');
    return savedPref !== null ? savedPref === 'true' : false;
  });
  const [activeExplanation, setActiveExplanation] = useState<Explanation | null>(null);
  
  // Register a new help topic
  const registerTopic = (topic: HelpTopic) => {
    setTopics(prev => {
      // Check if topic already exists
      const exists = prev.some(t => t.id === topic.id);
      if (exists) {
        return prev.map(t => t.id === topic.id ? topic : t);
      }
      return [...prev, topic];
    });
  };
  
  // Register an explanation for inline help
  const registerExplanation = (explanation: Explanation) => {
    setExplanations(prev => {
      // Check if explanation already exists
      const exists = prev.some(e => e.id === explanation.id);
      if (exists) {
        return prev.map(e => e.id === explanation.id ? explanation : e);
      }
      return [...prev, explanation];
    });
  };
  
  // Help panel controls
  const openHelpPanel = () => {
    setIsHelpPanelOpen(true);
  };
  
  const closeHelpPanel = () => {
    setIsHelpPanelOpen(false);
  };
  
  const openTopicInPanel = (topicId: string) => {
    setActiveTopicId(topicId);
    openHelpPanel();
  };
  
  // Toggle inline help
  const toggleInlineHelp = () => {
    setEnableInlineHelp(prev => {
      const newValue = !prev;
      localStorage.setItem('mcp-enable-inline-help', String(newValue));
      return newValue;
    });
  };
  
  // Show/hide explanations
  const showExplanation = (id: string) => {
    const explanation = explanations.find(e => e.id === id);
    if (explanation) {
      setActiveExplanation(explanation);
    }
  };
  
  const hideExplanation = () => {
    setActiveExplanation(null);
  };
  
  // Filter topics based on search query
  const searchResults = searchQuery.trim() === '' 
    ? [] 
    : topics.filter(topic => {
        const query = searchQuery.toLowerCase();
        return (
          topic.title.toLowerCase().includes(query) ||
          topic.content.toLowerCase().includes(query) ||
          topic.keywords.some(keyword => keyword.toLowerCase().includes(query))
        );
      });
  
  // Get active topic
  const activeTopic = activeTopicId
    ? topics.find(topic => topic.id === activeTopicId)
    : null;
  
  return (
    <HelpContext.Provider
      value={{
        topics,
        explanations,
        registerTopic,
        registerExplanation,
        
        isHelpPanelOpen,
        openHelpPanel,
        closeHelpPanel,
        openTopicInPanel,
        
        activeTopicId,
        searchQuery,
        setSearchQuery,
        searchResults,
        
        enableInlineHelp,
        toggleInlineHelp,
        activeExplanation,
        showExplanation,
        hideExplanation,
      }}
    >
      {children}
      {isHelpPanelOpen && (
        <HelpPanel activeTopic={activeTopic} onClose={closeHelpPanel} />
      )}
      {activeExplanation && <ExplanationTooltip />}
      {enableInlineHelp && <InlineHelpOverlay />}
    </HelpContext.Provider>
  );
};

// Help panel component
interface HelpPanelProps {
  activeTopic: HelpTopic | null;
  onClose: () => void;
}

const HelpPanel: React.FC<HelpPanelProps> = ({ activeTopic, onClose }) => {
  const { topics, searchQuery, setSearchQuery, searchResults, openTopicInPanel } = useHelp();
  
  // Group topics by category for sidebar
  const categories = topics.reduce<Record<string, HelpTopic[]>>((acc, topic) => {
    if (!acc[topic.category]) {
      acc[topic.category] = [];
    }
    acc[topic.category].push(topic);
    return acc;
  }, {});
  
  return (
    <div className="help-panel-overlay" onClick={onClose}>
      <div className="help-panel" onClick={e => e.stopPropagation()}>
        <div className="help-panel-sidebar">
          <div className="help-panel-search">
            <input
              type="text"
              placeholder="Search help topics..."
              value={searchQuery}
              onChange={e => setSearchQuery(e.target.value)}
              className="help-panel-search-input"
            />
          </div>
          
          <div className="help-panel-nav">
            {searchQuery.trim() !== '' ? (
              <div className="help-panel-search-results">
                <h3 className="help-panel-category-title">Search Results</h3>
                
                {searchResults.length > 0 ? (
                  <ul className="help-panel-topic-list">
                    {searchResults.map(topic => (
                      <li key={topic.id} className="help-panel-topic-item">
                        <button
                          className={`help-panel-topic-button ${
                            activeTopic?.id === topic.id ? 'active' : ''
                          }`}
                          onClick={() => openTopicInPanel(topic.id)}
                        >
                          {topic.title}
                        </button>
                      </li>
                    ))}
                  </ul>
                ) : (
                  <p className="help-panel-no-results">No results found</p>
                )}
              </div>
            ) : (
              <>
                {Object.entries(categories).map(([category, categoryTopics]) => (
                  <div key={category} className="help-panel-category">
                    <h3 className="help-panel-category-title">{category}</h3>
                    <ul className="help-panel-topic-list">
                      {categoryTopics.map(topic => (
                        <li key={topic.id} className="help-panel-topic-item">
                          <button
                            className={`help-panel-topic-button ${
                              activeTopic?.id === topic.id ? 'active' : ''
                            }`}
                            onClick={() => openTopicInPanel(topic.id)}
                          >
                            {topic.title}
                          </button>
                        </li>
                      ))}
                    </ul>
                  </div>
                ))}
              </>
            )}
          </div>
        </div>
        
        <div className="help-panel-content">
          <div className="help-panel-header">
            <h2 className="help-panel-title">
              {activeTopic ? activeTopic.title : 'Help Center'}
            </h2>
            <button className="help-panel-close" onClick={onClose}>
              &times;
            </button>
          </div>
          
          <div className="help-panel-body">
            {activeTopic ? (
              <>
                <div 
                  className="help-topic-content"
                  dangerouslySetInnerHTML={{ __html: activeTopic.content }}
                />
                
                {activeTopic.related && activeTopic.related.length > 0 && (
                  <div className="help-topic-related">
                    <h3>Related Topics</h3>
                    <ul className="help-topic-related-list">
                      {activeTopic.related.map(relatedId => {
                        const relatedTopic = topics.find(t => t.id === relatedId);
                        if (!relatedTopic) return null;
                        
                        return (
                          <li key={relatedId} className="help-topic-related-item">
                            <button
                              className="help-topic-related-button"
                              onClick={() => openTopicInPanel(relatedId)}
                            >
                              {relatedTopic.title}
                            </button>
                          </li>
                        );
                      })}
                    </ul>
                  </div>
                )}
              </>
            ) : (
              <div className="help-panel-welcome">
                <h3>Welcome to the Help Center</h3>
                <p>
                  Select a topic from the sidebar or search for help on a specific topic.
                </p>
              </div>
            )}
          </div>
        </div>
      </div>
    </div>
  );
};

// Explanation tooltip component
const ExplanationTooltip: React.FC = () => {
  const { activeExplanation, hideExplanation } = useHelp();
  
  if (!activeExplanation) return null;
  
  // Position the tooltip
  let tooltipPosition = { top: 0, left: 0 };
  
  // If there's a target selector, position near the target element
  if (activeExplanation.targetSelector) {
    const targetElement = document.querySelector(activeExplanation.targetSelector);
    if (targetElement) {
      const rect = targetElement.getBoundingClientRect();
      tooltipPosition = {
        top: rect.bottom + 10,
        left: rect.left + rect.width / 2,
      };
    }
  } else {
    // Otherwise, center in viewport
    tooltipPosition = {
      top: window.innerHeight / 2,
      left: window.innerWidth / 2,
    };
  }
  
  return (
    <div
      className="explanation-tooltip"
      style={{
        top: `${tooltipPosition.top}px`,
        left: `${tooltipPosition.left}px`,
      }}
    >
      <div className="explanation-tooltip-header">
        <h3 className="explanation-tooltip-title">{activeExplanation.title}</h3>
        <button
          className="explanation-tooltip-close"
          onClick={hideExplanation}
          aria-label="Close explanation"
        >
          &times;
        </button>
      </div>
      <div className="explanation-tooltip-content">
        {activeExplanation.content}
      </div>
    </div>
  );
};

// Inline help overlay - shows help icons near elements with explanations
const InlineHelpOverlay: React.FC = () => {
  const { explanations, showExplanation } = useHelp();
  
  return (
    <div className="inline-help-overlay">
      {explanations
        .filter(explanation => explanation.targetSelector)
        .map(explanation => {
          const targetElement = document.querySelector(explanation.targetSelector!);
          if (!targetElement) return null;
          
          const rect = targetElement.getBoundingClientRect();
          
          return (
            <button
              key={explanation.id}
              className="inline-help-icon"
              onClick={() => showExplanation(explanation.id)}
              aria-label={`Help for ${explanation.title}`}
              style={{
                top: `${rect.top}px`,
                left: `${rect.right + 5}px`,
              }}
            >
              ?
            </button>
          );
        })}
    </div>
  );
};

export default HelpProvider;
</file>

<file path="src-frontend/src/help/HelpTrigger.tsx">
import React from 'react';
import { useHelp } from './HelpProvider';

interface HelpTriggerProps {
  topicId: string;
  className?: string;
}

const HelpTrigger: React.FC<HelpTriggerProps> = ({ topicId, className = '' }) => {
  const { openTopicInPanel, topics } = useHelp();
  
  const topic = topics.find(t => t.id === topicId);
  if (!topic) return null;
  
  return (
    <button
      className={`help-trigger ${className}`}
      onClick={() => openTopicInPanel(topicId)}
      aria-label={`Help: ${topic.title}`}
      title={topic.title}
    >
      ?
    </button>
  );
};

export default HelpTrigger;
</file>

<file path="src-frontend/src/help/index.ts">
export * from './HelpProvider';
export { default as HelpButton } from './HelpButton';
export { default as HelpTrigger } from './HelpTrigger';
</file>

<file path="src-frontend/src/hooks/useCollaboration.ts">
// useCollaboration.ts
//
// This hook provides access to the collaboration context,
// making it easy to use collaboration features throughout the application.
// It also adds optimizations like batching and retry logic.

import { useContext, useEffect, useCallback } from 'react';
import { invoke } from '@tauri-apps/api/tauri';
import { CollaborationContext } from '../components/collaboration/context/CollaborationContext';
import { createRetryFunction } from '../utils/retry';
import { createBatcher } from '../utils/batch';
import { throttle } from '../utils/throttle';

// Create retry-enabled versions of invoke for critical operations
const invokeWithRetry = createRetryFunction(invoke, {
  maxRetries: 3,
  retryDelay: 500,
  backoffFactor: 1.5,
  // Only retry on network-related errors, not validation errors
  retryCondition: (error) => {
    const errorStr = String(error).toLowerCase();
    return (
      errorStr.includes('network') ||
      errorStr.includes('timeout') ||
      errorStr.includes('connection') ||
      errorStr.includes('server')
    );
  },
});

// Enhanced useCollaboration hook with optimizations
export const useCollaboration = () => {
  const context = useContext(CollaborationContext);
  
  if (!context) {
    throw new Error('useCollaboration must be used within a CollaborationProvider');
  }
  
  // Create optimized versions of critical functions
  
  // Throttled cursor position updates
  const throttledUpdateCursor = useCallback(
    throttle(context.updateCursorPosition, 50), // 50ms throttle
    [context.updateCursorPosition]
  );
  
  // Setup a batch processor for selection updates
  const batchedSelectionUpdater = useCallback(() => {
    // Create a function to process batches of selection updates
    const processSelectionBatch = async (selectionUpdates: any[]) => {
      // Group updates by selection range (startId/endId/etc)
      // and only process the latest update for each range
      const latestUpdates = new Map();
      
      for (const update of selectionUpdates) {
        const key = `${update.startId}-${update.endId}`;
        latestUpdates.set(key, update);
      }
      
      // Process each unique update
      for (const update of latestUpdates.values()) {
        try {
          await context.updateSelection(
            update.startId,
            update.endId,
            update.startOffset,
            update.endOffset
          );
        } catch (error) {
          console.error('Error updating selection:', error);
        }
      }
    };
    
    // Create a batcher that will group selection updates
    return createBatcher(processSelectionBatch, {
      maxBatchSize: 5,
      maxDelayMs: 100,
    });
  }, [context.updateSelection]);
  
  // Use retry for critical operations
  const createSessionWithRetry = useCallback(
    async (name: string, conversationId: string) => {
      return invokeWithRetry('create_session', { name, conversationId })
        .then((session) => {
          context.refreshUsers();
          return session;
        });
    },
    [context.refreshUsers]
  );
  
  const joinSessionWithRetry = useCallback(
    async (sessionId: string) => {
      return invokeWithRetry('join_session', { sessionId })
        .then((session) => {
          context.refreshUsers();
          return session;
        });
    },
    [context.refreshUsers]
  );
  
  // Enhanced context with optimized functions
  const enhancedContext = {
    ...context,
    updateCursorPosition: throttledUpdateCursor,
    batchUpdateSelection: batchedSelectionUpdater(),
    createSessionWithRetry,
    joinSessionWithRetry,
  };
  
  return enhancedContext;
};

export default useCollaboration;
</file>

<file path="src-frontend/src/hooks/useCommandPalette.ts">
import { useState, useEffect, useCallback } from 'react';
import { CommandItem } from '../components/CommandPalette';

export const useCommandPalette = (initialCommands: CommandItem[] = []) => {
  const [isOpen, setIsOpen] = useState(false);
  const [commands, setCommands] = useState<CommandItem[]>(initialCommands);
  
  const registerCommand = useCallback((command: CommandItem) => {
    setCommands(prev => {
      // Don't add duplicate commands (based on ID)
      if (prev.some(cmd => cmd.id === command.id)) {
        return prev;
      }
      return [...prev, command];
    });
    
    // Return unregister function
    return () => {
      setCommands(prev => prev.filter(cmd => cmd.id !== command.id));
    };
  }, []);
  
  const registerCommands = useCallback((newCommands: CommandItem[]) => {
    setCommands(prev => {
      const uniqueCommands = newCommands.filter(
        newCmd => !prev.some(cmd => cmd.id === newCmd.id)
      );
      return [...prev, ...uniqueCommands];
    });
    
    // Return unregister function
    return () => {
      const ids = newCommands.map(cmd => cmd.id);
      setCommands(prev => prev.filter(cmd => !ids.includes(cmd.id)));
    };
  }, []);
  
  const open = useCallback(() => {
    setIsOpen(true);
  }, []);
  
  const close = useCallback(() => {
    setIsOpen(false);
  }, []);
  
  // Add global keyboard shortcut to open the command palette (Cmd+K or Ctrl+K)
  useEffect(() => {
    const handleKeyDown = (e: KeyboardEvent) => {
      if ((e.metaKey || e.ctrlKey) && e.key === 'k') {
        e.preventDefault();
        setIsOpen(prev => !prev);
      }
    };
    
    window.addEventListener('keydown', handleKeyDown);
    
    return () => {
      window.addEventListener('keydown', handleKeyDown);
    };
  }, []);
  
  return {
    isOpen,
    commands,
    registerCommand,
    registerCommands,
    open,
    close,
  };
};

export default useCommandPalette;
</file>

<file path="src-frontend/src/interactions/feedback.css">
/* Feedback and micro-interactions styles */

/* Feedback notifications */
.feedback-notification {
  position: fixed;
  bottom: var(--spacing-lg);
  left: 50%;
  transform: translateX(-50%);
  padding: var(--spacing-sm) var(--spacing-md);
  border-radius: var(--radius-md);
  color: white;
  font-size: var(--font-size-md);
  font-weight: 500;
  box-shadow: var(--shadow-md);
  z-index: var(--z-tooltip);
  animation: slideUpFade 0.3s var(--animation-easing-spring);
}

.feedback-notification.success {
  background-color: var(--color-success);
}

.feedback-notification.error {
  background-color: var(--color-error);
}

.feedback-notification.fade-out {
  animation: fadeOut 0.3s var(--animation-easing-standard) forwards;
}

@keyframes slideUpFade {
  from {
    opacity: 0;
    transform: translate(-50%, 20px);
  }
  to {
    opacity: 1;
    transform: translate(-50%, 0);
  }
}

/* Micro-interactions */

/* Press effect */
.press {
  animation: pressEffect 0.2s var(--animation-easing-standard);
}

@keyframes pressEffect {
  0% {
    transform: scale(1);
  }
  50% {
    transform: scale(0.95);
  }
  100% {
    transform: scale(1);
  }
}

/* Pulse effect */
.pulse {
  animation: pulseEffect 1s var(--animation-easing-standard);
}

@keyframes pulseEffect {
  0% {
    transform: scale(1);
    box-shadow: 0 0 0 0 rgba(var(--color-primary-rgb), 0.4);
  }
  70% {
    transform: scale(1.05);
    box-shadow: 0 0 0 10px rgba(var(--color-primary-rgb), 0);
  }
  100% {
    transform: scale(1);
    box-shadow: 0 0 0 0 rgba(var(--color-primary-rgb), 0);
  }
}

/* Shake effect */
.shake {
  animation: shakeEffect 0.5s cubic-bezier(0.36, 0.07, 0.19, 0.97) both;
}

@keyframes shakeEffect {
  0%, 100% {
    transform: translateX(0);
  }
  10%, 30%, 50%, 70%, 90% {
    transform: translateX(-5px);
  }
  20%, 40%, 60%, 80% {
    transform: translateX(5px);
  }
}

/* Bounce effect */
.bounce {
  animation: bounceEffect 0.8s var(--animation-easing-spring);
}

@keyframes bounceEffect {
  0%, 20%, 50%, 80%, 100% {
    transform: translateY(0);
  }
  40% {
    transform: translateY(-15px);
  }
  60% {
    transform: translateY(-7px);
  }
}

/* Wiggle effect */
.wiggle {
  animation: wiggleEffect 0.8s var(--animation-easing-spring);
}

@keyframes wiggleEffect {
  0%, 100% {
    transform: rotate(0deg);
  }
  20% {
    transform: rotate(-10deg);
  }
  40% {
    transform: rotate(5deg);
  }
  60% {
    transform: rotate(-5deg);
  }
  80% {
    transform: rotate(2deg);
  }
}

/* Tada effect */
.tada {
  animation: tadaEffect 1s var(--animation-easing-standard);
}

@keyframes tadaEffect {
  0% {
    transform: scale(1);
  }
  10%, 20% {
    transform: scale(0.9) rotate(-3deg);
  }
  30%, 50%, 70%, 90% {
    transform: scale(1.1) rotate(3deg);
  }
  40%, 60%, 80% {
    transform: scale(1.1) rotate(-3deg);
  }
  100% {
    transform: scale(1) rotate(0);
  }
}

/* Jello effect */
.jello {
  animation: jelloEffect 1s var(--animation-easing-standard);
}

@keyframes jelloEffect {
  0%, 11.1%, 100% {
    transform: none;
  }
  22.2% {
    transform: skewX(-12.5deg) skewY(-12.5deg);
  }
  33.3% {
    transform: skewX(6.25deg) skewY(6.25deg);
  }
  44.4% {
    transform: skewX(-3.125deg) skewY(-3.125deg);
  }
  55.5% {
    transform: skewX(1.5625deg) skewY(1.5625deg);
  }
  66.6% {
    transform: skewX(-0.78125deg) skewY(-0.78125deg);
  }
  77.7% {
    transform: skewX(0.390625deg) skewY(0.390625deg);
  }
  88.8% {
    transform: skewX(-0.1953125deg) skewY(-0.1953125deg);
  }
}

/* Heartbeat effect */
.heartbeat {
  animation: heartbeatEffect 1.5s var(--animation-easing-standard);
}

@keyframes heartbeatEffect {
  0% {
    transform: scale(1);
  }
  14% {
    transform: scale(1.1);
  }
  28% {
    transform: scale(1);
  }
  42% {
    transform: scale(1.15);
  }
  70% {
    transform: scale(1);
  }
}

/* Ripple effect */
.ripple-container {
  position: relative;
  overflow: hidden;
}

.ripple {
  position: absolute;
  border-radius: 50%;
  background-color: rgba(255, 255, 255, 0.3);
  transform: scale(0);
  animation: rippleEffect 0.6s linear;
  pointer-events: none;
}

@keyframes rippleEffect {
  to {
    transform: scale(4);
    opacity: 0;
  }
}

/* Hover effects */
.hover-float {
  transition: transform 0.3s var(--animation-easing-standard);
}

.hover-float:hover {
  transform: translateY(-5px);
}

.hover-grow {
  transition: transform 0.3s var(--animation-easing-standard);
}

.hover-grow:hover {
  transform: scale(1.05);
}

.hover-shadow {
  transition: box-shadow 0.3s var(--animation-easing-standard);
}

.hover-shadow:hover {
  box-shadow: 0 10px 20px rgba(0, 0, 0, 0.1);
}

.hover-rotate {
  transition: transform 0.3s var(--animation-easing-standard);
}

.hover-rotate:hover {
  transform: rotate(5deg);
}

/* Feedback points indicator */
.points-indicator {
  position: fixed;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  font-size: var(--font-size-xl);
  font-weight: bold;
  color: var(--color-primary);
  z-index: var(--z-tooltip);
  pointer-events: none;
  opacity: 0;
}

.points-indicator.positive {
  animation: pointsPositive 1.5s var(--animation-easing-standard) forwards;
}

.points-indicator.negative {
  color: var(--color-error);
  animation: pointsNegative 1.5s var(--animation-easing-standard) forwards;
}

@keyframes pointsPositive {
  0% {
    opacity: 0;
    transform: translate(-50%, -50%) scale(0.5);
  }
  20% {
    opacity: 1;
    transform: translate(-50%, -50%) scale(1.2);
  }
  80% {
    opacity: 1;
    transform: translate(-50%, -120%) scale(1);
  }
  100% {
    opacity: 0;
    transform: translate(-50%, -150%) scale(1);
  }
}

@keyframes pointsNegative {
  0% {
    opacity: 0;
    transform: translate(-50%, -50%) scale(0.5);
  }
  20% {
    opacity: 1;
    transform: translate(-50%, -50%) scale(1.2);
  }
  80% {
    opacity: 1;
    transform: translate(-50%, 20%) scale(1);
  }
  100% {
    opacity: 0;
    transform: translate(-50%, 50%) scale(1);
  }
}
</file>

<file path="src-frontend/src/interactions/index.ts">
import './feedback.css';

export { default as useMicroInteraction } from './useMicroInteraction';
export { default as usePressEffect } from './usePressEffect';
export { default as useHoverEffect } from './useHoverEffect';
export { default as useRippleEffect } from './useRippleEffect';
export { default as useFeedback } from './useFeedback';
</file>

<file path="src-frontend/src/interactions/useFeedback.ts">
import { useCallback } from 'react';
import { useDisclosure } from '../disclosure/ProgressiveDisclosure';

interface FeedbackOptions {
  successPoints?: number;
  errorPoints?: number;
  successMessage?: string;
  errorMessage?: string;
  showNotification?: boolean;
}

/**
 * A hook for providing interactive feedback when users complete actions
 * 
 * @param options - Options for customizing the feedback
 * @returns Functions for triggering success and error feedback
 */
export const useFeedback = (options: FeedbackOptions = {}) => {
  const {
    successPoints = 5,
    errorPoints = 0,
    successMessage = 'Success!',
    errorMessage = 'Error occurred',
    showNotification = true,
  } = options;
  
  const { addPoints } = useDisclosure();
  
  // Show a temporary notification
  const showFeedbackNotification = useCallback((message: string, isSuccess: boolean) => {
    if (!showNotification) return;
    
    // Create notification element
    const notification = document.createElement('div');
    notification.className = `feedback-notification ${isSuccess ? 'success' : 'error'}`;
    notification.textContent = message;
    
    // Add to document
    document.body.appendChild(notification);
    
    // Remove after animation completes
    setTimeout(() => {
      notification.classList.add('fade-out');
      setTimeout(() => {
        if (document.body.contains(notification)) {
          document.body.removeChild(notification);
        }
      }, 300);
    }, 2000);
  }, [showNotification]);
  
  // Trigger success feedback
  const triggerSuccess = useCallback((customMessage?: string) => {
    // Add points for successful action
    if (successPoints > 0) {
      addPoints(successPoints);
    }
    
    // Show notification
    showFeedbackNotification(customMessage || successMessage, true);
    
    // Play success sound if available
    if (typeof window !== 'undefined' && window.Audio) {
      try {
        const audio = new Audio('/sounds/success.mp3');
        audio.volume = 0.5;
        audio.play().catch(() => {
          // Ignore errors - browser might block autoplay
        });
      } catch (e) {
        // Ignore errors - audio might not be supported
      }
    }
  }, [addPoints, successMessage, successPoints, showFeedbackNotification]);
  
  // Trigger error feedback
  const triggerError = useCallback((customMessage?: string) => {
    // Add/remove points for failed action
    if (errorPoints !== 0) {
      addPoints(errorPoints);
    }
    
    // Show notification
    showFeedbackNotification(customMessage || errorMessage, false);
    
    // Play error sound if available
    if (typeof window !== 'undefined' && window.Audio) {
      try {
        const audio = new Audio('/sounds/error.mp3');
        audio.volume = 0.5;
        audio.play().catch(() => {
          // Ignore errors - browser might block autoplay
        });
      } catch (e) {
        // Ignore errors - audio might not be supported
      }
    }
    
    // Add a shake animation to the body to provide haptic-like feedback
    document.body.classList.add('shake');
    setTimeout(() => {
      document.body.classList.remove('shake');
    }, 500);
  }, [addPoints, errorMessage, errorPoints, showFeedbackNotification]);
  
  return {
    triggerSuccess,
    triggerError,
  };
};

export default useFeedback;
</file>

<file path="src-frontend/src/interactions/useHoverEffect.ts">
import { useState, useCallback } from 'react';

interface HoverEffectOptions {
  delayEnter?: number;
  delayLeave?: number;
}

/**
 * A hook for adding hover effects with customizable delays
 * 
 * @param options - Options for customizing the hover effect
 * @returns isHovered state and props to spread on the target element
 */
export const useHoverEffect = (options: HoverEffectOptions = {}) => {
  const {
    delayEnter = 0,
    delayLeave = 0,
  } = options;
  
  const [isHovered, setIsHovered] = useState(false);
  const [hoverTimer, setHoverTimer] = useState<NodeJS.Timeout | null>(null);
  
  const handleMouseEnter = useCallback(() => {
    // Clear any existing timer
    if (hoverTimer) {
      clearTimeout(hoverTimer);
    }
    
    if (delayEnter > 0) {
      // Set a timer to activate the hover state
      const timer = setTimeout(() => {
        setIsHovered(true);
        setHoverTimer(null);
      }, delayEnter);
      
      setHoverTimer(timer);
    } else {
      // No delay, activate immediately
      setIsHovered(true);
    }
  }, [delayEnter, hoverTimer]);
  
  const handleMouseLeave = useCallback(() => {
    // Clear any existing timer
    if (hoverTimer) {
      clearTimeout(hoverTimer);
    }
    
    if (delayLeave > 0) {
      // Set a timer to deactivate the hover state
      const timer = setTimeout(() => {
        setIsHovered(false);
        setHoverTimer(null);
      }, delayLeave);
      
      setHoverTimer(timer);
    } else {
      // No delay, deactivate immediately
      setIsHovered(false);
    }
  }, [delayLeave, hoverTimer]);
  
  // Clean up timers if component unmounts
  const reset = useCallback(() => {
    if (hoverTimer) {
      clearTimeout(hoverTimer);
    }
    setIsHovered(false);
    setHoverTimer(null);
  }, [hoverTimer]);
  
  // Props to spread on the target element
  const props = {
    onMouseEnter: handleMouseEnter,
    onMouseLeave: handleMouseLeave,
  };
  
  return {
    isHovered,
    props,
    reset,
  };
};

export default useHoverEffect;
</file>

<file path="src-frontend/src/interactions/useMicroInteraction.ts">
import { useState, useCallback, useRef, useEffect } from 'react';

type MicroInteractionType = 
  | 'press' 
  | 'pulse' 
  | 'shake' 
  | 'bounce' 
  | 'wiggle' 
  | 'tada' 
  | 'jello' 
  | 'heartbeat';

interface MicroInteractionOptions {
  duration?: number;
  delay?: number;
  count?: number;
}

/**
 * A hook for adding micro-interactions to UI elements
 * 
 * @param type - The type of micro-interaction
 * @param options - Options for customizing the interaction
 * @returns An object with the current animation class, a trigger function, and a reset function
 */
export const useMicroInteraction = (
  type: MicroInteractionType,
  options: MicroInteractionOptions = {}
) => {
  const {
    duration = 500,
    delay = 0,
    count = 1,
  } = options;
  
  const [isAnimating, setIsAnimating] = useState(false);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  const counterRef = useRef(0);
  
  // Clear any existing timers when component unmounts
  useEffect(() => {
    return () => {
      if (timerRef.current) {
        clearTimeout(timerRef.current);
      }
    };
  }, []);
  
  // Reset the animation state
  const reset = useCallback(() => {
    setIsAnimating(false);
    counterRef.current = 0;
    
    if (timerRef.current) {
      clearTimeout(timerRef.current);
      timerRef.current = null;
    }
  }, []);
  
  // Trigger the animation
  const trigger = useCallback(() => {
    if (isAnimating && count === 1) {
      // If already animating and we only want to play once, just return
      return;
    }
    
    // Reset any existing animation
    reset();
    
    // Start the animation after the specified delay
    timerRef.current = setTimeout(() => {
      setIsAnimating(true);
      counterRef.current = 1;
      
      // Set a timeout to end the animation
      if (count !== Infinity) {
        timerRef.current = setTimeout(() => {
          setIsAnimating(false);
          timerRef.current = null;
        }, duration);
      }
      
      // For repeating animations
      if (count > 1) {
        const intervalId = setInterval(() => {
          counterRef.current += 1;
          
          // Force a re-render to restart the animation
          setIsAnimating(false);
          setTimeout(() => setIsAnimating(true), 10);
          
          if (counterRef.current >= count) {
            clearInterval(intervalId);
            
            // End the animation after the last iteration
            setTimeout(() => {
              setIsAnimating(false);
            }, duration);
          }
        }, duration);
        
        // Store the interval ID to clear it when unmounting
        timerRef.current = intervalId as unknown as NodeJS.Timeout;
      }
    }, delay);
  }, [isAnimating, count, duration, delay, reset]);
  
  // Determine the current animation class
  const animationClass = isAnimating ? type : '';
  
  // Additional properties for the element
  const props = {
    className: animationClass,
    style: isAnimating ? { animationDuration: `${duration}ms` } : undefined,
  };
  
  return {
    isAnimating,
    animationClass,
    props,
    trigger,
    reset,
  };
};

export default useMicroInteraction;
</file>

<file path="src-frontend/src/interactions/usePressEffect.ts">
import { useState, useCallback } from 'react';

/**
 * A hook for adding press/click feedback effect to interactive elements
 * 
 * @returns Props to spread on the target element and a reset function
 */
export const usePressEffect = () => {
  const [isPressed, setIsPressed] = useState(false);
  
  const handleMouseDown = useCallback(() => {
    setIsPressed(true);
  }, []);
  
  const handleMouseUp = useCallback(() => {
    setIsPressed(false);
  }, []);
  
  const handleMouseLeave = useCallback(() => {
    setIsPressed(false);
  }, []);
  
  const reset = useCallback(() => {
    setIsPressed(false);
  }, []);
  
  // Props to spread on the target element
  const props = {
    className: isPressed ? 'button-press' : '',
    onMouseDown: handleMouseDown,
    onMouseUp: handleMouseUp,
    onMouseLeave: handleMouseLeave,
    onTouchStart: handleMouseDown,
    onTouchEnd: handleMouseUp,
    onTouchCancel: handleMouseUp,
  };
  
  return {
    isPressed,
    props,
    reset,
  };
};

export default usePressEffect;
</file>

<file path="src-frontend/src/interactions/useRippleEffect.tsx">
import React, { useState, useCallback, useRef, CSSProperties } from 'react';

interface RippleStyle extends CSSProperties {
  top: number;
  left: number;
  width: number;
  height: number;
}

interface Ripple {
  id: number;
  style: RippleStyle;
}

interface RippleEffectOptions {
  color?: string;
  duration?: number;
}

/**
 * A hook for adding ripple effects (similar to Material Design) to elements
 * 
 * @param options - Options for customizing the ripple effect
 * @returns Props to spread on the target element and a component to render inside
 */
export const useRippleEffect = (options: RippleEffectOptions = {}) => {
  const {
    color = 'rgba(255, 255, 255, 0.3)',
    duration = 500,
  } = options;
  
  const [ripples, setRipples] = useState<Ripple[]>([]);
  const nextId = useRef(0);
  
  const addRipple = useCallback((e: React.MouseEvent | React.TouchEvent) => {
    const target = e.currentTarget as HTMLElement;
    const rect = target.getBoundingClientRect();
    
    let pageX, pageY;
    
    if ('touches' in e) {
      pageX = e.touches[0].pageX;
      pageY = e.touches[0].pageY;
    } else {
      pageX = e.pageX;
      pageY = e.pageY;
    }
    
    const left = pageX - (rect.left + window.scrollX);
    const top = pageY - (rect.top + window.scrollY);
    
    const size = Math.max(rect.width, rect.height) * 2;
    
    const newRipple: Ripple = {
      id: nextId.current++,
      style: {
        top: top - size / 2,
        left: left - size / 2,
        width: size,
        height: size,
      },
    };
    
    setRipples(prev => [...prev, newRipple]);
    
    // Remove the ripple after the animation completes
    setTimeout(() => {
      setRipples(prev => prev.filter(r => r.id !== newRipple.id));
    }, duration + 100); // Add 100ms buffer
  }, [duration]);
  
  // Clear all ripples
  const reset = useCallback(() => {
    setRipples([]);
  }, []);
  
  // Props to spread on the target element
  const props = {
    onClick: addRipple,
    style: { position: 'relative', overflow: 'hidden' } as CSSProperties,
  };
  
  // Component to render inside the target
  const RippleEffect: React.FC = () => (
    <>
      {ripples.map(ripple => (
        <span
          key={ripple.id}
          style={{
            position: 'absolute',
            borderRadius: '50%',
            backgroundColor: color,
            opacity: 0.6,
            transform: 'scale(0)',
            animation: `ripple ${duration}ms ease-out`,
            ...ripple.style,
          }}
        />
      ))}
      <style>
        {`
          @keyframes ripple {
            to {
              opacity: 0;
              transform: scale(2);
            }
          }
        `}
      </style>
    </>
  );
  
  return {
    ripples,
    props,
    reset,
    RippleEffect,
  };
};

export default useRippleEffect;
</file>

<file path="src-frontend/src/keyboard/index.ts">
export * from './KeyboardNavigation';
</file>

<file path="src-frontend/src/keyboard/KeyboardNavigation.css">
.keyboard-shortcuts-overlay {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.5);
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: var(--z-modal);
  animation: fadeIn var(--animation-duration-fast) var(--animation-easing-standard);
}

.keyboard-shortcuts-dialog {
  width: 600px;
  max-width: 90vw;
  max-height: 80vh;
  background-color: var(--color-surface);
  border-radius: var(--radius-lg);
  box-shadow: var(--shadow-lg);
  display: flex;
  flex-direction: column;
  animation: scaleIn var(--animation-duration-normal) var(--animation-easing-spring);
}

.keyboard-shortcuts-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: var(--spacing-md) var(--spacing-lg);
  border-bottom: 1px solid var(--color-border);
}

.keyboard-shortcuts-header h2 {
  margin: 0;
  font-size: var(--font-size-xl);
  color: var(--color-on-surface);
}

.keyboard-shortcuts-close {
  background: none;
  border: none;
  font-size: 1.5rem;
  line-height: 1;
  padding: var(--spacing-xs);
  cursor: pointer;
  color: var(--color-on-surface-variant);
  transition: color var(--animation-duration-fast) var(--animation-easing-standard);
}

.keyboard-shortcuts-close:hover {
  color: var(--color-on-surface);
}

.keyboard-shortcuts-content {
  padding: var(--spacing-md) var(--spacing-lg);
  overflow-y: auto;
  flex: 1;
}

.keyboard-shortcuts-section {
  margin-bottom: var(--spacing-lg);
}

.keyboard-shortcuts-section:last-child {
  margin-bottom: 0;
}

.keyboard-shortcuts-scope {
  font-size: var(--font-size-lg);
  margin: 0 0 var(--spacing-sm) 0;
  padding-bottom: var(--spacing-xs);
  border-bottom: 1px solid var(--color-border);
  color: var(--color-on-surface);
  display: flex;
  align-items: center;
  gap: var(--spacing-sm);
}

.keyboard-shortcuts-active-tag {
  font-size: var(--font-size-xs);
  background-color: var(--color-primary);
  color: white;
  padding: 2px var(--spacing-xs);
  border-radius: var(--radius-sm);
  font-weight: normal;
}

.keyboard-shortcuts-list {
  list-style: none;
  margin: 0;
  padding: 0;
}

.keyboard-shortcuts-item {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: var(--spacing-sm) 0;
  border-bottom: 1px solid var(--color-divider);
}

.keyboard-shortcuts-item:last-child {
  border-bottom: none;
}

.keyboard-shortcuts-description {
  font-size: var(--font-size-md);
  color: var(--color-on-surface);
}

.keyboard-shortcuts-keys {
  font-size: var(--font-size-sm);
}

.keyboard-shortcuts-keys kbd {
  display: inline-block;
  padding: var(--spacing-xxs) var(--spacing-xs);
  font-family: var(--font-family);
  border: 1px solid var(--color-border);
  border-radius: var(--radius-sm);
  box-shadow: 0 1px 0 var(--color-border);
  background-color: var(--color-surface-variant);
  color: var(--color-on-surface);
}

.keyboard-shortcuts-footer {
  padding: var(--spacing-md) var(--spacing-lg);
  border-top: 1px solid var(--color-border);
  text-align: center;
  color: var(--color-on-surface-variant);
  font-size: var(--font-size-sm);
}

.keyboard-shortcuts-footer kbd {
  display: inline-block;
  padding: 2px var(--spacing-xs);
  font-family: var(--font-family);
  font-size: var(--font-size-xs);
  border: 1px solid var(--color-border);
  border-radius: var(--radius-sm);
  box-shadow: 0 1px 0 var(--color-border);
  background-color: var(--color-surface-variant);
  color: var(--color-on-surface);
}

.keyboard-focus-outline:focus-visible {
  outline: 2px solid var(--color-primary);
  outline-offset: 2px;
}

/* Visual indicator for keyboard navigation */
.keyboard-focus-outline:focus:not(:focus-visible) {
  outline: none;
}

.keyboard-navigation-indicator {
  position: fixed;
  bottom: var(--spacing-md);
  right: var(--spacing-md);
  background-color: var(--color-surface);
  border: 1px solid var(--color-border);
  border-radius: var(--radius-md);
  padding: var(--spacing-xs) var(--spacing-sm);
  font-size: var(--font-size-sm);
  color: var(--color-on-surface-variant);
  box-shadow: var(--shadow-md);
  opacity: 0;
  transform: translateY(10px);
  transition: opacity var(--animation-duration-normal) var(--animation-easing-standard),
              transform var(--animation-duration-normal) var(--animation-easing-standard);
}

.keyboard-navigation-indicator.active {
  opacity: 1;
  transform: translateY(0);
}
</file>

<file path="src-frontend/src/keyboard/KeyboardNavigation.tsx">
import React, { createContext, useContext, useState, useEffect, ReactNode } from 'react';
import './KeyboardNavigation.css';

// Key action types
export type KeyAction = {
  key: string;
  altKey?: boolean;
  ctrlKey?: boolean;
  shiftKey?: boolean;
  metaKey?: boolean;
  description: string;
  handler: () => void;
  disabled?: boolean;
  scope?: string;
};

// Interface for the keyboard context
interface KeyboardContextType {
  registerAction: (action: KeyAction) => () => void;
  unregisterAction: (key: string) => void;
  activeScope: string;
  setActiveScope: (scope: string) => void;
  showShortcutsDialog: () => void;
  hideShortcutsDialog: () => void;
  isShortcutsDialogOpen: boolean;
  getFormattedShortcut: (action: KeyAction) => string;
  actions: KeyAction[];
}

const KeyboardContext = createContext<KeyboardContextType>({
  registerAction: () => () => {},
  unregisterAction: () => {},
  activeScope: 'global',
  setActiveScope: () => {},
  showShortcutsDialog: () => {},
  hideShortcutsDialog: () => {},
  isShortcutsDialogOpen: false,
  getFormattedShortcut: () => '',
  actions: [],
});

export const useKeyboard = () => useContext(KeyboardContext);

// Helper function to format keyboard shortcuts for display
const formatShortcut = (action: KeyAction): string => {
  const parts: string[] = [];
  
  if (action.ctrlKey) parts.push('Ctrl');
  if (action.altKey) parts.push('Alt');
  if (action.shiftKey) parts.push('Shift');
  if (action.metaKey) parts.push('⌘');
  
  parts.push(action.key.toUpperCase());
  
  return parts.join(' + ');
};

// Match a keyboard event against a registered action
const isMatchingKeyEvent = (event: KeyboardEvent, action: KeyAction): boolean => {
  return (
    event.key.toLowerCase() === action.key.toLowerCase() &&
    Boolean(event.altKey) === Boolean(action.altKey) &&
    Boolean(event.ctrlKey) === Boolean(action.ctrlKey) &&
    Boolean(event.shiftKey) === Boolean(action.shiftKey) &&
    Boolean(event.metaKey) === Boolean(action.metaKey)
  );
};

interface KeyboardProviderProps {
  children: ReactNode;
}

export const KeyboardProvider: React.FC<KeyboardProviderProps> = ({ children }) => {
  const [actions, setActions] = useState<KeyAction[]>([]);
  const [activeScope, setActiveScope] = useState<string>('global');
  const [isShortcutsDialogOpen, setIsShortcutsDialogOpen] = useState<boolean>(false);
  
  // Register a keyboard action
  const registerAction = (action: KeyAction) => {
    const scopedAction = {
      ...action,
      scope: action.scope || 'global',
    };
    
    setActions(prevActions => {
      // Filter out any existing actions with the same key combination and scope
      const filteredActions = prevActions.filter(
        act => !(
          act.key === scopedAction.key &&
          act.altKey === scopedAction.altKey &&
          act.ctrlKey === scopedAction.ctrlKey &&
          act.shiftKey === scopedAction.shiftKey &&
          act.metaKey === scopedAction.metaKey &&
          act.scope === scopedAction.scope
        )
      );
      
      return [...filteredActions, scopedAction];
    });
    
    // Return a function to unregister this action
    return () => {
      setActions(prevActions => 
        prevActions.filter(a => a !== scopedAction)
      );
    };
  };
  
  // Unregister a keyboard action by key
  const unregisterAction = (key: string) => {
    setActions(prevActions => 
      prevActions.filter(action => action.key !== key)
    );
  };
  
  // Handle keyboard events
  useEffect(() => {
    const handleKeyDown = (event: KeyboardEvent) => {
      // Skip if we're in an input element
      if (
        event.target instanceof HTMLInputElement ||
        event.target instanceof HTMLTextAreaElement ||
        event.target instanceof HTMLSelectElement ||
        (event.target as HTMLElement).isContentEditable
      ) {
        return;
      }
      
      // Global action to show keyboard shortcuts dialog
      if (event.key === '?' && event.shiftKey) {
        event.preventDefault();
        setIsShortcutsDialogOpen(true);
        return;
      }
      
      // Find matching action in current scope or global scope
      const matchingAction = actions.find(
        action => 
          !action.disabled && 
          isMatchingKeyEvent(event, action) && 
          (action.scope === activeScope || action.scope === 'global')
      );
      
      if (matchingAction) {
        event.preventDefault();
        matchingAction.handler();
      }
    };
    
    window.addEventListener('keydown', handleKeyDown);
    
    return () => {
      window.removeEventListener('keydown', handleKeyDown);
    };
  }, [actions, activeScope]);
  
  // Show and hide keyboard shortcuts dialog
  const showShortcutsDialog = () => setIsShortcutsDialogOpen(true);
  const hideShortcutsDialog = () => setIsShortcutsDialogOpen(false);
  
  return (
    <KeyboardContext.Provider
      value={{
        registerAction,
        unregisterAction,
        activeScope,
        setActiveScope,
        showShortcutsDialog,
        hideShortcutsDialog,
        isShortcutsDialogOpen,
        getFormattedShortcut: formatShortcut,
        actions,
      }}
    >
      {children}
      {isShortcutsDialogOpen && (
        <KeyboardShortcutsDialog onClose={hideShortcutsDialog} />
      )}
    </KeyboardContext.Provider>
  );
};

// Component to display keyboard shortcuts
const KeyboardShortcutsDialog: React.FC<{ onClose: () => void }> = ({ onClose }) => {
  const { actions, activeScope, getFormattedShortcut } = useKeyboard();
  
  // Group actions by scope
  const groupedActions = actions.reduce<Record<string, KeyAction[]>>(
    (groups, action) => {
      const scope = action.scope || 'global';
      if (!groups[scope]) {
        groups[scope] = [];
      }
      groups[scope].push(action);
      return groups;
    }, 
    {}
  );
  
  // Format scope name for display
  const formatScopeName = (scope: string): string => {
    return scope
      .replace(/([A-Z])/g, ' $1')
      .replace(/^./, str => str.toUpperCase());
  };
  
  return (
    <div className="keyboard-shortcuts-overlay" onClick={onClose}>
      <div className="keyboard-shortcuts-dialog" onClick={e => e.stopPropagation()}>
        <div className="keyboard-shortcuts-header">
          <h2>Keyboard Shortcuts</h2>
          <button className="keyboard-shortcuts-close" onClick={onClose}>
            &times;
          </button>
        </div>
        
        <div className="keyboard-shortcuts-content">
          {Object.entries(groupedActions).map(([scope, scopeActions]) => (
            <div key={scope} className="keyboard-shortcuts-section">
              <h3 className="keyboard-shortcuts-scope">
                {formatScopeName(scope)}
                {scope === activeScope && scope !== 'global' && (
                  <span className="keyboard-shortcuts-active-tag">Active</span>
                )}
              </h3>
              
              <ul className="keyboard-shortcuts-list">
                {scopeActions.map((action, index) => (
                  <li key={index} className="keyboard-shortcuts-item">
                    <span className="keyboard-shortcuts-description">
                      {action.description}
                    </span>
                    <span className="keyboard-shortcuts-keys">
                      <kbd>{getFormattedShortcut(action)}</kbd>
                    </span>
                  </li>
                ))}
              </ul>
            </div>
          ))}
        </div>
        
        <div className="keyboard-shortcuts-footer">
          <p>Press <kbd>?</kbd> to toggle this dialog at any time</p>
        </div>
      </div>
    </div>
  );
};

export default KeyboardProvider;
</file>

<file path="src-frontend/src/main.tsx">
import React from 'react';
import ReactDOM from 'react-dom/client';
import App from './App';
import './index.css';

// Performance measurement for bootstrap time
const startTime = performance.now();

// Log startup metrics
document.addEventListener('DOMContentLoaded', () => {
  const loadTime = performance.now() - startTime;
  console.log(`DOM loaded in ${loadTime.toFixed(2)}ms`);
});

// Initial render must be as fast as possible (<500ms goal)
ReactDOM.createRoot(document.getElementById('root') as HTMLElement).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
);

// Log total render time
window.addEventListener('load', () => {
  const totalTime = performance.now() - startTime;
  console.log(`Total load time: ${totalTime.toFixed(2)}ms`);
  
  // Send metrics to backend (would be implemented in real app)
  // invoke('log_performance', { metric: 'initial_load', value: totalTime });
});
</file>

<file path="src-frontend/src/styles/collaboration.css">
/* Collaboration Module Styles */

/* CollaborationPanel */
.collaboration-panel {
  width: 300px;
  height: 100%;
  display: flex;
  flex-direction: column;
  background-color: #fff;
  border-left: 1px solid #e0e0e0;
  box-shadow: -2px 0 10px rgba(0, 0, 0, 0.1);
  overflow: hidden;
}

.collaboration-panel-header {
  padding: 12px 16px;
  border-bottom: 1px solid #e0e0e0;
  display: flex;
  justify-content: space-between;
  align-items: center;
  background-color: #f5f5f5;
}

.collaboration-panel-title {
  margin: 0;
  font-size: 16px;
  font-weight: 500;
}

.collaboration-panel-content {
  flex: 1;
  overflow-y: auto;
  padding: 16px;
}

.collaboration-panel-footer {
  padding: 12px 16px;
  border-top: 1px solid #e0e0e0;
  background-color: #f5f5f5;
}

/* User List */
.user-list {
  display: flex;
  flex-direction: column;
  gap: 8px;
}

.user-item {
  padding: 10px;
  border-radius: 6px;
  background-color: #f9f9f9;
  border: 1px solid #e0e0e0;
  display: flex;
  align-items: center;
  justify-content: space-between;
}

.user-item-info {
  display: flex;
  align-items: center;
  gap: 10px;
}

.user-avatar {
  width: 32px;
  height: 32px;
  border-radius: 50%;
  overflow: hidden;
  display: flex;
  align-items: center;
  justify-content: center;
  font-weight: 500;
  color: #fff;
}

.user-details {
  display: flex;
  flex-direction: column;
}

.user-name {
  font-weight: 500;
}

.user-role {
  font-size: 12px;
  color: #757575;
}

.user-actions {
  display: flex;
  gap: 8px;
}

/* Cursor Overlay */
.cursor-overlay {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  pointer-events: none;
  z-index: 1000;
}

.cursor-wrapper {
  position: absolute;
  transform: translate(-50%, -50%);
  pointer-events: none;
  transition: left 0.1s ease, top 0.1s ease;
}

.cursor-icon {
  position: relative;
  width: 24px;
  height: 24px;
}

.cursor-badge {
  position: absolute;
  left: 20px;
  top: -10px;
  padding: 4px 8px;
  border-radius: 4px;
  color: #fff;
  font-size: 12px;
  font-weight: 500;
  white-space: nowrap;
  transform: translateY(-100%);
  box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
  display: flex;
  align-items: center;
  gap: 5px;
}

.cursor-badge::after {
  content: '';
  position: absolute;
  bottom: -5px;
  left: 10px;
  width: 0;
  height: 0;
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: 5px solid currentColor;
}

/* Selection Overlay */
.selection-highlight {
  position: absolute;
  border-radius: 2px;
  opacity: 0.3;
  pointer-events: none;
}

/* Call Controls */
.call-controls {
  display: flex;
  justify-content: center;
  gap: 16px;
  padding: 16px;
  border-top: 1px solid #e0e0e0;
}

.call-button {
  width: 48px;
  height: 48px;
  border-radius: 50%;
  border: none;
  cursor: pointer;
  display: flex;
  align-items: center;
  justify-content: center;
}

.call-button.primary {
  background-color: #2196f3;
  color: white;
}

.call-button.danger {
  background-color: #f44336;
  color: white;
}

.call-button.secondary {
  background-color: #e0e0e0;
  color: #212121;
}

.call-button.active {
  background-color: #4caf50;
  color: white;
}

.call-button.inactive {
  background-color: #f44336;
  color: white;
}

.call-participants {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(150px, 1fr));
  gap: 16px;
  padding: 16px;
}

.participant-card {
  background-color: #f5f5f5;
  border-radius: 8px;
  overflow: hidden;
  display: flex;
  flex-direction: column;
}

.video-container {
  position: relative;
  width: 100%;
  padding-bottom: 75%; /* 4:3 aspect ratio */
  background-color: #e0e0e0;
}

.video-placeholder {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 24px;
  font-weight: bold;
  color: #9e9e9e;
}

.participant-info {
  padding: 8px;
  text-align: center;
}

.participant-name {
  font-weight: 500;
  font-size: 14px;
  margin-bottom: 4px;
}

.participant-status {
  font-size: 12px;
  color: #757575;
}

.media-controls {
  position: absolute;
  bottom: 8px;
  right: 8px;
  display: flex;
  gap: 4px;
}

.media-indicator {
  width: 24px;
  height: 24px;
  border-radius: 50%;
  background-color: rgba(0, 0, 0, 0.5);
  display: flex;
  align-items: center;
  justify-content: center;
  color: white;
}

/* Dialogs */
.collaboration-dialog-overlay {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.5);
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: 1100;
}

.collaboration-dialog {
  background-color: white;
  border-radius: 8px;
  width: 400px;
  max-width: 90%;
  padding: 24px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
}

.collaboration-dialog-title {
  margin: 0 0 16px 0;
  font-size: 18px;
  font-weight: 500;
}

.collaboration-form-group {
  margin-bottom: 16px;
}

.collaboration-label {
  display: block;
  margin-bottom: 8px;
  font-weight: 500;
}

.collaboration-input,
.collaboration-select {
  width: 100%;
  padding: 8px 12px;
  border-radius: 4px;
  border: 1px solid #bdbdbd;
  font-size: 14px;
}

.collaboration-button-group {
  display: flex;
  justify-content: flex-end;
  gap: 12px;
  margin-top: 24px;
}

.collaboration-button {
  padding: 8px 16px;
  border-radius: 4px;
  border: none;
  cursor: pointer;
  font-size: 14px;
  font-weight: 500;
}

.collaboration-button.primary {
  background-color: #2196f3;
  color: white;
}

.collaboration-button.secondary {
  background-color: #e0e0e0;
  color: #212121;
}

.collaboration-button.danger {
  background-color: #f44336;
  color: white;
}

.collaboration-error {
  background-color: #ffebee;
  color: #d32f2f;
  padding: 12px;
  border-radius: 4px;
  margin-bottom: 16px;
  font-size: 14px;
}

/* Tab navigation */
.collaboration-tabs {
  display: flex;
  border-bottom: 1px solid #e0e0e0;
}

.collaboration-tab {
  padding: 8px 16px;
  background-color: transparent;
  border: none;
  border-bottom: 2px solid transparent;
  cursor: pointer;
  font-size: 14px;
  font-weight: 500;
}

.collaboration-tab.active {
  border-bottom-color: #2196f3;
  background-color: #e3f2fd;
  font-weight: 600;
}

/* Integration in main UI */
.collaboration-status-button {
  position: relative;
  width: 32px;
  height: 32px;
  border-radius: 50%;
  background-color: transparent;
  border: none;
  cursor: pointer;
  display: flex;
  align-items: center;
  justify-content: center;
  margin-left: 8px;
}

.collaboration-status-indicator {
  position: absolute;
  bottom: 0;
  right: 0;
  width: 10px;
  height: 10px;
  border-radius: 50%;
  border: 2px solid white;
}

.collaboration-panel-container {
  position: absolute;
  top: 0;
  right: 0;
  width: 300px;
  height: 100%;
  z-index: 100;
  animation: slideIn 0.3s ease;
}

@keyframes slideIn {
  from {
    transform: translateX(100%);
  }
  to {
    transform: translateX(0);
  }
}
</file>

<file path="src-frontend/src/styles/observability.css">
/* Observability Components Styling */

/* Common */
.loading {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  height: 300px;
}

.loading::after {
  content: "";
  width: 40px;
  height: 40px;
  border: 4px solid #f3f3f3;
  border-top: 4px solid #3498db;
  border-radius: 50%;
  animation: spin 1s linear infinite;
}

@keyframes spin {
  0% { transform: rotate(0deg); }
  100% { transform: rotate(360deg); }
}

.error {
  background-color: #fff1f0;
  border: 1px solid #ffccc7;
  border-radius: 4px;
  padding: 16px;
  margin-bottom: 20px;
  color: #cf1322;
}

.feature-disabled {
  background-color: #f5f5f5;
  border: 1px solid #d9d9d9;
  border-radius: 4px;
  padding: 16px;
  margin-bottom: 20px;
}

/* Buttons */
.primary-button {
  background-color: #1890ff;
  color: white;
  border: none;
  padding: 8px 16px;
  border-radius: 4px;
  cursor: pointer;
  font-weight: 500;
}

.primary-button:hover {
  background-color: #40a9ff;
}

.secondary-button {
  background-color: white;
  color: #1890ff;
  border: 1px solid #1890ff;
  padding: 8px 16px;
  border-radius: 4px;
  cursor: pointer;
  font-weight: 500;
}

.secondary-button:hover {
  background-color: #e6f7ff;
}

.danger-button {
  background-color: white;
  color: #ff4d4f;
  border: 1px solid #ff4d4f;
  padding: 8px 16px;
  border-radius: 4px;
  cursor: pointer;
  font-weight: 500;
}

.danger-button:hover {
  background-color: #fff1f0;
}

/* Toggle Switch */
.switch {
  position: relative;
  display: inline-block;
  width: 50px;
  height: 24px;
}

.switch input {
  opacity: 0;
  width: 0;
  height: 0;
}

.slider {
  position: absolute;
  cursor: pointer;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: #ccc;
  transition: .4s;
}

.slider:before {
  position: absolute;
  content: "";
  height: 16px;
  width: 16px;
  left: 4px;
  bottom: 4px;
  background-color: white;
  transition: .4s;
}

input:checked + .slider {
  background-color: #1890ff;
}

input:focus + .slider {
  box-shadow: 0 0 1px #1890ff;
}

input:checked + .slider:before {
  transform: translateX(26px);
}

.slider.round {
  border-radius: 34px;
}

.slider.round:before {
  border-radius: 50%;
}

/* Alerts */
.alert-info {
  background-color: #e6f7ff;
  border: 1px solid #91d5ff;
  border-radius: 4px;
  padding: 8px 16px;
  margin-bottom: 16px;
  color: #096dd9;
}

.alert-warning {
  background-color: #fffbe6;
  border: 1px solid #ffe58f;
  border-radius: 4px;
  padding: 8px 16px;
  margin-bottom: 16px;
  color: #d48806;
}

/* Progress Bar */
.progress-bar {
  width: 100%;
  height: 8px;
  background-color: #f5f5f5;
  border-radius: 4px;
  overflow: hidden;
}

.progress-fill {
  height: 100%;
  background-color: #1890ff;
  border-radius: 4px;
}

.progress-fill.error {
  background-color: #ff4d4f;
}

.progress-fill.success {
  background-color: #52c41a;
}

/* Feature Chips */
.feature-chip {
  display: inline-block;
  padding: 2px 8px;
  margin: 2px;
  border-radius: 16px;
  background-color: #e6f7ff;
  color: #1890ff;
  font-size: 12px;
}

.feature-chip.highlight {
  background-color: #1890ff;
  color: white;
}

.feature-chip.outline {
  background-color: white;
  border: 1px solid #1890ff;
  color: #1890ff;
}

/* Tabs */
.dashboard-tabs {
  margin-bottom: 20px;
  border-bottom: 1px solid #f0f0f0;
  display: flex;
}

.dashboard-tabs button {
  padding: 8px 16px;
  margin-right: 16px;
  background: none;
  border: none;
  border-bottom: 2px solid transparent;
  cursor: pointer;
  font-weight: 500;
  color: #595959;
}

.dashboard-tabs button.active {
  color: #1890ff;
  border-bottom: 2px solid #1890ff;
}

.dashboard-tabs button:hover:not(.active) {
  color: #40a9ff;
}

/* Resource Dashboard */
.resource-dashboard {
  padding: 16px;
}

.dashboard-header {
  margin-bottom: 24px;
}

.system-info {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
  gap: 16px;
  margin-bottom: 16px;
  background-color: #f5f5f5;
  border-radius: 4px;
  padding: 16px;
}

.info-item strong {
  font-weight: 500;
  margin-right: 8px;
}

.overview-charts {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(500px, 1fr));
  gap: 16px;
}

.chart-container {
  background-color: white;
  border-radius: 4px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  padding: 16px;
  margin-bottom: 16px;
}

.chart-container.full-width {
  grid-column: 1 / -1;
}

.chart-container h3 {
  margin-top: 0;
  margin-bottom: 16px;
}

.stats-grid {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
  gap: 16px;
  margin-top: 24px;
}

.stat-card {
  background-color: white;
  border-radius: 4px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  padding: 16px;
}

.stat-card h4 {
  margin-top: 0;
  margin-bottom: 8px;
  color: #8c8c8c;
}

.stat-card .stat-value {
  font-size: 24px;
  font-weight: 500;
}

.dashboard-controls {
  margin-top: 24px;
  display: flex;
  align-items: center;
  justify-content: flex-end;
}

.dashboard-controls label {
  margin-right: 16px;
}

.dashboard-controls select {
  margin-left: 8px;
  padding: 4px 8px;
  border-radius: 4px;
  border: 1px solid #d9d9d9;
}

/* Privacy Settings */
.privacy-settings {
  padding: 16px;
}

.telemetry-main-card {
  background-color: white;
  border-radius: 4px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  padding: 16px;
  margin-bottom: 24px;
}

.telemetry-main-toggle {
  display: flex;
  align-items: center;
  margin-bottom: 16px;
}

.telemetry-main-toggle h3 {
  margin: 0;
  margin-right: 16px;
}

.telemetry-main-toggle span {
  margin-left: 8px;
}

.client-id-section {
  display: flex;
  align-items: center;
  justify-content: space-between;
  margin-top: 16px;
  padding-top: 16px;
  border-top: 1px solid #f0f0f0;
}

.client-id-section code {
  background-color: #f5f5f5;
  padding: 4px 8px;
  border-radius: 4px;
  font-family: monospace;
}

.category-toggles {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
  gap: 16px;
}

.category-card {
  background-color: white;
  border-radius: 4px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  padding: 16px;
}

.category-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  margin-bottom: 8px;
}

.category-header h4 {
  margin: 0;
}

.subcategory-list {
  margin-top: 16px;
  padding-top: 16px;
  border-top: 1px solid #f0f0f0;
}

.subcategory-list h5 {
  margin-top: 0;
  margin-bottom: 8px;
}

.subcategory-list ul {
  margin: 0;
  padding-left: 20px;
}

.settings-actions {
  margin-top: 24px;
  display: flex;
  justify-content: flex-end;
}

.settings-actions button {
  margin-left: 8px;
}

.data-control-card {
  background-color: white;
  border-radius: 4px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  padding: 16px;
  margin-top: 24px;
}

.confirm-delete {
  margin-top: 16px;
}

.confirm-delete .warning {
  color: #ff4d4f;
  font-weight: 500;
  margin-bottom: 8px;
}

.confirm-buttons {
  display: flex;
  gap: 8px;
}

/* Canary Dashboard */
.canary-dashboard {
  padding: 16px;
}

.user-canary-status {
  margin-bottom: 24px;
}

.status-card {
  background-color: white;
  border-radius: 4px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  padding: 16px;
}

.canary-opt-in {
  margin-top: 16px;
}

.canary-opt-in select {
  width: 100%;
  padding: 8px;
  border-radius: 4px;
  border: 1px solid #d9d9d9;
}

.canary-groups {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
  gap: 16px;
}

.canary-group-card {
  background-color: white;
  border-radius: 4px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  padding: 16px;
}

.group-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  margin-bottom: 8px;
}

.group-header h3 {
  margin: 0;
}

.group-stats {
  display: flex;
  gap: 16px;
  margin-top: 16px;
  margin-bottom: 16px;
}

.stat {
  flex: 1;
  text-align: center;
  background-color: #f5f5f5;
  border-radius: 4px;
  padding: 8px;
}

.stat h4 {
  margin: 0;
  margin-bottom: 4px;
  font-size: 12px;
  color: #8c8c8c;
}

.stat .value {
  font-size: 20px;
  font-weight: 500;
}

.rollout-percentage {
  margin-bottom: 16px;
}

.rollout-percentage input {
  width: 100%;
  margin: 8px 0;
}

.percentage-labels {
  display: flex;
  justify-content: space-between;
  font-size: 12px;
  color: #8c8c8c;
}

.active-features {
  margin-top: 16px;
  padding-top: 16px;
  border-top: 1px solid #f0f0f0;
}

.active-features h4 {
  margin-top: 0;
  margin-bottom: 8px;
}

.feature-chips {
  display: flex;
  flex-wrap: wrap;
  gap: 4px;
}

.canary-features {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(400px, 1fr));
  gap: 16px;
}

.feature-card {
  background-color: white;
  border-radius: 4px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  padding: 16px;
}

.feature-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  margin-bottom: 8px;
}

.feature-header h3 {
  margin: 0;
}

.feature-meta {
  display: flex;
  align-items: center;
  justify-content: space-between;
  margin: 16px 0;
}

.dates {
  display: flex;
  flex-direction: column;
  font-size: 12px;
  color: #8c8c8c;
}

.feature-percentage {
  margin-bottom: 16px;
}

.feature-dependencies {
  margin-top: 16px;
  padding-top: 16px;
  border-top: 1px solid #f0f0f0;
}

.feature-dependencies h4 {
  margin-top: 0;
  margin-bottom: 8px;
}

.feature-actions {
  margin-top: 16px;
  display: flex;
  gap: 8px;
}

.canary-metrics {
  display: flex;
  flex-direction: column;
  gap: 24px;
}

.metrics-card {
  background-color: white;
  border-radius: 4px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  padding: 16px;
}

.metrics-grid {
  display: grid;
  grid-template-columns: 1fr 1fr;
  grid-template-rows: auto auto;
  gap: 16px;
  margin: 16px 0;
}

.performance-chart {
  grid-column: 1 / -1;
}

.rate-bars {
  display: flex;
  flex-direction: column;
  gap: 16px;
  margin: 16px 0;
}

.rate-bar {
  display: flex;
  flex-direction: column;
  gap: 4px;
}

.bar-label {
  font-size: 12px;
}

.metrics-actions {
  margin-top: 16px;
  display: flex;
  gap: 8px;
}

.no-metrics, .no-canary-features {
  background-color: #f5f5f5;
  border-radius: 4px;
  padding: 24px;
  text-align: center;
  color: #8c8c8c;
}
</file>

<file path="src-frontend/src/styles/whiteboard.css">
/* Collaborative Whiteboard Styles */

.collaborative-whiteboard {
  display: flex;
  flex-direction: column;
  border: 1px solid #e0e0e0;
  border-radius: 8px;
  overflow: hidden;
  background-color: #fff;
  box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
}

.whiteboard-toolbar {
  display: flex;
  padding: 8px;
  background-color: #f5f5f5;
  border-bottom: 1px solid #e0e0e0;
  gap: 16px;
  align-items: center;
}

.tool-group {
  display: flex;
  gap: 4px;
}

.tool-button {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  border-radius: 4px;
  border: 1px solid #e0e0e0;
  background-color: white;
  cursor: pointer;
  transition: all 0.2s ease;
}

.tool-button:hover {
  background-color: #f0f0f0;
}

.tool-button.active {
  background-color: #e3f2fd;
  border-color: #2196f3;
  color: #2196f3;
}

.color-group {
  display: flex;
  align-items: center;
}

.color-group input[type="color"] {
  width: 32px;
  height: 32px;
  border: 1px solid #e0e0e0;
  border-radius: 4px;
  padding: 0;
  background-color: white;
  cursor: pointer;
}

.size-group {
  display: flex;
  align-items: center;
  flex: 1;
}

.size-group input[type="range"] {
  width: 100%;
  max-width: 200px;
}

.whiteboard-canvas {
  flex: 1;
  background-color: white;
  cursor: crosshair;
  touch-action: none;
}

/* User cursors on the whiteboard */
.whiteboard-cursor {
  position: absolute;
  pointer-events: none;
  z-index: 1000;
}

.whiteboard-cursor-icon {
  width: 24px;
  height: 24px;
  transform: translate(-50%, -50%);
}

.whiteboard-user-badge {
  position: absolute;
  left: 12px;
  top: -4px;
  transform: translateY(-100%);
  padding: 2px 6px;
  border-radius: 4px;
  font-size: 12px;
  white-space: nowrap;
  color: white;
  opacity: 0.9;
}

/* Responsive adjustments */
@media (max-width: 768px) {
  .whiteboard-toolbar {
    flex-wrap: wrap;
  }
  
  .size-group {
    width: 100%;
    margin-top: 8px;
  }
  
  .size-group input[type="range"] {
    max-width: none;
  }
}
</file>

<file path="src-frontend/src/theme/index.ts">
export * from './ThemeContext';
export * from './ThemeToggle';
</file>

<file path="src-frontend/src/theme/ThemeContext.tsx">
import React, { createContext, useContext, useState, useEffect } from 'react';

export type ThemeType = 'light' | 'dark' | 'system';

interface ThemeContextType {
  theme: ThemeType;
  setTheme: (theme: ThemeType) => void;
  actualTheme: 'light' | 'dark'; // The actual applied theme after system preference is applied
}

const ThemeContext = createContext<ThemeContextType>({
  theme: 'system',
  setTheme: () => {},
  actualTheme: 'light',
});

export const useTheme = () => useContext(ThemeContext);

export const ThemeProvider: React.FC<{ children: React.ReactNode }> = ({ children }) => {
  // Get saved theme from localStorage or default to system
  const getSavedTheme = (): ThemeType => {
    const savedTheme = localStorage.getItem('mcp-theme');
    return (savedTheme as ThemeType) || 'system';
  };

  const [theme, setTheme] = useState<ThemeType>(getSavedTheme());
  const [actualTheme, setActualTheme] = useState<'light' | 'dark'>('light');

  // Function to determine if system prefers dark mode
  const systemPrefersDark = () => {
    return window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
  };

  // Update the theme
  const updateTheme = (newTheme: ThemeType) => {
    setTheme(newTheme);
    localStorage.setItem('mcp-theme', newTheme);
    
    // Determine the actual theme to apply
    if (newTheme === 'system') {
      setActualTheme(systemPrefersDark() ? 'dark' : 'light');
    } else {
      setActualTheme(newTheme);
    }
  };

  // Set up the initial theme
  useEffect(() => {
    updateTheme(theme);
    
    // Listen for system theme changes
    const mediaQuery = window.matchMedia('(prefers-color-scheme: dark)');
    
    const handleSystemThemeChange = (e: MediaQueryListEvent) => {
      if (theme === 'system') {
        setActualTheme(e.matches ? 'dark' : 'light');
      }
    };
    
    mediaQuery.addEventListener('change', handleSystemThemeChange);
    
    return () => {
      mediaQuery.removeEventListener('change', handleSystemThemeChange);
    };
  }, [theme]);

  // Apply the theme to the document
  useEffect(() => {
    document.documentElement.setAttribute('data-theme', actualTheme);
    
    // Also add a class for easier styling
    if (actualTheme === 'dark') {
      document.documentElement.classList.add('dark-theme');
      document.documentElement.classList.remove('light-theme');
    } else {
      document.documentElement.classList.add('light-theme');
      document.documentElement.classList.remove('dark-theme');
    }
  }, [actualTheme]);

  return (
    <ThemeContext.Provider value={{ theme, setTheme: updateTheme, actualTheme }}>
      {children}
    </ThemeContext.Provider>
  );
};
</file>

<file path="src-frontend/src/theme/ThemeToggle.css">
.theme-toggle {
  display: flex;
  align-items: center;
}

.theme-selector {
  padding: var(--spacing-xs) var(--spacing-sm);
  border-radius: var(--radius-md);
  background-color: var(--color-surface-variant);
  color: var(--color-on-surface);
  border: 1px solid var(--color-border);
  font-size: var(--font-size-sm);
  cursor: pointer;
  transition: all var(--transition-fast);
}

.theme-selector:hover {
  border-color: var(--color-primary);
}

.theme-selector:focus {
  outline: none;
  border-color: var(--color-primary);
  box-shadow: 0 0 0 2px var(--color-primary-light);
}
</file>

<file path="src-frontend/src/theme/ThemeToggle.tsx">
import React from 'react';
import { useTheme, ThemeType } from './ThemeContext';
import './ThemeToggle.css';

export const ThemeToggle: React.FC = () => {
  const { theme, setTheme } = useTheme();
  
  const handleThemeChange = (e: React.ChangeEvent<HTMLSelectElement>) => {
    setTheme(e.target.value as ThemeType);
  };
  
  return (
    <div className="theme-toggle">
      <select 
        value={theme}
        onChange={handleThemeChange}
        className="theme-selector"
        aria-label="Select theme"
      >
        <option value="light">Light</option>
        <option value="dark">Dark</option>
        <option value="system">System</option>
      </select>
    </div>
  );
};

export default ThemeToggle;
</file>

<file path="src-frontend/src/theme/variables.css">
:root {
  /* Base colors - light theme default */
  --color-primary: #7963d2;
  --color-primary-light: #9883e6;
  --color-primary-dark: #5946a9;
  
  --color-secondary: #4a8cfa;
  --color-secondary-light: #72a9ff;
  --color-secondary-dark: #2a6cd8;
  
  --color-success: #34c759;
  --color-warning: #ffcc00;
  --color-error: #ff3b30;
  --color-info: #5ac8fa;
  
  /* UI color scheme - light mode */
  --color-background: #ffffff;
  --color-surface: #f8f9fa;
  --color-surface-variant: #f0f1f4;
  
  --color-on-background: #1c1c1e;
  --color-on-surface: #2c2c2e;
  --color-on-surface-variant: #6c6c70;
  
  --color-border: #e6e6e6;
  --color-divider: #ebebeb;
  
  /* Typography */
  --font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen,
    Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
  
  --font-size-xs: 0.75rem;   /* 12px */
  --font-size-sm: 0.875rem;  /* 14px */
  --font-size-md: 1rem;      /* 16px */
  --font-size-lg: 1.125rem;  /* 18px */
  --font-size-xl: 1.25rem;   /* 20px */
  --font-size-2xl: 1.5rem;   /* 24px */
  --font-size-3xl: 1.875rem; /* 30px */
  
  /* Spacing */
  --spacing-xxs: 0.25rem;  /* 4px */
  --spacing-xs: 0.5rem;    /* 8px */
  --spacing-sm: 0.75rem;   /* 12px */
  --spacing-md: 1rem;      /* 16px */
  --spacing-lg: 1.5rem;    /* 24px */
  --spacing-xl: 2rem;      /* 32px */
  --spacing-2xl: 2.5rem;   /* 40px */
  --spacing-3xl: 3rem;     /* 48px */
  
  /* Shadows */
  --shadow-sm: 0 1px 2px rgba(0, 0, 0, 0.05);
  --shadow-md: 0 4px 6px rgba(0, 0, 0, 0.05), 0 1px 3px rgba(0, 0, 0, 0.1);
  --shadow-lg: 0 10px 15px rgba(0, 0, 0, 0.05), 0 4px 6px rgba(0, 0, 0, 0.05);
  
  /* Border Radius */
  --radius-sm: 0.25rem;  /* 4px */
  --radius-md: 0.5rem;   /* 8px */
  --radius-lg: 0.75rem;  /* 12px */
  --radius-xl: 1rem;     /* 16px */
  --radius-full: 9999px;
  
  /* Transition */
  --transition-fast: 150ms ease;
  --transition-normal: 250ms ease;
  --transition-slow: 350ms ease;
  
  /* Z-index layers */
  --z-below: -1;
  --z-normal: 1;
  --z-overlay: 10;
  --z-dropdown: 20;
  --z-sticky: 30;
  --z-drawer: 40;
  --z-modal: 50;
  --z-tooltip: 60;
}

/* Dark theme */
:root.dark-theme {
  --color-primary: #8b7de0;
  --color-primary-light: #a99df1;
  --color-primary-dark: #6c5dc8;
  
  --color-secondary: #5c9aff;
  --color-secondary-light: #81b3ff;
  --color-secondary-dark: #4080e6;
  
  /* Dark mode UI colors */
  --color-background: #1c1c1e;
  --color-surface: #2c2c2e;
  --color-surface-variant: #3a3a3c;
  
  --color-on-background: #f2f2f7;
  --color-on-surface: #e5e5ea;
  --color-on-surface-variant: #aeaeb2;
  
  --color-border: #3a3a3c;
  --color-divider: #444446;
  
  /* Adjust shadows for dark mode */
  --shadow-sm: 0 1px 2px rgba(0, 0, 0, 0.2);
  --shadow-md: 0 4px 6px rgba(0, 0, 0, 0.2), 0 1px 3px rgba(0, 0, 0, 0.3);
  --shadow-lg: 0 10px 15px rgba(0, 0, 0, 0.2), 0 4px 6px rgba(0, 0, 0, 0.2);
}
</file>

<file path="src-frontend/src/tours/index.ts">
export * from './TourProvider';
export { default as TourButton } from './TourButton';
export { default as TourList } from './TourList';
</file>

<file path="src-frontend/src/tours/TourButton.tsx">
import React from 'react';
import { useTour } from './TourProvider';

interface TourButtonProps {
  tourId: string;
  className?: string;
}

const TourButton: React.FC<TourButtonProps> = ({ tourId, className = '' }) => {
  const { startTour, availableTours, completedTours } = useTour();
  
  const tour = availableTours.find(t => t.id === tourId);
  const isCompleted = completedTours.includes(tourId);
  
  if (!tour) return null;
  
  return (
    <button
      className={`tour-start-button ${className} ${isCompleted ? 'tour-start-button-completed' : ''}`}
      onClick={() => startTour(tourId)}
      aria-label={`Start ${tour.name} tour`}
    >
      <svg
        xmlns="http://www.w3.org/2000/svg"
        viewBox="0 0 24 24"
        fill="none"
        stroke="currentColor"
        strokeWidth="2"
        strokeLinecap="round"
        strokeLinejoin="round"
      >
        <circle cx="12" cy="12" r="10" />
        <line x1="12" y1="16" x2="12" y2="12" />
        <line x1="12" y1="8" x2="12.01" y2="8" />
      </svg>
      {isCompleted ? 'Replay Tour' : 'Take Tour'}
    </button>
  );
};

export default TourButton;
</file>

<file path="src-frontend/src/tours/TourList.tsx">
import React from 'react';
import { useTour } from './TourProvider';

const TourList: React.FC = () => {
  const { availableTours, completedTours, startTour } = useTour();
  
  if (availableTours.length === 0) {
    return <p>No tours available.</p>;
  }
  
  return (
    <ul className="tour-list">
      {availableTours.map(tour => {
        const isCompleted = completedTours.includes(tour.id);
        
        return (
          <li 
            key={tour.id} 
            className={`tour-list-item ${isCompleted ? 'tour-list-item-completed' : ''}`}
            onClick={() => startTour(tour.id)}
          >
            <span className="tour-list-item-name">{tour.name}</span>
            <span 
              className={`tour-list-item-badge ${
                isCompleted 
                  ? 'tour-list-item-badge-completed' 
                  : 'tour-list-item-badge-new'
              }`}
            >
              {isCompleted ? 'Completed' : 'New'}
            </span>
          </li>
        );
      })}
    </ul>
  );
};

export default TourList;
</file>

<file path="src-frontend/src/tours/TourProvider.tsx">
import React, { createContext, useContext, useState, useEffect, ReactNode } from 'react';
import './tours.css';

// Step interface for tour
export interface TourStep {
  target: string;
  title: string;
  content: string;
  placement?: 'top' | 'right' | 'bottom' | 'left';
  disableOverlay?: boolean;
}

// Tour interface
export interface Tour {
  id: string;
  name: string;
  steps: TourStep[];
}

// Progress tracking
interface TourProgress {
  tourId: string;
  currentStep: number;
  completed: boolean;
}

// Context interface
interface TourContextType {
  activeTour: TourProgress | null;
  availableTours: Tour[];
  completedTours: string[];
  startTour: (tourId: string) => void;
  endTour: () => void;
  nextStep: () => void;
  prevStep: () => void;
  skipTour: () => void;
  registerTour: (tour: Tour) => void;
  resetTourHistory: () => void;
}

const TourContext = createContext<TourContextType>({
  activeTour: null,
  availableTours: [],
  completedTours: [],
  startTour: () => {},
  endTour: () => {},
  nextStep: () => {},
  prevStep: () => {},
  skipTour: () => {},
  registerTour: () => {},
  resetTourHistory: () => {},
});

export const useTour = () => useContext(TourContext);

interface TourProviderProps {
  children: ReactNode;
}

export const TourProvider: React.FC<TourProviderProps> = ({ children }) => {
  const [availableTours, setAvailableTours] = useState<Tour[]>([]);
  const [activeTour, setActiveTour] = useState<TourProgress | null>(null);
  const [completedTours, setCompletedTours] = useState<string[]>(() => {
    const saved = localStorage.getItem('mcp-completed-tours');
    return saved ? JSON.parse(saved) : [];
  });

  // Save completed tours to localStorage
  useEffect(() => {
    localStorage.setItem('mcp-completed-tours', JSON.stringify(completedTours));
  }, [completedTours]);

  // Register a new tour
  const registerTour = (tour: Tour) => {
    setAvailableTours(prev => {
      // Check if tour already exists
      const exists = prev.some(t => t.id === tour.id);
      if (exists) {
        return prev.map(t => t.id === tour.id ? tour : t);
      }
      return [...prev, tour];
    });
  };

  // Start a tour
  const startTour = (tourId: string) => {
    const tour = availableTours.find(t => t.id === tourId);
    if (!tour) {
      console.error(`Tour with id ${tourId} not found`);
      return;
    }

    setActiveTour({
      tourId,
      currentStep: 0,
      completed: false,
    });
  };

  // End a tour
  const endTour = () => {
    if (activeTour) {
      setCompletedTours(prev => {
        if (!prev.includes(activeTour.tourId)) {
          return [...prev, activeTour.tourId];
        }
        return prev;
      });
    }
    setActiveTour(null);
  };

  // Go to next step
  const nextStep = () => {
    if (!activeTour) return;

    const tour = availableTours.find(t => t.id === activeTour.tourId);
    if (!tour) return;

    if (activeTour.currentStep < tour.steps.length - 1) {
      setActiveTour({
        ...activeTour,
        currentStep: activeTour.currentStep + 1,
      });
    } else {
      setActiveTour({
        ...activeTour,
        completed: true,
      });
      endTour();
    }
  };

  // Go to previous step
  const prevStep = () => {
    if (!activeTour) return;

    if (activeTour.currentStep > 0) {
      setActiveTour({
        ...activeTour,
        currentStep: activeTour.currentStep - 1,
      });
    }
  };

  // Skip the tour
  const skipTour = () => {
    endTour();
  };

  // Reset tour history
  const resetTourHistory = () => {
    setCompletedTours([]);
    localStorage.removeItem('mcp-completed-tours');
  };

  // Scroll to the target element and highlight it
  useEffect(() => {
    if (!activeTour) return;

    const tour = availableTours.find(t => t.id === activeTour.tourId);
    if (!tour) return;

    const step = tour.steps[activeTour.currentStep];
    if (!step) return;

    const target = document.querySelector(step.target);
    if (!target) return;

    // Scroll to target if needed
    target.scrollIntoView({
      behavior: 'smooth',
      block: 'center',
    });

    // Add highlight class to target
    target.classList.add('tour-target');

    return () => {
      target.classList.remove('tour-target');
    };
  }, [activeTour, availableTours]);

  return (
    <TourContext.Provider
      value={{
        activeTour,
        availableTours,
        completedTours,
        startTour,
        endTour,
        nextStep,
        prevStep,
        skipTour,
        registerTour,
        resetTourHistory,
      }}
    >
      {children}
      {activeTour && <TourOverlay />}
    </TourContext.Provider>
  );
};

// Helper function to calculate best placement
const calculateBestPlacement = (targetRect: DOMRect): 'top' | 'right' | 'bottom' | 'left' => {
  const windowHeight = window.innerHeight;
  const windowWidth = window.innerWidth;
  
  // Calculate available space in each direction
  const topSpace = targetRect.top;
  const rightSpace = windowWidth - (targetRect.left + targetRect.width);
  const bottomSpace = windowHeight - (targetRect.top + targetRect.height);
  const leftSpace = targetRect.left;
  
  // Find direction with most space
  const spaces = [
    { direction: 'bottom', space: bottomSpace },
    { direction: 'left', space: leftSpace },
    { direction: 'right', space: rightSpace },
    { direction: 'top', space: topSpace },
  ];
  
  spaces.sort((a, b) => b.space - a.space);
  
  return spaces[0].direction as 'top' | 'right' | 'bottom' | 'left';
};

// Helper function to calculate tooltip position
const calculateTooltipPosition = (
  targetRect: DOMRect, 
  placement: 'top' | 'right' | 'bottom' | 'left'
): { top: number; left: number; } => {
  const OFFSET = 12; // Distance between target and tooltip
  
  switch (placement) {
    case 'top':
      return {
        top: targetRect.top - OFFSET,
        left: targetRect.left + targetRect.width / 2,
      };
    case 'right':
      return {
        top: targetRect.top + targetRect.height / 2,
        left: targetRect.right + OFFSET,
      };
    case 'bottom':
      return {
        top: targetRect.bottom + OFFSET,
        left: targetRect.left + targetRect.width / 2,
      };
    case 'left':
      return {
        top: targetRect.top + targetRect.height / 2,
        left: targetRect.left - OFFSET,
      };
  }
};

// Tour overlay component
const TourOverlay: React.FC = () => {
  const { activeTour, availableTours, nextStep, prevStep, skipTour } = useTour();

  if (!activeTour) return null;

  const tour = availableTours.find(t => t.id === activeTour.tourId);
  if (!tour) return null;

  const step = tour.steps[activeTour.currentStep];
  if (!step) return null;

  const target = document.querySelector(step.target);
  if (!target) return null;

  // Get target position for tooltip placement
  const targetRect = target.getBoundingClientRect();
  const placement = step.placement || calculateBestPlacement(targetRect);

  // Calculate tooltip position
  const tooltipPosition = calculateTooltipPosition(targetRect, placement);

  return (
    <div className="tour-overlay">
      {!step.disableOverlay && <div className="tour-overlay-background" />}
      
      <div 
        className={`tour-tooltip tour-tooltip-${placement}`}
        style={{
          top: `${tooltipPosition.top}px`,
          left: `${tooltipPosition.left}px`,
        }}
      >
        <div className="tour-tooltip-arrow" />
        
        <div className="tour-tooltip-content">
          <div className="tour-tooltip-header">
            <h3>{step.title}</h3>
            <button 
              className="tour-tooltip-close" 
              onClick={skipTour}
              aria-label="Close tour"
            >
              &times;
            </button>
          </div>
          
          <div className="tour-tooltip-body">
            {step.content}
          </div>
          
          <div className="tour-tooltip-footer">
            <div className="tour-tooltip-progress">
              {activeTour.currentStep + 1} of {tour.steps.length}
            </div>
            
            <div className="tour-tooltip-actions">
              {activeTour.currentStep > 0 && (
                <button 
                  className="tour-tooltip-button tour-tooltip-button-secondary" 
                  onClick={prevStep}
                >
                  Previous
                </button>
              )}
              
              <button 
                className="tour-tooltip-button tour-tooltip-button-primary" 
                onClick={nextStep}
              >
                {activeTour.currentStep < tour.steps.length - 1 ? 'Next' : 'Finish'}
              </button>
              
              <button
                className="tour-tooltip-button tour-tooltip-button-text"
                onClick={skipTour}
              >
                Skip
              </button>
            </div>
          </div>
        </div>
      </div>
      
      {/* Highlight the target element */}
      <div 
        className="tour-target-highlight"
        style={{
          top: `${targetRect.top}px`,
          left: `${targetRect.left}px`,
          width: `${targetRect.width}px`,
          height: `${targetRect.height}px`,
        }}
      />
    </div>
  );
};

export default TourProvider;
</file>

<file path="src-frontend/src/tours/tours.css">
/* Tour styles */

.tour-overlay {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  z-index: var(--z-tooltip);
  pointer-events: none;
}

.tour-overlay-background {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background-color: rgba(0, 0, 0, 0.5);
  animation: fadeIn var(--animation-duration-fast) var(--animation-easing-standard);
}

.tour-target {
  position: relative;
  z-index: calc(var(--z-tooltip) + 1);
}

.tour-target-highlight {
  position: absolute;
  z-index: calc(var(--z-tooltip) + 1);
  border-radius: var(--radius-md);
  box-shadow: 0 0 0 4px var(--color-primary), 0 0 0 10000px rgba(0, 0, 0, 0.5);
  pointer-events: none;
  animation: pulse 2s infinite var(--animation-easing-standard);
}

.tour-tooltip {
  position: absolute;
  z-index: calc(var(--z-tooltip) + 2);
  width: 320px;
  transform: translate(-50%, -50%);
  pointer-events: auto;
  animation: scaleIn var(--animation-duration-normal) var(--animation-easing-spring);
}

.tour-tooltip-top {
  transform: translate(-50%, -100%);
  margin-top: -12px;
}

.tour-tooltip-right {
  transform: translateY(-50%);
  margin-left: 12px;
}

.tour-tooltip-bottom {
  transform: translate(-50%, 0);
  margin-top: 12px;
}

.tour-tooltip-left {
  transform: translate(-100%, -50%);
  margin-left: -12px;
}

.tour-tooltip-content {
  background-color: var(--color-surface);
  border-radius: var(--radius-lg);
  box-shadow: var(--shadow-lg);
  overflow: hidden;
  border: 1px solid var(--color-border);
}

.tour-tooltip-arrow {
  position: absolute;
  width: 12px;
  height: 12px;
  background-color: var(--color-surface);
  transform: rotate(45deg);
  border: 1px solid var(--color-border);
}

.tour-tooltip-top .tour-tooltip-arrow {
  bottom: -6px;
  left: 50%;
  margin-left: -6px;
  border-top: none;
  border-left: none;
}

.tour-tooltip-right .tour-tooltip-arrow {
  left: -6px;
  top: 50%;
  margin-top: -6px;
  border-right: none;
  border-bottom: none;
}

.tour-tooltip-bottom .tour-tooltip-arrow {
  top: -6px;
  left: 50%;
  margin-left: -6px;
  border-bottom: none;
  border-right: none;
}

.tour-tooltip-left .tour-tooltip-arrow {
  right: -6px;
  top: 50%;
  margin-top: -6px;
  border-left: none;
  border-top: none;
}

.tour-tooltip-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: var(--spacing-md) var(--spacing-md) var(--spacing-sm);
  border-bottom: 1px solid var(--color-divider);
}

.tour-tooltip-header h3 {
  margin: 0;
  font-size: var(--font-size-lg);
  color: var(--color-on-surface);
}

.tour-tooltip-close {
  background: none;
  border: none;
  font-size: 1.25rem;
  line-height: 1;
  padding: 0;
  cursor: pointer;
  color: var(--color-on-surface-variant);
  transition: color var(--animation-duration-fast) var(--animation-easing-standard);
}

.tour-tooltip-close:hover {
  color: var(--color-on-surface);
}

.tour-tooltip-body {
  padding: var(--spacing-md);
  color: var(--color-on-surface);
  font-size: var(--font-size-md);
  line-height: 1.5;
}

.tour-tooltip-footer {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: var(--spacing-sm) var(--spacing-md);
  border-top: 1px solid var(--color-divider);
  background-color: var(--color-surface-variant);
}

.tour-tooltip-progress {
  font-size: var(--font-size-sm);
  color: var(--color-on-surface-variant);
}

.tour-tooltip-actions {
  display: flex;
  align-items: center;
  gap: var(--spacing-xs);
}

.tour-tooltip-button {
  padding: var(--spacing-xs) var(--spacing-sm);
  border-radius: var(--radius-md);
  font-size: var(--font-size-sm);
  cursor: pointer;
  transition: all var(--animation-duration-fast) var(--animation-easing-standard);
}

.tour-tooltip-button-primary {
  background-color: var(--color-primary);
  color: white;
  border: none;
}

.tour-tooltip-button-primary:hover {
  background-color: var(--color-primary-dark);
}

.tour-tooltip-button-secondary {
  background-color: var(--color-surface);
  color: var(--color-on-surface);
  border: 1px solid var(--color-border);
}

.tour-tooltip-button-secondary:hover {
  background-color: var(--color-surface-variant);
}

.tour-tooltip-button-text {
  background: none;
  border: none;
  color: var(--color-on-surface-variant);
  padding: var(--spacing-xs) var(--spacing-xxs);
}

.tour-tooltip-button-text:hover {
  color: var(--color-on-surface);
  text-decoration: underline;
}

/* Button to start tour */
.tour-start-button {
  display: inline-flex;
  align-items: center;
  gap: var(--spacing-xs);
  padding: var(--spacing-xs) var(--spacing-sm);
  background-color: var(--color-surface-variant);
  color: var(--color-on-surface);
  border: 1px solid var(--color-border);
  border-radius: var(--radius-md);
  font-size: var(--font-size-sm);
  cursor: pointer;
  transition: all var(--animation-duration-fast) var(--animation-easing-standard);
}

.tour-start-button:hover {
  background-color: var(--color-primary);
  color: white;
  border-color: var(--color-primary);
}

.tour-start-button svg {
  width: 16px;
  height: 16px;
}

/* Tour list component */
.tour-list {
  margin: 0;
  padding: 0;
  list-style: none;
}

.tour-list-item {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: var(--spacing-sm) var(--spacing-md);
  border-radius: var(--radius-md);
  margin-bottom: var(--spacing-xs);
  cursor: pointer;
  transition: background-color var(--animation-duration-fast) var(--animation-easing-standard);
}

.tour-list-item:hover {
  background-color: var(--color-surface-variant);
}

.tour-list-item-completed {
  opacity: 0.7;
}

.tour-list-item-name {
  font-weight: 500;
  color: var(--color-on-surface);
}

.tour-list-item-badge {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  padding: 2px var(--spacing-xs);
  border-radius: var(--radius-sm);
  font-size: var(--font-size-xs);
  font-weight: 500;
}

.tour-list-item-badge-completed {
  background-color: var(--color-success);
  color: white;
}

.tour-list-item-badge-new {
  background-color: var(--color-primary);
  color: white;
}
</file>

<file path="src-frontend/src/utils/batch.ts">
/**
 * Creates a function that batches multiple calls into one and processes them together.
 * 
 * @param processBatch The function to process the batch of items
 * @param options Configuration options
 * @returns A function that can be called to add items to the batch
 */
export function createBatcher<T>(
  processBatch: (items: T[]) => Promise<void>,
  options: {
    maxBatchSize?: number;
    maxDelayMs?: number;
  } = {}
): (item: T) => void {
  const { maxBatchSize = 10, maxDelayMs = 100 } = options;
  
  let batch: T[] = [];
  let timeoutId: ReturnType<typeof setTimeout> | null = null;
  
  // Process the current batch
  const flush = async () => {
    if (batch.length === 0) return;
    
    const itemsToProcess = [...batch];
    batch = [];
    timeoutId = null;
    
    try {
      await processBatch(itemsToProcess);
    } catch (error) {
      console.error('Error processing batch:', error);
    }
  };
  
  // Return a function that adds items to the batch
  return (item: T) => {
    batch.push(item);
    
    // If we've reached the max batch size, flush immediately
    if (batch.length >= maxBatchSize) {
      if (timeoutId) {
        clearTimeout(timeoutId);
        timeoutId = null;
      }
      flush();
      return;
    }
    
    // Otherwise, set a timeout to flush after the max delay
    if (!timeoutId) {
      timeoutId = setTimeout(flush, maxDelayMs);
    }
  };
}
</file>

<file path="src-frontend/src/utils/lazyLoad.tsx">
import React, { lazy, Suspense, ComponentType } from 'react';

interface LazyLoadOptions {
  fallback?: React.ReactNode;
  errorComponent?: React.ComponentType<{ error: Error }>;
}

// Default error boundary component
const DefaultErrorComponent: React.FC<{ error: Error }> = ({ error }) => (
  <div className="error-boundary">
    <h3>Something went wrong loading this component</h3>
    <p>{error.message}</p>
    <button onClick={() => window.location.reload()}>
      Reload Application
    </button>
  </div>
);

// Default loading fallback
const DefaultFallback = (
  <div className="loading-component">
    <div className="loading-spinner"></div>
    <p>Loading component...</p>
  </div>
);

// Error boundary component
class ErrorBoundary extends React.Component<
  { children: React.ReactNode; errorComponent: React.ComponentType<{ error: Error }> },
  { hasError: boolean; error: Error | null }
> {
  constructor(props: { children: React.ReactNode; errorComponent: React.ComponentType<{ error: Error }> }) {
    super(props);
    this.state = { hasError: false, error: null };
  }

  static getDerivedStateFromError(error: Error) {
    return { hasError: true, error };
  }

  render() {
    if (this.state.hasError && this.state.error) {
      const ErrorComponent = this.props.errorComponent;
      return <ErrorComponent error={this.state.error} />;
    }

    return this.props.children;
  }
}

// Function to create a lazy-loaded component with error boundary
export function lazyLoad<T extends ComponentType<any>>(
  importFunc: () => Promise<{ default: T }>,
  options: LazyLoadOptions = {}
) {
  const LazyComponent = lazy(importFunc);
  const fallback = options.fallback || DefaultFallback;
  const ErrorComponent = options.errorComponent || DefaultErrorComponent;

  return (props: React.ComponentProps<T>) => (
    <ErrorBoundary errorComponent={ErrorComponent}>
      <Suspense fallback={fallback}>
        <LazyComponent {...props} />
      </Suspense>
    </ErrorBoundary>
  );
}

export default lazyLoad;
</file>

<file path="src-frontend/src/utils/retry.ts">
/**
 * Executes a function with retry logic for handling transient failures.
 * 
 * @param operation The function to execute with retries
 * @param options Configuration options for the retry mechanism
 * @returns A promise that resolves with the result of the operation
 */
export async function withRetry<T>(
  operation: () => Promise<T>,
  options: {
    maxRetries?: number;
    retryDelay?: number;
    backoffFactor?: number;
    retryCondition?: (error: any) => boolean;
  } = {}
): Promise<T> {
  const {
    maxRetries = 3,
    retryDelay = 500,
    backoffFactor = 2,
    retryCondition = () => true,
  } = options;
  
  let retries = 0;
  let delay = retryDelay;
  
  while (true) {
    try {
      return await operation();
    } catch (error) {
      // Check if we've exceeded the maximum number of retries
      if (retries >= maxRetries || !retryCondition(error)) {
        throw error;
      }
      
      // Increment the retry count
      retries++;
      
      // Log the retry
      console.warn(`Operation failed, retrying (${retries}/${maxRetries})...`, error);
      
      // Wait before retrying
      await new Promise(resolve => setTimeout(resolve, delay));
      
      // Increase the delay for the next retry using exponential backoff
      delay *= backoffFactor;
    }
  }
}

/**
 * Creates a version of a function that includes retry logic.
 * 
 * @param fn The function to wrap with retry logic
 * @param options Configuration options for the retry mechanism
 * @returns A new function that includes retry logic
 */
export function createRetryFunction<T extends (...args: any[]) => Promise<any>>(
  fn: T,
  options: {
    maxRetries?: number;
    retryDelay?: number;
    backoffFactor?: number;
    retryCondition?: (error: any) => boolean;
  } = {}
): T {
  return ((...args: Parameters<T>) => {
    return withRetry(() => fn(...args), options);
  }) as T;
}
</file>

<file path="src-frontend/src/utils/throttle.ts">
/**
 * Creates a throttled function that only invokes func at most once per
 * every wait milliseconds.
 *
 * @param func The function to throttle
 * @param wait The number of milliseconds to throttle invocations to
 * @returns The throttled function
 */
export function throttle<T extends (...args: any[]) => any>(
  func: T,
  wait: number
): (...args: Parameters<T>) => void {
  let lastCallTime = 0;
  let lastArgs: Parameters<T> | null = null;
  let timeout: ReturnType<typeof setTimeout> | null = null;
  
  return function throttled(...args: Parameters<T>): void {
    const now = Date.now();
    const timeSinceLastCall = now - lastCallTime;
    
    // Update the last args for deferred execution
    lastArgs = args;
    
    // If enough time has elapsed, call the function immediately
    if (timeSinceLastCall >= wait) {
      lastCallTime = now;
      func(...args);
      return;
    }
    
    // Otherwise, schedule a deferred execution
    if (timeout === null) {
      timeout = setTimeout(() => {
        if (lastArgs !== null) {
          lastCallTime = Date.now();
          func(...lastArgs);
          lastArgs = null;
          timeout = null;
        }
      }, wait - timeSinceLastCall);
    }
  };
}
</file>

<file path="src-frontend/ui-enhancements-summary.md">
# MCP Client UI Enhancements - Summary

This document outlines all the UI enhancements implemented for the MCP client as specified in the requirements: microinteraction system, progressive disclosure, contextual help, animation framework, keyboard navigation, guided tours, and accessibility.

## 1. Animation Framework

A comprehensive animation system has been implemented to provide subtle interface feedback throughout the application.

### Key Features:
- **AnimationProvider**: Context provider for managing animation preferences
- **Customizable Animation Speeds**: Normal, Slow, Fast options
- **Reduced Motion Support**: Respects user's OS preferences
- **Rich Animation Library**: Includes fade, scale, slide, attention-grabbing effects
- **Animation Utilities**: CSS classes to easily apply animations
- **Duration Controls**: Various duration presets for different animation needs

## 2. Keyboard Navigation System

A robust keyboard navigation system allows users to effectively navigate and interact with the application without relying on a mouse.

### Key Features:
- **KeyboardProvider**: Manages keyboard shortcuts and focus state
- **Configurable Shortcuts**: Register, scope, and manage keyboard actions
- **Visual Indicator Support**: Shows keyboard navigation state
- **Keyboard Shortcuts Dialog**: Help screen showing available shortcuts
- **Focus Management**: Improved focus indicators and trapping
- **Scope-based Actions**: Context-aware keyboard shortcuts

## 3. Guided Tours

An interactive tour system walks users through the interface, helping them discover features and learn how to use the application.

### Key Features:
- **TourProvider**: Manages tour state and progression
- **Step-by-Step Guidance**: Multi-step tours with highlighting
- **Tour Persistence**: Remembers completed tours
- **Flexible Positioning**: Tooltips can be positioned around elements
- **Tour Components**: TourButton, TourList for easy integration
- **Discoverable Interface**: Helps users find and learn features

## 4. Accessibility Layer

A comprehensive accessibility layer ensures the application is usable by everyone, regardless of abilities.

### Key Features:
- **AccessibilityProvider**: Manages accessibility preferences
- **High Contrast Mode**: Increases contrast for better readability
- **Large Text Mode**: Increases font sizes throughout the application
- **Reduced Motion Support**: Minimizes animations and transitions
- **Screen Reader Enhancements**: Improved ARIA support and labels
- **Focus Indicators**: Enhanced keyboard focus visibility
- **Dyslexic Font Support**: Optional font for easier reading
- **Accessibility Panel**: Easy access to all accessibility settings

## 5. Contextual Help System

A context-aware help system provides assistance to users exactly when and where they need it.

### Key Features:
- **HelpProvider**: Manages help topics and contextual explanations
- **Help Panel**: Searchable help documentation
- **Contextual Tooltips**: In-context explanations
- **Inline Help Mode**: Shows help indicators near UI elements
- **Topic Categories**: Organized help content
- **Related Topics**: Discovers related help content
- **HelpTrigger Component**: Easily add help to any UI element

## 6. Progressive Disclosure

A progressive disclosure system gradually reveals advanced features as users become more experienced, preventing overwhelming new users.

### Key Features:
- **ProgressiveDisclosureProvider**: Manages user level and feature access
- **Level-Based Features**: Basic, Intermediate, Advanced, Expert tiers
- **Points System**: Rewards user engagement
- **Override Toggle**: Option to show all features regardless of level
- **Level Progress Indicator**: Shows progress toward next level
- **Feature Discovery**: Gradually introduces new capabilities
- **ProgressiveFeature Component**: Conditionally renders based on level

## 7. Microinteraction System

A microinteraction system provides subtle visual feedback that makes the interface feel responsive and alive.

### Key Features:
- **Interaction Hooks**: Reusable hooks for common interactions
  - `useMicroInteraction`: General-purpose animated effects
  - `usePressEffect`: Button press feedback
  - `useHoverEffect`: Delayed hover effects
  - `useRippleEffect`: Material-style ripple effect
  - `useFeedback`: Success/error feedback with points
- **Animation Effects**: Press, pulse, shake, bounce, etc.
- **Visual Feedback**: Success and error states
- **Points Indicator**: Shows earned points
- **Ripple Effects**: Interactive ripple animations
- **Hover Transitions**: Smooth state transitions

## Implementation Details

### File Structure
```
/src
  /animation         - Animation framework
  /keyboard          - Keyboard navigation system
  /accessibility     - Accessibility features
  /tours             - Guided tour system
  /help              - Contextual help system
  /disclosure        - Progressive disclosure
  /interactions      - Microinteraction system
  ...
```

### Integration
- All systems are integrated through context providers in App.tsx
- Each system is tree-shakable and modular for optimal performance
- Components use hooks to access the systems as needed
- Custom React hooks encapsulate behavior for reusability

### Settings Integration
The Settings component has been enhanced to showcase all of these features, allowing users to:
- Customize animation preferences
- View and customize keyboard shortcuts
- Take guided tours of the interface
- Access contextual help
- View their user level progress
- Experience microinteractions throughout the interface
- Configure accessibility features

## Next Steps

### Additional Enhancements
- Add more comprehensive tours for each major feature
- Expand the help documentation with more topics
- Create more advanced microinteractions for specific use cases
- Implement guided workflows for complex tasks
- Add more accessibility features like color blindness support

### Integration with Backend
- Connect feature access to user accounts/permissions
- Store user preferences on the server
- Track feature discovery across sessions
- Collect analytics on feature usage to improve disclosure
</file>

<file path="src-frontend/vite.config.ts">
import { defineConfig } from 'vite';
import react from '@vitejs/plugin-react';

// https://vitejs.dev/config/
export default defineConfig(async () => ({
  plugins: [react()],

  // Vite options tailored for Tauri development
  clearScreen: false,
  server: {
    port: 3000,
    strictPort: true,
  },
  build: {
    // Tauri uses Chromium on Windows and WebKit on macOS and Linux
    target: ['es2021', 'chrome100', 'safari13'],
    minify: !process.env.TAURI_DEBUG ? 'esbuild' : false,
    sourcemap: !!process.env.TAURI_DEBUG,
    
    // Optimize for fast startup
    rollupOptions: {
      output: {
        // Create chunk sizes optimized for fast startup
        manualChunks: {
          'vendor': ['react', 'react-dom'],
          'shell': ['./src/components/Shell.tsx'],
        }
      }
    },
  },
  
  // Optimize preload strategy
  optimizeDeps: {
    include: ['react', 'react-dom'],
  },
}));
</file>

<file path="src-tauri/src/commands/mod.rs">
//! Commands for the Tauri application
//!
//! This module provides command implementations that can be called from the frontend.

pub mod offline;
</file>

<file path="src-tauri/src/commands/offline.rs">
//! Tauri commands for offline features
//! 
//! This module provides commands for interacting with offline features,
//! including local LLM integration.

use std::sync::Arc;
use serde::{Deserialize, Serialize};
use tauri::{command, State};
use tokio::sync::Mutex;

use crate::offline::llm::{
    LLMManager, LLMConfig, ProviderType,
    provider::{GenerationOptions, ModelInfo, DownloadStatus},
};
use crate::offline::OfflineManager;

/// Application state containing the offline manager
pub struct AppState {
    /// Offline manager
    pub offline_manager: Arc<Mutex<OfflineManager>>,
}

/// Response for offline commands
#[derive(Serialize)]
pub struct CommandResponse<T> {
    /// Whether the operation was successful
    pub success: bool,
    /// Optional error message
    pub error: Option<String>,
    /// Optional data
    pub data: Option<T>,
}

impl<T> CommandResponse<T> {
    /// Create a success response
    pub fn success(data: T) -> Self {
        Self {
            success: true,
            error: None,
            data: Some(data),
        }
    }
    
    /// Create an error response
    pub fn error(message: impl Into<String>) -> Self {
        Self {
            success: false,
            error: Some(message.into()),
            data: None,
        }
    }
}

/// Request for configuring the LLM
#[derive(Deserialize)]
pub struct ConfigureLLMRequest {
    /// Provider type to use
    pub provider_type: Option<ProviderType>,
    /// Provider-specific configuration as a JSON object
    pub provider_config: Option<serde_json::Value>,
    /// Default model to use
    pub default_model: Option<String>,
}

/// Request for generating text
#[derive(Deserialize)]
pub struct GenerateTextRequest {
    /// Model to use
    pub model_id: Option<String>,
    /// Prompt for generation
    pub prompt: String,
    /// Maximum number of tokens to generate
    pub max_tokens: Option<u32>,
    /// Temperature for sampling
    pub temperature: Option<f32>,
    /// Top-p sampling
    pub top_p: Option<f32>,
}

/// Configure the LLM provider
#[command]
pub async fn configure_llm(
    state: State<'_, AppState>,
    request: ConfigureLLMRequest,
) -> CommandResponse<bool> {
    let offline_manager = state.offline_manager.lock().await;
    let mut llm_manager = offline_manager.llm_manager().lock().await;
    
    // Create a new configuration based on the current one and the request
    let mut config = LLMConfig::default();
    
    if let Some(provider_type) = request.provider_type {
        config.provider_type = provider_type;
    }
    
    if let Some(provider_config) = request.provider_config {
        config.provider_config = provider_config;
    }
    
    if let Some(default_model) = request.default_model {
        config.default_model = Some(default_model);
    }
    
    // Reinitialize the manager with the new configuration
    *llm_manager = LLMManager::with_config(config);
    
    match llm_manager.initialize().await {
        Ok(_) => CommandResponse::success(true),
        Err(e) => CommandResponse::error(format!("Failed to configure LLM: {}", e)),
    }
}

/// List available models
#[command]
pub async fn list_available_models(
    state: State<'_, AppState>,
) -> CommandResponse<Vec<ModelInfo>> {
    let offline_manager = state.offline_manager.lock().await;
    let llm_manager = offline_manager.llm_manager().lock().await;
    
    match llm_manager.list_available_models().await {
        Ok(models) => CommandResponse::success(models),
        Err(e) => CommandResponse::error(format!("Failed to list models: {}", e)),
    }
}

/// List downloaded models
#[command]
pub async fn list_downloaded_models(
    state: State<'_, AppState>,
) -> CommandResponse<Vec<ModelInfo>> {
    let offline_manager = state.offline_manager.lock().await;
    let llm_manager = offline_manager.llm_manager().lock().await;
    
    match llm_manager.list_downloaded_models().await {
        Ok(models) => CommandResponse::success(models),
        Err(e) => CommandResponse::error(format!("Failed to list models: {}", e)),
    }
}

/// Get model info
#[command]
pub async fn get_model_info(
    state: State<'_, AppState>,
    model_id: String,
) -> CommandResponse<ModelInfo> {
    let offline_manager = state.offline_manager.lock().await;
    let llm_manager = offline_manager.llm_manager().lock().await;
    
    match llm_manager.get_model_info(&model_id).await {
        Ok(model) => CommandResponse::success(model),
        Err(e) => CommandResponse::error(format!("Failed to get model info: {}", e)),
    }
}

/// Download a model
#[command]
pub async fn download_model(
    state: State<'_, AppState>,
    model_id: String,
) -> CommandResponse<bool> {
    let offline_manager = state.offline_manager.lock().await;
    let llm_manager = offline_manager.llm_manager().lock().await;
    
    match llm_manager.download_model(&model_id).await {
        Ok(_) => CommandResponse::success(true),
        Err(e) => CommandResponse::error(format!("Failed to download model: {}", e)),
    }
}

/// Get download status
#[command]
pub async fn get_download_status(
    state: State<'_, AppState>,
    model_id: String,
) -> CommandResponse<DownloadStatus> {
    let offline_manager = state.offline_manager.lock().await;
    let llm_manager = offline_manager.llm_manager().lock().await;
    let provider = match llm_manager.get_provider() {
        Ok(p) => p,
        Err(e) => return CommandResponse::error(format!("Failed to get provider: {}", e)),
    };
    
    match provider.get_download_status(&model_id).await {
        Ok(status) => CommandResponse::success(status),
        Err(e) => CommandResponse::error(format!("Failed to get download status: {}", e)),
    }
}

/// Check if a model is loaded
#[command]
pub async fn is_model_loaded(
    state: State<'_, AppState>,
    model_id: String,
) -> CommandResponse<bool> {
    let offline_manager = state.offline_manager.lock().await;
    let llm_manager = offline_manager.llm_manager().lock().await;
    
    match llm_manager.is_model_loaded(&model_id).await {
        Ok(loaded) => CommandResponse::success(loaded),
        Err(e) => CommandResponse::error(format!("Failed to check if model is loaded: {}", e)),
    }
}

/// Load a model
#[command]
pub async fn load_model(
    state: State<'_, AppState>,
    model_id: String,
) -> CommandResponse<bool> {
    let offline_manager = state.offline_manager.lock().await;
    let llm_manager = offline_manager.llm_manager().lock().await;
    
    match llm_manager.load_model(&model_id).await {
        Ok(_) => CommandResponse::success(true),
        Err(e) => CommandResponse::error(format!("Failed to load model: {}", e)),
    }
}

/// Delete a model
#[command]
pub async fn delete_model(
    state: State<'_, AppState>,
    model_id: String,
) -> CommandResponse<bool> {
    let offline_manager = state.offline_manager.lock().await;
    let llm_manager = offline_manager.llm_manager().lock().await;
    let provider = match llm_manager.get_provider() {
        Ok(p) => p,
        Err(e) => return CommandResponse::error(format!("Failed to get provider: {}", e)),
    };
    
    match provider.delete_model(&model_id).await {
        Ok(_) => CommandResponse::success(true),
        Err(e) => CommandResponse::error(format!("Failed to delete model: {}", e)),
    }
}

/// Generate text
#[command]
pub async fn generate_text(
    state: State<'_, AppState>,
    request: GenerateTextRequest,
) -> CommandResponse<String> {
    let offline_manager = state.offline_manager.lock().await;
    
    // Create options from the request
    let options = GenerationOptions {
        max_tokens: request.max_tokens,
        temperature: request.temperature,
        top_p: request.top_p,
        stream: false,
        additional_params: Default::default(),
    };
    
    // Generate text
    match offline_manager.generate_text(
        &request.prompt,
        request.model_id.as_deref(),
    ).await {
        Ok(text) => CommandResponse::success(text),
        Err(e) => CommandResponse::error(format!("Failed to generate text: {}", e)),
    }
}

/// Check network connectivity
#[command]
pub async fn check_network(
    state: State<'_, AppState>,
) -> CommandResponse<bool> {
    let mut offline_manager = state.offline_manager.lock().await;
    
    match offline_manager.check_network().await {
        Ok(status) => {
            use crate::offline::NetworkStatus;
            let is_connected = status == NetworkStatus::Connected;
            CommandResponse::success(is_connected)
        },
        Err(e) => CommandResponse::error(format!("Failed to check network: {}", e)),
    }
}

/// Get offline mode status
#[command]
pub async fn get_offline_status(
    state: State<'_, AppState>,
) -> CommandResponse<bool> {
    let offline_manager = state.offline_manager.lock().await;
    CommandResponse::success(offline_manager.is_enabled())
}

/// Set offline mode
#[command]
pub async fn set_offline_mode(
    state: State<'_, AppState>,
    enabled: bool,
) -> CommandResponse<bool> {
    let mut offline_manager = state.offline_manager.lock().await;
    offline_manager.set_enabled(enabled);
    CommandResponse::success(enabled)
}

/// Initialize the offline manager
pub async fn init_offline_manager() -> Arc<Mutex<OfflineManager>> {
    crate::offline::create_offline_manager().await
}

/// Register all commands with Tauri
pub fn register_commands(app: &mut tauri::App) -> Result<(), tauri::Error> {
    let offline_manager = tauri::async_runtime::block_on(init_offline_manager());
    
    app.manage(AppState {
        offline_manager,
    });
    
    Ok(())
}
</file>

<file path="src-tauri/src/monitoring/resources.rs">
use sysinfo::{System, SystemExt, ProcessExt, CpuExt};
use serde::{Serialize, Deserialize};
use std::sync::{Arc, Mutex};
use std::thread;
use std::time::{Duration, SystemTime, Instant};
use crate::observability::metrics;

#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct ResourceMetrics {
    pub timestamp: u64,
    pub cpu: f32,
    pub memory: u64,  // in bytes
    pub fps: Option<f32>,
    pub message_count: u32,
    pub api_latency: f64,  // in milliseconds
    pub api_calls: u32,
}

pub struct ResourceMonitor {
    system: Arc<Mutex<System>>,
    metrics_history: Arc<Mutex<Vec<ResourceMetrics>>>,
    max_history_size: usize,
    pid: u32,
    is_running: Arc<Mutex<bool>>,
    start_time: Instant,
}

impl ResourceMonitor {
    pub fn new(pid: u32, max_history_size: usize) -> Self {
        let mut system = System::new_all();
        system.refresh_all();
        
        let system = Arc::new(Mutex::new(system));
        let metrics_history = Arc::new(Mutex::new(Vec::with_capacity(max_history_size)));
        let is_running = Arc::new(Mutex::new(false));
        
        Self {
            system,
            metrics_history,
            max_history_size,
            pid,
            is_running,
            start_time: Instant::now(),
        }
    }
    
    pub fn start(&self, interval_ms: u64) {
        let system = Arc::clone(&self.system);
        let metrics_history = Arc::clone(&self.metrics_history);
        let is_running = Arc::clone(&self.is_running);
        let pid = self.pid;
        let max_history_size = self.max_history_size;
        
        // Set running state
        *is_running.lock().unwrap() = true;
        
        thread::spawn(move || {
            while *is_running.lock().unwrap() {
                // Refresh system info
                let mut system = system.lock().unwrap();
                system.refresh_all();
                
                // Get process info (our own process)
                let process = system.process(sysinfo::Pid::from(pid));
                
                if let Some(process) = process {
                    // Calculate CPU usage (as percentage)
                    let cpu_usage = process.cpu_usage();
                    
                    // Get memory usage (in bytes)
                    let memory_usage = process.memory();
                    
                    // Get current timestamp
                    let timestamp = SystemTime::now()
                        .duration_since(SystemTime::UNIX_EPOCH)
                        .unwrap()
                        .as_millis() as u64;
                    
                    // Create metrics record
                    let metrics = ResourceMetrics {
                        timestamp,
                        cpu: cpu_usage,
                        memory: memory_usage,
                        fps: None, // Will be updated from the UI
                        message_count: 0, // Will be updated from the message service
                        api_latency: 0.0, // Will be updated from the API service
                        api_calls: 0, // Will be updated from the API service
                    };
                    
                    // Record these metrics for telemetry as well
                    let mut tags = std::collections::HashMap::new();
                    tags.insert("pid".to_string(), pid.to_string());
                    metrics::record_gauge("cpu_usage", cpu_usage as f64, Some(tags.clone()));
                    metrics::record_gauge("memory_usage", memory_usage as f64, Some(tags));
                    
                    // Add to history
                    let mut history = metrics_history.lock().unwrap();
                    history.push(metrics);
                    
                    // Trim history if needed
                    if history.len() > max_history_size {
                        history.remove(0);
                    }
                }
                
                // Sleep for the specified interval
                drop(system);
                thread::sleep(Duration::from_millis(interval_ms));
            }
        });
    }
    
    pub fn stop(&self) {
        let mut is_running = self.is_running.lock().unwrap();
        *is_running = false;
    }
    
    pub fn get_metrics(&self, time_range_ms: Option<u64>) -> Vec<ResourceMetrics> {
        let history = self.metrics_history.lock().unwrap();
        
        if let Some(time_range) = time_range_ms {
            // Calculate cutoff timestamp
            let now = SystemTime::now()
                .duration_since(SystemTime::UNIX_EPOCH)
                .unwrap()
                .as_millis() as u64;
                
            let cutoff = if now > time_range { now - time_range } else { 0 };
            
            // Filter history by timestamp
            history.iter()
                .filter(|metrics| metrics.timestamp >= cutoff)
                .cloned()
                .collect()
        } else {
            // Return all history
            history.clone()
        }
    }
    
    pub fn update_metrics(&self, 
        fps: Option<f32>, 
        message_count: Option<u32>, 
        api_latency: Option<f64>, 
        api_calls: Option<u32>
    ) {
        let mut history = self.metrics_history.lock().unwrap();
        
        if let Some(latest) = history.last_mut() {
            if let Some(fps_value) = fps {
                latest.fps = Some(fps_value);
            }
            
            if let Some(message_count_value) = message_count {
                latest.message_count = message_count_value;
            }
            
            if let Some(api_latency_value) = api_latency {
                latest.api_latency = api_latency_value;
            }
            
            if let Some(api_calls_value) = api_calls {
                latest.api_calls = api_calls_value;
            }
        }
    }
    
    pub fn get_uptime(&self) -> Duration {
        self.start_time.elapsed()
    }
    
    pub fn get_current_memory_usage(&self) -> u64 {
        let system = self.system.lock().unwrap();
        if let Some(process) = system.process(sysinfo::Pid::from(self.pid)) {
            process.memory()
        } else {
            0
        }
    }
    
    pub fn get_current_cpu_usage(&self) -> f32 {
        let system = self.system.lock().unwrap();
        if let Some(process) = system.process(sysinfo::Pid::from(self.pid)) {
            process.cpu_usage()
        } else {
            0.0
        }
    }
    
    pub fn get_system_info(&self) -> SystemInfo {
        let system = self.system.lock().unwrap();
        
        SystemInfo {
            total_memory: system.total_memory(),
            used_memory: system.used_memory(),
            total_swap: system.total_swap(),
            used_swap: system.used_swap(),
            cpu_count: system.cpus().len() as u32,
            system_name: system.name().unwrap_or_else(|| "Unknown".to_string()),
            kernel_version: system.kernel_version().unwrap_or_else(|| "Unknown".to_string()),
            os_version: system.os_version().unwrap_or_else(|| "Unknown".to_string()),
            host_name: system.host_name().unwrap_or_else(|| "Unknown".to_string()),
        }
    }
}

#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct SystemInfo {
    pub total_memory: u64,
    pub used_memory: u64,
    pub total_swap: u64,
    pub used_swap: u64,
    pub cpu_count: u32,
    pub system_name: String,
    pub kernel_version: String,
    pub os_version: String,
    pub host_name: String,
}

// Create a global resource monitor
lazy_static::lazy_static! {
    pub static ref RESOURCE_MONITOR: Mutex<ResourceMonitor> = {
        let pid = std::process::id();
        let monitor = ResourceMonitor::new(pid, 10000); // Store up to 10000 data points
        monitor.start(1000); // Update every second
        Mutex::new(monitor)
    };
}

// Tauri commands
#[tauri::command]
pub fn get_resource_metrics(time_range: Option<u64>) -> Vec<ResourceMetrics> {
    let monitor = RESOURCE_MONITOR.lock().unwrap();
    monitor.get_metrics(time_range)
}

#[tauri::command]
pub fn get_system_info() -> SystemInfo {
    let monitor = RESOURCE_MONITOR.lock().unwrap();
    monitor.get_system_info()
}

#[tauri::command]
pub fn update_resource_metrics(
    fps: Option<f32>, 
    message_count: Option<u32>, 
    api_latency: Option<f64>, 
    api_calls: Option<u32>
) {
    let monitor = RESOURCE_MONITOR.lock().unwrap();
    monitor.update_metrics(fps, message_count, api_latency, api_calls);
}

#[tauri::command]
pub fn get_uptime() -> u64 {
    let monitor = RESOURCE_MONITOR.lock().unwrap();
    monitor.get_uptime().as_secs()
}

#[tauri::command]
pub fn report_startup_time(time_ms: f64) {
    let mut tags = std::collections::HashMap::new();
    tags.insert("type".to_string(), "frontend".to_string());
    metrics::record_histogram("startup_time", time_ms, Some(tags));
    
    println!("Frontend startup time: {:.2}ms", time_ms);
}

#[tauri::command]
pub fn report_frame_rate(fps: f32) {
    let monitor = RESOURCE_MONITOR.lock().unwrap();
    monitor.update_metrics(Some(fps), None, None, None);
    
    let mut tags = std::collections::HashMap::new();
    tags.insert("type".to_string(), "ui".to_string());
    metrics::record_gauge("fps", fps as f64, Some(tags));
}

#[tauri::command]
pub fn report_resource_metrics(
    resource_type: String,
    url: String,
    duration: f64,
    size: u64
) {
    let mut tags = std::collections::HashMap::new();
    tags.insert("type".to_string(), resource_type.clone());
    tags.insert("url".to_string(), url);
    
    metrics::record_histogram("resource_load_time", duration, Some(tags.clone()));
    metrics::record_histogram("resource_size", size as f64, Some(tags));
}

#[tauri::command]
pub fn report_page_metrics(
    load_time: f64,
    dom_content_loaded: f64,
    first_paint: f64
) {
    let mut tags = std::collections::HashMap::new();
    tags.insert("type".to_string(), "page".to_string());
    
    metrics::record_histogram("page_load_time", load_time, Some(tags.clone()));
    metrics::record_histogram("dom_content_loaded", dom_content_loaded, Some(tags.clone()));
    metrics::record_histogram("first_paint", first_paint, Some(tags));
}
</file>

<file path="src-tauri/src/offline/llm/factory.rs">
//! Factory for creating and managing LLM providers
//!
//! This module provides functions for instantiating, checking availability, and
//! managing local LLM providers. It includes a registry of available providers
//! and utilities for determining which providers are currently usable.

use std::collections::HashMap;
use std::sync::Arc;
use std::time::Duration;
use async_trait::async_trait;
use log::{debug, error, info, warn};
use once_cell::sync::Lazy;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use tokio::sync::RwLock;
use tokio::time::timeout;

use crate::offline::llm::provider::{LocalLLMProvider, LLMProviderError, LLMProviderResult};
use crate::offline::llm::providers::{ollama::OllamaProvider, localai::LocalAIProvider};
use crate::offline::llm::types::{ProviderType, ProviderConfig};

/// The registry for supported LLM providers
static PROVIDER_REGISTRY: Lazy<RwLock<ProviderRegistry>> = Lazy::new(|| {
    RwLock::new(ProviderRegistry::new())
});

/// A registry of all supported LLM providers
#[derive(Debug)]
pub struct ProviderRegistry {
    /// All registered providers
    providers: HashMap<ProviderType, ProviderInfo>,
    /// HTTP client for checking provider availability
    client: Client,
}

/// Information about a provider
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProviderInfo {
    /// Type of the provider
    pub provider_type: ProviderType,
    /// Display name of the provider
    pub name: String,
    /// Description of the provider
    pub description: String,
    /// Version of the provider
    pub version: String,
    /// Default endpoint URL for the provider
    pub default_endpoint: String,
    /// Whether the provider supports text generation
    pub supports_text_generation: bool,
    /// Whether the provider supports chat completion
    pub supports_chat: bool,
    /// Whether the provider supports embeddings
    pub supports_embeddings: bool,
    /// Whether the provider requires an API key
    pub requires_api_key: bool,
    /// Health check endpoint for the provider
    pub health_check_endpoint: Option<String>,
    /// Current availability status
    #[serde(skip)]
    pub available: bool,
}

/// Result from checking provider availability
#[derive(Debug, Clone)]
pub struct AvailabilityResult {
    /// Whether the provider is available
    pub available: bool,
    /// The detected version if available
    pub version: Option<String>,
    /// Error message if the provider is not available
    pub error: Option<String>,
    /// Response time in milliseconds if available
    pub response_time_ms: Option<u64>,
}

impl ProviderRegistry {
    /// Create a new provider registry with default providers registered
    pub fn new() -> Self {
        let client = Client::builder()
            .timeout(Duration::from_secs(3))
            .build()
            .unwrap_or_else(|_| Client::new());
        
        let mut registry = Self {
            providers: HashMap::new(),
            client,
        };
        
        // Register default providers
        registry.register_ollama();
        registry.register_local_ai();
        
        registry
    }
    
    /// Register the Ollama provider
    fn register_ollama(&mut self) {
        let info = ProviderInfo {
            provider_type: ProviderType::Ollama,
            name: "Ollama".to_string(),
            description: "Ollama allows you to run open-source large language models locally.".to_string(),
            version: "0.0.0".to_string(), // Will be updated when checked
            default_endpoint: "http://localhost:11434".to_string(),
            supports_text_generation: true,
            supports_chat: true,
            supports_embeddings: false,
            requires_api_key: false,
            health_check_endpoint: Some("/api/version".to_string()),
            available: false,
        };
        
        self.providers.insert(ProviderType::Ollama, info);
    }
    
    /// Register the LocalAI provider
    fn register_local_ai(&mut self) {
        let info = ProviderInfo {
            provider_type: ProviderType::LocalAI,
            name: "LocalAI".to_string(),
            description: "LocalAI is a drop-in replacement for OpenAI, running models locally.".to_string(),
            version: "0.0.0".to_string(), // Will be updated when checked
            default_endpoint: "http://localhost:8080".to_string(),
            supports_text_generation: true,
            supports_chat: true,
            supports_embeddings: true,
            requires_api_key: false,
            health_check_endpoint: Some("/health".to_string()),
            available: false,
        };
        
        self.providers.insert(ProviderType::LocalAI, info);
    }
    
    /// Get provider information by type
    pub fn get_provider_info(&self, provider_type: &ProviderType) -> Option<&ProviderInfo> {
        self.providers.get(provider_type)
    }
    
    /// Get all registered providers
    pub fn get_all_providers(&self) -> Vec<&ProviderInfo> {
        self.providers.values().collect()
    }
    
    /// Get all available providers
    pub fn get_available_providers(&self) -> Vec<&ProviderInfo> {
        self.providers.values().filter(|p| p.available).collect()
    }
    
    /// Check if a provider is registered
    pub fn is_provider_registered(&self, provider_type: &ProviderType) -> bool {
        self.providers.contains_key(provider_type)
    }
    
    /// Check if a provider is available
    pub async fn check_provider_availability(&mut self, provider_type: &ProviderType, endpoint_url: Option<&str>) -> AvailabilityResult {
        let provider = match self.providers.get_mut(provider_type) {
            Some(provider) => provider,
            None => {
                return AvailabilityResult {
                    available: false,
                    version: None,
                    error: Some(format!("Provider type {:?} is not registered", provider_type)),
                    response_time_ms: None,
                };
            }
        };
        
        let base_url = endpoint_url.unwrap_or(&provider.default_endpoint);
        
        if let Some(health_endpoint) = &provider.health_check_endpoint {
            let url = format!("{}{}", base_url, health_endpoint);
            debug!("Checking availability of {} at {}", provider.name, url);
            
            let start_time = std::time::Instant::now();
            let result = match timeout(Duration::from_secs(5), self.client.get(&url).send()).await {
                Ok(Ok(response)) => {
                    let response_time = start_time.elapsed().as_millis() as u64;
                    
                    if response.status().is_success() {
                        // Try to extract version information
                        let mut version = None;
                        
                        match response.json::<serde_json::Value>().await {
                            Ok(json) => {
                                // Check for version field based on provider type
                                match provider_type {
                                    ProviderType::Ollama => {
                                        if let Some(v) = json.get("version").and_then(|v| v.as_str()) {
                                            version = Some(v.to_string());
                                            provider.version = v.to_string();
                                        }
                                    },
                                    ProviderType::LocalAI => {
                                        if let Some(v) = json.get("version").and_then(|v| v.as_str()) {
                                            version = Some(v.to_string());
                                            provider.version = v.to_string();
                                        }
                                    },
                                    _ => {}
                                }
                            },
                            Err(e) => {
                                debug!("Failed to parse response from {}: {}", provider.name, e);
                            }
                        }
                        
                        // Update availability status
                        provider.available = true;
                        
                        AvailabilityResult {
                            available: true,
                            version,
                            error: None,
                            response_time_ms: Some(response_time),
                        }
                    } else {
                        // Service responded but returned an error
                        provider.available = false;
                        
                        AvailabilityResult {
                            available: false,
                            version: None,
                            error: Some(format!("Provider returned HTTP {}", response.status())),
                            response_time_ms: Some(response_time),
                        }
                    }
                },
                Ok(Err(e)) => {
                    // Connection error
                    provider.available = false;
                    
                    AvailabilityResult {
                        available: false,
                        version: None,
                        error: Some(format!("Connection error: {}", e)),
                        response_time_ms: None,
                    }
                },
                Err(_) => {
                    // Timeout
                    provider.available = false;
                    
                    AvailabilityResult {
                        available: false,
                        version: None,
                        error: Some("Connection timed out".to_string()),
                        response_time_ms: None,
                    }
                }
            };
            
            result
        } else {
            // No health check endpoint defined
            AvailabilityResult {
                available: false,
                version: None,
                error: Some("No health check endpoint defined for this provider".to_string()),
                response_time_ms: None,
            }
        }
    }
    
    /// Check availability of all registered providers
    pub async fn check_all_providers(&mut self) -> HashMap<ProviderType, AvailabilityResult> {
        let mut results = HashMap::new();
        
        for provider_type in self.providers.keys().cloned().collect::<Vec<_>>() {
            let result = self.check_provider_availability(&provider_type, None).await;
            results.insert(provider_type, result);
        }
        
        results
    }
}

/// Get a reference to the provider registry
pub async fn get_provider_registry() -> tokio::sync::RwLockReadGuard<'static, ProviderRegistry> {
    PROVIDER_REGISTRY.read().await
}

/// Get a mutable reference to the provider registry
pub async fn get_provider_registry_mut() -> tokio::sync::RwLockWriteGuard<'static, ProviderRegistry> {
    PROVIDER_REGISTRY.write().await
}

/// Check availability of all registered providers
pub async fn check_all_providers() -> HashMap<ProviderType, AvailabilityResult> {
    let mut registry = get_provider_registry_mut().await;
    registry.check_all_providers().await
}

/// Check if a specific provider is available
pub async fn is_provider_available(provider_type: ProviderType, endpoint_url: Option<&str>) -> bool {
    let mut registry = get_provider_registry_mut().await;
    registry.check_provider_availability(&provider_type, endpoint_url).await.available
}

/// Create a provider instance based on configuration
pub async fn create_provider(config: &ProviderConfig) -> LLMProviderResult<Box<dyn LocalLLMProvider>> {
    let provider_type = &config.provider_type;
    
    // Check if provider is registered
    let registry = get_provider_registry().await;
    if !registry.is_provider_registered(provider_type) {
        error!("Provider type {:?} is not registered", provider_type);
        return Err(LLMProviderError::ProviderError(
            format!("Provider type {:?} is not registered", provider_type)
        ));
    }
    drop(registry);
    
    // Check if provider is available
    let endpoint_url = Some(&config.endpoint_url);
    if !is_provider_available(config.provider_type, endpoint_url).await {
        let error_message = match config.provider_type {
            ProviderType::Ollama => format!(
                "Ollama is not available at {}. Make sure Ollama is installed and running.",
                config.endpoint_url
            ),
            ProviderType::LocalAI => format!(
                "LocalAI is not available at {}. Make sure LocalAI is installed and running.",
                config.endpoint_url
            ),
            ProviderType::LlamaCpp => format!(
                "llama.cpp is not available. Make sure it's properly set up.",
            ),
            ProviderType::Custom(ref name) => format!(
                "Custom provider '{}' is not available at {}.",
                name, config.endpoint_url
            ),
        };
        
        warn!("{}", error_message);
        return Err(LLMProviderError::ProviderError(error_message));
    }
    
    // Create provider based on type
    let mut provider: Box<dyn LocalLLMProvider> = match config.provider_type {
        ProviderType::Ollama => {
            debug!("Creating Ollama provider with endpoint {}", config.endpoint_url);
            Box::new(OllamaProvider::with_base_url(&config.endpoint_url))
        },
        ProviderType::LocalAI => {
            debug!("Creating LocalAI provider with endpoint {}", config.endpoint_url);
            Box::new(LocalAIProvider::with_base_url(&config.endpoint_url))
        },
        ProviderType::LlamaCpp => {
            debug!("llama.cpp provider not implemented yet, falling back to Ollama");
            warn!("llama.cpp provider requested but not implemented, falling back to Ollama");
            Box::new(OllamaProvider::with_base_url(&config.endpoint_url))
        },
        ProviderType::Custom(_) => {
            debug!("Custom provider not implemented yet, falling back to Ollama");
            warn!("Custom provider requested but not implemented, falling back to Ollama");
            Box::new(OllamaProvider::with_base_url(&config.endpoint_url))
        },
    };
    
    // Initialize the provider
    debug!("Initializing provider with config: {:?}", config);
    
    // Create initialization config
    let mut init_config = serde_json::Map::new();
    
    // Add endpoint URL
    init_config.insert("base_url".to_string(), serde_json::Value::String(config.endpoint_url.clone()));
    
    // Add API key if present
    if let Some(api_key) = &config.api_key {
        init_config.insert("api_key".to_string(), serde_json::Value::String(api_key.clone()));
    }
    
    // Add timeout settings
    init_config.insert(
        "request_timeout_seconds".to_string(), 
        serde_json::Value::Number(serde_json::Number::from(config.request_timeout_seconds))
    );
    
    // Add any custom options
    for (key, value) in &config.custom_options {
        init_config.insert(key.clone(), value.clone());
    }
    
    // Initialize provider
    match provider.initialize(serde_json::Value::Object(init_config)).await {
        Ok(_) => {
            info!("Successfully initialized {:?} provider", config.provider_type);
            Ok(provider)
        },
        Err(e) => {
            error!("Failed to initialize {:?} provider: {}", config.provider_type, e);
            Err(e)
        },
    }
}

/// Create a provider with default configuration
pub async fn create_default_provider() -> LLMProviderResult<Box<dyn LocalLLMProvider>> {
    // Check which providers are available
    let results = check_all_providers().await;
    
    // Try to find an available provider
    let available_provider = results.iter()
        .filter(|(_, result)| result.available)
        .map(|(provider_type, _)| provider_type)
        .next();
    
    match available_provider {
        Some(provider_type) => {
            let registry = get_provider_registry().await;
            let info = registry.get_provider_info(provider_type)
                .expect("Provider info should exist for available provider");
            
            // Create configuration for the available provider
            let config = ProviderConfig {
                provider_type: *provider_type,
                endpoint_url: info.default_endpoint.clone(),
                api_key: None,
                max_concurrent_requests: 5,
                request_timeout_seconds: 30,
                enable_retry: true,
                max_retries: 3,
                retry_backoff_seconds: 1.5,
                custom_options: HashMap::new(),
            };
            
            create_provider(&config).await
        },
        None => {
            // No providers available
            // Try Ollama as a default fallback
            warn!("No providers available, trying Ollama as fallback");
            let config = ProviderConfig {
                provider_type: ProviderType::Ollama,
                endpoint_url: "http://localhost:11434".to_string(),
                api_key: None,
                max_concurrent_requests: 5,
                request_timeout_seconds: 30,
                enable_retry: true,
                max_retries: 3,
                retry_backoff_seconds: 1.5,
                custom_options: HashMap::new(),
            };
            
            create_provider(&config).await
        },
    }
}

/// Get a list of all available providers
pub async fn get_available_providers() -> Vec<ProviderInfo> {
    let mut registry = get_provider_registry_mut().await;
    let results = registry.check_all_providers().await;
    
    // Filter for available providers and get their info
    registry.get_available_providers().cloned().collect()
}

/// Get information about all registered providers
pub async fn get_all_providers() -> Vec<ProviderInfo> {
    let registry = get_provider_registry().await;
    registry.get_all_providers().cloned().collect()
}

/// Initialize the factory and check all providers
pub async fn initialize() {
    info!("Initializing LLM provider factory");
    
    // Force initialization of the registry
    let _ = PROVIDER_REGISTRY.clone();
    
    // Check all providers
    let results = check_all_providers().await;
    
    // Log available providers
    let available_count = results.values().filter(|r| r.available).count();
    info!("Found {} available LLM providers", available_count);
    
    for (provider_type, result) in results {
        if result.available {
            info!(
                "Provider {:?} is available (version: {})",
                provider_type,
                result.version.unwrap_or_else(|| "unknown".to_string())
            );
        } else {
            if let Some(error) = result.error {
                debug!("Provider {:?} is not available: {}", provider_type, error);
            } else {
                debug!("Provider {:?} is not available", provider_type);
            }
        }
    }
}
</file>

<file path="src-tauri/src/offline/llm/mod.rs">
//! Local LLM integration module for offline capabilities
//!
//! This module provides support for using locally-hosted Large Language Models
//! for generating text when online services are unavailable.

pub mod provider;
pub mod types;
pub mod providers;
pub mod factory;

use std::sync::Arc;
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use thiserror::Error;

use self::provider::{
    CompletionResponse, LLMProviderError, 
    LLMProviderResult, LocalLLMProvider, ModelInfo,
};
use self::providers::ollama::OllamaProvider;
use self::types::{ProviderType, ProviderConfig, GenerationOptions, GenerationRequest, GenerationResponse};

/// Errors that can occur in the LLM module
#[derive(Error, Debug)]
pub enum LLMError {
    /// No provider is available
    #[error("No LLM provider available")]
    NoProviderAvailable,
    
    /// Provider-specific error
    #[error("Provider error: {0}")]
    ProviderError(#[from] LLMProviderError),
    
    /// Error communicating with LLM
    #[error("LLM communication error: {0}")]
    CommunicationError(String),
    
    /// Unexpected error
    #[error("Unexpected error: {0}")]
    Unexpected(String),
}

/// Result type for LLM operations
pub type LLMResult<T> = Result<T, LLMError>;

/// Configuration for the LLM module
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMConfig {
    /// Type of provider to use
    pub provider_type: ProviderType,
    /// Provider-specific configuration
    pub provider_config: serde_json::Value,
    /// Default model to use for generation
    pub default_model: Option<String>,
    /// Whether to enable debug logging
    pub enable_debug: bool,
}

impl Default for LLMConfig {
    fn default() -> Self {
        Self {
            provider_type: ProviderType::Ollama,
            provider_config: serde_json::json!({
                "base_url": "http://localhost:11434",
            }),
            default_model: None,
            enable_debug: false,
        }
    }
}

/// LLM manager that handles provider selection and configuration
pub struct LLMManager {
    /// Current configuration
    config: LLMConfig,
    /// Current provider
    provider: Option<Box<dyn LocalLLMProvider>>,
}

impl LLMManager {
    /// Create a new LLM manager with the default configuration
    pub fn new() -> Self {
        Self {
            config: LLMConfig::default(),
            provider: None,
        }
    }
    
    /// Create a new LLM manager with a specific configuration
    pub fn with_config(config: LLMConfig) -> Self {
        Self {
            config,
            provider: None,
        }
    }
    
    /// Initialize the LLM manager using the factory
    pub async fn initialize(&mut self) -> LLMResult<()> {
        let provider_config = ProviderConfig {
            provider_type: self.config.provider_type,
            endpoint_url: match self.config.provider_config.get("base_url") {
                Some(val) => val.as_str().unwrap_or("http://localhost:11434").to_string(),
                None => "http://localhost:11434".to_string(),
            },
            api_key: match self.config.provider_config.get("api_key") {
                Some(val) => val.as_str().map(|s| s.to_string()),
                None => None,
            },
            // Set reasonable defaults for remaining fields
            max_concurrent_requests: 5,
            request_timeout_seconds: 30,
            enable_retry: true,
            max_retries: 3,
            retry_backoff_seconds: 1.5,
            // Convert any remaining custom options
            custom_options: {
                let mut custom = std::collections::HashMap::new();
                if let Some(obj) = self.config.provider_config.as_object() {
                    for (key, val) in obj {
                        if key != "base_url" && key != "api_key" {
                            custom.insert(key.clone(), val.clone());
                        }
                    }
                }
                custom
            },
        };
        
        // Use the factory to create the provider
        match factory::create_provider(&provider_config).await {
            Ok(provider) => {
                self.provider = Some(provider);
                Ok(())
            },
            Err(e) => {
                // Try to use default provider as fallback
                log::warn!("Failed to create configured provider: {}. Trying default provider...", e);
                match factory::create_default_provider().await {
                    Ok(provider) => {
                        self.provider = Some(provider);
                        Ok(())
                    },
                    Err(e) => {
                        log::error!("Failed to create default provider: {}", e);
                        Err(LLMError::ProviderError(e))
                    },
                }
            },
        }
    }
    
    /// Get a reference to the current provider
    pub(crate) fn get_provider(&self) -> LLMResult<&dyn LocalLLMProvider> {
        match &self.provider {
            Some(provider) => Ok(provider.as_ref()),
            None => Err(LLMError::NoProviderAvailable),
        }
    }
    
    /// List available models
    pub async fn list_available_models(&self) -> LLMResult<Vec<ModelInfo>> {
        let provider = self.get_provider()?;
        Ok(provider.list_available_models().await?)
    }
    
    /// List downloaded models
    pub async fn list_downloaded_models(&self) -> LLMResult<Vec<ModelInfo>> {
        let provider = self.get_provider()?;
        Ok(provider.list_downloaded_models().await?)
    }
    
    /// Get model info
    pub async fn get_model_info(&self, model_id: &str) -> LLMResult<ModelInfo> {
        let provider = self.get_provider()?;
        Ok(provider.get_model_info(model_id).await?)
    }
    
    /// Download a model
    pub async fn download_model(&self, model_id: &str) -> LLMResult<()> {
        let provider = self.get_provider()?;
        Ok(provider.download_model(model_id).await?)
    }
    
    /// Check if a model is loaded
    pub async fn is_model_loaded(&self, model_id: &str) -> LLMResult<bool> {
        let provider = self.get_provider()?;
        Ok(provider.is_model_loaded(model_id).await?)
    }
    
    /// Load a model
    pub async fn load_model(&self, model_id: &str) -> LLMResult<()> {
        let provider = self.get_provider()?;
        Ok(provider.load_model(model_id).await?)
    }
    
    /// Generate text with a model
    pub async fn generate_text(
        &self,
        model_id: Option<&str>,
        prompt: &str,
        options: Option<GenerationOptions>,
    ) -> LLMResult<CompletionResponse> {
        let provider = self.get_provider()?;
        
        // Determine which model to use
        let model_id = match model_id {
            Some(id) => id,
            None => match &self.config.default_model {
                Some(id) => id,
                None => return Err(LLMError::Unexpected("No model specified and no default model configured".to_string())),
            },
        };
        
        // Determine options to use
        let options = options.unwrap_or_default();
        
        // Generate text
        Ok(provider.generate_text(model_id, prompt, options).await?)
    }
    
    /// Process a generation request
    pub async fn process_request(&self, request: GenerationRequest) -> LLMResult<GenerationResponse> {
        let provider = self.get_provider()?;
        
        match &request.request_type {
            types::GenerationRequestType::TextCompletion { prompt } => {
                let completion = provider.generate_text(&request.model_id, prompt, request.options).await?;
                
                // Convert to GenerationResponse
                let created_at = chrono::Utc::now().to_rfc3339();
                let response = GenerationResponse {
                    text: completion.text,
                    response_type: types::ResponseType::TextCompletion,
                    usage: types::TokenUsage {
                        prompt_tokens: completion.usage.prompt_tokens,
                        completion_tokens: completion.usage.completion_tokens,
                        total_tokens: completion.usage.total_tokens,
                    },
                    truncated: completion.reached_max_tokens,
                    finish_reason: if completion.reached_max_tokens {
                        Some("length".to_string())
                    } else {
                        Some("stop".to_string())
                    },
                    model_id: request.model_id,
                    created_at,
                    metadata: completion.metadata,
                };
                
                Ok(response)
            },
            types::GenerationRequestType::ChatCompletion { messages } => {
                // Convert chat messages to a prompt
                let mut prompt = String::new();
                for message in messages {
                    match message.role.as_str() {
                        "system" => {
                            prompt.push_str(&format!("[SYSTEM]: {}\n\n", message.content));
                        },
                        "user" => {
                            prompt.push_str(&format!("[USER]: {}\n\n", message.content));
                        },
                        "assistant" => {
                            prompt.push_str(&format!("[ASSISTANT]: {}\n\n", message.content));
                        },
                        _ => {
                            prompt.push_str(&format!("[{}]: {}\n\n", message.role, message.content));
                        },
                    }
                }
                prompt.push_str("[ASSISTANT]: ");
                
                // Generate completion
                let completion = provider.generate_text(&request.model_id, &prompt, request.options).await?;
                
                // Convert to GenerationResponse
                let created_at = chrono::Utc::now().to_rfc3339();
                let response = GenerationResponse {
                    text: completion.text,
                    response_type: types::ResponseType::ChatCompletion,
                    usage: types::TokenUsage {
                        prompt_tokens: completion.usage.prompt_tokens,
                        completion_tokens: completion.usage.completion_tokens,
                        total_tokens: completion.usage.total_tokens,
                    },
                    truncated: completion.reached_max_tokens,
                    finish_reason: if completion.reached_max_tokens {
                        Some("length".to_string())
                    } else {
                        Some("stop".to_string())
                    },
                    model_id: request.model_id,
                    created_at,
                    metadata: completion.metadata,
                };
                
                Ok(response)
            },
        }
    }
    
    /// Get available providers
    pub async fn get_available_providers() -> LLMResult<Vec<factory::ProviderInfo>> {
        match factory::get_available_providers().await {
            info if !info.is_empty() => Ok(info),
            _ => {
                log::warn!("No LLM providers available");
                Err(LLMError::NoProviderAvailable)
            }
        }
    }
    
    /// Check if a provider is available
    pub async fn is_provider_available(provider_type: ProviderType, endpoint_url: Option<&str>) -> bool {
        factory::is_provider_available(provider_type, endpoint_url).await
    }
}

/// Create a shared LLM manager with the default configuration
pub async fn create_llm_manager() -> Arc<tokio::sync::Mutex<LLMManager>> {
    // Initialize the factory
    factory::initialize().await;
    
    let mut manager = LLMManager::new();
    if let Err(e) = manager.initialize().await {
        log::error!("Failed to initialize LLM manager: {}", e);
    }
    
    Arc::new(tokio::sync::Mutex::new(manager))
}

/// Get a list of all available providers
pub async fn get_available_providers() -> Vec<factory::ProviderInfo> {
    factory::get_available_providers().await
}

/// Check if a provider is available
pub async fn is_provider_available(provider_type: ProviderType, endpoint_url: Option<&str>) -> bool {
    factory::is_provider_available(provider_type, endpoint_url).await
}
</file>

<file path="src-tauri/src/offline/llm/ollama.rs">
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use async_trait::async_trait;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use tokio::sync::RwLock;

use super::provider::{
    CompletionResponse, CompletionStream, DownloadStatus, GenerationOptions,
    LLMProviderError, LLMProviderResult, LocalLLMProvider, ModelConfig, ModelInfo, TokenUsage,
};

/// Ollama API response for listing models
#[derive(Debug, Deserialize)]
struct OllamaListModelsResponse {
    models: Vec<OllamaModelInfo>,
}

/// Ollama API model information
#[derive(Debug, Deserialize)]
struct OllamaModelInfo {
    name: String,
    modified_at: String,
    size: u64,
    digest: String,
    details: Option<OllamaModelDetails>,
}

/// Ollama API model details
#[derive(Debug, Deserialize)]
struct OllamaModelDetails {
    format: String,
    family: String,
    families: Option<Vec<String>>,
    parameter_size: Option<String>,
    quantization_level: Option<String>,
}

/// Ollama API generation request
#[derive(Debug, Serialize)]
struct OllamaGenerationRequest {
    model: String,
    prompt: String,
    stream: bool,
    #[serde(skip_serializing_if = "Option::is_none")]
    options: Option<HashMap<String, serde_json::Value>>,
}

/// Ollama API generation response
#[derive(Debug, Deserialize)]
struct OllamaGenerationResponse {
    model: String,
    response: String,
    done: bool,
    context: Option<Vec<u32>>,
    total_duration: Option<u64>,
    load_duration: Option<u64>,
    prompt_eval_duration: Option<u64>,
    eval_count: Option<u32>,
    eval_duration: Option<u64>,
}

/// Ollama provider implementation
pub struct OllamaProvider {
    /// Base URL for the Ollama API
    base_url: String,
    /// HTTP client for API requests
    client: Client,
    /// Map of model IDs to download status
    download_status: Arc<RwLock<HashMap<String, DownloadStatus>>>,
    /// Map of model IDs to model configurations
    model_configs: Arc<RwLock<HashMap<String, ModelConfig>>>,
    /// Whether the provider has been initialized
    initialized: Arc<Mutex<bool>>,
}

impl OllamaProvider {
    /// Create a new Ollama provider with the default URL
    pub fn new() -> Self {
        Self::with_base_url("http://localhost:11434")
    }

    /// Create a new Ollama provider with a custom URL
    pub fn with_base_url(base_url: &str) -> Self {
        Self {
            base_url: base_url.to_string(),
            client: Client::new(),
            download_status: Arc::new(RwLock::new(HashMap::new())),
            model_configs: Arc::new(RwLock::new(HashMap::new())),
            initialized: Arc::new(Mutex::new(false)),
        }
    }

    /// Check if the provider is initialized
    fn ensure_initialized(&self) -> LLMProviderResult<()> {
        let initialized = *self.initialized.lock().map_err(|e| {
            LLMProviderError::Unexpected(format!("Failed to acquire lock: {}", e))
        })?;

        if !initialized {
            return Err(LLMProviderError::NotInitialized(
                "Ollama provider not initialized. Call initialize() first.".to_string(),
            ));
        }

        Ok(())
    }

    /// Convert Ollama model info to the common ModelInfo format
    fn convert_to_model_info(&self, ollama_info: OllamaModelInfo, is_downloaded: bool) -> ModelInfo {
        let mut provider_metadata = HashMap::new();
        
        if let Some(details) = ollama_info.details {
            provider_metadata.insert("format".to_string(), serde_json::to_value(details.format).unwrap_or_default());
            provider_metadata.insert("family".to_string(), serde_json::to_value(details.family).unwrap_or_default());
            
            if let Some(families) = details.families {
                provider_metadata.insert("families".to_string(), serde_json::to_value(families).unwrap_or_default());
            }
            
            if let Some(param_size) = details.parameter_size {
                provider_metadata.insert("parameter_size".to_string(), serde_json::to_value(param_size).unwrap_or_default());
            }
            
            if let Some(quant_level) = details.quantization_level {
                provider_metadata.insert("quantization_level".to_string(), serde_json::to_value(quant_level).unwrap_or_default());
            }
        }
        
        provider_metadata.insert("digest".to_string(), serde_json::to_value(ollama_info.digest).unwrap_or_default());
        provider_metadata.insert("modified_at".to_string(), serde_json::to_value(ollama_info.modified_at).unwrap_or_default());

        ModelInfo {
            id: ollama_info.name.clone(),
            name: ollama_info.name,
            description: "".to_string(), // Ollama API doesn't provide descriptions
            size_bytes: ollama_info.size,
            is_downloaded,
            provider_metadata,
        }
    }
}

/// Ollama implementation of CompletionStream
pub struct OllamaCompletionStream {
    /// HTTP response stream
    response: reqwest::Response,
    /// Total completion text accumulated so far
    accumulated_text: String,
    /// Whether we've reached the end of the stream
    completed: bool,
}

#[async_trait]
impl CompletionStream for OllamaCompletionStream {
    async fn next_chunk(&mut self) -> Option<LLMProviderResult<String>> {
        if self.completed {
            return None;
        }

        match self.response.chunk().await {
            Ok(Some(chunk)) => {
                let chunk_str = match std::str::from_utf8(&chunk) {
                    Ok(s) => s,
                    Err(e) => return Some(Err(LLMProviderError::Unexpected(format!(
                        "Failed to decode response chunk: {}", e
                    )))),
                };

                // Parse the JSON response
                match serde_json::from_str::<OllamaGenerationResponse>(chunk_str) {
                    Ok(response) => {
                        self.accumulated_text.push_str(&response.response);
                        
                        if response.done {
                            self.completed = true;
                        }
                        
                        Some(Ok(response.response))
                    },
                    Err(e) => Some(Err(LLMProviderError::Unexpected(format!(
                        "Failed to parse response JSON: {}", e
                    )))),
                }
            },
            Ok(None) => {
                self.completed = true;
                None
            },
            Err(e) => Some(Err(LLMProviderError::NetworkError(format!(
                "Failed to get response chunk: {}", e
            )))),
        }
    }
}

#[async_trait]
impl LocalLLMProvider for OllamaProvider {
    async fn initialize(&mut self, config: serde_json::Value) -> LLMProviderResult<()> {
        // Extract configuration options
        let config_obj = match config.as_object() {
            Some(obj) => obj,
            None => return Err(LLMProviderError::ProviderError(
                "Configuration must be a JSON object".to_string(),
            )),
        };

        // Override base URL if provided
        if let Some(base_url) = config_obj.get("base_url").and_then(|v| v.as_str()) {
            self.base_url = base_url.to_string();
        }

        // Test connection to Ollama API
        let health_url = format!("{}/api/health", self.base_url);
        match self.client.get(&health_url).send().await {
            Ok(response) => {
                if !response.status().is_success() {
                    return Err(LLMProviderError::ProviderError(format!(
                        "Failed to connect to Ollama API: HTTP {}", response.status()
                    )));
                }
            },
            Err(e) => {
                return Err(LLMProviderError::NetworkError(format!(
                    "Failed to connect to Ollama API: {}", e
                )));
            }
        }

        // Mark as initialized
        let mut initialized = self.initialized.lock().map_err(|e| {
            LLMProviderError::Unexpected(format!("Failed to acquire lock: {}", e))
        })?;
        *initialized = true;

        Ok(())
    }

    fn provider_name(&self) -> &str {
        "Ollama"
    }

    async fn list_available_models(&self) -> LLMProviderResult<Vec<ModelInfo>> {
        self.ensure_initialized()?;

        let url = format!("{}/api/tags", self.base_url);
        let response = self.client.get(&url).send().await.map_err(|e| {
            LLMProviderError::NetworkError(format!("Failed to fetch models: {}", e))
        })?;

        if !response.status().is_success() {
            return Err(LLMProviderError::ProviderError(format!(
                "Failed to fetch models: HTTP {}", response.status()
            )));
        }

        let models_response: OllamaListModelsResponse = response.json().await.map_err(|e| {
            LLMProviderError::ProviderError(format!("Failed to parse models response: {}", e))
        })?;

        let models = models_response.models.into_iter()
            .map(|m| self.convert_to_model_info(m, true))
            .collect();

        Ok(models)
    }

    async fn list_downloaded_models(&self) -> LLMProviderResult<Vec<ModelInfo>> {
        // For Ollama, all listed models are downloaded models
        self.list_available_models().await
    }

    async fn download_model(&self, model_id: &str) -> LLMProviderResult<()> {
        self.ensure_initialized()?;

        // Update download status
        {
            let mut status_map = self.download_status.write().await;
            status_map.insert(model_id.to_string(), DownloadStatus::InProgress { percent: 0.0 });
        }

        // Construct the pull request
        let url = format!("{}/api/pull", self.base_url);
        let request = serde_json::json!({
            "name": model_id,
            "stream": true,
        });

        // Start download in a separate task
        let model_id = model_id.to_string();
        let client = self.client.clone();
        let base_url = self.base_url.clone();
        let download_status = self.download_status.clone();

        tokio::spawn(async move {
            let result = client.post(&url).json(&request).send().await;
            
            match result {
                Ok(response) => {
                    if !response.status().is_success() {
                        let mut status_map = download_status.write().await;
                        status_map.insert(model_id, DownloadStatus::Failed { 
                            reason: format!("HTTP error: {}", response.status()) 
                        });
                        return;
                    }

                    // Stream the response to track progress
                    let mut stream = response.bytes_stream();
                    while let Some(chunk_result) = futures_util::StreamExt::next(&mut stream).await {
                        match chunk_result {
                            Ok(chunk) => {
                                if let Ok(text) = std::str::from_utf8(&chunk) {
                                    if let Ok(json) = serde_json::from_str::<serde_json::Value>(text) {
                                        // Extract progress information if available
                                        if let Some(completed) = json.get("completed").and_then(|v| v.as_f64()) {
                                            if let Some(total) = json.get("total").and_then(|v| v.as_f64()) {
                                                if total > 0.0 {
                                                    let percent = (completed / total * 100.0) as f32;
                                                    let mut status_map = download_status.write().await;
                                                    status_map.insert(model_id.clone(), DownloadStatus::InProgress { percent });
                                                }
                                            }
                                        }
                                        
                                        // Check for completion
                                        if json.get("status").and_then(|v| v.as_str()) == Some("success") {
                                            let mut status_map = download_status.write().await;
                                            status_map.insert(model_id.clone(), DownloadStatus::Completed);
                                            break;
                                        }
                                    }
                                }
                            },
                            Err(e) => {
                                let mut status_map = download_status.write().await;
                                status_map.insert(model_id.clone(), DownloadStatus::Failed { 
                                    reason: format!("Failed to read response: {}", e) 
                                });
                                break;
                            }
                        }
                    }
                },
                Err(e) => {
                    let mut status_map = download_status.write().await;
                    status_map.insert(model_id, DownloadStatus::Failed { 
                        reason: format!("Network error: {}", e) 
                    });
                }
            }
        });

        Ok(())
    }

    async fn get_download_status(&self, model_id: &str) -> LLMProviderResult<DownloadStatus> {
        self.ensure_initialized()?;

        let status_map = self.download_status.read().await;
        match status_map.get(model_id) {
            Some(status) => Ok(status.clone()),
            None => {
                // If no status is recorded, check if the model exists already
                let models = self.list_downloaded_models().await?;
                if models.iter().any(|m| m.id == model_id) {
                    Ok(DownloadStatus::Completed)
                } else {
                    Ok(DownloadStatus::NotStarted)
                }
            }
        }
    }

    async fn cancel_download(&self, model_id: &str) -> LLMProviderResult<()> {
        self.ensure_initialized()?;

        // Ollama doesn't have a direct API to cancel downloads
        // We can only update our status to indicate cancellation
        let mut status_map = self.download_status.write().await;
        if let Some(status) = status_map.get(model_id) {
            match status {
                DownloadStatus::InProgress { .. } => {
                    status_map.insert(model_id.to_string(), DownloadStatus::Cancelled);
                    Ok(())
                },
                _ => Err(LLMProviderError::ProviderError(
                    "Model is not currently downloading".to_string()
                )),
            }
        } else {
            Err(LLMProviderError::ModelNotFound(model_id.to_string()))
        }
    }

    async fn delete_model(&self, model_id: &str) -> LLMProviderResult<()> {
        self.ensure_initialized()?;

        let url = format!("{}/api/delete", self.base_url);
        let request = serde_json::json!({
            "name": model_id,
        });

        let response = self.client.delete(&url).json(&request).send().await.map_err(|e| {
            LLMProviderError::NetworkError(format!("Failed to delete model: {}", e))
        })?;

        if !response.status().is_success() {
            return Err(LLMProviderError::ProviderError(format!(
                "Failed to delete model: HTTP {}", response.status()
            )));
        }

        // Remove status and config for the deleted model
        {
            let mut status_map = self.download_status.write().await;
            status_map.remove(model_id);
        }
        {
            let mut config_map = self.model_configs.write().await;
            config_map.remove(model_id);
        }

        Ok(())
    }

    async fn is_model_loaded(&self, model_id: &str) -> LLMProviderResult<bool> {
        self.ensure_initialized()?;

        // Ollama doesn't have a direct API to check if a model is loaded
        // We can send a simple generation request to see if it responds quickly
        let url = format!("{}/api/generate", self.base_url);
        let request = OllamaGenerationRequest {
            model: model_id.to_string(),
            prompt: "".to_string(),
            stream: false,
            options: Some(HashMap::from([
                ("num_predict".to_string(), serde_json::json!(1)),
            ])),
        };

        match self.client.post(&url).json(&request).send().await {
            Ok(response) => {
                if response.status().is_success() {
                    Ok(true)
                } else if response.status() == reqwest::StatusCode::NOT_FOUND {
                    Ok(false)
                } else {
                    Err(LLMProviderError::ProviderError(format!(
                        "Failed to check model status: HTTP {}", response.status()
                    )))
                }
            },
            Err(e) => Err(LLMProviderError::NetworkError(format!(
                "Failed to check model status: {}", e
            ))),
        }
    }

    async fn load_model(&self, model_id: &str) -> LLMProviderResult<()> {
        self.ensure_initialized()?;

        // Ollama automatically loads models when used, but we can "warm up" the model
        // by sending a trivial request
        let url = format!("{}/api/generate", self.base_url);
        let request = OllamaGenerationRequest {
            model: model_id.to_string(),
            prompt: "".to_string(),
            stream: false,
            options: Some(HashMap::from([
                ("num_predict".to_string(), serde_json::json!(1)),
            ])),
        };

        let response = self.client.post(&url).json(&request).send().await.map_err(|e| {
            LLMProviderError::NetworkError(format!("Failed to load model: {}", e))
        })?;

        if !response.status().is_success() {
            return Err(LLMProviderError::ProviderError(format!(
                "Failed to load model: HTTP {}", response.status()
            )));
        }

        Ok(())
    }

    async fn unload_model(&self, model_id: &str) -> LLMProviderResult<()> {
        self.ensure_initialized()?;
        
        // Ollama doesn't have a direct API to unload models
        // Models are automatically unloaded when not used for a while
        Ok(())
    }

    async fn generate_text(
        &self,
        model_id: &str,
        prompt: &str,
        options: GenerationOptions,
    ) -> LLMProviderResult<CompletionResponse> {
        self.ensure_initialized()?;

        // Prepare the generation request
        let mut ollama_options = HashMap::new();
        
        if let Some(max_tokens) = options.max_tokens {
            ollama_options.insert("num_predict".to_string(), serde_json::json!(max_tokens));
        }
        
        if let Some(temperature) = options.temperature {
            ollama_options.insert("temperature".to_string(), serde_json::json!(temperature));
        }
        
        if let Some(top_p) = options.top_p {
            ollama_options.insert("top_p".to_string(), serde_json::json!(top_p));
        }
        
        // Add any additional parameters
        for (key, value) in options.additional_params {
            ollama_options.insert(key, value);
        }

        let url = format!("{}/api/generate", self.base_url);
        let request = OllamaGenerationRequest {
            model: model_id.to_string(),
            prompt: prompt.to_string(),
            stream: false,
            options: Some(ollama_options),
        };

        // Send the request
        let response = self.client.post(&url).json(&request).send().await.map_err(|e| {
            LLMProviderError::NetworkError(format!("Failed to generate text: {}", e))
        })?;

        if !response.status().is_success() {
            return Err(LLMProviderError::ProviderError(format!(
                "Failed to generate text: HTTP {}", response.status()
            )));
        }

        // Parse the response
        let ollama_response: OllamaGenerationResponse = response.json().await.map_err(|e| {
            LLMProviderError::ProviderError(format!("Failed to parse generation response: {}", e))
        })?;

        // Create the completion response
        let mut metadata = HashMap::new();
        
        if let Some(context) = &ollama_response.context {
            metadata.insert("context".to_string(), serde_json::to_value(context).unwrap_or_default());
        }
        
        if let Some(total_duration) = ollama_response.total_duration {
            metadata.insert("total_duration_ms".to_string(), serde_json::json!(total_duration));
        }
        
        if let Some(load_duration) = ollama_response.load_duration {
            metadata.insert("load_duration_ms".to_string(), serde_json::json!(load_duration));
        }
        
        if let Some(prompt_eval_duration) = ollama_response.prompt_eval_duration {
            metadata.insert("prompt_eval_duration_ms".to_string(), serde_json::json!(prompt_eval_duration));
        }
        
        if let Some(eval_count) = ollama_response.eval_count {
            metadata.insert("eval_count".to_string(), serde_json::json!(eval_count));
        }
        
        if let Some(eval_duration) = ollama_response.eval_duration {
            metadata.insert("eval_duration_ms".to_string(), serde_json::json!(eval_duration));
        }

        let usage = TokenUsage {
            // Ollama doesn't provide token counts, so we have to estimate
            prompt_tokens: (prompt.len() / 4) as u32, // Very rough estimate
            completion_tokens: (ollama_response.response.len() / 4) as u32, // Very rough estimate
            total_tokens: ((prompt.len() + ollama_response.response.len()) / 4) as u32,
        };

        Ok(CompletionResponse {
            text: ollama_response.response,
            reached_max_tokens: false, // Ollama doesn't indicate this
            usage,
            metadata,
        })
    }

    async fn generate_text_streaming(
        &self,
        model_id: &str,
        prompt: &str,
        options: GenerationOptions,
    ) -> LLMProviderResult<Box<dyn CompletionStream>> {
        self.ensure_initialized()?;

        // Prepare the generation request with streaming enabled
        let mut ollama_options = HashMap::new();
        
        if let Some(max_tokens) = options.max_tokens {
            ollama_options.insert("num_predict".to_string(), serde_json::json!(max_tokens));
        }
        
        if let Some(temperature) = options.temperature {
            ollama_options.insert("temperature".to_string(), serde_json::json!(temperature));
        }
        
        if let Some(top_p) = options.top_p {
            ollama_options.insert("top_p".to_string(), serde_json::json!(top_p));
        }
        
        // Add any additional parameters
        for (key, value) in options.additional_params {
            ollama_options.insert(key, value);
        }

        let url = format!("{}/api/generate", self.base_url);
        let request = OllamaGenerationRequest {
            model: model_id.to_string(),
            prompt: prompt.to_string(),
            stream: true,
            options: Some(ollama_options),
        };

        // Send the request
        let response = self.client.post(&url).json(&request).send().await.map_err(|e| {
            LLMProviderError::NetworkError(format!("Failed to generate text: {}", e))
        })?;

        if !response.status().is_success() {
            return Err(LLMProviderError::ProviderError(format!(
                "Failed to generate text: HTTP {}", response.status()
            )));
        }

        // Create the streaming response handler
        Ok(Box::new(OllamaCompletionStream {
            response,
            accumulated_text: String::new(),
            completed: false,
        }))
    }

    async fn get_model_config(&self, model_id: &str) -> LLMProviderResult<ModelConfig> {
        self.ensure_initialized()?;

        // Check if we have a cached config
        let config_map = self.model_configs.read().await;
        if let Some(config) = config_map.get(model_id) {
            return Ok(config.clone());
        }

        // Ollama doesn't have a direct API to get model config
        // We'll return a default config
        Ok(ModelConfig {
            id: model_id.to_string(),
            parameters: HashMap::new(),
        })
    }

    async fn update_model_config(&self, config: ModelConfig) -> LLMProviderResult<()> {
        self.ensure_initialized()?;

        // Store the config in our cache
        let mut config_map = self.model_configs.write().await;
        config_map.insert(config.id.clone(), config);

        // Note: Ollama doesn't have a direct API to update model config
        // We're just storing it locally for now
        Ok(())
    }

    async fn get_model_info(&self, model_id: &str) -> LLMProviderResult<ModelInfo> {
        self.ensure_initialized()?;

        // Get all models and find the one we want
        let models = self.list_available_models().await?;
        for model in models {
            if model.id == model_id {
                return Ok(model);
            }
        }

        Err(LLMProviderError::ModelNotFound(model_id.to_string()))
    }
}
</file>

<file path="src-tauri/src/offline/llm/provider.rs">
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use thiserror::Error;

/// Represents errors that can occur when interacting with a local LLM provider.
#[derive(Error, Debug)]
pub enum LLMProviderError {
    /// Returned when a requested model does not exist
    #[error("Model not found: {0}")]
    ModelNotFound(String),
    
    /// Returned when there's an error during model download
    #[error("Model download failed: {0}")]
    DownloadFailed(String),
    
    /// Returned when there's an error during text generation
    #[error("Text generation failed: {0}")]
    GenerationFailed(String),
    
    /// Returned when an operation fails due to network issues
    #[error("Network error: {0}")]
    NetworkError(String),
    
    /// Returned when a provider-specific operation fails
    #[error("Provider error: {0}")]
    ProviderError(String),
    
    /// Returned when the provider is not properly initialized
    #[error("Provider not initialized: {0}")]
    NotInitialized(String),
    
    /// Represents an unexpected error
    #[error("Unexpected error: {0}")]
    Unexpected(String),
}

/// Result type for LLM provider operations
pub type LLMProviderResult<T> = Result<T, LLMProviderError>;

/// Represents the download status of a model
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum DownloadStatus {
    /// Model download has not started
    NotStarted,
    /// Model download is in progress with percentage
    InProgress { percent: f32 },
    /// Model download has completed successfully
    Completed,
    /// Model download has failed with error message
    Failed { reason: String },
    /// Model download has been cancelled
    Cancelled,
}

/// Represents the configuration for a model
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelConfig {
    /// Model identifier
    pub id: String,
    /// Configuration parameters for the model
    pub parameters: HashMap<String, serde_json::Value>,
}

/// Represents model metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelInfo {
    /// Unique identifier for the model
    pub id: String,
    /// Display name of the model
    pub name: String,
    /// Model description
    pub description: String,
    /// Model size in bytes
    pub size_bytes: u64,
    /// Whether the model is currently downloaded and available locally
    pub is_downloaded: bool,
    /// Provider-specific model information
    pub provider_metadata: HashMap<String, serde_json::Value>,
}

/// Options for text generation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GenerationOptions {
    /// Maximum number of tokens to generate
    pub max_tokens: Option<u32>,
    /// Temperature for sampling (0.0 to 1.0, lower is more deterministic)
    pub temperature: Option<f32>,
    /// Top-p sampling
    pub top_p: Option<f32>,
    /// Whether to stream the generated text
    pub stream: bool,
    /// Additional provider-specific parameters
    pub additional_params: HashMap<String, serde_json::Value>,
}

impl Default for GenerationOptions {
    fn default() -> Self {
        Self {
            max_tokens: Some(1024),
            temperature: Some(0.7),
            top_p: Some(0.9),
            stream: false,
            additional_params: HashMap::new(),
        }
    }
}

/// Represents a completion response from a model
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CompletionResponse {
    /// The generated text
    pub text: String,
    /// Whether the generation was stopped due to reaching max tokens
    pub reached_max_tokens: bool,
    /// Token usage statistics
    pub usage: TokenUsage,
    /// Provider-specific metadata about the generation
    pub metadata: HashMap<String, serde_json::Value>,
}

/// Represents token usage statistics for a generation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TokenUsage {
    /// Number of tokens in the prompt
    pub prompt_tokens: u32,
    /// Number of tokens in the completion
    pub completion_tokens: u32,
    /// Total number of tokens used
    pub total_tokens: u32,
}

/// Trait for streaming completions
#[async_trait]
pub trait CompletionStream: Send + Sync {
    /// Get the next chunk of the completion
    async fn next_chunk(&mut self) -> Option<LLMProviderResult<String>>;
}

/// Represents a local LLM provider, abstracting different implementations like Ollama, LocalAI, etc.
#[async_trait]
pub trait LocalLLMProvider: Send + Sync {
    /// Initialize the provider with the given configuration
    /// 
    /// # Arguments
    /// 
    /// * `config` - Provider-specific configuration as a JSON value
    /// 
    /// # Returns
    /// 
    /// * `LLMProviderResult<()>` - Result indicating success or failure
    async fn initialize(&mut self, config: serde_json::Value) -> LLMProviderResult<()>;
    
    /// Get provider name
    /// 
    /// # Returns
    /// 
    /// * `&str` - Name of the provider (e.g., "Ollama", "LocalAI")
    fn provider_name(&self) -> &str;
    
    /// List all available models from this provider
    /// 
    /// This will return both downloaded/available models and models that can be downloaded.
    /// 
    /// # Returns
    /// 
    /// * `LLMProviderResult<Vec<ModelInfo>>` - Information about available models
    async fn list_available_models(&self) -> LLMProviderResult<Vec<ModelInfo>>;
    
    /// List models that are currently downloaded and ready to use
    /// 
    /// # Returns
    /// 
    /// * `LLMProviderResult<Vec<ModelInfo>>` - Information about downloaded models
    async fn list_downloaded_models(&self) -> LLMProviderResult<Vec<ModelInfo>>;
    
    /// Download a model by its identifier
    /// 
    /// # Arguments
    /// 
    /// * `model_id` - Identifier of the model to download
    /// 
    /// # Returns
    /// 
    /// * `LLMProviderResult<()>` - Result indicating success or failure
    async fn download_model(&self, model_id: &str) -> LLMProviderResult<()>;
    
    /// Get the download status of a model
    /// 
    /// # Arguments
    /// 
    /// * `model_id` - Identifier of the model to check
    /// 
    /// # Returns
    /// 
    /// * `LLMProviderResult<DownloadStatus>` - Current download status
    async fn get_download_status(&self, model_id: &str) -> LLMProviderResult<DownloadStatus>;
    
    /// Cancel an in-progress model download
    /// 
    /// # Arguments
    /// 
    /// * `model_id` - Identifier of the model whose download should be cancelled
    /// 
    /// # Returns
    /// 
    /// * `LLMProviderResult<()>` - Result indicating success or failure
    async fn cancel_download(&self, model_id: &str) -> LLMProviderResult<()>;
    
    /// Delete a downloaded model
    /// 
    /// # Arguments
    /// 
    /// * `model_id` - Identifier of the model to delete
    /// 
    /// # Returns
    /// 
    /// * `LLMProviderResult<()>` - Result indicating success or failure
    async fn delete_model(&self, model_id: &str) -> LLMProviderResult<()>;
    
    /// Check if a model is currently loaded and ready for inference
    /// 
    /// # Arguments
    /// 
    /// * `model_id` - Identifier of the model to check
    /// 
    /// # Returns
    /// 
    /// * `LLMProviderResult<bool>` - Whether the model is loaded
    async fn is_model_loaded(&self, model_id: &str) -> LLMProviderResult<bool>;
    
    /// Explicitly load a model into memory for faster inference
    /// 
    /// # Arguments
    /// 
    /// * `model_id` - Identifier of the model to load
    /// 
    /// # Returns
    /// 
    /// * `LLMProviderResult<()>` - Result indicating success or failure
    async fn load_model(&self, model_id: &str) -> LLMProviderResult<()>;
    
    /// Unload a model from memory
    /// 
    /// # Arguments
    /// 
    /// * `model_id` - Identifier of the model to unload
    /// 
    /// # Returns
    /// 
    /// * `LLMProviderResult<()>` - Result indicating success or failure
    async fn unload_model(&self, model_id: &str) -> LLMProviderResult<()>;
    
    /// Generate text with a model
    /// 
    /// # Arguments
    /// 
    /// * `model_id` - Identifier of the model to use
    /// * `prompt` - Input prompt for text generation
    /// * `options` - Options for text generation
    /// 
    /// # Returns
    /// 
    /// * `LLMProviderResult<CompletionResponse>` - Generated completion
    async fn generate_text(
        &self,
        model_id: &str,
        prompt: &str,
        options: GenerationOptions,
    ) -> LLMProviderResult<CompletionResponse>;
    
    /// Generate text with streaming response
    /// 
    /// # Arguments
    /// 
    /// * `model_id` - Identifier of the model to use
    /// * `prompt` - Input prompt for text generation
    /// * `options` - Options for text generation
    /// 
    /// # Returns
    /// 
    /// * `LLMProviderResult<Box<dyn CompletionStream>>` - Stream of generated text chunks
    async fn generate_text_streaming(
        &self,
        model_id: &str,
        prompt: &str,
        options: GenerationOptions,
    ) -> LLMProviderResult<Box<dyn CompletionStream>>;
    
    /// Get the current configuration for a model
    /// 
    /// # Arguments
    /// 
    /// * `model_id` - Identifier of the model
    /// 
    /// # Returns
    /// 
    /// * `LLMProviderResult<ModelConfig>` - Current model configuration
    async fn get_model_config(&self, model_id: &str) -> LLMProviderResult<ModelConfig>;
    
    /// Update the configuration for a model
    /// 
    /// # Arguments
    /// 
    /// * `config` - New configuration for the model
    /// 
    /// # Returns
    /// 
    /// * `LLMProviderResult<()>` - Result indicating success or failure
    async fn update_model_config(&self, config: ModelConfig) -> LLMProviderResult<()>;
    
    /// Get detailed information about a specific model
    /// 
    /// # Arguments
    /// 
    /// * `model_id` - Identifier of the model
    /// 
    /// # Returns
    /// 
    /// * `LLMProviderResult<ModelInfo>` - Detailed model information
    async fn get_model_info(&self, model_id: &str) -> LLMProviderResult<ModelInfo>;
}
</file>

<file path="src-tauri/src/offline/llm/providers/localai.rs">
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use async_trait::async_trait;
use chrono::Utc;
use futures_util::StreamExt;
use log::{debug, error, info, warn};
use reqwest::{Client, StatusCode, header};
use serde::{Deserialize, Serialize};
use tokio::sync::RwLock;

use crate::offline::llm::provider::{
    CompletionResponse, CompletionStream, LocalLLMProvider,
    LLMProviderError, LLMProviderResult, ModelConfig, ModelInfo, TokenUsage,
};
use crate::offline::llm::types::{DownloadStatus, GenerationOptions, ProviderType};

// LocalAI API - OpenAI compatibility types

/// OpenAI compatible model object
#[derive(Debug, Deserialize)]
struct OpenAIModel {
    id: String,
    object: String,
    created: u64,
    owned_by: String,
    #[serde(default)]
    permission: Vec<OpenAIPermission>,
    root: Option<String>,
    parent: Option<String>,
}

/// OpenAI compatible permission object
#[derive(Debug, Deserialize)]
struct OpenAIPermission {
    id: String,
    object: String,
    created: u64,
    allow_create_engine: bool,
    allow_sampling: bool,
    allow_logprobs: bool,
    allow_search_indices: bool,
    allow_view: bool,
    allow_fine_tuning: bool,
    organization: String,
    group: Option<String>,
    is_blocking: bool,
}

/// OpenAI compatible model list response
#[derive(Debug, Deserialize)]
struct OpenAIModelList {
    object: String,
    data: Vec<OpenAIModel>,
}

/// OpenAI compatible completion request
#[derive(Debug, Serialize)]
struct OpenAICompletionRequest {
    model: String,
    prompt: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    max_tokens: Option<u32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    temperature: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    top_p: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    frequency_penalty: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    presence_penalty: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    stop: Option<Vec<String>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    n: Option<u32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    logprobs: Option<u32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    best_of: Option<u32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    echo: Option<bool>,
    #[serde(skip_serializing_if = "Option::is_none")]
    seed: Option<u64>,
    #[serde(skip_serializing_if = "Option::is_none")]
    stream: Option<bool>,
    #[serde(skip_serializing_if = "Option::is_none")]
    user: Option<String>,
}

/// OpenAI compatible chat message
#[derive(Debug, Serialize, Deserialize, Clone)]
struct OpenAIChatMessage {
    role: String,
    content: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    name: Option<String>,
}

/// OpenAI compatible chat completion request
#[derive(Debug, Serialize)]
struct OpenAIChatCompletionRequest {
    model: String,
    messages: Vec<OpenAIChatMessage>,
    #[serde(skip_serializing_if = "Option::is_none")]
    max_tokens: Option<u32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    temperature: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    top_p: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    frequency_penalty: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    presence_penalty: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    stop: Option<Vec<String>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    n: Option<u32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    stream: Option<bool>,
    #[serde(skip_serializing_if = "Option::is_none")]
    seed: Option<u64>,
    #[serde(skip_serializing_if = "Option::is_none")]
    user: Option<String>,
}

/// OpenAI compatible chat completion response
#[derive(Debug, Deserialize)]
struct OpenAIChatCompletionResponse {
    id: String,
    object: String,
    created: u64,
    model: String,
    choices: Vec<OpenAIChatCompletionChoice>,
    usage: Option<OpenAIUsage>,
}

/// OpenAI compatible chat completion choice
#[derive(Debug, Deserialize)]
struct OpenAIChatCompletionChoice {
    index: u32,
    message: OpenAIChatMessage,
    finish_reason: Option<String>,
}

/// OpenAI compatible completion response
#[derive(Debug, Deserialize)]
struct OpenAICompletionResponse {
    id: String,
    object: String,
    created: u64,
    model: String,
    choices: Vec<OpenAICompletionChoice>,
    usage: Option<OpenAIUsage>,
}

/// OpenAI compatible completion choice
#[derive(Debug, Deserialize)]
struct OpenAICompletionChoice {
    text: String,
    index: u32,
    logprobs: Option<serde_json::Value>,
    finish_reason: Option<String>,
}

/// OpenAI compatible usage information
#[derive(Debug, Deserialize)]
struct OpenAIUsage {
    prompt_tokens: u32,
    completion_tokens: u32,
    total_tokens: u32,
}

/// OpenAI compatible chat completion delta
#[derive(Debug, Deserialize)]
struct OpenAIChatCompletionDelta {
    content: Option<String>,
    role: Option<String>,
}

/// OpenAI compatible streaming chat completion chunk
#[derive(Debug, Deserialize)]
struct OpenAIChatCompletionChunk {
    id: String,
    object: String,
    created: u64,
    model: String,
    choices: Vec<OpenAIChatCompletionStreamChoice>,
}

/// OpenAI compatible streaming chat completion choice
#[derive(Debug, Deserialize)]
struct OpenAIChatCompletionStreamChoice {
    index: u32,
    delta: OpenAIChatCompletionDelta,
    finish_reason: Option<String>,
}

/// OpenAI compatible streaming completion chunk
#[derive(Debug, Deserialize)]
struct OpenAICompletionChunk {
    id: String,
    object: String,
    created: u64,
    model: String,
    choices: Vec<OpenAICompletionStreamChoice>,
}

/// OpenAI compatible streaming completion choice
#[derive(Debug, Deserialize)]
struct OpenAICompletionStreamChoice {
    text: String,
    index: u32,
    logprobs: Option<serde_json::Value>,
    finish_reason: Option<String>,
}

/// LocalAI-specific API types

/// LocalAI model pull request
#[derive(Debug, Serialize)]
struct LocalAIPullRequest {
    name: String,
}

/// LocalAI model pull response
#[derive(Debug, Deserialize)]
struct LocalAIPullResponse {
    status: String,
    error: Option<String>,
}

/// LocalAI model gallery item
#[derive(Debug, Deserialize)]
struct LocalAIGalleryModel {
    name: String,
    description: Option<String>,
    license: Option<String>,
    urls: Vec<String>,
    tags: Option<Vec<String>>,
    parameters: Option<HashMap<String, serde_json::Value>>,
}

/// LocalAI model gallery response
#[derive(Debug, Deserialize)]
struct LocalAIGalleryResponse {
    models: Vec<LocalAIGalleryModel>,
}

/// LocalAI health endpoint response
#[derive(Debug, Deserialize)]
struct LocalAIHealthResponse {
    status: String,
    version: Option<String>,
}

/// Download statistics for models
#[derive(Debug, Clone)]
struct ModelDownloadStats {
    start_time: chrono::DateTime<Utc>,
    model_id: String,
    status: DownloadStatus,
    // For non-standard providers like LocalAI we don't have download progress 
    // info, so we use a simple status tracking system
}

impl ModelDownloadStats {
    fn new(model_id: &str) -> Self {
        Self {
            start_time: Utc::now(),
            model_id: model_id.to_string(),
            status: DownloadStatus::InProgress { 
                percent: 0.0,
                bytes_downloaded: None,
                total_bytes: None,
                eta_seconds: None,
                bytes_per_second: None,
            },
        }
    }
    
    fn set_completed(&mut self) {
        let duration = Utc::now() - self.start_time;
        self.status = DownloadStatus::Completed { 
            completed_at: Some(Utc::now().to_rfc3339()),
            duration_seconds: Some(duration.num_milliseconds() as f32 / 1000.0),
        };
    }
    
    fn set_failed(&mut self, reason: &str) {
        self.status = DownloadStatus::Failed { 
            reason: reason.to_string(),
            error_code: None,
            failed_at: Some(Utc::now().to_rfc3339()),
        };
    }
    
    fn set_cancelled(&mut self) {
        self.status = DownloadStatus::Cancelled { 
            cancelled_at: Some(Utc::now().to_rfc3339()),
        };
    }
}

/// LocalAI provider implementation
pub struct LocalAIProvider {
    /// Base URL for the LocalAI API
    base_url: String,
    /// HTTP client for API requests
    client: Client,
    /// API key (if required)
    api_key: Option<String>,
    /// Map of model IDs to download status
    download_status: Arc<RwLock<HashMap<String, DownloadStatus>>>,
    /// Map of model IDs to download statistics
    download_stats: Arc<RwLock<HashMap<String, ModelDownloadStats>>>,
    /// Map of model IDs to model configurations
    model_configs: Arc<RwLock<HashMap<String, ModelConfig>>>,
    /// Whether the provider has been initialized
    initialized: Arc<Mutex<bool>>,
    /// Model capabilities from gallery info
    model_capabilities: Arc<RwLock<HashMap<String, HashMap<String, bool>>>>,
}

impl LocalAIProvider {
    /// Create a new LocalAI provider with the default URL
    pub fn new() -> Self {
        Self::with_base_url("http://localhost:8080")
    }

    /// Create a new LocalAI provider with a custom URL
    pub fn with_base_url(base_url: &str) -> Self {
        Self {
            base_url: base_url.to_string(),
            client: Client::builder()
                .timeout(std::time::Duration::from_secs(60))
                .build()
                .unwrap_or_else(|_| Client::new()),
            api_key: None,
            download_status: Arc::new(RwLock::new(HashMap::new())),
            download_stats: Arc::new(RwLock::new(HashMap::new())),
            model_configs: Arc::new(RwLock::new(HashMap::new())),
            initialized: Arc::new(Mutex::new(false)),
            model_capabilities: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    /// Check if the provider is initialized
    fn ensure_initialized(&self) -> LLMProviderResult<()> {
        let initialized = *self.initialized.lock().map_err(|e| {
            LLMProviderError::Unexpected(format!("Failed to acquire lock: {}", e))
        })?;

        if !initialized {
            return Err(LLMProviderError::NotInitialized(
                "LocalAI provider not initialized. Call initialize() first.".to_string(),
            ));
        }

        Ok(())
    }
    
    /// Create a request builder with appropriate headers including API key if set
    fn create_request(&self, method: reqwest::Method, url: &str) -> reqwest::RequestBuilder {
        let mut builder = self.client.request(method, url);
        
        // Add API key if available
        if let Some(api_key) = &self.api_key {
            builder = builder.header(header::AUTHORIZATION, format!("Bearer {}", api_key));
        }
        
        builder
    }

    /// Convert OpenAI model info to the common ModelInfo format
    fn convert_to_model_info(&self, openai_model: OpenAIModel) -> ModelInfo {
        let mut provider_metadata = HashMap::new();
        let model_id = openai_model.id.clone();
        
        provider_metadata.insert("object".to_string(), serde_json::to_value(openai_model.object).unwrap_or_default());
        provider_metadata.insert("created".to_string(), serde_json::to_value(openai_model.created).unwrap_or_default());
        provider_metadata.insert("owned_by".to_string(), serde_json::to_value(openai_model.owned_by).unwrap_or_default());
        
        if let Some(root) = openai_model.root {
            provider_metadata.insert("root".to_string(), serde_json::to_value(root).unwrap_or_default());
        }
        
        if let Some(parent) = openai_model.parent {
            provider_metadata.insert("parent".to_string(), serde_json::to_value(parent).unwrap_or_default());
        }
        
        // Create tags from model name components
        let tags = model_id.split(':')
            .skip(1)  // Skip the base name
            .filter(|&s| !s.is_empty())
            .map(|s| s.to_string())
            .collect::<Vec<_>>();

        // Extract base name without tags
        let base_name = model_id.split(':').next().unwrap_or(&model_id).to_string();
        
        // Determine capabilities from our stored capabilities map
        let mut supports_text_generation = true;  // Assume text generation by default
        let mut supports_completion = true;       // Assume completion by default
        let mut supports_chat = true;             // Assume chat by default
        let mut supports_embeddings = false;      // Don't assume embeddings by default
        let mut supports_image_generation = false; // Don't assume image generation by default
        
        // Look up capabilities if we have them
        if let Ok(capabilities_map) = self.model_capabilities.read() {
            if let Some(capabilities) = capabilities_map.get(&model_id) {
                supports_text_generation = *capabilities.get("text_generation").unwrap_or(&true);
                supports_completion = *capabilities.get("completion").unwrap_or(&true);
                supports_chat = *capabilities.get("chat").unwrap_or(&true);
                supports_embeddings = *capabilities.get("embeddings").unwrap_or(&false);
                supports_image_generation = *capabilities.get("image_generation").unwrap_or(&false);
            }
        }
        
        // Try to infer parameter count from model name
        let param_count = model_id.to_lowercase()
            .replace(&base_name.to_lowercase(), "")
            .replace(':', "")
            .replace('-', "")
            .chars()
            .filter(|c| c.is_alphanumeric())
            .collect::<String>()
            .find(|c: char| c.is_numeric())
            .and_then(|_| {
                // Extract numeric part that might represent parameters
                let numeric_part = model_id.to_lowercase()
                    .chars()
                    .filter(|&c| c.is_digit(10) || c == '.')
                    .collect::<String>();
                
                // Check if it has 'b' suffix for billions
                if model_id.to_lowercase().contains('b') {
                    numeric_part.parse::<f64>().ok()
                } else {
                    None
                }
            });

        // Create model info
        ModelInfo {
            id: model_id.clone(),
            name: base_name,
            description: format!("LocalAI model{}", 
                if let Some(params) = param_count {
                    format!(" ({:.1}B parameters)", params)
                } else {
                    "".to_string()
                }
            ),
            size_bytes: 0, // LocalAI API doesn't provide size info
            is_downloaded: true, // If it's listed, it's available
            provider_metadata,
            provider: ProviderType::LocalAI,
            supports_text_generation,
            supports_completion,
            supports_chat,
            supports_embeddings,
            supports_image_generation,
            quantization: None, // LocalAI API doesn't provide quantization info
            parameter_count_b: param_count,
            context_length: None, // LocalAI API doesn't provide context window info
            model_family: None, // LocalAI API doesn't provide family info
            created_at: Some(chrono::DateTime::from_timestamp(openai_model.created as i64, 0)
                .map_or_else(
                    || Utc::now().to_rfc3339(),
                    |dt| dt.to_rfc3339()
                )),
            tags,
            license: None, // LocalAI API doesn't provide license info directly
        }
    }
    
    /// Fetch model gallery information to determine capabilities
    async fn fetch_model_gallery(&self) -> LLMProviderResult<()> {
        debug!("Fetching model gallery from LocalAI");
        
        let url = format!("{}/api/models/available", self.base_url);
        let response = match self.create_request(reqwest::Method::GET, &url).send().await {
            Ok(response) => response,
            Err(e) => {
                // This is non-critical, so just log and return
                warn!("Failed to fetch model gallery: {}. Some model capabilities may not be detected.", e);
                return Ok(());
            }
        };
        
        if !response.status().is_success() {
            warn!("Failed to fetch model gallery: HTTP {}. Some model capabilities may not be detected.", 
                response.status());
            return Ok(());
        }
        
        // Parse the response
        match response.json::<LocalAIGalleryResponse>().await {
            Ok(gallery) => {
                let mut capabilities_map = self.model_capabilities.write().await;
                
                for model in gallery.models {
                    let mut capabilities = HashMap::new();
                    
                    // Process tags to determine capabilities
                    if let Some(tags) = model.tags {
                        // Check for specific capabilities in tags
                        capabilities.insert("text_generation".to_string(), 
                            tags.iter().any(|t| t.contains("text") || t.contains("llm")));
                        capabilities.insert("chat".to_string(), 
                            tags.iter().any(|t| t.contains("chat")));
                        capabilities.insert("embeddings".to_string(), 
                            tags.iter().any(|t| t.contains("embed")));
                        capabilities.insert("image_generation".to_string(), 
                            tags.iter().any(|t| t.contains("image") || t.contains("diffusion")));
                    }
                    
                    capabilities_map.insert(model.name, capabilities);
                }
                
                debug!("Successfully fetched model gallery with {} models", gallery.models.len());
            },
            Err(e) => {
                warn!("Failed to parse model gallery: {}. Some model capabilities may not be detected.", e);
            }
        }
        
        Ok(())
    }
    
    /// Set download status for a model
    async fn set_download_status(&self, model_id: &str, status: DownloadStatus) {
        let mut status_map = self.download_status.write().await;
        status_map.insert(model_id.to_string(), status);
    }
}

/// Streaming completion implementation for LocalAI using chat completions
pub struct LocalAIChatCompletionStream {
    /// HTTP response stream
    response: reqwest::Response,
    /// Total text accumulated so far
    accumulated_text: String,
    /// Whether we've reached the end of the stream
    completed: bool,
}

impl LocalAIChatCompletionStream {
    /// Parse a chunk from the event stream
    fn parse_chunk(&mut self, chunk: &str) -> LLMProviderResult<Option<String>> {
        // SSE format: "data: {...}\n\n"
        let data = chunk.trim().strip_prefix("data: ").unwrap_or(chunk);
        
        // Check for the "[DONE]" marker
        if data == "[DONE]" {
            self.completed = true;
            return Ok(None);
        }
        
        // Parse the JSON
        match serde_json::from_str::<OpenAIChatCompletionChunk>(data) {
            Ok(chunk_data) => {
                // Extract the text content
                if let Some(choice) = chunk_data.choices.first() {
                    if let Some(content) = &choice.delta.content {
                        self.accumulated_text.push_str(content);
                        return Ok(Some(content.clone()));
                    }
                    
                    // Check for finish reason
                    if choice.finish_reason.is_some() {
                        self.completed = true;
                    }
                }
                
                Ok(None)
            },
            Err(e) => {
                // If we can't parse as chat completion, try as regular completion
                match serde_json::from_str::<OpenAICompletionChunk>(data) {
                    Ok(completion_chunk) => {
                        if let Some(choice) = completion_chunk.choices.first() {
                            self.accumulated_text.push_str(&choice.text);
                            
                            // Check for finish reason
                            if choice.finish_reason.is_some() {
                                self.completed = true;
                            }
                            
                            return Ok(Some(choice.text.clone()));
                        }
                        
                        Ok(None)
                    },
                    Err(e2) => {
                        warn!("Failed to parse chunk as chat or completion: {} / {}", e, e2);
                        Err(LLMProviderError::Unexpected(format!(
                            "Failed to parse streaming response: {}", e
                        )))
                    }
                }
            }
        }
    }
}

#[async_trait]
impl CompletionStream for LocalAIChatCompletionStream {
    async fn next_chunk(&mut self) -> Option<LLMProviderResult<String>> {
        if self.completed {
            return None;
        }

        match self.response.chunk().await {
            Ok(Some(chunk)) => {
                let chunk_str = match std::str::from_utf8(&chunk) {
                    Ok(s) => s,
                    Err(e) => return Some(Err(LLMProviderError::Unexpected(format!(
                        "Failed to decode response chunk: {}", e
                    )))),
                };
                
                match self.parse_chunk(chunk_str) {
                    Ok(Some(text)) => Some(Ok(text)),
                    Ok(None) => self.next_chunk().await, // No content in this chunk, try next
                    Err(e) => Some(Err(e)),
                }
            },
            Ok(None) => {
                self.completed = true;
                None
            },
            Err(e) => Some(Err(LLMProviderError::NetworkError(format!(
                "Failed to get response chunk: {}", e
            )))),
        }
    }
}

#[async_trait]
impl LocalLLMProvider for LocalAIProvider {
    async fn initialize(&mut self, config: serde_json::Value) -> LLMProviderResult<()> {
        // Extract configuration options
        let config_obj = match config.as_object() {
            Some(obj) => obj,
            None => {
                warn!("LocalAI provider configuration is not a JSON object");
                return Err(LLMProviderError::ProviderError(
                    "Configuration must be a JSON object".to_string(),
                ));
            }
        };

        // Override base URL if provided
        if let Some(base_url) = config_obj.get("base_url").and_then(|v| v.as_str()) {
            debug!("Setting LocalAI base URL to: {}", base_url);
            self.base_url = base_url.to_string();
        }
        
        // Set API key if provided
        if let Some(api_key) = config_obj.get("api_key").and_then(|v| v.as_str()) {
            debug!("Setting LocalAI API key");
            self.api_key = Some(api_key.to_string());
        }

        // Test connection to LocalAI API
        info!("Testing connection to LocalAI API at {}", self.base_url);
        let health_url = format!("{}/health", self.base_url);
        
        match self.create_request(reqwest::Method::GET, &health_url).send().await {
            Ok(response) => {
                if !response.status().is_success() {
                    error!("Failed to connect to LocalAI API: HTTP {}", response.status());
                    return Err(LLMProviderError::ProviderError(format!(
                        "Failed to connect to LocalAI API: HTTP {}", response.status()
                    )));
                }
                
                // Try to parse the health response
                match response.json::<LocalAIHealthResponse>().await {
                    Ok(health_data) => {
                        if let Some(version) = health_data.version {
                            info!("Connected to LocalAI (version: {})", version);
                        } else {
                            info!("Connected to LocalAI (unknown version)");
                        }
                    },
                    Err(e) => {
                        warn!("Connected to LocalAI but couldn't parse health info: {}", e);
                    }
                }
            },
            Err(e) => {
                error!("Failed to connect to LocalAI API: {}", e);
                return Err(LLMProviderError::NetworkError(format!(
                    "Failed to connect to LocalAI API: {}. Make sure LocalAI is running at {}.", 
                    e, self.base_url
                )));
            }
        }
        
        // Fetch model gallery to determine capabilities
        self.fetch_model_gallery().await?;

        // Mark as initialized
        match self.initialized.lock() {
            Ok(mut initialized) => {
                *initialized = true;
                info!("LocalAI provider initialized successfully");
            },
            Err(e) => {
                error!("Failed to acquire lock when initializing LocalAI provider: {}", e);
                return Err(LLMProviderError::Unexpected(format!(
                    "Failed to acquire lock: {}", e
                )));
            }
        }

        Ok(())
    }

    fn provider_name(&self) -> &str {
        "LocalAI"
    }

    async fn list_available_models(&self) -> LLMProviderResult<Vec<ModelInfo>> {
        self.ensure_initialized()?;
        debug!("Listing available models from LocalAI");

        let url = format!("{}/v1/models", self.base_url);
        let response = match self.create_request(reqwest::Method::GET, &url).send().await {
            Ok(response) => response,
            Err(e) => {
                error!("Failed to fetch models from LocalAI: {}", e);
                return Err(LLMProviderError::NetworkError(format!(
                    "Failed to fetch models: {}", e
                )));
            }
        };

        if !response.status().is_success() {
            error!("Failed to fetch models from LocalAI: HTTP {}", response.status());
            return Err(LLMProviderError::ProviderError(format!(
                "Failed to fetch models: HTTP {}", response.status()
            )));
        }

        // Parse the response
        let models_response: OpenAIModelList = match response.json().await {
            Ok(data) => data,
            Err(e) => {
                error!("Failed to parse models response from LocalAI: {}", e);
                return Err(LLMProviderError::ProviderError(format!(
                    "Failed to parse models response: {}", e
                )));
            }
        };

        // Convert to our model format
        let models = models_response.data.into_iter()
            .map(|m| self.convert_to_model_info(m))
            .collect::<Vec<_>>();

        info!("Found {} models from LocalAI", models.len());
        Ok(models)
    }

    async fn list_downloaded_models(&self) -> LLMProviderResult<Vec<ModelInfo>> {
        // For LocalAI, all listed models are already downloaded and ready to use
        self.list_available_models().await
    }

    async fn download_model(&self, model_id: &str) -> LLMProviderResult<()> {
        self.ensure_initialized()?;
        info!("Starting download of model '{}' from LocalAI", model_id);

        // Update download status
        self.set_download_status(model_id, DownloadStatus::InProgress { 
            percent: 0.0,
            bytes_downloaded: None,
            total_bytes: None,
            eta_seconds: None,
            bytes_per_second: None,
        }).await;
        
        // Initialize download stats
        {
            let mut stats_map = self.download_stats.write().await;
            stats_map.insert(model_id.to_string(), ModelDownloadStats::new(model_id));
        }

        // Construct the pull request
        let url = format!("{}/api/models/apply", self.base_url);
        let request = LocalAIPullRequest {
            name: model_id.to_string(),
        };

        // Start download in a separate task
        let model_id = model_id.to_string();
        let client = self.client.clone();
        let base_url = self.base_url.clone();
        let api_key = self.api_key.clone();
        let download_status = self.download_status.clone();
        let download_stats = self.download_stats.clone();

        tokio::spawn(async move {
            let mut request_builder = client.post(&url);
            
            // Add API key if available
            if let Some(api_key) = &api_key {
                request_builder = request_builder.header(header::AUTHORIZATION, format!("Bearer {}", api_key));
            }
            
            // Send the request
            let result = request_builder.json(&request).send().await;
            
            match result {
                Ok(response) => {
                    if !response.status().is_success() {
                        error!("Failed to download model '{}': HTTP {}", model_id, response.status());
                        
                        // Update status to failed
                        let mut stats_map = download_stats.write().await;
                        if let Some(stats) = stats_map.get_mut(&model_id) {
                            stats.set_failed(&format!("HTTP error: {}", response.status()));
                            // Update the download status
                            let mut status_map = download_status.write().await;
                            status_map.insert(model_id.clone(), stats.status.clone());
                        }
                        return;
                    }

                    // Parse the response
                    match response.json::<LocalAIPullResponse>().await {
                        Ok(pull_response) => {
                            if pull_response.status == "success" {
                                info!("Successfully downloaded model '{}'", model_id);
                                
                                // Update status to completed
                                let mut stats_map = download_stats.write().await;
                                if let Some(stats) = stats_map.get_mut(&model_id) {
                                    stats.set_completed();
                                    // Update the download status
                                    let mut status_map = download_status.write().await;
                                    status_map.insert(model_id.clone(), stats.status.clone());
                                }
                            } else {
                                let error_msg = pull_response.error.unwrap_or_else(|| "Unknown error".to_string());
                                error!("Error downloading model '{}': {}", model_id, error_msg);
                                
                                // Update status to failed
                                let mut stats_map = download_stats.write().await;
                                if let Some(stats) = stats_map.get_mut(&model_id) {
                                    stats.set_failed(&error_msg);
                                    // Update the download status
                                    let mut status_map = download_status.write().await;
                                    status_map.insert(model_id.clone(), stats.status.clone());
                                }
                            }
                        },
                        Err(e) => {
                            error!("Failed to parse pull response for model '{}': {}", model_id, e);
                            
                            // Update status to failed
                            let mut stats_map = download_stats.write().await;
                            if let Some(stats) = stats_map.get_mut(&model_id) {
                                stats.set_failed(&format!("Failed to parse response: {}", e));
                                // Update the download status
                                let mut status_map = download_status.write().await;
                                status_map.insert(model_id.clone(), stats.status.clone());
                            }
                        }
                    }
                },
                Err(e) => {
                    error!("Network error while downloading model '{}': {}", model_id, e);
                    
                    // Update status to failed
                    let mut stats_map = download_stats.write().await;
                    if let Some(stats) = stats_map.get_mut(&model_id) {
                        stats.set_failed(&format!("Network error: {}", e));
                        // Update the download status
                        let mut status_map = download_status.write().await;
                        status_map.insert(model_id.clone(), stats.status.clone());
                    }
                }
            }
        });

        Ok(())
    }

    async fn get_download_status(&self, model_id: &str) -> LLMProviderResult<DownloadStatus> {
        self.ensure_initialized()?;
        debug!("Getting download status for model '{}'", model_id);

        let status_map = self.download_status.read().await;
        match status_map.get(model_id) {
            Some(status) => Ok(status.clone()),
            None => {
                // If no status is recorded, check if the model exists already
                debug!("No download status found for model '{}', checking if it exists", model_id);
                let models = self.list_downloaded_models().await?;
                if models.iter().any(|m| m.id == model_id) {
                    debug!("Model '{}' is already downloaded", model_id);
                    Ok(DownloadStatus::Completed {
                        completed_at: None,
                        duration_seconds: None,
                    })
                } else {
                    debug!("Model '{}' is not downloaded", model_id);
                    Ok(DownloadStatus::NotStarted)
                }
            }
        }
    }

    async fn cancel_download(&self, model_id: &str) -> LLMProviderResult<()> {
        self.ensure_initialized()?;
        debug!("Cancelling download of model '{}'", model_id);

        // LocalAI doesn't have a direct API to cancel downloads
        // We can only update our status to indicate cancellation
        let mut status_map = self.download_status.write().await;
        match status_map.get(model_id) {
            Some(status) => {
                match status {
                    DownloadStatus::InProgress { .. } => {
                        info!("Marking download of model '{}' as cancelled", model_id);
                        status_map.insert(model_id.to_string(), DownloadStatus::Cancelled {
                            cancelled_at: Some(Utc::now().to_rfc3339()),
                        });
                        
                        // Update stats
                        let mut stats_map = self.download_stats.write().await;
                        if let Some(stats) = stats_map.get_mut(model_id) {
                            stats.set_cancelled();
                        }
                        
                        Ok(())
                    },
                    _ => {
                        warn!("Cannot cancel download of model '{}' as it is not in progress", model_id);
                        Err(LLMProviderError::ProviderError(
                            "Model is not currently downloading".to_string()
                        ))
                    },
                }
            }
            None => {
                warn!("Cannot cancel download of model '{}' as it is not found", model_id);
                Err(LLMProviderError::ModelNotFound(model_id.to_string()))
            }
        }
    }

    async fn delete_model(&self, model_id: &str) -> LLMProviderResult<()> {
        self.ensure_initialized()?;
        info!("Deleting model '{}'", model_id);

        // LocalAI doesn't have a convenient API to delete models
        // We'll try the undocumented endpoint
        let url = format!("{}/api/models/delete", self.base_url);
        let request = serde_json::json!({
            "name": model_id,
        });

        let response = match self.create_request(reqwest::Method::POST, &url).json(&request).send().await {
            Ok(response) => response,
            Err(e) => {
                error!("Failed to delete model '{}': {}", model_id, e);
                return Err(LLMProviderError::NetworkError(format!(
                    "Failed to delete model: {}", e
                )));
            }
        };

        match response.status() {
            StatusCode::OK => {
                info!("Successfully deleted model '{}'", model_id);
                
                // Remove status and config for the deleted model
                {
                    let mut status_map = self.download_status.write().await;
                    status_map.remove(model_id);
                }
                {
                    let mut stats_map = self.download_stats.write().await;
                    stats_map.remove(model_id);
                }
                {
                    let mut config_map = self.model_configs.write().await;
                    config_map.remove(model_id);
                }
                
                Ok(())
            },
            StatusCode::NOT_FOUND => {
                warn!("Model '{}' not found for deletion", model_id);
                Err(LLMProviderError::ModelNotFound(model_id.to_string()))
            },
            _ => {
                error!("Failed to delete model '{}': HTTP {}", model_id, response.status());
                
                // Try to parse error message
                let error_text = match response.text().await {
                    Ok(text) => text,
                    Err(_) => format!("HTTP {}", response.status()),
                };
                
                Err(LLMProviderError::ProviderError(format!(
                    "Failed to delete model: {}", error_text
                )))
            }
        }
    }

    async fn is_model_loaded(&self, model_id: &str) -> LLMProviderResult<bool> {
        self.ensure_initialized()?;
        debug!("Checking if model '{}' is loaded", model_id);

        // LocalAI doesn't explicitly distinguish between available and loaded models
        // If a model is listed, we'll assume it's loaded or can be loaded immediately
        let models = self.list_available_models().await?;
        Ok(models.iter().any(|m| m.id == model_id))
    }

    async fn load_model(&self, model_id: &str) -> LLMProviderResult<()> {
        self.ensure_initialized()?;
        info!("Loading model '{}'", model_id);

        // LocalAI automatically loads models when used
        // Check if the model exists
        if self.is_model_loaded(model_id).await? {
            Ok(())
        } else {
            Err(LLMProviderError::ModelNotFound(model_id.to_string()))
        }
    }

    async fn unload_model(&self, model_id: &str) -> LLMProviderResult<()> {
        self.ensure_initialized()?;
        debug!("Unloading model '{}' (no-op in LocalAI)", model_id);
        
        // LocalAI doesn't have a direct API to unload models
        // Models are automatically unloaded when not used
        Ok(())
    }

    async fn generate_text(
        &self,
        model_id: &str,
        prompt: &str,
        options: GenerationOptions,
    ) -> LLMProviderResult<CompletionResponse> {
        self.ensure_initialized()?;
        debug!("Generating text with model '{}'", model_id);

        // Check if we should use chat completions or text completions
        // Try chat completions first, then fall back to text completions if needed
        match self.generate_chat_completion(model_id, prompt, options.clone()).await {
            Ok(completion) => Ok(completion),
            Err(e) => {
                debug!("Chat completion failed: {}. Trying text completion...", e);
                self.generate_text_completion(model_id, prompt, options).await
            }
        }
    }
    
    async fn generate_text_streaming(
        &self,
        model_id: &str,
        prompt: &str,
        options: GenerationOptions,
    ) -> LLMProviderResult<Box<dyn CompletionStream>> {
        self.ensure_initialized()?;
        debug!("Generating streaming text with model '{}'", model_id);

        // Prepare the chat request with streaming enabled
        let mut messages = Vec::new();
        messages.push(OpenAIChatMessage {
            role: "user".to_string(),
            content: prompt.to_string(),
            name: None,
        });
        
        let request = OpenAIChatCompletionRequest {
            model: model_id.to_string(),
            messages,
            max_tokens: options.max_tokens,
            temperature: options.temperature,
            top_p: options.top_p,
            frequency_penalty: options.frequency_penalty,
            presence_penalty: options.presence_penalty,
            stop: options.stop_sequences,
            n: Some(1),
            stream: Some(true),
            seed: options.seed,
            user: None,
        };
        
        let url = format!("{}/v1/chat/completions", self.base_url);
        
        // Send the request
        let response = match self.create_request(reqwest::Method::POST, &url)
            .json(&request)
            .send()
            .await 
        {
            Ok(response) => {
                if !response.status().is_success() {
                    let error_text = match response.text().await {
                        Ok(text) => text,
                        Err(_) => format!("HTTP {}", response.status()),
                    };
                    
                    error!("Failed to generate streaming chat completion: {}", error_text);
                    return Err(LLMProviderError::GenerationFailed(error_text));
                }
                
                response
            },
            Err(e) => {
                error!("Network error while generating streaming chat completion: {}", e);
                return Err(LLMProviderError::NetworkError(format!(
                    "Failed to generate text: {}", e
                )));
            }
        };
        
        // Create streaming handler
        Ok(Box::new(LocalAIChatCompletionStream {
            response,
            accumulated_text: String::new(),
            completed: false,
        }))
    }

    async fn get_model_config(&self, model_id: &str) -> LLMProviderResult<ModelConfig> {
        self.ensure_initialized()?;
        debug!("Getting configuration for model '{}'", model_id);

        // Check if we have a cached config
        let config_map = self.model_configs.read().await;
        if let Some(config) = config_map.get(model_id) {
            return Ok(config.clone());
        }

        // LocalAI doesn't have a direct API to get model config
        // We'll return a default config
        Ok(ModelConfig {
            id: model_id.to_string(),
            parameters: HashMap::new(),
        })
    }

    async fn update_model_config(&self, config: ModelConfig) -> LLMProviderResult<()> {
        self.ensure_initialized()?;
        debug!("Updating configuration for model '{}'", config.id);

        // Store the config in our cache
        let mut config_map = self.model_configs.write().await;
        config_map.insert(config.id.clone(), config);

        // LocalAI doesn't have a direct API to update model config
        // We're just storing it locally for now
        Ok(())
    }

    async fn get_model_info(&self, model_id: &str) -> LLMProviderResult<ModelInfo> {
        self.ensure_initialized()?;
        debug!("Getting information for model '{}'", model_id);

        // Get all models and find the one we want
        let models = self.list_available_models().await?;
        for model in models {
            if model.id == model_id {
                return Ok(model);
            }
        }

        error!("Model '{}' not found", model_id);
        Err(LLMProviderError::ModelNotFound(model_id.to_string()))
    }
}

impl LocalAIProvider {
    /// Generate text using the chat completions endpoint
    async fn generate_chat_completion(
        &self,
        model_id: &str,
        prompt: &str,
        options: GenerationOptions,
    ) -> LLMProviderResult<CompletionResponse> {
        debug!("Generating chat completion with model '{}'", model_id);
        
        // Prepare the chat request
        let mut messages = Vec::new();
        messages.push(OpenAIChatMessage {
            role: "user".to_string(),
            content: prompt.to_string(),
            name: None,
        });
        
        let request = OpenAIChatCompletionRequest {
            model: model_id.to_string(),
            messages,
            max_tokens: options.max_tokens,
            temperature: options.temperature,
            top_p: options.top_p,
            frequency_penalty: options.frequency_penalty,
            presence_penalty: options.presence_penalty,
            stop: options.stop_sequences,
            n: Some(1),
            stream: None,
            seed: options.seed,
            user: None,
        };
        
        let url = format!("{}/v1/chat/completions", self.base_url);
        
        // Send the request
        let response = match self.create_request(reqwest::Method::POST, &url)
            .json(&request)
            .send()
            .await 
        {
            Ok(response) => {
                if !response.status().is_success() {
                    let error_text = match response.text().await {
                        Ok(text) => text,
                        Err(_) => format!("HTTP {}", response.status()),
                    };
                    
                    return Err(LLMProviderError::GenerationFailed(error_text));
                }
                
                response
            },
            Err(e) => {
                return Err(LLMProviderError::NetworkError(format!(
                    "Failed to generate text: {}", e
                )));
            }
        };
        
        // Parse the response
        let chat_response: OpenAIChatCompletionResponse = match response.json().await {
            Ok(data) => data,
            Err(e) => {
                return Err(LLMProviderError::ProviderError(format!(
                    "Failed to parse generation response: {}", e
                )));
            }
        };
        
        // Extract the completion text
        let choice = match chat_response.choices.first() {
            Some(choice) => choice,
            None => {
                return Err(LLMProviderError::ProviderError(
                    "No completion choices returned".to_string()
                ));
            }
        };
        
        // Create metadata
        let mut metadata = HashMap::new();
        metadata.insert("id".to_string(), serde_json::to_value(chat_response.id).unwrap_or_default());
        metadata.insert("object".to_string(), serde_json::to_value(chat_response.object).unwrap_or_default());
        metadata.insert("created".to_string(), serde_json::to_value(chat_response.created).unwrap_or_default());
        
        if let Some(finish_reason) = &choice.finish_reason {
            metadata.insert("finish_reason".to_string(), serde_json::to_value(finish_reason).unwrap_or_default());
        }
        
        // Create usage info
        let usage = chat_response.usage.unwrap_or(OpenAIUsage {
            prompt_tokens: 0,
            completion_tokens: 0,
            total_tokens: 0,
        });
        
        // Create the completion response
        Ok(CompletionResponse {
            text: choice.message.content.clone(),
            reached_max_tokens: choice.finish_reason.as_deref() == Some("length"),
            usage: TokenUsage {
                prompt_tokens: usage.prompt_tokens,
                completion_tokens: usage.completion_tokens,
                total_tokens: usage.total_tokens,
            },
            metadata,
        })
    }
    
    /// Generate text using the completions endpoint
    async fn generate_text_completion(
        &self,
        model_id: &str,
        prompt: &str,
        options: GenerationOptions,
    ) -> LLMProviderResult<CompletionResponse> {
        debug!("Generating text completion with model '{}'", model_id);
        
        // Prepare the completion request
        let request = OpenAICompletionRequest {
            model: model_id.to_string(),
            prompt: prompt.to_string(),
            max_tokens: options.max_tokens,
            temperature: options.temperature,
            top_p: options.top_p,
            frequency_penalty: options.frequency_penalty,
            presence_penalty: options.presence_penalty,
            stop: options.stop_sequences,
            n: Some(1),
            logprobs: None,
            best_of: None,
            echo: None,
            seed: options.seed,
            stream: None,
            user: None,
        };
        
        let url = format!("{}/v1/completions", self.base_url);
        
        // Send the request
        let response = match self.create_request(reqwest::Method::POST, &url)
            .json(&request)
            .send()
            .await 
        {
            Ok(response) => {
                if !response.status().is_success() {
                    let error_text = match response.text().await {
                        Ok(text) => text,
                        Err(_) => format!("HTTP {}", response.status()),
                    };
                    
                    return Err(LLMProviderError::GenerationFailed(error_text));
                }
                
                response
            },
            Err(e) => {
                return Err(LLMProviderError::NetworkError(format!(
                    "Failed to generate text: {}", e
                )));
            }
        };
        
        // Parse the response
        let completion_response: OpenAICompletionResponse = match response.json().await {
            Ok(data) => data,
            Err(e) => {
                return Err(LLMProviderError::ProviderError(format!(
                    "Failed to parse generation response: {}", e
                )));
            }
        };
        
        // Extract the completion text
        let choice = match completion_response.choices.first() {
            Some(choice) => choice,
            None => {
                return Err(LLMProviderError::ProviderError(
                    "No completion choices returned".to_string()
                ));
            }
        };
        
        // Create metadata
        let mut metadata = HashMap::new();
        metadata.insert("id".to_string(), serde_json::to_value(completion_response.id).unwrap_or_default());
        metadata.insert("object".to_string(), serde_json::to_value(completion_response.object).unwrap_or_default());
        metadata.insert("created".to_string(), serde_json::to_value(completion_response.created).unwrap_or_default());
        
        if let Some(finish_reason) = &choice.finish_reason {
            metadata.insert("finish_reason".to_string(), serde_json::to_value(finish_reason).unwrap_or_default());
        }
        
        if let Some(logprobs) = &choice.logprobs {
            metadata.insert("logprobs".to_string(), logprobs.clone());
        }
        
        // Create usage info
        let usage = completion_response.usage.unwrap_or(OpenAIUsage {
            prompt_tokens: 0,
            completion_tokens: 0,
            total_tokens: 0,
        });
        
        // Create the completion response
        Ok(CompletionResponse {
            text: choice.text.clone(),
            reached_max_tokens: choice.finish_reason.as_deref() == Some("length"),
            usage: TokenUsage {
                prompt_tokens: usage.prompt_tokens,
                completion_tokens: usage.completion_tokens,
                total_tokens: usage.total_tokens,
            },
            metadata,
        })
    }
}
</file>

<file path="src-tauri/src/offline/llm/providers/mod.rs">
//! Providers for local LLM integration
//!
//! This module contains concrete implementations of the `LocalLLMProvider` trait
//! for different local LLM providers such as Ollama, LocalAI, etc.

pub mod ollama;
pub mod localai;

// Re-export providers for easier access
pub use ollama::OllamaProvider;
pub use localai::LocalAIProvider;
</file>

<file path="src-tauri/src/offline/llm/providers/ollama.rs">
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use async_trait::async_trait;
use chrono::Utc;
use futures_util::StreamExt;
use log::{debug, error, info, warn};
use reqwest::{Client, StatusCode};
use serde::{Deserialize, Serialize};
use tokio::sync::RwLock;

use crate::offline::llm::provider::{
    CompletionResponse, CompletionStream, LocalLLMProvider,
    LLMProviderError, LLMProviderResult, ModelConfig, ModelInfo, TokenUsage,
};
use crate::offline::llm::types::{DownloadStatus, GenerationOptions, ProviderType};

/// Ollama API response for listing models
#[derive(Debug, Deserialize)]
struct OllamaListModelsResponse {
    models: Vec<OllamaModelInfo>,
}

/// Ollama API model information
#[derive(Debug, Deserialize)]
struct OllamaModelInfo {
    name: String,
    modified_at: String,
    size: u64,
    digest: String,
    details: Option<OllamaModelDetails>,
}

/// Ollama API model details
#[derive(Debug, Deserialize)]
struct OllamaModelDetails {
    format: String,
    family: String,
    families: Option<Vec<String>>,
    parameter_size: Option<String>,
    quantization_level: Option<String>,
}

/// Ollama API generation request
#[derive(Debug, Serialize)]
struct OllamaGenerationRequest {
    model: String,
    prompt: String,
    stream: bool,
    #[serde(skip_serializing_if = "Option::is_none")]
    options: Option<HashMap<String, serde_json::Value>>,
}

/// Ollama API generation response
#[derive(Debug, Deserialize)]
struct OllamaGenerationResponse {
    model: String,
    response: String,
    done: bool,
    context: Option<Vec<u32>>,
    total_duration: Option<u64>,
    load_duration: Option<u64>,
    prompt_eval_duration: Option<u64>,
    eval_count: Option<u32>,
    eval_duration: Option<u64>,
}

/// Ollama API pull request
#[derive(Debug, Serialize)]
struct OllamaPullRequest {
    name: String,
    stream: bool,
    insecure: Option<bool>,
}

/// Ollama API pull response
#[derive(Debug, Deserialize)]
struct OllamaPullResponse {
    status: String,
    digest: Option<String>,
    total: Option<u64>,
    completed: Option<u64>,
    error: Option<String>,
}

/// Ollama API delete request
#[derive(Debug, Serialize)]
struct OllamaDeleteRequest {
    name: String,
}

/// Statistics for a running download
#[derive(Debug, Clone)]
struct DownloadStats {
    /// Start time of the download
    start_time: chrono::DateTime<Utc>,
    /// Last update time
    last_update: chrono::DateTime<Utc>,
    /// Total bytes to download
    total_bytes: Option<u64>,
    /// Bytes downloaded so far
    bytes_downloaded: Option<u64>,
    /// Download speed in bytes per second
    bytes_per_second: Option<f32>,
    /// Sample of recent download speeds for averaging
    speed_samples: Vec<f32>,
}

impl DownloadStats {
    fn new() -> Self {
        let now = Utc::now();
        Self {
            start_time: now,
            last_update: now,
            total_bytes: None,
            bytes_downloaded: None,
            bytes_per_second: None,
            speed_samples: Vec::with_capacity(10),
        }
    }
    
    fn update(&mut self, completed: Option<u64>, total: Option<u64>) {
        let now = Utc::now();
        let time_elapsed = (now - self.last_update).num_milliseconds() as f32 / 1000.0;
        
        // Update bytes
        if let Some(total) = total {
            self.total_bytes = Some(total);
        }
        
        // Calculate speed if we have new completed data
        if let Some(completed) = completed {
            if let Some(old_completed) = self.bytes_downloaded {
                if time_elapsed > 0.0 && completed > old_completed {
                    let bytes_delta = (completed - old_completed) as f32;
                    let speed = bytes_delta / time_elapsed;
                    
                    // Add to speed samples, keeping a fixed window
                    self.speed_samples.push(speed);
                    if self.speed_samples.len() > 10 {
                        self.speed_samples.remove(0);
                    }
                    
                    // Calculate average speed
                    let avg_speed = self.speed_samples.iter().sum::<f32>() / self.speed_samples.len() as f32;
                    self.bytes_per_second = Some(avg_speed);
                }
            }
            
            self.bytes_downloaded = Some(completed);
        }
        
        self.last_update = now;
    }
    
    fn eta_seconds(&self) -> Option<f32> {
        if let (Some(total), Some(completed), Some(speed)) = (self.total_bytes, self.bytes_downloaded, self.bytes_per_second) {
            if speed > 0.0 && completed < total {
                let remaining_bytes = (total - completed) as f32;
                return Some(remaining_bytes / speed);
            }
        }
        None
    }
    
    fn to_download_status(&self) -> DownloadStatus {
        let total = self.total_bytes;
        let completed = self.bytes_downloaded;
        
        if let (Some(total), Some(completed)) = (total, completed) {
            if total > 0 {
                let percent = (completed as f32 / total as f32) * 100.0;
                return DownloadStatus::InProgress {
                    percent,
                    bytes_downloaded: completed,
                    total_bytes: total,
                    eta_seconds: self.eta_seconds(),
                    bytes_per_second: self.bytes_per_second,
                };
            }
        }
        
        // Default to 0% if we don't have enough info
        DownloadStatus::InProgress {
            percent: 0.0,
            bytes_downloaded: completed,
            total_bytes: total,
            eta_seconds: None,
            bytes_per_second: None,
        }
    }
}

/// Ollama provider implementation
pub struct OllamaProvider {
    /// Base URL for the Ollama API
    base_url: String,
    /// HTTP client for API requests
    client: Client,
    /// Map of model IDs to download status
    download_status: Arc<RwLock<HashMap<String, DownloadStatus>>>,
    /// Map of model IDs to download statistics
    download_stats: Arc<RwLock<HashMap<String, DownloadStats>>>,
    /// Map of model IDs to model configurations
    model_configs: Arc<RwLock<HashMap<String, ModelConfig>>>,
    /// Whether the provider has been initialized
    initialized: Arc<Mutex<bool>>,
}

impl OllamaProvider {
    /// Create a new Ollama provider with the default URL
    pub fn new() -> Self {
        Self::with_base_url("http://localhost:11434")
    }

    /// Create a new Ollama provider with a custom URL
    pub fn with_base_url(base_url: &str) -> Self {
        Self {
            base_url: base_url.to_string(),
            client: Client::builder()
                .timeout(std::time::Duration::from_secs(60))
                .build()
                .unwrap_or_else(|_| Client::new()),
            download_status: Arc::new(RwLock::new(HashMap::new())),
            download_stats: Arc::new(RwLock::new(HashMap::new())),
            model_configs: Arc::new(RwLock::new(HashMap::new())),
            initialized: Arc::new(Mutex::new(false)),
        }
    }

    /// Check if the provider is initialized
    fn ensure_initialized(&self) -> LLMProviderResult<()> {
        let initialized = *self.initialized.lock().map_err(|e| {
            LLMProviderError::Unexpected(format!("Failed to acquire lock: {}", e))
        })?;

        if !initialized {
            return Err(LLMProviderError::NotInitialized(
                "Ollama provider not initialized. Call initialize() first.".to_string(),
            ));
        }

        Ok(())
    }

    /// Convert Ollama model info to the common ModelInfo format
    fn convert_to_model_info(&self, ollama_info: OllamaModelInfo, is_downloaded: bool) -> ModelInfo {
        let mut provider_metadata = HashMap::new();
        let mut parameter_count = None;
        let mut quant_level = None;
        let mut model_family = None;
        
        if let Some(details) = ollama_info.details {
            provider_metadata.insert("format".to_string(), serde_json::to_value(details.format).unwrap_or_default());
            
            // Extract the model family
            model_family = Some(details.family.clone());
            provider_metadata.insert("family".to_string(), serde_json::to_value(details.family).unwrap_or_default());
            
            if let Some(families) = details.families {
                provider_metadata.insert("families".to_string(), serde_json::to_value(families).unwrap_or_default());
            }
            
            // Extract parameter count
            if let Some(param_size) = details.parameter_size {
                provider_metadata.insert("parameter_size".to_string(), serde_json::to_value(&param_size).unwrap_or_default());
                
                // Parse the parameter size (usually in format like "7B")
                if let Some(size_str) = param_size.trim_end_matches('B').parse::<f64>().ok() {
                    parameter_count = Some(size_str);
                }
            }
            
            // Extract quantization level
            if let Some(q_level) = details.quantization_level {
                provider_metadata.insert("quantization_level".to_string(), serde_json::to_value(&q_level).unwrap_or_default());
                quant_level = Some(q_level);
            }
        }
        
        provider_metadata.insert("digest".to_string(), serde_json::to_value(ollama_info.digest).unwrap_or_default());
        provider_metadata.insert("modified_at".to_string(), serde_json::to_value(ollama_info.modified_at).unwrap_or_default());

        // Create tags from model name components
        let tags = ollama_info.name.split(':')
            .skip(1)  // Skip the base name
            .filter(|&s| !s.is_empty())
            .map(|s| s.to_string())
            .collect::<Vec<_>>();

        // Extract base name without tags
        let base_name = ollama_info.name.split(':').next().unwrap_or(&ollama_info.name).to_string();
        
        // Determine what capabilities the model has
        // For Ollama, we'll assume all models support text generation and chat
        let supports_text_generation = true;
        let supports_completion = true;
        let supports_chat = true;
        let supports_embeddings = false;  // Ollama doesn't support embeddings yet
        let supports_image_generation = false;  // Ollama doesn't support image generation yet

        ModelInfo {
            id: ollama_info.name.clone(),
            name: format!("{}{}", 
                base_name,
                if !quant_level.is_none() { 
                    format!(" ({})", quant_level.unwrap_or_default()) 
                } else { 
                    "".to_string() 
                }
            ),
            description: format!("{}{} model", 
                if let Some(params) = parameter_count {
                    format!("{:.1}B parameter ", params)
                } else {
                    "".to_string()
                },
                model_family.unwrap_or_else(|| "LLM".to_string())
            ),
            size_bytes: ollama_info.size,
            is_downloaded,
            provider_metadata,
            provider: ProviderType::Ollama,
            supports_text_generation,
            supports_completion,
            supports_chat,
            supports_embeddings,
            supports_image_generation,
            quantization: quant_level,
            parameter_count_b: parameter_count,
            context_length: None,  // Ollama doesn't provide this info
            model_family,
            created_at: None,  // Ollama doesn't provide this info
            tags,
            license: None,  // Ollama doesn't provide this info
        }
    }
    
    /// Set the download status for a model
    async fn set_download_status(&self, model_id: &str, status: DownloadStatus) {
        let mut status_map = self.download_status.write().await;
        status_map.insert(model_id.to_string(), status);
    }

    /// Cancel any running download for a model
    async fn _cleanup_download_tasks(&self, model_id: &str) {
        // Remove from status maps
        {
            let mut status_map = self.download_status.write().await;
            status_map.remove(model_id);
        }
        {
            let mut stats_map = self.download_stats.write().await;
            stats_map.remove(model_id);
        }
    }
}

/// Ollama implementation of CompletionStream
pub struct OllamaCompletionStream {
    /// HTTP response stream
    response: reqwest::Response,
    /// Total completion text accumulated so far
    accumulated_text: String,
    /// Whether we've reached the end of the stream
    completed: bool,
}

#[async_trait]
impl CompletionStream for OllamaCompletionStream {
    async fn next_chunk(&mut self) -> Option<LLMProviderResult<String>> {
        if self.completed {
            return None;
        }

        match self.response.chunk().await {
            Ok(Some(chunk)) => {
                let chunk_str = match std::str::from_utf8(&chunk) {
                    Ok(s) => s,
                    Err(e) => return Some(Err(LLMProviderError::Unexpected(format!(
                        "Failed to decode response chunk: {}", e
                    )))),
                };

                // Parse the JSON response
                match serde_json::from_str::<OllamaGenerationResponse>(chunk_str) {
                    Ok(response) => {
                        self.accumulated_text.push_str(&response.response);
                        
                        if response.done {
                            self.completed = true;
                        }
                        
                        Some(Ok(response.response))
                    },
                    Err(e) => Some(Err(LLMProviderError::Unexpected(format!(
                        "Failed to parse response JSON: {}", e
                    )))),
                }
            },
            Ok(None) => {
                self.completed = true;
                None
            },
            Err(e) => Some(Err(LLMProviderError::NetworkError(format!(
                "Failed to get response chunk: {}", e
            )))),
        }
    }
}

#[async_trait]
impl LocalLLMProvider for OllamaProvider {
    async fn initialize(&mut self, config: serde_json::Value) -> LLMProviderResult<()> {
        // Extract configuration options
        let config_obj = match config.as_object() {
            Some(obj) => obj,
            None => {
                warn!("Ollama provider configuration is not a JSON object");
                return Err(LLMProviderError::ProviderError(
                    "Configuration must be a JSON object".to_string(),
                ));
            }
        };

        // Override base URL if provided
        if let Some(base_url) = config_obj.get("base_url").and_then(|v| v.as_str()) {
            debug!("Setting Ollama base URL to: {}", base_url);
            self.base_url = base_url.to_string();
        }

        // Test connection to Ollama API
        info!("Testing connection to Ollama API at {}", self.base_url);
        let health_url = format!("{}/api/version", self.base_url);
        
        match self.client.get(&health_url).send().await {
            Ok(response) => {
                if !response.status().is_success() {
                    error!("Failed to connect to Ollama API: HTTP {}", response.status());
                    return Err(LLMProviderError::ProviderError(format!(
                        "Failed to connect to Ollama API: HTTP {}", response.status()
                    )));
                }
                
                // Try to parse the version response
                match response.json::<serde_json::Value>().await {
                    Ok(version_data) => {
                        if let Some(version) = version_data.get("version").and_then(|v| v.as_str()) {
                            info!("Connected to Ollama (version: {})", version);
                        } else {
                            info!("Connected to Ollama (unknown version)");
                        }
                    },
                    Err(e) => {
                        warn!("Connected to Ollama but couldn't parse version info: {}", e);
                    }
                }
            },
            Err(e) => {
                error!("Failed to connect to Ollama API: {}", e);
                return Err(LLMProviderError::NetworkError(format!(
                    "Failed to connect to Ollama API: {}. Make sure Ollama is running at {}.", 
                    e, self.base_url
                )));
            }
        }

        // Mark as initialized
        match self.initialized.lock() {
            Ok(mut initialized) => {
                *initialized = true;
                info!("Ollama provider initialized successfully");
            },
            Err(e) => {
                error!("Failed to acquire lock when initializing Ollama provider: {}", e);
                return Err(LLMProviderError::Unexpected(format!(
                    "Failed to acquire lock: {}", e
                )));
            }
        }

        Ok(())
    }

    fn provider_name(&self) -> &str {
        "Ollama"
    }

    async fn list_available_models(&self) -> LLMProviderResult<Vec<ModelInfo>> {
        self.ensure_initialized()?;
        debug!("Listing available models from Ollama");

        let url = format!("{}/api/tags", self.base_url);
        let response = match self.client.get(&url).send().await {
            Ok(response) => response,
            Err(e) => {
                error!("Failed to fetch models from Ollama: {}", e);
                return Err(LLMProviderError::NetworkError(format!(
                    "Failed to fetch models: {}", e
                )));
            }
        };

        if !response.status().is_success() {
            error!("Failed to fetch models from Ollama: HTTP {}", response.status());
            return Err(LLMProviderError::ProviderError(format!(
                "Failed to fetch models: HTTP {}", response.status()
            )));
        }

        let models_response: OllamaListModelsResponse = match response.json().await {
            Ok(data) => data,
            Err(e) => {
                error!("Failed to parse models response from Ollama: {}", e);
                return Err(LLMProviderError::ProviderError(format!(
                    "Failed to parse models response: {}", e
                )));
            }
        };

        let models = models_response.models.into_iter()
            .map(|m| self.convert_to_model_info(m, true))
            .collect::<Vec<_>>();

        info!("Found {} models from Ollama", models.len());
        Ok(models)
    }

    async fn list_downloaded_models(&self) -> LLMProviderResult<Vec<ModelInfo>> {
        // For Ollama, all listed models are downloaded models
        debug!("Listing downloaded models from Ollama");
        self.list_available_models().await
    }

    async fn download_model(&self, model_id: &str) -> LLMProviderResult<()> {
        self.ensure_initialized()?;
        info!("Starting download of model '{}' from Ollama", model_id);

        // Update download status
        self.set_download_status(model_id, DownloadStatus::InProgress { 
            percent: 0.0,
            bytes_downloaded: None,
            total_bytes: None,
            eta_seconds: None,
            bytes_per_second: None,
        }).await;
        
        // Initialize download stats
        {
            let mut stats_map = self.download_stats.write().await;
            stats_map.insert(model_id.to_string(), DownloadStats::new());
        }

        // Construct the pull request
        let url = format!("{}/api/pull", self.base_url);
        let request = OllamaPullRequest {
            name: model_id.to_string(),
            stream: true,
            insecure: None,
        };

        // Start download in a separate task
        let model_id = model_id.to_string();
        let client = self.client.clone();
        let download_status = self.download_status.clone();
        let download_stats = self.download_stats.clone();

        tokio::spawn(async move {
            debug!("Sending pull request for model '{}'", model_id);
            let result = client.post(&url).json(&request).send().await;
            
            match result {
                Ok(response) => {
                    if !response.status().is_success() {
                        error!("Failed to download model '{}': HTTP {}", model_id, response.status());
                        let mut status_map = download_status.write().await;
                        status_map.insert(model_id, DownloadStatus::Failed { 
                            reason: format!("HTTP error: {}", response.status()),
                            error_code: Some(response.status().to_string()),
                            failed_at: Some(Utc::now().to_rfc3339()),
                        });
                        return;
                    }

                    // Stream the response to track progress
                    debug!("Streaming download response for model '{}'", model_id);
                    let mut stream = response.bytes_stream();
                    while let Some(chunk_result) = stream.next().await {
                        match chunk_result {
                            Ok(chunk) => {
                                if let Ok(text) = std::str::from_utf8(&chunk) {
                                    match serde_json::from_str::<OllamaPullResponse>(text) {
                                        Ok(pull_response) => {
                                            match pull_response.status.as_str() {
                                                "pulling manifest" => {
                                                    debug!("Pulling manifest for model '{}'", model_id);
                                                },
                                                "pulling layers" => {
                                                    // Extract progress information if available
                                                    let total = pull_response.total;
                                                    let completed = pull_response.completed;
                                                    
                                                    // Update stats
                                                    {
                                                        let mut stats_map = download_stats.write().await;
                                                        if let Some(stats) = stats_map.get_mut(&model_id) {
                                                            stats.update(completed, total);
                                                            
                                                            // Update status from stats
                                                            let status = stats.to_download_status();
                                                            let mut status_map = download_status.write().await;
                                                            status_map.insert(model_id.clone(), status);
                                                        }
                                                    }
                                                },
                                                "success" => {
                                                    info!("Successfully downloaded model '{}'", model_id);
                                                    let mut status_map = download_status.write().await;
                                                    status_map.insert(model_id.clone(), DownloadStatus::Completed { 
                                                        completed_at: Some(Utc::now().to_rfc3339()),
                                                        duration_seconds: {
                                                            let stats_map = download_stats.read().await;
                                                            stats_map.get(&model_id).map(|stats| {
                                                                let duration = Utc::now() - stats.start_time;
                                                                Some(duration.num_milliseconds() as f32 / 1000.0)
                                                            }).flatten()
                                                        }
                                                    });
                                                    break;
                                                },
                                                "error" => {
                                                    let error_msg = pull_response.error.unwrap_or_else(|| "Unknown error".to_string());
                                                    error!("Error downloading model '{}': {}", model_id, error_msg);
                                                    let mut status_map = download_status.write().await;
                                                    status_map.insert(model_id.clone(), DownloadStatus::Failed { 
                                                        reason: error_msg,
                                                        error_code: None,
                                                        failed_at: Some(Utc::now().to_rfc3339()),
                                                    });
                                                    break;
                                                },
                                                _ => {
                                                    debug!("Unhandled pull status for model '{}': {}", model_id, pull_response.status);
                                                }
                                            }
                                        },
                                        Err(e) => {
                                            warn!("Failed to parse Ollama pull response for model '{}': {}", model_id, e);
                                            warn!("Response text: {}", text);
                                        }
                                    }
                                }
                            },
                            Err(e) => {
                                error!("Failed to read response chunk for model '{}': {}", model_id, e);
                                let mut status_map = download_status.write().await;
                                status_map.insert(model_id.clone(), DownloadStatus::Failed { 
                                    reason: format!("Failed to read response: {}", e),
                                    error_code: None,
                                    failed_at: Some(Utc::now().to_rfc3339()),
                                });
                                break;
                            }
                        }
                    }
                },
                Err(e) => {
                    error!("Network error while downloading model '{}': {}", model_id, e);
                    let mut status_map = download_status.write().await;
                    status_map.insert(model_id, DownloadStatus::Failed { 
                        reason: format!("Network error: {}", e),
                        error_code: None,
                        failed_at: Some(Utc::now().to_rfc3339()),
                    });
                }
            }
        });

        Ok(())
    }

    async fn get_download_status(&self, model_id: &str) -> LLMProviderResult<DownloadStatus> {
        self.ensure_initialized()?;
        debug!("Getting download status for model '{}'", model_id);

        let status_map = self.download_status.read().await;
        match status_map.get(model_id) {
            Some(status) => Ok(status.clone()),
            None => {
                // If no status is recorded, check if the model exists already
                debug!("No download status found for model '{}', checking if it exists", model_id);
                let models = self.list_downloaded_models().await?;
                if models.iter().any(|m| m.id == model_id) {
                    debug!("Model '{}' is already downloaded", model_id);
                    Ok(DownloadStatus::Completed {
                        completed_at: None,
                        duration_seconds: None,
                    })
                } else {
                    debug!("Model '{}' is not downloaded", model_id);
                    Ok(DownloadStatus::NotStarted)
                }
            }
        }
    }

    async fn cancel_download(&self, model_id: &str) -> LLMProviderResult<()> {
        self.ensure_initialized()?;
        debug!("Cancelling download of model '{}'", model_id);

        // Ollama doesn't have a direct API to cancel downloads
        // We can only update our status to indicate cancellation
        let mut status_map = self.download_status.write().await;
        match status_map.get(model_id) {
            Some(status) => {
                match status {
                    DownloadStatus::InProgress { .. } => {
                        info!("Marking download of model '{}' as cancelled", model_id);
                        status_map.insert(model_id.to_string(), DownloadStatus::Cancelled {
                            cancelled_at: Some(Utc::now().to_rfc3339()),
                        });
                        Ok(())
                    },
                    _ => {
                        warn!("Cannot cancel download of model '{}' as it is not in progress", model_id);
                        Err(LLMProviderError::ProviderError(
                            "Model is not currently downloading".to_string()
                        ))
                    },
                }
            }
            None => {
                warn!("Cannot cancel download of model '{}' as it is not found", model_id);
                Err(LLMProviderError::ModelNotFound(model_id.to_string()))
            }
        }
    }

    async fn delete_model(&self, model_id: &str) -> LLMProviderResult<()> {
        self.ensure_initialized()?;
        info!("Deleting model '{}'", model_id);

        let url = format!("{}/api/delete", self.base_url);
        let request = OllamaDeleteRequest {
            name: model_id.to_string(),
        };

        let response = match self.client.delete(&url).json(&request).send().await {
            Ok(response) => response,
            Err(e) => {
                error!("Failed to delete model '{}': {}", model_id, e);
                return Err(LLMProviderError::NetworkError(format!(
                    "Failed to delete model: {}", e
                )));
            }
        };

        match response.status() {
            StatusCode::OK => {
                info!("Successfully deleted model '{}'", model_id);
                
                // Remove status and config for the deleted model
                {
                    let mut status_map = self.download_status.write().await;
                    status_map.remove(model_id);
                }
                {
                    let mut stats_map = self.download_stats.write().await;
                    stats_map.remove(model_id);
                }
                {
                    let mut config_map = self.model_configs.write().await;
                    config_map.remove(model_id);
                }
                
                Ok(())
            },
            StatusCode::NOT_FOUND => {
                warn!("Model '{}' not found for deletion", model_id);
                Err(LLMProviderError::ModelNotFound(model_id.to_string()))
            },
            _ => {
                error!("Failed to delete model '{}': HTTP {}", model_id, response.status());
                Err(LLMProviderError::ProviderError(format!(
                    "Failed to delete model: HTTP {}", response.status()
                )))
            }
        }
    }

    async fn is_model_loaded(&self, model_id: &str) -> LLMProviderResult<bool> {
        self.ensure_initialized()?;
        debug!("Checking if model '{}' is loaded", model_id);

        // Ollama doesn't have a direct API to check if a model is loaded
        // We can send a simple generation request to see if it responds quickly
        let url = format!("{}/api/generate", self.base_url);
        let request = OllamaGenerationRequest {
            model: model_id.to_string(),
            prompt: "".to_string(),
            stream: false,
            options: Some(HashMap::from([
                ("num_predict".to_string(), serde_json::json!(1)),
            ])),
        };

        match self.client.post(&url).json(&request).send().await {
            Ok(response) => {
                match response.status() {
                    StatusCode::OK => {
                        debug!("Model '{}' is loaded", model_id);
                        Ok(true)
                    },
                    StatusCode::NOT_FOUND => {
                        debug!("Model '{}' is not loaded", model_id);
                        Ok(false)
                    },
                    _ => {
                        warn!("Failed to check if model '{}' is loaded: HTTP {}", model_id, response.status());
                        Err(LLMProviderError::ProviderError(format!(
                            "Failed to check model status: HTTP {}", response.status()
                        )))
                    }
                }
            },
            Err(e) => {
                error!("Failed to check if model '{}' is loaded: {}", model_id, e);
                Err(LLMProviderError::NetworkError(format!(
                    "Failed to check model status: {}", e
                )))
            },
        }
    }

    async fn load_model(&self, model_id: &str) -> LLMProviderResult<()> {
        self.ensure_initialized()?;
        info!("Loading model '{}'", model_id);

        // Ollama automatically loads models when used, but we can "warm up" the model
        // by sending a trivial request
        let url = format!("{}/api/generate", self.base_url);
        let request = OllamaGenerationRequest {
            model: model_id.to_string(),
            prompt: "".to_string(),
            stream: false,
            options: Some(HashMap::from([
                ("num_predict".to_string(), serde_json::json!(1)),
            ])),
        };

        let response = match self.client.post(&url).json(&request).send().await {
            Ok(response) => response,
            Err(e) => {
                error!("Failed to load model '{}': {}", model_id, e);
                return Err(LLMProviderError::NetworkError(format!(
                    "Failed to load model: {}", e
                )));
            }
        };

        if !response.status().is_success() {
            error!("Failed to load model '{}': HTTP {}", model_id, response.status());
            if response.status() == StatusCode::NOT_FOUND {
                return Err(LLMProviderError::ModelNotFound(model_id.to_string()));
            } else {
                return Err(LLMProviderError::ProviderError(format!(
                    "Failed to load model: HTTP {}", response.status()
                )));
            }
        }

        info!("Successfully loaded model '{}'", model_id);
        Ok(())
    }

    async fn unload_model(&self, model_id: &str) -> LLMProviderResult<()> {
        self.ensure_initialized()?;
        debug!("Unloading model '{}' (no-op in Ollama)", model_id);
        
        // Ollama doesn't have a direct API to unload models
        // Models are automatically unloaded when not used for a while
        Ok(())
    }

    async fn generate_text(
        &self,
        model_id: &str,
        prompt: &str,
        options: GenerationOptions,
    ) -> LLMProviderResult<CompletionResponse> {
        self.ensure_initialized()?;
        debug!("Generating text with model '{}'", model_id);

        // Prepare the generation request
        let mut ollama_options = HashMap::new();
        
        if let Some(max_tokens) = options.max_tokens {
            ollama_options.insert("num_predict".to_string(), serde_json::json!(max_tokens));
        }
        
        if let Some(temperature) = options.temperature {
            ollama_options.insert("temperature".to_string(), serde_json::json!(temperature));
        }
        
        if let Some(top_p) = options.top_p {
            ollama_options.insert("top_p".to_string(), serde_json::json!(top_p));
        }
        
        if let Some(top_k) = options.top_k {
            ollama_options.insert("top_k".to_string(), serde_json::json!(top_k));
        }
        
        if let Some(fp) = options.frequency_penalty {
            ollama_options.insert("frequency_penalty".to_string(), serde_json::json!(fp));
        }
        
        if let Some(pp) = options.presence_penalty {
            ollama_options.insert("presence_penalty".to_string(), serde_json::json!(pp));
        }
        
        if let Some(seed) = options.seed {
            ollama_options.insert("seed".to_string(), serde_json::json!(seed));
        }
        
        if let Some(stop_sequences) = &options.stop_sequences {
            if !stop_sequences.is_empty() {
                ollama_options.insert("stop".to_string(), serde_json::json!(stop_sequences));
            }
        }
        
        // Add any additional parameters
        for (key, value) in options.additional_params {
            ollama_options.insert(key, value);
        }

        let url = format!("{}/api/generate", self.base_url);
        let request = OllamaGenerationRequest {
            model: model_id.to_string(),
            prompt: prompt.to_string(),
            stream: false,
            options: if ollama_options.is_empty() { None } else { Some(ollama_options) },
        };

        // Send the request
        debug!("Sending generation request to Ollama for model '{}'", model_id);
        let response = match self.client.post(&url).json(&request).send().await {
            Ok(response) => response,
            Err(e) => {
                error!("Failed to generate text with model '{}': {}", model_id, e);
                return Err(LLMProviderError::NetworkError(format!(
                    "Failed to generate text: {}", e
                )));
            }
        };

        if !response.status().is_success() {
            error!("Failed to generate text with model '{}': HTTP {}", model_id, response.status());
            match response.status() {
                StatusCode::NOT_FOUND => {
                    return Err(LLMProviderError::ModelNotFound(model_id.to_string()));
                },
                StatusCode::BAD_REQUEST => {
                    // Try to extract error message from response
                    let error_text = match response.text().await {
                        Ok(text) => {
                            match serde_json::from_str::<serde_json::Value>(&text) {
                                Ok(json) => {
                                    json.get("error").and_then(|e| e.as_str())
                                        .unwrap_or(&text).to_string()
                                },
                                Err(_) => text,
                            }
                        },
                        Err(_) => "Bad request".to_string(),
                    };
                    return Err(LLMProviderError::GenerationFailed(error_text));
                },
                _ => {
                    return Err(LLMProviderError::ProviderError(format!(
                        "Failed to generate text: HTTP {}", response.status()
                    )));
                }
            }
        }

        // Parse the response
        let ollama_response: OllamaGenerationResponse = match response.json().await {
            Ok(data) => data,
            Err(e) => {
                error!("Failed to parse generation response for model '{}': {}", model_id, e);
                return Err(LLMProviderError::ProviderError(format!(
                    "Failed to parse generation response: {}", e
                )));
            }
        };

        // Create the completion response
        debug!("Successfully generated text with model '{}'", model_id);
        let mut metadata = HashMap::new();
        
        if let Some(context) = &ollama_response.context {
            metadata.insert("context".to_string(), serde_json::to_value(context).unwrap_or_default());
        }
        
        if let Some(total_duration) = ollama_response.total_duration {
            metadata.insert("total_duration_ms".to_string(), serde_json::json!(total_duration));
        }
        
        if let Some(load_duration) = ollama_response.load_duration {
            metadata.insert("load_duration_ms".to_string(), serde_json::json!(load_duration));
        }
        
        if let Some(prompt_eval_duration) = ollama_response.prompt_eval_duration {
            metadata.insert("prompt_eval_duration_ms".to_string(), serde_json::json!(prompt_eval_duration));
        }
        
        if let Some(eval_count) = ollama_response.eval_count {
            metadata.insert("eval_count".to_string(), serde_json::json!(eval_count));
        }
        
        if let Some(eval_duration) = ollama_response.eval_duration {
            metadata.insert("eval_duration_ms".to_string(), serde_json::json!(eval_duration));
        }

        // Ollama doesn't provide token counts, so we have to estimate
        let prompt_tokens = (prompt.len() / 4) as u32; // Very rough estimate
        let completion_tokens = (ollama_response.response.len() / 4) as u32; // Very rough estimate
        
        let usage = TokenUsage {
            prompt_tokens,
            completion_tokens,
            total_tokens: prompt_tokens + completion_tokens,
        };

        Ok(CompletionResponse {
            text: ollama_response.response,
            reached_max_tokens: false, // Ollama doesn't indicate this
            usage,
            metadata,
        })
    }

    async fn generate_text_streaming(
        &self,
        model_id: &str,
        prompt: &str,
        options: GenerationOptions,
    ) -> LLMProviderResult<Box<dyn CompletionStream>> {
        self.ensure_initialized()?;
        debug!("Generating text (streaming) with model '{}'", model_id);

        // Prepare the generation request with streaming enabled
        let mut ollama_options = HashMap::new();
        
        if let Some(max_tokens) = options.max_tokens {
            ollama_options.insert("num_predict".to_string(), serde_json::json!(max_tokens));
        }
        
        if let Some(temperature) = options.temperature {
            ollama_options.insert("temperature".to_string(), serde_json::json!(temperature));
        }
        
        if let Some(top_p) = options.top_p {
            ollama_options.insert("top_p".to_string(), serde_json::json!(top_p));
        }
        
        if let Some(top_k) = options.top_k {
            ollama_options.insert("top_k".to_string(), serde_json::json!(top_k));
        }
        
        if let Some(fp) = options.frequency_penalty {
            ollama_options.insert("frequency_penalty".to_string(), serde_json::json!(fp));
        }
        
        if let Some(pp) = options.presence_penalty {
            ollama_options.insert("presence_penalty".to_string(), serde_json::json!(pp));
        }
        
        if let Some(seed) = options.seed {
            ollama_options.insert("seed".to_string(), serde_json::json!(seed));
        }
        
        if let Some(stop_sequences) = &options.stop_sequences {
            if !stop_sequences.is_empty() {
                ollama_options.insert("stop".to_string(), serde_json::json!(stop_sequences));
            }
        }
        
        // Add any additional parameters
        for (key, value) in options.additional_params {
            ollama_options.insert(key, value);
        }

        let url = format!("{}/api/generate", self.base_url);
        let request = OllamaGenerationRequest {
            model: model_id.to_string(),
            prompt: prompt.to_string(),
            stream: true,
            options: if ollama_options.is_empty() { None } else { Some(ollama_options) },
        };

        // Send the request
        debug!("Sending streaming generation request to Ollama for model '{}'", model_id);
        let response = match self.client.post(&url).json(&request).send().await {
            Ok(response) => response,
            Err(e) => {
                error!("Failed to generate streaming text with model '{}': {}", model_id, e);
                return Err(LLMProviderError::NetworkError(format!(
                    "Failed to generate text: {}", e
                )));
            }
        };

        if !response.status().is_success() {
            error!("Failed to generate streaming text with model '{}': HTTP {}", model_id, response.status());
            match response.status() {
                StatusCode::NOT_FOUND => {
                    return Err(LLMProviderError::ModelNotFound(model_id.to_string()));
                },
                StatusCode::BAD_REQUEST => {
                    // Try to extract error message from response
                    let error_text = match response.text().await {
                        Ok(text) => {
                            match serde_json::from_str::<serde_json::Value>(&text) {
                                Ok(json) => {
                                    json.get("error").and_then(|e| e.as_str())
                                        .unwrap_or(&text).to_string()
                                },
                                Err(_) => text,
                            }
                        },
                        Err(_) => "Bad request".to_string(),
                    };
                    return Err(LLMProviderError::GenerationFailed(error_text));
                },
                _ => {
                    return Err(LLMProviderError::ProviderError(format!(
                        "Failed to generate text: HTTP {}", response.status()
                    )));
                }
            }
        }

        // Create the streaming response handler
        debug!("Successfully created streaming response for model '{}'", model_id);
        Ok(Box::new(OllamaCompletionStream {
            response,
            accumulated_text: String::new(),
            completed: false,
        }))
    }

    async fn get_model_config(&self, model_id: &str) -> LLMProviderResult<ModelConfig> {
        self.ensure_initialized()?;
        debug!("Getting configuration for model '{}'", model_id);

        // Check if we have a cached config
        let config_map = self.model_configs.read().await;
        if let Some(config) = config_map.get(model_id) {
            return Ok(config.clone());
        }

        // Ollama doesn't have a direct API to get model config
        // We'll return a default config
        Ok(ModelConfig {
            id: model_id.to_string(),
            parameters: HashMap::new(),
        })
    }

    async fn update_model_config(&self, config: ModelConfig) -> LLMProviderResult<()> {
        self.ensure_initialized()?;
        debug!("Updating configuration for model '{}'", config.id);

        // Store the config in our cache
        let mut config_map = self.model_configs.write().await;
        config_map.insert(config.id.clone(), config);

        // Note: Ollama doesn't have a direct API to update model config
        // We're just storing it locally for now
        Ok(())
    }

    async fn get_model_info(&self, model_id: &str) -> LLMProviderResult<ModelInfo> {
        self.ensure_initialized()?;
        debug!("Getting information for model '{}'", model_id);

        // Get all models and find the one we want
        let models = self.list_available_models().await?;
        for model in models {
            if model.id == model_id {
                return Ok(model);
            }
        }

        error!("Model '{}' not found", model_id);
        Err(LLMProviderError::ModelNotFound(model_id.to_string()))
    }
}
</file>

<file path="src-tauri/src/offline/llm/types.rs">
use std::collections::HashMap;
use serde::{Deserialize, Serialize};
use thiserror::Error;

/// Represents the various types of local LLM providers that the system supports.
/// 
/// This enum allows the system to identify and instantiate the appropriate
/// provider implementation based on user selection or configuration.
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq)]
#[serde(rename_all = "camelCase")]
pub enum ProviderType {
    /// Ollama provider (https://ollama.ai/)
    /// A lightweight local LLM server that supports various models
    Ollama,
    
    /// LocalAI provider (https://localai.io/)
    /// A drop-in replacement for OpenAI API that runs locally
    LocalAI,
    
    /// llama.cpp provider (direct integration)
    /// Direct integration with the llama.cpp library for maximum performance
    LlamaCpp,
    
    /// Custom provider implementation
    /// For extensibility and third-party providers
    Custom(String),
}

impl Default for ProviderType {
    fn default() -> Self {
        ProviderType::Ollama
    }
}

impl std::fmt::Display for ProviderType {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            ProviderType::Ollama => write!(f, "Ollama"),
            ProviderType::LocalAI => write!(f, "LocalAI"),
            ProviderType::LlamaCpp => write!(f, "llama.cpp"),
            ProviderType::Custom(name) => write!(f, "Custom ({})", name),
        }
    }
}

/// Configuration options for LLM providers.
/// 
/// This struct contains common configuration options applicable to most providers,
/// as well as provider-specific options that can be passed through the custom_options field.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct ProviderConfig {
    /// The type of provider this configuration is for
    pub provider_type: ProviderType,
    
    /// The base URL/endpoint for the provider's API
    pub endpoint_url: String,
    
    /// API key for providers that require authentication
    #[serde(skip_serializing_if = "Option::is_none")]
    pub api_key: Option<String>,
    
    /// Maximum number of concurrent requests to the provider
    #[serde(default = "default_concurrency")]
    pub max_concurrent_requests: usize,
    
    /// Timeout in seconds for requests to the provider
    #[serde(default = "default_timeout")]
    pub request_timeout_seconds: u64,
    
    /// Whether to enable request retry on failure
    #[serde(default)]
    pub enable_retry: bool,
    
    /// Maximum number of retries for failed requests
    #[serde(default = "default_max_retries")]
    pub max_retries: usize,
    
    /// Backoff factor for retry delays (in seconds)
    #[serde(default = "default_backoff")]
    pub retry_backoff_seconds: f64,
    
    /// Additional provider-specific configuration options
    #[serde(default)]
    pub custom_options: HashMap<String, serde_json::Value>,
}

fn default_concurrency() -> usize { 5 }
fn default_timeout() -> u64 { 30 }
fn default_max_retries() -> usize { 3 }
fn default_backoff() -> f64 { 1.5 }

impl Default for ProviderConfig {
    fn default() -> Self {
        ProviderConfig {
            provider_type: ProviderType::default(),
            endpoint_url: "http://localhost:11434".to_string(),
            api_key: None,
            max_concurrent_requests: default_concurrency(),
            request_timeout_seconds: default_timeout(),
            enable_retry: true,
            max_retries: default_max_retries(),
            retry_backoff_seconds: default_backoff(),
            custom_options: HashMap::new(),
        }
    }
}

/// Information about a model available from a provider.
///
/// This struct provides a uniform representation of model information across
/// different providers, allowing the application to display and manage models
/// in a consistent way regardless of the provider.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct ModelInfo {
    /// Unique identifier for the model
    pub id: String,
    
    /// Display name of the model
    pub name: String,
    
    /// Model description
    pub description: String,
    
    /// Provider that supplies this model
    pub provider: ProviderType,
    
    /// Model size in bytes
    pub size_bytes: u64,
    
    /// Whether the model is currently downloaded and available locally
    pub is_downloaded: bool,
    
    /// Whether the model supports text generation
    pub supports_text_generation: bool,
    
    /// Whether the model supports text completion (vs chat)
    pub supports_completion: bool,
    
    /// Whether the model supports chat completion
    pub supports_chat: bool,
    
    /// Whether the model supports embeddings
    pub supports_embeddings: bool,
    
    /// Whether the model supports image generation
    pub supports_image_generation: bool,
    
    /// Quantization level if the model is quantized (e.g., "Q4_K_M")
    #[serde(skip_serializing_if = "Option::is_none")]
    pub quantization: Option<String>,
    
    /// Parameter count of the model (in billions)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub parameter_count_b: Option<f64>,
    
    /// Context window size (max tokens)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub context_length: Option<usize>,
    
    /// The model's family/architecture (e.g., "llama", "mistral", "gpt2")
    #[serde(skip_serializing_if = "Option::is_none")]
    pub model_family: Option<String>,
    
    /// Creation date of the model (ISO 8601 format)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub created_at: Option<String>,
    
    /// Tags associated with the model
    #[serde(default)]
    pub tags: Vec<String>,
    
    /// License information for the model
    #[serde(skip_serializing_if = "Option::is_none")]
    pub license: Option<String>,
    
    /// Provider-specific model information that doesn't fit into the standard fields
    #[serde(default)]
    pub provider_specific: HashMap<String, serde_json::Value>,
}

/// Message type for chat-based requests.
///
/// This struct represents a single message in a chat-based interaction,
/// following the format used by many LLM providers.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct ChatMessage {
    /// Role of the message sender (system, user, assistant)
    pub role: String,
    
    /// Content of the message
    pub content: String,
    
    /// Optional name of the message sender
    #[serde(skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
}

/// Options for text generation.
///
/// This struct contains parameters that control the behavior of text generation,
/// including creativity, response length, and other settings.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GenerationOptions {
    /// Maximum number of tokens to generate
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_tokens: Option<u32>,
    
    /// Temperature parameter (0.0 to 2.0, higher = more creative)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>,
    
    /// Top-p sampling parameter (0.0 to 1.0)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f32>,
    
    /// Top-k sampling parameter
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_k: Option<u32>,
    
    /// Frequency penalty (-2.0 to 2.0)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub frequency_penalty: Option<f32>,
    
    /// Presence penalty (-2.0 to 2.0)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub presence_penalty: Option<f32>,
    
    /// List of token IDs to never generate
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stop_token_ids: Option<Vec<u32>>,
    
    /// List of sequences to stop generation when encountered
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stop_sequences: Option<Vec<String>>,
    
    /// Whether to stream the response token-by-token
    #[serde(default)]
    pub stream: bool,
    
    /// Whether to return logprobs for generated tokens
    #[serde(default)]
    pub logprobs: bool,
    
    /// Number of top logprobs to return per token
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_logprobs: Option<u32>,
    
    /// Seed for deterministic generation
    #[serde(skip_serializing_if = "Option::is_none")]
    pub seed: Option<u64>,
    
    /// Additional provider-specific parameters
    #[serde(default)]
    pub additional_params: HashMap<String, serde_json::Value>,
}

impl Default for GenerationOptions {
    fn default() -> Self {
        Self {
            max_tokens: Some(1024),
            temperature: Some(0.7),
            top_p: Some(0.9),
            top_k: None,
            frequency_penalty: None,
            presence_penalty: None,
            stop_token_ids: None,
            stop_sequences: None,
            stream: false,
            logprobs: false,
            top_logprobs: None,
            seed: None,
            additional_params: HashMap::new(),
        }
    }
}

/// Request for text generation.
///
/// This struct contains all the information needed for a text generation request,
/// including the prompt, model, and generation parameters.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GenerationRequest {
    /// ID of the model to use
    pub model_id: String,
    
    /// Type of generation request (text, chat, etc.)
    pub request_type: GenerationRequestType,
    
    /// Generation options
    #[serde(default)]
    pub options: GenerationOptions,
}

/// Type of generation request.
///
/// This enum represents the different types of generation requests that can be made,
/// such as text completion, chat, etc.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase", tag = "type")]
pub enum GenerationRequestType {
    /// Text completion with a single prompt
    #[serde(rename_all = "camelCase")]
    TextCompletion {
        /// The prompt text to complete
        prompt: String,
    },
    
    /// Chat completion with a sequence of messages
    #[serde(rename_all = "camelCase")]
    ChatCompletion {
        /// The chat messages
        messages: Vec<ChatMessage>,
    },
}

/// Token usage information.
///
/// This struct tracks token usage for billing and rate limiting purposes.
#[derive(Debug, Clone, Serialize, Deserialize, Default)]
#[serde(rename_all = "camelCase")]
pub struct TokenUsage {
    /// Number of tokens in the prompt
    #[serde(default)]
    pub prompt_tokens: u32,
    
    /// Number of tokens in the completion
    #[serde(default)]
    pub completion_tokens: u32,
    
    /// Total number of tokens used
    #[serde(default)]
    pub total_tokens: u32,
}

/// Response from text generation.
///
/// This struct contains the generated text and metadata from the generation process.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GenerationResponse {
    /// The generated text
    pub text: String,
    
    /// Type of the response (text, chat)
    pub response_type: ResponseType,
    
    /// Token usage information
    #[serde(default)]
    pub usage: TokenUsage,
    
    /// Whether the generation was stopped due to reaching max tokens
    #[serde(default)]
    pub truncated: bool,
    
    /// Reason for stopping generation (if any)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub finish_reason: Option<String>,
    
    /// ID of the model that generated the text
    pub model_id: String,
    
    /// Creation timestamp (ISO 8601 format)
    pub created_at: String,
    
    /// Provider-specific metadata
    #[serde(default)]
    pub metadata: HashMap<String, serde_json::Value>,
}

/// Type of response.
///
/// This enum represents the different types of responses that can be generated,
/// matching the request types.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub enum ResponseType {
    /// Text completion response
    TextCompletion,
    
    /// Chat completion response
    ChatCompletion,
}

/// Status of a model download.
///
/// This enum tracks the status and progress of model downloads.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase", tag = "status")]
pub enum DownloadStatus {
    /// Model download has not started
    NotStarted,
    
    /// Model download is in progress
    #[serde(rename_all = "camelCase")]
    InProgress {
        /// Download progress percentage (0.0 to 100.0)
        percent: f32,
        
        /// Downloaded bytes
        bytes_downloaded: Option<u64>,
        
        /// Total bytes to download
        total_bytes: Option<u64>,
        
        /// Estimated time remaining in seconds
        eta_seconds: Option<f32>,
        
        /// Download speed in bytes per second
        bytes_per_second: Option<f32>,
    },
    
    /// Model download has completed successfully
    Completed {
        /// Timestamp when download completed (ISO 8601 format)
        completed_at: Option<String>,
        
        /// Time taken to download in seconds
        duration_seconds: Option<f32>,
    },
    
    /// Model download has failed
    #[serde(rename_all = "camelCase")]
    Failed {
        /// Reason for failure
        reason: String,
        
        /// Error code (if available)
        error_code: Option<String>,
        
        /// Timestamp when failure occurred (ISO 8601 format)
        failed_at: Option<String>,
    },
    
    /// Model download has been cancelled
    Cancelled {
        /// Timestamp when download was cancelled (ISO 8601 format)
        cancelled_at: Option<String>,
    },
}

/// Error types for LLM provider operations.
///
/// This enum represents the different types of errors that can occur
/// when interacting with LLM providers.
#[derive(Error, Debug)]
pub enum ProviderError {
    /// The requested model was not found
    #[error("Model not found: {0}")]
    ModelNotFound(String),
    
    /// The model download failed
    #[error("Model download failed: {0}")]
    DownloadFailed(String),
    
    /// The text generation failed
    #[error("Text generation failed: {0}")]
    GenerationFailed(String),
    
    /// A network error occurred
    #[error("Network error: {0}")]
    NetworkError(String),
    
    /// Authentication error
    #[error("Authentication error: {0}")]
    AuthError(String),
    
    /// Rate limit exceeded
    #[error("Rate limit exceeded: {0}")]
    RateLimitExceeded(String),
    
    /// Provider error
    #[error("Provider error: {0}")]
    ProviderError(String),
    
    /// Provider not initialized properly
    #[error("Provider not initialized: {0}")]
    NotInitialized(String),
    
    /// Unsupported operation
    #[error("Unsupported operation: {0}")]
    UnsupportedOperation(String),
    
    /// Invalid configuration
    #[error("Invalid configuration: {0}")]
    InvalidConfig(String),
    
    /// Unexpected error
    #[error("Unexpected error: {0}")]
    Unexpected(String),
}

/// Result type for provider operations
pub type ProviderResult<T> = Result<T, ProviderError>;
</file>

<file path="src-tauri/src/offline/mod.rs">
//! Offline capabilities module
//!
//! This module provides functionality for offline operation of the MCP client,
//! including local LLM inference, checkpointing, and synchronization mechanisms.

pub mod llm;

use std::sync::Arc;
use std::collections::HashMap;
use log::{debug, error, info, warn};
use serde::{Deserialize, Serialize};
use thiserror::Error;
use tokio::sync::Mutex;

use self::llm::{
    LLMManager, create_llm_manager, LLMConfig, LLMError, LLMResult,
    types::{ProviderType, ProviderConfig, GenerationOptions, GenerationRequest},
    factory::{ProviderInfo, AvailabilityResult},
};

/// Errors that can occur in the offline module
#[derive(Error, Debug)]
pub enum OfflineError {
    /// LLM-related error
    #[error("LLM error: {0}")]
    LLMError(#[from] llm::LLMError),
    
    /// Network connectivity error
    #[error("Network error: {0}")]
    NetworkError(String),
    
    /// Checkpoint-related error
    #[error("Checkpoint error: {0}")]
    CheckpointError(String),
    
    /// Synchronization error
    #[error("Sync error: {0}")]
    SyncError(String),
    
    /// Provider-related error
    #[error("Provider error: {0}")]
    ProviderError(String),
    
    /// Configuration error
    #[error("Configuration error: {0}")]
    ConfigError(String),
    
    /// Unexpected error
    #[error("Unexpected error: {0}")]
    Unexpected(String),
}

/// Result type for offline operations
pub type OfflineResult<T> = Result<T, OfflineError>;

/// Configuration for the offline module
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OfflineConfig {
    /// Whether offline mode is enabled
    pub enabled: bool,
    /// Whether to automatically switch to offline mode when network is unavailable
    pub auto_switch: bool,
    /// Configuration for the LLM module
    pub llm_config: LLMProviderConfig,
    /// Maximum size of conversation history to keep in offline mode
    pub max_history_size: usize,
    /// Whether to enable debug logging
    pub enable_debug: bool,
}

/// Configuration for the LLM provider
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct LLMProviderConfig {
    /// Type of provider to use
    pub provider_type: ProviderType,
    /// Provider endpoint URL
    pub endpoint_url: String,
    /// API key for providers that require authentication
    #[serde(skip_serializing_if = "Option::is_none")]
    pub api_key: Option<String>,
    /// Default model to use for generation
    pub default_model: Option<String>,
    /// Whether to enable advanced configuration options
    #[serde(default)]
    pub enable_advanced_config: bool,
    /// Advanced provider configuration
    #[serde(default, skip_serializing_if = "HashMap::is_empty")]
    pub advanced_config: HashMap<String, serde_json::Value>,
}

impl Default for LLMProviderConfig {
    fn default() -> Self {
        Self {
            provider_type: ProviderType::Ollama,
            endpoint_url: "http://localhost:11434".to_string(),
            api_key: None,
            default_model: None,
            enable_advanced_config: false,
            advanced_config: HashMap::new(),
        }
    }
}

impl Default for OfflineConfig {
    fn default() -> Self {
        Self {
            enabled: false,
            auto_switch: true,
            llm_config: LLMProviderConfig::default(),
            max_history_size: 100,
            enable_debug: false,
        }
    }
}

/// Network connectivity status
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum NetworkStatus {
    /// Network is connected
    Connected,
    /// Network is disconnected
    Disconnected,
    /// Network status is unknown
    Unknown,
}

/// Offline manager that handles offline capabilities
pub struct OfflineManager {
    /// Configuration for the offline module
    config: OfflineConfig,
    /// Current network status
    network_status: NetworkStatus,
    /// LLM manager for local inference
    llm_manager: Arc<Mutex<LLMManager>>,
    /// Last provider availability check results
    provider_availability: HashMap<ProviderType, AvailabilityResult>,
}

impl OfflineManager {
    /// Create a new offline manager with the default configuration
    pub async fn new() -> Self {
        // Initialize the LLM factory
        llm::factory::initialize().await;
        
        // Start with default configuration
        let config = OfflineConfig::default();
        let llm_config = Self::convert_to_llm_config(&config.llm_config);
        
        Self {
            config,
            network_status: NetworkStatus::Unknown,
            llm_manager: create_llm_manager().await,
            provider_availability: HashMap::new(),
        }
    }
    
    /// Create a new offline manager with a specific configuration
    pub async fn with_config(config: OfflineConfig) -> Self {
        // Initialize the LLM factory
        llm::factory::initialize().await;
        
        let mut manager = Self {
            config,
            network_status: NetworkStatus::Unknown,
            llm_manager: create_llm_manager().await,
            provider_availability: HashMap::new(),
        };
        
        // Initialize the LLM manager with the config
        if let Err(e) = manager.initialize_llm().await {
            error!("Failed to initialize LLM manager: {}", e);
        }
        
        manager
    }
    
    /// Convert LLMProviderConfig to LLMConfig
    fn convert_to_llm_config(provider_config: &LLMProviderConfig) -> LLMConfig {
        // Create a JSON object for the provider config
        let mut config_obj = serde_json::Map::new();
        
        // Add base URL
        config_obj.insert("base_url".to_string(), serde_json::Value::String(provider_config.endpoint_url.clone()));
        
        // Add API key if present
        if let Some(api_key) = &provider_config.api_key {
            config_obj.insert("api_key".to_string(), serde_json::Value::String(api_key.clone()));
        }
        
        // Add any advanced configuration options
        if provider_config.enable_advanced_config {
            for (key, value) in &provider_config.advanced_config {
                config_obj.insert(key.clone(), value.clone());
            }
        }
        
        LLMConfig {
            provider_type: provider_config.provider_type,
            provider_config: serde_json::Value::Object(config_obj),
            default_model: provider_config.default_model.clone(),
            enable_debug: false,
        }
    }
    
    /// Convert LLMProviderConfig to ProviderConfig
    fn convert_to_provider_config(provider_config: &LLMProviderConfig) -> ProviderConfig {
        let mut custom_options = HashMap::new();
        
        if provider_config.enable_advanced_config {
            for (key, value) in &provider_config.advanced_config {
                custom_options.insert(key.clone(), value.clone());
            }
        }
        
        ProviderConfig {
            provider_type: provider_config.provider_type,
            endpoint_url: provider_config.endpoint_url.clone(),
            api_key: provider_config.api_key.clone(),
            max_concurrent_requests: 5,  // Default value
            request_timeout_seconds: 30, // Default value
            enable_retry: true,          // Default value
            max_retries: 3,              // Default value
            retry_backoff_seconds: 1.5,  // Default value
            custom_options,
        }
    }
    
    /// Initialize the LLM manager
    async fn initialize_llm(&mut self) -> OfflineResult<()> {
        info!("Initializing LLM manager with provider {:?}", self.config.llm_config.provider_type);
        
        // Check provider availability first
        self.check_provider_availability().await?;
        
        // Convert the provider config to LLMConfig
        let llm_config = Self::convert_to_llm_config(&self.config.llm_config);
        
        // Initialize the LLM manager
        let mut llm_manager = self.llm_manager.lock().await;
        *llm_manager = LLMManager::with_config(llm_config);
        
        match llm_manager.initialize().await {
            Ok(()) => {
                info!("Successfully initialized LLM manager");
                Ok(())
            },
            Err(e) => {
                // Try to use default provider as fallback
                error!("Failed to initialize LLM manager: {}. Trying default provider...", e);
                
                // Get available providers
                match llm::get_available_providers().await {
                    providers if !providers.is_empty() => {
                        let available_provider = &providers[0];
                        warn!(
                            "Using available provider {:?} as fallback",
                            available_provider.provider_type
                        );
                        
                        // Update the configuration
                        self.config.llm_config.provider_type = available_provider.provider_type;
                        self.config.llm_config.endpoint_url = available_provider.default_endpoint.clone();
                        
                        // Create new config
                        let fallback_config = Self::convert_to_llm_config(&self.config.llm_config);
                        
                        // Reinitialize
                        *llm_manager = LLMManager::with_config(fallback_config);
                        match llm_manager.initialize().await {
                            Ok(()) => {
                                info!("Successfully initialized LLM manager with fallback provider");
                                Ok(())
                            },
                            Err(e) => {
                                error!("Failed to initialize LLM manager with fallback provider: {}", e);
                                Err(OfflineError::LLMError(e))
                            },
                        }
                    },
                    _ => {
                        error!("No LLM providers available");
                        Err(OfflineError::ProviderError("No LLM providers available".to_string()))
                    },
                }
            },
        }
    }
    
    /// Check the availability of the current provider
    pub async fn check_provider_availability(&mut self) -> OfflineResult<bool> {
        let provider_type = self.config.llm_config.provider_type;
        let endpoint_url = Some(self.config.llm_config.endpoint_url.as_str());
        
        debug!("Checking availability of provider {:?}", provider_type);
        
        // Use the factory to check availability
        let is_available = llm::is_provider_available(provider_type, endpoint_url).await;
        
        if is_available {
            debug!("Provider {:?} is available", provider_type);
        } else {
            warn!("Provider {:?} is not available", provider_type);
        }
        
        // Get detailed availability results
        let mut registry = llm::factory::get_provider_registry_mut().await;
        let result = registry.check_provider_availability(&provider_type, endpoint_url).await;
        self.provider_availability.insert(provider_type, result.clone());
        
        Ok(is_available)
    }
    
    /// Get availability status for all providers
    pub async fn get_all_provider_availability(&mut self) -> OfflineResult<HashMap<ProviderType, AvailabilityResult>> {
        debug!("Checking availability of all providers");
        
        // Use the factory to check all providers
        let results = llm::factory::check_all_providers().await;
        
        // Store results
        self.provider_availability = results.clone();
        
        Ok(results)
    }
    
    /// Get a list of all available providers
    pub async fn get_available_providers(&mut self) -> OfflineResult<Vec<ProviderInfo>> {
        debug!("Getting list of available providers");
        
        // Use the factory to get available providers
        let providers = llm::get_available_providers().await;
        
        Ok(providers)
    }
    
    /// Get a list of all registered providers
    pub async fn get_all_providers(&self) -> OfflineResult<Vec<ProviderInfo>> {
        debug!("Getting list of all registered providers");
        
        // Use the factory to get all providers
        let providers = llm::factory::get_all_providers().await;
        
        Ok(providers)
    }
    
    /// Change the provider type and reconfigure
    pub async fn change_provider(&mut self, provider_type: ProviderType, endpoint_url: &str) -> OfflineResult<()> {
        debug!("Changing provider to {:?} at {}", provider_type, endpoint_url);
        
        // Check if the provider is available
        let is_available = llm::is_provider_available(provider_type, Some(endpoint_url)).await;
        
        if !is_available {
            warn!("Provider {:?} is not available at {}", provider_type, endpoint_url);
            return Err(OfflineError::ProviderError(format!(
                "Provider {:?} is not available at {}",
                provider_type, endpoint_url
            )));
        }
        
        // Update the configuration
        self.config.llm_config.provider_type = provider_type;
        self.config.llm_config.endpoint_url = endpoint_url.to_string();
        
        // Reinitialize the LLM manager
        self.initialize_llm().await
    }
    
    /// Get the current provider configuration
    pub fn get_provider_config(&self) -> LLMProviderConfig {
        self.config.llm_config.clone()
    }
    
    /// Update the provider configuration
    pub async fn update_provider_config(&mut self, config: LLMProviderConfig) -> OfflineResult<()> {
        debug!("Updating provider configuration: {:?}", config);
        
        // Check if the configuration changed
        if self.config.llm_config == config {
            debug!("Provider configuration unchanged");
            return Ok(());
        }
        
        // Update the configuration
        self.config.llm_config = config;
        
        // Reinitialize the LLM manager
        self.initialize_llm().await
    }
    
    /// Check if offline mode is enabled
    pub fn is_enabled(&self) -> bool {
        self.config.enabled
    }
    
    /// Enable or disable offline mode
    pub fn set_enabled(&mut self, enabled: bool) {
        self.config.enabled = enabled;
    }
    
    /// Get the current network status
    pub fn get_network_status(&self) -> NetworkStatus {
        self.network_status
    }
    
    /// Update the network status
    pub fn update_network_status(&mut self, status: NetworkStatus) {
        self.network_status = status;
        
        // If auto-switch is enabled, update offline mode based on network status
        if self.config.auto_switch {
            match status {
                NetworkStatus::Connected => self.config.enabled = false,
                NetworkStatus::Disconnected => self.config.enabled = true,
                NetworkStatus::Unknown => {}, // Don't change anything
            }
        }
    }
    
    /// Update the configuration
    pub async fn update_config(&mut self, config: OfflineConfig) -> OfflineResult<()> {
        debug!("Updating offline configuration");
        
        // Check if LLM config changed
        let llm_config_changed = self.config.llm_config != config.llm_config;
        
        // Update the config
        self.config = config;
        
        // Reinitialize the LLM manager if needed
        if llm_config_changed {
            self.initialize_llm().await?;
        }
        
        Ok(())
    }
    
    /// Get a reference to the LLM manager
    pub fn llm_manager(&self) -> &Arc<Mutex<LLMManager>> {
        &self.llm_manager
    }
    
    /// Check if network is available
    pub async fn check_network(&mut self) -> OfflineResult<NetworkStatus> {
        debug!("Checking network connectivity");
        
        // Try to make a simple HTTP request to check connectivity
        let client = reqwest::Client::new();
        
        match client.get("https://anthropic.com")
            .timeout(std::time::Duration::from_secs(5))
            .send()
            .await 
        {
            Ok(_) => {
                debug!("Network is connected");
                self.update_network_status(NetworkStatus::Connected);
                Ok(NetworkStatus::Connected)
            },
            Err(e) => {
                debug!("Network is disconnected: {}", e);
                self.update_network_status(NetworkStatus::Disconnected);
                Ok(NetworkStatus::Disconnected)
            },
        }
    }
    
    /// Generate text using a local LLM (backward compatible)
    pub async fn generate_text(
        &self,
        prompt: &str,
        model_id: Option<&str>,
    ) -> OfflineResult<String> {
        debug!("Generating text with prompt: {:?}", prompt);
        let llm_manager = self.llm_manager.lock().await;
        
        let response = llm_manager.generate_text(model_id, prompt, None).await?;
        
        Ok(response.text)
    }
    
    /// Generate text with options
    pub async fn generate_text_with_options(
        &self,
        prompt: &str,
        model_id: Option<&str>,
        options: GenerationOptions,
    ) -> OfflineResult<String> {
        debug!("Generating text with options");
        let llm_manager = self.llm_manager.lock().await;
        
        let response = llm_manager.generate_text(model_id, prompt, Some(options)).await?;
        
        Ok(response.text)
    }
    
    /// Process a generation request
    pub async fn process_request(&self, request: GenerationRequest) -> OfflineResult<String> {
        debug!("Processing generation request");
        let llm_manager = self.llm_manager.lock().await;
        
        let response = llm_manager.process_request(request).await?;
        
        Ok(response.text)
    }
    
    /// List available models from the current provider
    pub async fn list_available_models(&self) -> OfflineResult<Vec<llm::provider::ModelInfo>> {
        debug!("Listing available models");
        let llm_manager = self.llm_manager.lock().await;
        
        match llm_manager.list_available_models().await {
            Ok(models) => Ok(models),
            Err(e) => Err(OfflineError::LLMError(e)),
        }
    }
    
    /// List downloaded models from the current provider
    pub async fn list_downloaded_models(&self) -> OfflineResult<Vec<llm::provider::ModelInfo>> {
        debug!("Listing downloaded models");
        let llm_manager = self.llm_manager.lock().await;
        
        match llm_manager.list_downloaded_models().await {
            Ok(models) => Ok(models),
            Err(e) => Err(OfflineError::LLMError(e)),
        }
    }
    
    /// Get model info
    pub async fn get_model_info(&self, model_id: &str) -> OfflineResult<llm::provider::ModelInfo> {
        debug!("Getting model info for {}", model_id);
        let llm_manager = self.llm_manager.lock().await;
        
        match llm_manager.get_model_info(model_id).await {
            Ok(info) => Ok(info),
            Err(e) => Err(OfflineError::LLMError(e)),
        }
    }
    
    /// Download a model
    pub async fn download_model(&self, model_id: &str) -> OfflineResult<()> {
        debug!("Downloading model {}", model_id);
        let llm_manager = self.llm_manager.lock().await;
        
        match llm_manager.download_model(model_id).await {
            Ok(()) => Ok(()),
            Err(e) => Err(OfflineError::LLMError(e)),
        }
    }
}

/// Create a shared offline manager with the default configuration
pub async fn create_offline_manager() -> Arc<Mutex<OfflineManager>> {
    Arc::new(Mutex::new(OfflineManager::new().await))
}
</file>

<file path="src-tui/Cargo.toml">
[package]
name = "mcp-tui"
version = "0.1.0"
edition = "2021"
description = "Text User Interface for the MCP client"

[dependencies]
# Common library
mcp-common = { path = "../src-common" }

# TUI framework
crossterm = "0.27.0"
ratatui = "0.24.0"
tui-textarea = "0.3.0"

# Async runtime
tokio = { version = "1.32", features = ["full"] }
futures = "0.3.28"

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Logging and error handling
log = "0.4.20"
env_logger = "0.10.0"
thiserror = "1.0.49"
anyhow = "1.0.75"

# Utilities
chrono = { version = "0.4.29", features = ["serde"] }
once_cell = "1.18.0"
uuid = { version = "1.4.1", features = ["v4", "serde"] }
dirs = "5.0.1"
</file>

<file path="src-tui/README.md">
# MCP TUI - Text User Interface for Claude MCP

A terminal-based user interface for interacting with Claude AI models through the Model Context Protocol (MCP).

## Features

- **Rich Terminal UI**: Full-featured interface built with Ratatui and Crossterm
- **Conversation Management**: View, create, select, and delete conversations
- **Real-time Streaming**: See Claude's responses as they're generated
- **Keyboard Navigation**: Vim-inspired keyboard shortcuts
- **Command Mode**: Quick access to commands via command palette
- **Multiple Views**: Conversations list, chat view, settings, help
- **Theming Support**: Customizable colors and styling
- **MCP Protocol**: Full implementation of the Model Context Protocol
- **Offline Support**: Automatic fallback to local models when offline

## Usage

```bash
# Start the TUI
mcp-tui

# Start with a specific conversation
mcp-tui --conversation CONVERSATION_ID
```

## Keyboard Shortcuts

### Global

- `q` - Quit application
- `?` - Show help screen
- `:` - Enter command mode
- `s` - Open settings

### Navigation

- `j/k` or Arrow keys - Navigate up/down in lists
- `Enter` - Select conversation
- `Esc` - Return to normal mode
- `Tab` - Cycle through sections

### Conversation Management

- `n` - Create new conversation
- `d` - Delete current conversation
- `r` - Reload conversations
- `PageUp/PageDown` - Scroll through history

### Chat Mode

- `Ctrl+Enter` - Send message
- `Esc` - Exit chat mode

## Command Mode

The TUI features a vim-like command mode activated with `:`. Available commands:

- `:quit` or `:q` - Quit the application
- `:new [title]` or `:n [title]` - Create a new conversation
- `:delete` or `:d` - Delete the current conversation
- `:reload` or `:r` - Reload conversations
- `:help` or `:h` - Show help screen
- `:settings` or `:s` - Open settings

## User Interface

```
┌─ Status Bar ───────────────────────────────────────────────────────────────┐
│ NORMAL | My Conversation | claude-3-opus-20240229                          │
├─ Conversations ──────────┬─ Chat ─────────────────────────────────────────┐
│                          │                                                 │
│ > Project Ideas          │ You: What are some project ideas for Rust?      │
│   Meeting Notes          │                                                 │
│   Brainstorming          │ Claude: Here are some project ideas for Rust:   │
│   Research               │                                                 │
│   Claude Exploration     │ 1. Command-line tools                           │
│   Coding Help            │ 2. WebAssembly applications                     │
│   Travel Planning        │ 3. Embedded systems                             │
│                          │ 4. Network services                             │
│                          │ 5. Game development                             │
│                          │                                                 │
│                          │ Would you like me to elaborate on any of these? │
│                          │                                                 │
│                          │                                                 │
│                          │                                                 │
├──────────────────────────┴─────────────────────────────────────────────────┤
│ Type a message...                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

## Installation

### From Source

```bash
# Clone the repository
git clone https://github.com/your-username/claude-mcp.git
cd claude-mcp

# Build the TUI
cd src-tui
cargo build --release

# Install (optional)
cargo install --path .
```

### Using Pre-built Binaries

Download the appropriate binary for your platform from the [Releases](https://github.com/your-username/claude-mcp/releases) page.

## Configuration

The TUI shares configuration with the main application and CLI. Configuration is stored in:

- Linux: `~/.config/mcp/config.json`
- macOS: `~/Library/Application Support/mcp/config.json`
- Windows: `%APPDATA%\mcp\config.json`

## Environment Variables

- `MCP_API_KEY`: Your Claude API key (overrides config file)
- `MCP_DEFAULT_MODEL`: Default model to use (overrides config file)
- `MCP_CONFIG_PATH`: Custom path to config file
- `MCP_TUI_COLORS`: Set to `true` to enable colors, `false` to disable
- `MCP_TUI_THEME`: Set the theme (`default`, `dark`, `light`, `high-contrast`)

## Advanced Features

### Custom Key Bindings

Create a file at `~/.config/mcp/tui-keybindings.json` to customize key bindings.

### Themes

The TUI supports multiple themes that can be selected in the settings screen or via the `MCP_TUI_THEME` environment variable.

### Local Model Integration

When offline, the TUI automatically switches to available local models, providing a seamless experience even without internet access.

## License

[MIT License](LICENSE)
</file>

<file path="src-tui/src/app/mod.rs">
use std::sync::Arc;
use crossterm::event::{KeyEvent, MouseEvent, KeyCode, KeyModifiers};
use ratatui::layout::Rect;
use tui_textarea::TextArea;
use tokio::sync::mpsc;

use crate::error::AppError;
use mcp_common::{
    models::{Conversation, Message, Model},
    service::ChatService,
};

// Result type used in the application
pub type AppResult<T> = std::result::Result<T, AppError>;

// Application mode enumeration
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum AppMode {
    Normal,      // Navigation, list selection
    Chatting,    // Active chat input
    Command,     // Command input
    Help,        // Help screen
    Settings,    // Settings screen
}

// Application state
pub struct App {
    // Services
    pub chat_service: Arc<ChatService>,
    
    // Application state
    pub should_quit: bool,
    pub mode: AppMode,
    pub size: Rect,
    
    // Conversations
    pub conversations: Vec<Conversation>,
    pub selected_conversation_idx: Option<usize>,
    pub current_conversation: Option<Conversation>,
    pub message_offset: usize,
    
    // Streaming state
    pub is_streaming: bool,
    pub stream_receiver: Option<mpsc::Receiver<Result<Message, String>>>,
    pub current_response: String,
    
    // Input fields
    pub input: TextArea<'static>,
    pub command_input: TextArea<'static>,
    pub status_message: Option<(String, bool)>, // (message, is_error)
    
    // Help
    pub show_help: bool,
    
    // Settings
    pub settings_open: bool,
    pub settings_idx: usize,
}

impl App {
    // Create a new application instance
    pub fn new(chat_service: Arc<ChatService>) -> Self {
        let mut app = Self {
            chat_service,
            should_quit: false,
            mode: AppMode::Normal,
            size: Rect::default(),
            conversations: Vec::new(),
            selected_conversation_idx: None,
            current_conversation: None,
            message_offset: 0,
            is_streaming: false,
            stream_receiver: None,
            current_response: String::new(),
            input: TextArea::default(),
            command_input: TextArea::default(),
            status_message: None,
            show_help: false,
            settings_open: false,
            settings_idx: 0,
        };
        
        // Configure TextArea
        app.input.set_placeholder_text("Type a message...");
        app.input.set_cursor_line_style(ratatui::style::Style::default());
        app.input.set_block(ratatui::widgets::Block::default());
        
        // Configure command input
        app.command_input.set_placeholder_text("Type a command...");
        app.command_input.set_cursor_line_style(ratatui::style::Style::default());
        app.command_input.set_block(ratatui::widgets::Block::default());
        
        app
    }
    
    // Initialize the application
    pub async fn initialize(&mut self) -> AppResult<()> {
        // Load conversations
        self.load_conversations().await?;
        
        // Set status message
        self.set_status("Welcome to Claude MCP TUI", false);
        
        Ok(())
    }
    
    // Handle application tick (time-based updates)
    pub fn tick(&mut self) {
        // Process streaming responses
        if self.is_streaming {
            if let Some(receiver) = &mut self.stream_receiver {
                // Try to receive new message chunks
                if let Ok(Some(result)) = receiver.try_recv() {
                    match result {
                        Ok(message) => {
                            // Update the current response
                            self.current_response = message.text();
                            
                            // Update the conversation with the response
                            if let Some(conversation) = &mut self.current_conversation {
                                // Check if there's already an assistant message at the end
                                let last_message_is_assistant = conversation.messages.last()
                                    .map(|m| m.role == "assistant")
                                    .unwrap_or(false);
                                
                                if last_message_is_assistant {
                                    // Update the last message
                                    if let Some(last) = conversation.messages.last_mut() {
                                        last.content = message.content.clone();
                                    }
                                } else {
                                    // Add a new message
                                    conversation.messages.push(message);
                                }
                            }
                        }
                        Err(e) => {
                            // Show error
                            self.set_status(&format!("Error: {}", e), true);
                            self.is_streaming = false;
                        }
                    }
                }
            } else {
                // No receiver, streaming should be false
                self.is_streaming = false;
            }
        }
        
        // Clear status message after a period of time
        if let Some((_, _)) = &self.status_message {
            // In a real implementation, we'd check against a timestamp
            // and clear after a certain duration
        }
    }
    
    // Handle keyboard events
    pub async fn handle_key_event(&mut self, key: KeyEvent) -> AppResult<bool> {
        match self.mode {
            AppMode::Normal => self.handle_normal_mode_key(key).await?,
            AppMode::Chatting => self.handle_chat_mode_key(key).await?,
            AppMode::Command => self.handle_command_mode_key(key).await?,
            AppMode::Help => self.handle_help_mode_key(key)?,
            AppMode::Settings => self.handle_settings_mode_key(key).await?,
        }
        
        Ok(self.should_quit)
    }
    
    // Handle mouse events
    pub fn handle_mouse_event(&mut self, _event: MouseEvent) {
        // Mouse handling code would go here
    }
    
    // Handle window resize
    pub fn resize(&mut self, width: u16, height: u16) {
        self.size = Rect::new(0, 0, width, height);
    }
    
    // Set status message
    pub fn set_status(&mut self, message: &str, is_error: bool) {
        self.status_message = Some((message.to_string(), is_error));
    }
    
    // Load conversations from the service
    async fn load_conversations(&mut self) -> AppResult<()> {
        match self.chat_service.list_conversations().await {
            Ok(conversations) => {
                self.conversations = conversations;
                
                // Select the first conversation if available
                if !self.conversations.is_empty() && self.selected_conversation_idx.is_none() {
                    self.selected_conversation_idx = Some(0);
                }
                
                Ok(())
            }
            Err(e) => {
                self.set_status(&format!("Failed to load conversations: {}", e), true);
                Err(AppError::Service(format!("Failed to load conversations: {}", e)))
            }
        }
    }
    
    // Load a specific conversation
    async fn load_conversation(&mut self, conversation_id: &str) -> AppResult<()> {
        match self.chat_service.get_conversation(conversation_id).await {
            Ok(conversation) => {
                self.current_conversation = Some(conversation);
                self.message_offset = 0;
                Ok(())
            }
            Err(e) => {
                self.set_status(&format!("Failed to load conversation: {}", e), true);
                Err(AppError::Service(format!("Failed to load conversation: {}", e)))
            }
        }
    }
    
    // Send a message in the current conversation
    async fn send_message(&mut self, content: &str) -> AppResult<()> {
        // Get the current conversation ID
        let conversation_id = if let Some(conversation) = &self.current_conversation {
            conversation.id.clone()
        } else if let Some(idx) = self.selected_conversation_idx {
            if let Some(conversation) = self.conversations.get(idx) {
                conversation.id.clone()
            } else {
                return Err(AppError::App("No conversation selected".to_string()));
            }
        } else {
            return Err(AppError::App("No conversation selected".to_string()));
        };
        
        // Add the user message to the conversation
        if let Some(conversation) = &mut self.current_conversation {
            let message = Message {
                id: format!("temp_{}", uuid::Uuid::new_v4()),
                role: "user".to_string(),
                content: vec![mcp_common::models::MessageContent {
                    content_type: "text".to_string(),
                    text: Some(content.to_string()),
                    source: None,
                }],
                model: None,
                stop_reason: None,
                stop_sequence: None,
                usage: None,
            };
            conversation.messages.push(message);
        }
        
        // Start streaming response
        match self.chat_service.send_message_streaming(&conversation_id, content).await {
            Ok(receiver) => {
                self.stream_receiver = Some(receiver);
                self.is_streaming = true;
                self.current_response = String::new();
                Ok(())
            }
            Err(e) => {
                self.set_status(&format!("Failed to send message: {}", e), true);
                Err(AppError::Service(format!("Failed to send message: {}", e)))
            }
        }
    }
    
    // Create a new conversation
    async fn create_conversation(&mut self, title: &str) -> AppResult<()> {
        match self.chat_service.create_conversation(title, None).await {
            Ok(conversation) => {
                // Add to list and select it
                self.conversations.insert(0, conversation.clone());
                self.selected_conversation_idx = Some(0);
                self.current_conversation = Some(conversation);
                self.set_status(&format!("Created conversation: {}", title), false);
                Ok(())
            }
            Err(e) => {
                self.set_status(&format!("Failed to create conversation: {}", e), true);
                Err(AppError::Service(format!("Failed to create conversation: {}", e)))
            }
        }
    }
    
    // Delete the current conversation
    async fn delete_conversation(&mut self) -> AppResult<()> {
        if let Some(idx) = self.selected_conversation_idx {
            if let Some(conversation) = self.conversations.get(idx) {
                let id = conversation.id.clone();
                let title = conversation.title.clone();
                
                match self.chat_service.delete_conversation(&id).await {
                    Ok(_) => {
                        // Remove from list
                        self.conversations.remove(idx);
                        
                        // Update selection
                        if self.conversations.is_empty() {
                            self.selected_conversation_idx = None;
                            self.current_conversation = None;
                        } else {
                            let new_idx = if idx >= self.conversations.len() {
                                self.conversations.len() - 1
                            } else {
                                idx
                            };
                            self.selected_conversation_idx = Some(new_idx);
                            
                            // Load the newly selected conversation
                            if let Some(conversation) = self.conversations.get(new_idx) {
                                self.load_conversation(&conversation.id).await?;
                            }
                        }
                        
                        self.set_status(&format!("Deleted conversation: {}", title), false);
                        Ok(())
                    }
                    Err(e) => {
                        self.set_status(&format!("Failed to delete conversation: {}", e), true);
                        Err(AppError::Service(format!("Failed to delete conversation: {}", e)))
                    }
                }
            } else {
                Err(AppError::App("Invalid conversation index".to_string()))
            }
        } else {
            Err(AppError::App("No conversation selected".to_string()))
        }
    }
    
    // Handle keys in normal mode (conversation navigation)
    async fn handle_normal_mode_key(&mut self, key: KeyEvent) -> AppResult<()> {
        match key.code {
            // Quit application
            KeyCode::Char('q') => {
                self.should_quit = true;
            }
            
            // Help screen
            KeyCode::Char('?') | KeyCode::F(1) => {
                self.show_help = true;
                self.mode = AppMode::Help;
            }
            
            // Settings screen
            KeyCode::Char('s') => {
                self.settings_open = true;
                self.mode = AppMode::Settings;
            }
            
            // Navigation - up/down
            KeyCode::Up | KeyCode::Char('k') => {
                if let Some(idx) = self.selected_conversation_idx {
                    if idx > 0 {
                        self.selected_conversation_idx = Some(idx - 1);
                    }
                }
            }
            KeyCode::Down | KeyCode::Char('j') => {
                if let Some(idx) = self.selected_conversation_idx {
                    if idx < self.conversations.len() - 1 {
                        self.selected_conversation_idx = Some(idx + 1);
                    }
                }
            }
            
            // Select conversation
            KeyCode::Enter => {
                if let Some(idx) = self.selected_conversation_idx {
                    if let Some(conversation) = self.conversations.get(idx) {
                        self.load_conversation(&conversation.id).await?;
                        self.mode = AppMode::Chatting;
                    }
                }
            }
            
            // Create new conversation
            KeyCode::Char('n') => {
                // Default name with timestamp
                let title = format!("Conversation {}", chrono::Local::now().format("%Y-%m-%d %H:%M"));
                self.create_conversation(&title).await?;
                self.mode = AppMode::Chatting;
            }
            
            // Delete conversation
            KeyCode::Char('d') => {
                if let Some(idx) = self.selected_conversation_idx {
                    if let Some(conversation) = self.conversations.get(idx) {
                        // In a real implementation, we'd ask for confirmation
                        self.delete_conversation().await?;
                    }
                }
            }
            
            // Command mode
            KeyCode::Char(':') => {
                self.command_input = TextArea::default();
                self.command_input.set_placeholder_text("Type a command...");
                self.mode = AppMode::Command;
            }
            
            // Scroll through conversation history
            KeyCode::PageUp => {
                if self.message_offset > 0 {
                    self.message_offset -= 1;
                }
            }
            KeyCode::PageDown => {
                if let Some(conversation) = &self.current_conversation {
                    if self.message_offset < conversation.messages.len() {
                        self.message_offset += 1;
                    }
                }
            }
            
            // Reload conversations
            KeyCode::Char('r') => {
                self.load_conversations().await?;
            }
            
            _ => {}
        }
        
        Ok(())
    }
    
    // Handle keys in chat mode (message input)
    async fn handle_chat_mode_key(&mut self, key: KeyEvent) -> AppResult<()> {
        match key.code {
            // Send message on Ctrl+Enter
            KeyCode::Enter if key.modifiers.contains(KeyModifiers::CONTROL) => {
                let content = self.input.lines().join("\n");
                if !content.is_empty() {
                    self.send_message(&content).await?;
                    self.input = TextArea::default();
                    self.input.set_placeholder_text("Type a message...");
                }
            }
            
            // Exit chat mode on Escape
            KeyCode::Esc => {
                self.mode = AppMode::Normal;
            }
            
            // Pass other keys to the text area
            _ => {
                self.input.input(key);
            }
        }
        
        Ok(())
    }
    
    // Handle keys in command mode
    async fn handle_command_mode_key(&mut self, key: KeyEvent) -> AppResult<()> {
        match key.code {
            // Execute command on Enter
            KeyCode::Enter => {
                let command = self.command_input.lines().join(" ").trim().to_string();
                self.mode = AppMode::Normal;
                
                if !command.is_empty() {
                    self.execute_command(&command).await?;
                }
            }
            
            // Exit command mode on Escape
            KeyCode::Esc => {
                self.mode = AppMode::Normal;
            }
            
            // Pass other keys to the text area
            _ => {
                self.command_input.input(key);
            }
        }
        
        Ok(())
    }
    
    // Handle keys in help mode
    fn handle_help_mode_key(&mut self, key: KeyEvent) -> AppResult<()> {
        match key.code {
            // Exit help mode on Escape or q
            KeyCode::Esc | KeyCode::Char('q') => {
                self.show_help = false;
                self.mode = AppMode::Normal;
            }
            _ => {}
        }
        
        Ok(())
    }
    
    // Handle keys in settings mode
    async fn handle_settings_mode_key(&mut self, key: KeyEvent) -> AppResult<()> {
        match key.code {
            // Exit settings mode on Escape
            KeyCode::Esc => {
                self.settings_open = false;
                self.mode = AppMode::Normal;
            }
            
            // Navigate settings
            KeyCode::Up | KeyCode::Char('k') => {
                if self.settings_idx > 0 {
                    self.settings_idx -= 1;
                }
            }
            KeyCode::Down | KeyCode::Char('j') => {
                // In a real implementation, we'd check against max settings
                self.settings_idx += 1;
            }
            
            // Toggle or modify settings
            KeyCode::Enter | KeyCode::Char(' ') => {
                // Toggle or modify the selected setting
                // In a real implementation, we'd handle different setting types
            }
            
            _ => {}
        }
        
        Ok(())
    }
    
    // Execute a command from the command prompt
    async fn execute_command(&mut self, command: &str) -> AppResult<()> {
        // Parse command
        let parts: Vec<&str> = command.split_whitespace().collect();
        if parts.is_empty() {
            return Ok(());
        }
        
        match parts[0] {
            "quit" | "q" => {
                self.should_quit = true;
            }
            "new" | "n" => {
                let title = if parts.len() > 1 {
                    parts[1..].join(" ")
                } else {
                    format!("Conversation {}", chrono::Local::now().format("%Y-%m-%d %H:%M"))
                };
                self.create_conversation(&title).await?;
                self.mode = AppMode::Chatting;
            }
            "delete" | "d" => {
                self.delete_conversation().await?;
            }
            "reload" | "r" => {
                self.load_conversations().await?;
            }
            "help" | "h" => {
                self.show_help = true;
                self.mode = AppMode::Help;
            }
            "settings" | "s" => {
                self.settings_open = true;
                self.mode = AppMode::Settings;
            }
            _ => {
                self.set_status(&format!("Unknown command: {}", parts[0]), true);
            }
        }
        
        Ok(())
    }
}
</file>

<file path="src-tui/src/ui/mod.rs">
use ratatui::{
    layout::{Constraint, Direction, Layout, Rect},
    style::{Color, Modifier, Style},
    text::{Line, Span, Text},
    widgets::{Block, Borders, List, ListItem, Paragraph, Wrap},
    Frame,
};

use crate::app::{App, AppMode};

/// Draw the user interface
pub fn draw(f: &mut Frame, app: &App) {
    // Create the layout
    let chunks = Layout::default()
        .direction(Direction::Vertical)
        .constraints([
            Constraint::Length(1),  // Status bar
            Constraint::Min(0),     // Main content
            Constraint::Length(3),  // Input box
        ])
        .split(f.size());
    
    // Draw the status bar
    draw_status_bar(f, app, chunks[0]);
    
    // Draw the main area
    draw_main_area(f, app, chunks[1]);
    
    // Draw the input box
    draw_input_box(f, app, chunks[2]);
    
    // Draw help screen if enabled
    if app.show_help {
        draw_help_screen(f, app);
    }
    
    // Draw settings screen if enabled
    if app.settings_open {
        draw_settings_screen(f, app);
    }
}

/// Draw the status bar
fn draw_status_bar(f: &mut Frame, app: &App, area: Rect) {
    let mut spans = vec![];
    
    // App mode
    let mode_str = match app.mode {
        AppMode::Normal => "NORMAL",
        AppMode::Chatting => "CHAT",
        AppMode::Command => "COMMAND",
        AppMode::Help => "HELP",
        AppMode::Settings => "SETTINGS",
    };
    
    spans.push(Span::styled(
        format!(" {} ", mode_str),
        Style::default().bg(Color::Blue).fg(Color::White),
    ));
    
    // Current conversation
    if let Some(conversation) = &app.current_conversation {
        spans.push(Span::raw(" "));
        spans.push(Span::styled(
            &conversation.title,
            Style::default().fg(Color::Green),
        ));
        
        if let Some(model) = &conversation.model {
            spans.push(Span::raw(" | "));
            spans.push(Span::styled(
                &model.name,
                Style::default().fg(Color::Yellow),
            ));
        }
    }
    
    // Streaming indicator
    if app.is_streaming {
        spans.push(Span::raw(" "));
        spans.push(Span::styled(
            " STREAMING ",
            Style::default().bg(Color::LightGreen).fg(Color::Black),
        ));
    }
    
    // Status message
    if let Some((message, is_error)) = &app.status_message {
        spans.push(Span::raw(" | "));
        spans.push(Span::styled(
            message,
            Style::default().fg(if *is_error { Color::Red } else { Color::Green }),
        ));
    }
    
    // Create the paragraph
    let paragraph = Paragraph::new(Line::from(spans));
    
    // Render the status bar
    f.render_widget(paragraph, area);
}

/// Draw the main content area
fn draw_main_area(f: &mut Frame, app: &App, area: Rect) {
    // Split into conversations list and chat area
    let chunks = Layout::default()
        .direction(Direction::Horizontal)
        .constraints([
            Constraint::Percentage(20), // Conversations list
            Constraint::Percentage(80), // Chat area
        ])
        .split(area);
    
    // Draw the conversations list
    draw_conversations_list(f, app, chunks[0]);
    
    // Draw the chat area
    draw_chat_area(f, app, chunks[1]);
}

/// Draw the conversations list
fn draw_conversations_list(f: &mut Frame, app: &App, area: Rect) {
    // Create list items
    let items: Vec<ListItem> = app
        .conversations
        .iter()
        .enumerate()
        .map(|(i, conversation)| {
            let style = if Some(i) == app.selected_conversation_idx {
                Style::default().bg(Color::Blue).fg(Color::White)
            } else {
                Style::default()
            };
            
            ListItem::new(conversation.title.clone()).style(style)
        })
        .collect();
    
    // Create the list
    let list = List::new(items)
        .block(Block::default().title("Conversations").borders(Borders::ALL))
        .highlight_style(
            Style::default()
                .bg(Color::Blue)
                .fg(Color::White)
                .add_modifier(Modifier::BOLD),
        );
    
    // Render the list
    f.render_widget(list, area);
}

/// Draw the chat area
fn draw_chat_area(f: &mut Frame, app: &App, area: Rect) {
    // Create the chat box
    let chat_box = Block::default()
        .title("Chat")
        .borders(Borders::ALL);
    
    // Render the chat box
    f.render_widget(chat_box, area);
    
    // Inner area for messages
    let inner_area = chat_box.inner(area);
    
    // Display conversation messages
    if let Some(conversation) = &app.current_conversation {
        let messages = &conversation.messages;
        
        if !messages.is_empty() {
            let mut text_spans = Vec::new();
            
            for message in messages {
                // Determine style based on role
                let (prefix, style) = match message.role.as_str() {
                    "user" => (
                        "You: ",
                        Style::default().fg(Color::Green),
                    ),
                    "assistant" => (
                        "Claude: ",
                        Style::default().fg(Color::Blue),
                    ),
                    "system" => (
                        "System: ",
                        Style::default().fg(Color::Yellow),
                    ),
                    _ => (
                        "Unknown: ",
                        Style::default(),
                    ),
                };
                
                // Add sender with style
                text_spans.push(Line::from(Span::styled(
                    prefix,
                    style.add_modifier(Modifier::BOLD),
                )));
                
                // Add message content
                for content in &message.content {
                    if let Some(text) = &content.text {
                        // Split by lines and add each as a span
                        for line in text.lines() {
                            text_spans.push(Line::from(line));
                        }
                    }
                }
                
                // Add separator
                text_spans.push(Line::from(""));
            }
            
            // Create the text widget
            let text = Text::from(text_spans);
            let paragraph = Paragraph::new(text)
                .wrap(Wrap { trim: false });
            
            // Render the messages
            f.render_widget(paragraph, inner_area);
        }
    }
}

/// Draw the input box
fn draw_input_box(f: &mut Frame, app: &App, area: Rect) {
    // Create the input box
    let input_box = Block::default()
        .title(match app.mode {
            AppMode::Chatting => "Message",
            AppMode::Command => "Command",
            _ => "Input",
        })
        .borders(Borders::ALL);
    
    // Set the block
    match app.mode {
        AppMode::Chatting => {
            app.input.set_block(input_box);
            f.render_widget(app.input.widget(), area);
        }
        AppMode::Command => {
            app.command_input.set_block(input_box);
            f.render_widget(app.command_input.widget(), area);
        }
        _ => {
            let text = match app.mode {
                AppMode::Normal => "Press Enter to chat, n for new, d to delete",
                AppMode::Help => "Press q to exit help",
                AppMode::Settings => "Press Esc to exit settings",
                _ => "",
            };
            
            let paragraph = Paragraph::new(text)
                .block(input_box);
            
            f.render_widget(paragraph, area);
        }
    }
}

/// Draw the help screen
fn draw_help_screen(f: &mut Frame, app: &App) {
    // Create a centered popup
    let area = centered_rect(60, 60, f.size());
    
    // Create the help box
    let help_box = Block::default()
        .title("Help")
        .borders(Borders::ALL);
    
    // Render the help box
    f.render_widget(help_box, area);
    
    // Inner area for help content
    let inner_area = help_box.inner(area);
    
    // Help text
    let text = Text::from(vec![
        Line::from("Claude MCP TUI Commands"),
        Line::from(""),
        Line::from("General:"),
        Line::from("  q         - Quit application"),
        Line::from("  ?         - Show this help"),
        Line::from("  :         - Enter command mode"),
        Line::from(""),
        Line::from("Navigation:"),
        Line::from("  j/k       - Move up/down in lists"),
        Line::from("  Enter     - Select conversation"),
        Line::from("  Esc       - Return to normal mode"),
        Line::from(""),
        Line::from("Conversations:"),
        Line::from("  n         - Create new conversation"),
        Line::from("  d         - Delete current conversation"),
        Line::from("  r         - Reload conversations"),
        Line::from(""),
        Line::from("Chat:"),
        Line::from("  Ctrl+Enter - Send message"),
        Line::from("  PageUp/Down - Scroll through history"),
        Line::from(""),
        Line::from("Settings:"),
        Line::from("  s         - Open settings"),
    ]);
    
    // Create the text widget
    let paragraph = Paragraph::new(text);
    
    // Render the help content
    f.render_widget(paragraph, inner_area);
}

/// Draw the settings screen
fn draw_settings_screen(f: &mut Frame, app: &App) {
    // Create a centered popup
    let area = centered_rect(60, 60, f.size());
    
    // Create the settings box
    let settings_box = Block::default()
        .title("Settings")
        .borders(Borders::ALL);
    
    // Render the settings box
    f.render_widget(settings_box, area);
    
    // Inner area for settings content
    let inner_area = settings_box.inner(area);
    
    // Settings list
    let items = vec![
        ListItem::new("API Key Configuration"),
        ListItem::new("Default Model: Claude-3-Opus"),
        ListItem::new("Enable Message Streaming: Yes"),
        ListItem::new("Dark Mode: Enabled"),
        ListItem::new("Show System Messages: Yes"),
    ];
    
    // Create the list
    let list = List::new(items)
        .highlight_style(
            Style::default()
                .bg(Color::Blue)
                .fg(Color::White)
                .add_modifier(Modifier::BOLD),
        )
        .highlight_symbol("> ");
    
    // Render the settings list
    f.render_stateful_widget(
        list,
        inner_area,
        &mut ratatui::widgets::ListState::default().with_selected(Some(app.settings_idx)),
    );
}

/// Helper function to create a centered rect
fn centered_rect(percent_x: u16, percent_y: u16, r: Rect) -> Rect {
    let popup_layout = Layout::default()
        .direction(Direction::Vertical)
        .constraints([
            Constraint::Percentage((100 - percent_y) / 2),
            Constraint::Percentage(percent_y),
            Constraint::Percentage((100 - percent_y) / 2),
        ])
        .split(r);
    
    Layout::default()
        .direction(Direction::Horizontal)
        .constraints([
            Constraint::Percentage((100 - percent_x) / 2),
            Constraint::Percentage(percent_x),
            Constraint::Percentage((100 - percent_x) / 2),
        ])
        .split(popup_layout[1])[1]
}
</file>

<file path="src-tui/src/util/mod.rs">
use chrono::{DateTime, Local};
use mcp_common::models::Message;
use ratatui::style::{Color, Style};

/// Format a timestamp as a readable string
pub fn format_timestamp(timestamp: &DateTime<Local>) -> String {
    timestamp.format("%Y-%m-%d %H:%M").to_string()
}

/// Get the color for a message role
pub fn get_role_color(role: &str) -> Color {
    match role {
        "user" => Color::Green,
        "assistant" => Color::Blue,
        "system" => Color::Yellow,
        _ => Color::White,
    }
}

/// Format a message for display
pub fn format_message(message: &Message) -> String {
    let prefix = match message.role.as_str() {
        "user" => "You",
        "assistant" => "Claude",
        "system" => "System",
        _ => "Unknown",
    };
    
    let content = message.content.iter()
        .filter_map(|c| c.text.clone())
        .collect::<Vec<String>>()
        .join("\n");
    
    format!("{}: {}", prefix, content)
}

/// Get the style for a message role
pub fn get_role_style(role: &str) -> Style {
    Style::default().fg(get_role_color(role))
}

/// Truncate a string to a maximum length with ellipsis
pub fn truncate_string(s: &str, max_len: usize) -> String {
    if s.len() <= max_len {
        s.to_string()
    } else {
        format!("{}...", &s[..max_len - 3])
    }
}

/// Generate a unique ID
pub fn generate_id() -> String {
    uuid::Uuid::new_v4().to_string()
}

/// Parse a command string into tokens
pub fn parse_command(command: &str) -> Vec<String> {
    let mut tokens = Vec::new();
    let mut current_token = String::new();
    let mut in_quotes = false;
    let mut escape_next = false;
    
    for c in command.chars() {
        if escape_next {
            current_token.push(c);
            escape_next = false;
        } else if c == '\\' {
            escape_next = true;
        } else if c == '"' {
            in_quotes = !in_quotes;
        } else if c.is_whitespace() && !in_quotes {
            if !current_token.is_empty() {
                tokens.push(current_token);
                current_token = String::new();
            }
        } else {
            current_token.push(c);
        }
    }
    
    if !current_token.is_empty() {
        tokens.push(current_token);
    }
    
    tokens
}
</file>

<file path="src/ai/claude/api.rs">
use log::{debug, error, warn};
use reqwest::{Client, Response, StatusCode};
use reqwest::header::{HeaderMap, HeaderValue, AUTHORIZATION, CONTENT_TYPE};
use serde::{Deserialize, Serialize};
use serde_json::Value;
use std::time::Duration;
use tokio_stream::Stream;
use futures_util::StreamExt;

/// Claude API response
#[derive(Debug, Clone, Deserialize)]
pub struct ClaudeResponse {
    /// Response ID
    pub id: String,
    
    /// Response type
    #[serde(rename = "type")]
    pub response_type: String,
    
    /// Response role
    pub role: String,
    
    /// Model used
    pub model: String,
    
    /// Stop reason
    pub stop_reason: Option<String>,
    
    /// Content parts
    pub content: Vec<Value>,
    
    /// Usage statistics
    pub usage: Value,
}

/// Claude API delta response (for streaming)
#[derive(Debug, Clone, Deserialize)]
pub struct ClaudeDeltaResponse {
    /// Response type
    #[serde(rename = "type")]
    pub response_type: String,
    
    /// Message ID
    pub message_id: String,
    
    /// Delta content
    pub delta: Value,
    
    /// Usage (only present in the final chunk)
    pub usage: Option<Value>,
}

/// Claude API error
#[derive(Debug, Clone, Deserialize)]
pub struct ClaudeError {
    /// Error type
    #[serde(rename = "type")]
    pub error_type: String,
    
    /// Error message
    pub message: String,
}

/// Claude API client
#[derive(Clone)]
pub struct ClaudeApi {
    /// HTTP client
    client: Client,
    
    /// API base URL
    base_url: String,
    
    /// API key
    api_key: String,
    
    /// API version
    api_version: String,
}

impl ClaudeApi {
    /// Create a new Claude API client
    pub fn new(api_key: String, base_url: &str) -> Self {
        // Create client with default settings
        let client = Client::builder()
            .timeout(Duration::from_secs(120))
            .build()
            .unwrap_or_else(|_| Client::new());
        
        Self {
            client,
            base_url: base_url.to_string(),
            api_key,
            api_version: "2023-06-01".to_string(),
        }
    }
    
    /// Set API key
    pub fn set_api_key(&mut self, api_key: String) {
        self.api_key = api_key;
    }
    
    /// Set API version
    pub fn set_api_version(&mut self, api_version: String) {
        self.api_version = api_version;
    }
    
    /// Create default headers
    fn create_headers(&self) -> HeaderMap {
        let mut headers = HeaderMap::new();
        
        // Add authentication header
        headers.insert(
            AUTHORIZATION,
            HeaderValue::from_str(&format!("Bearer {}", self.api_key))
                .unwrap_or_else(|_| HeaderValue::from_static("")),
        );
        
        // Add content type
        headers.insert(
            CONTENT_TYPE,
            HeaderValue::from_static("application/json"),
        );
        
        // Add API version
        headers.insert(
            "anthropic-version",
            HeaderValue::from_str(&self.api_version)
                .unwrap_or_else(|_| HeaderValue::from_static("2023-06-01")),
        );
        
        headers
    }
    
    /// Create a new message
    pub async fn create_message(&self, body: &Value) -> Result<ClaudeResponse, Box<dyn std::error::Error>> {
        let url = format!("{}/v1/messages", self.base_url);
        
        let response = self.client
            .post(&url)
            .headers(self.create_headers())
            .json(body)
            .send()
            .await?;
        
        self.handle_response(response).await
    }
    
    /// Create a streaming message
    pub async fn create_message_stream(
        &self,
        body: &Value,
    ) -> Result<impl Stream<Item = Result<ClaudeDeltaResponse, Box<dyn std::error::Error + Send + Sync>>>, Box<dyn std::error::Error>> {
        let url = format!("{}/v1/messages", self.base_url);
        
        let response = self.client
            .post(&url)
            .headers(self.create_headers())
            .json(body)
            .send()
            .await?;
        
        if !response.status().is_success() {
            return Err(self.handle_error_response(response).await?);
        }
        
        let stream = response.bytes_stream().map(|result| {
            result.map_err(|err| Box::new(err) as Box<dyn std::error::Error + Send + Sync>)
                .and_then(|bytes| {
                    // Parse SSE event
                    let text = String::from_utf8_lossy(&bytes);
                    let lines: Vec<&str> = text.lines().collect();
                    
                    for line in lines {
                        if line.starts_with("data: ") {
                            let data = &line[6..]; // Skip "data: "
                            
                            if data == "[DONE]" {
                                continue;
                            }
                            
                            match serde_json::from_str::<ClaudeDeltaResponse>(data) {
                                Ok(response) => return Ok(response),
                                Err(e) => return Err(Box::new(e) as Box<dyn std::error::Error + Send + Sync>),
                            }
                        }
                    }
                    
                    Err(Box::new(std::io::Error::new(
                        std::io::ErrorKind::InvalidData,
                        "Invalid SSE event",
                    )) as Box<dyn std::error::Error + Send + Sync>)
                })
        });
        
        Ok(stream)
    }
    
    /// Handle API response
    async fn handle_response(&self, response: Response) -> Result<ClaudeResponse, Box<dyn std::error::Error>> {
        match response.status() {
            StatusCode::OK | StatusCode::CREATED => {
                let claude_response = response.json::<ClaudeResponse>().await?;
                Ok(claude_response)
            }
            _ => Err(self.handle_error_response(response).await?),
        }
    }
    
    /// Handle error response
    async fn handle_error_response(&self, response: Response) -> Result<Box<dyn std::error::Error>, Box<dyn std::error::Error>> {
        let status = response.status();
        let body = response.text().await?;
        
        // Try to parse as Claude error
        match serde_json::from_str::<ClaudeError>(&body) {
            Ok(error) => {
                Err(Box::new(std::io::Error::new(
                    std::io::ErrorKind::Other,
                    format!("Claude API error ({}): {}", error.error_type, error.message),
                )))
            }
            Err(_) => {
                Err(Box::new(std::io::Error::new(
                    std::io::ErrorKind::Other,
                    format!("HTTP error {}: {}", status, body),
                )))
            }
        }
    }
}

/// Claude API client with convenience methods
#[derive(Clone)]
pub struct ClaudeApiClient {
    /// Inner API client
    api: ClaudeApi,
}

impl ClaudeApiClient {
    /// Create a new Claude API client
    pub fn new(api_key: String, base_url: &str) -> Self {
        Self {
            api: ClaudeApi::new(api_key, base_url),
        }
    }
    
    /// Set API key
    pub fn set_api_key(&mut self, api_key: String) {
        self.api.set_api_key(api_key);
    }
    
    /// Create a new message
    pub async fn create_message(&self, body: &Value) -> Result<ClaudeResponse, Box<dyn std::error::Error>> {
        self.api.create_message(body).await
    }
    
    /// Create a streaming message
    pub async fn create_message_stream(
        &self,
        body: &Value,
    ) -> Result<impl Stream<Item = Result<ClaudeDeltaResponse, Box<dyn std::error::Error + Send + Sync>>>, Box<dyn std::error::Error>> {
        self.api.create_message_stream(body).await
    }
}
</file>

<file path="src/ai/claude/mcp.rs">
use crate::ai::{ModelError, ModelProviderConfig};
use crate::models::messages::{Message, MessageError};
use crate::protocols::mcp::{McpClient, McpConfig, McpError};
use crate::utils::events::{events, get_event_system};
use log::{debug, error, info, warn};
use std::sync::{Arc, Mutex};
use tokio::sync::mpsc;

/// Claude MCP client
#[derive(Clone)]
pub struct ClaudeMcpClient {
    /// MCP client
    client: Arc<McpClient>,
    
    /// Provider configuration
    config: ModelProviderConfig,
}

impl ClaudeMcpClient {
    /// Create a new Claude MCP client
    pub fn new(provider_config: &ModelProviderConfig) -> Result<Self, ModelError> {
        // Create MCP configuration
        let mcp_config = McpConfig::with_api_key(provider_config.api_key.clone())
            .with_url("wss://api.anthropic.com/v1/messages")
            .with_model(provider_config.default_model.clone());
        
        // Create MCP client
        let client = match McpClient::new(mcp_config) {
            Ok(client) => client,
            Err(e) => {
                error!("Failed to create MCP client: {:?}", e);
                return Err(ModelError::SystemError);
            }
        };
        
        Ok(Self {
            client: Arc::new(client),
            config: provider_config.clone(),
        })
    }
    
    /// Connect to MCP server
    pub async fn connect(&self) -> Result<(), ModelError> {
        match self.client.connect().await {
            Ok(_) => Ok(()),
            Err(e) => {
                error!("Failed to connect to MCP server: {}", e);
                Err(ModelError::NetworkError)
            }
        }
    }
    
    /// Disconnect from MCP server
    pub async fn disconnect(&self) -> Result<(), ModelError> {
        match self.client.disconnect().await {
            Ok(_) => Ok(()),
            Err(e) => {
                error!("Failed to disconnect from MCP server: {}", e);
                Err(ModelError::SystemError)
            }
        }
    }
    
    /// Complete a message
    pub async fn complete(&self, model_id: &str, message: Message) -> Result<Message, MessageError> {
        // Check if connected
        if !self.client.is_connected() {
            // Try to connect
            match self.connect().await {
                Ok(_) => {}
                Err(e) => {
                    return Err(MessageError::NetworkError(format!(
                        "Failed to connect to MCP server: {:?}",
                        e
                    )));
                }
            }
        }
        
        // Send message with model ID in metadata
        let mut message_with_metadata = message;
        
        if message_with_metadata.metadata.is_none() {
            message_with_metadata.metadata = Some(std::collections::HashMap::new());
        }
        
        if let Some(metadata) = &mut message_with_metadata.metadata {
            metadata.insert(
                "model".to_string(),
                serde_json::to_value(model_id).unwrap(),
            );
        }
        
        // Send through MCP client
        self.client.send(message_with_metadata).await
    }
    
    /// Stream a message
    pub async fn stream(
        &self,
        model_id: &str,
        message: Message,
    ) -> Result<mpsc::Receiver<Result<Message, MessageError>>, MessageError> {
        // Check if connected
        if !self.client.is_connected() {
            // Try to connect
            match self.connect().await {
                Ok(_) => {}
                Err(e) => {
                    return Err(MessageError::NetworkError(format!(
                        "Failed to connect to MCP server: {:?}",
                        e
                    )));
                }
            }
        }
        
        // Send message with model ID in metadata
        let mut message_with_metadata = message;
        
        if message_with_metadata.metadata.is_none() {
            message_with_metadata.metadata = Some(std::collections::HashMap::new());
        }
        
        if let Some(metadata) = &mut message_with_metadata.metadata {
            metadata.insert(
                "model".to_string(),
                serde_json::to_value(model_id).unwrap(),
            );
        }
        
        // Stream through MCP client
        self.client.stream(message_with_metadata).await
    }
}
</file>

<file path="src/ai/claude/mod.rs">
mod api;
mod mcp;
mod streaming;

use self::api::{ClaudeApi, ClaudeApiClient, ClaudeResponse};
use self::mcp::ClaudeMcpClient;
use self::streaming::ClaudeStreamHandler;
use crate::ai::{ModelError, ModelProvider, ModelProviderConfig, ModelStatus, ProviderType};
use crate::models::messages::{ContentType, Message, MessageContent, MessageError, MessageRole};
use crate::models::Model;
use crate::services::auth::get_auth_service;
use crate::utils::config;
use crate::utils::events::{events, get_event_system};
use async_trait::async_trait;
use log::{debug, error, info, warn};
use std::collections::HashMap;
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, SystemTime};
use tokio::sync::mpsc;
use uuid::Uuid;

/// Claude AI provider
pub struct ClaudeProvider {
    /// Provider configuration
    config: ModelProviderConfig,
    
    /// Claude API client
    api_client: ClaudeApiClient,
    
    /// Claude MCP client
    mcp_client: Option<ClaudeMcpClient>,
    
    /// Available models cache
    models: Arc<RwLock<Vec<Model>>>,
    
    /// Model status cache
    model_status: Arc<RwLock<HashMap<String, ModelStatus>>>,
    
    /// Active streaming sessions
    active_streams: Arc<Mutex<HashMap<String, ClaudeStreamHandler>>>,
}

impl ClaudeProvider {
    /// Create a new Claude provider
    pub fn new() -> Result<Self, ModelError> {
        // Load configuration
        let config = config::get_config();
        let config_guard = config.lock().unwrap();
        
        let api_key = config_guard
            .get_string("api.key")
            .unwrap_or_else(|| String::new());
        
        if api_key.is_empty() {
            warn!("Claude API key not found in configuration");
            return Err(ModelError::AuthError);
        }
        
        let base_url = config_guard
            .get_string("api.base_url")
            .unwrap_or_else(|| "https://api.anthropic.com".to_string());
        
        let provider_config = ModelProviderConfig {
            provider_type: ProviderType::Claude,
            name: "Claude".to_string(),
            base_url,
            api_key: api_key.clone(),
            organization_id: None, // Could be loaded from config if needed
            timeout: Duration::from_secs(120),
            default_model: "claude-3-opus-20240229".to_string(),
            fallback_model: Some("claude-3-haiku-20240307".to_string()),
            enable_mcp: true,
            enable_streaming: true,
            settings: serde_json::Map::new(),
        };
        
        // Create API client
        let api_client = ClaudeApiClient::new(api_key, &base_url);
        
        // Create MCP client if enabled
        let mcp_client = if provider_config.enable_mcp {
            match ClaudeMcpClient::new(&provider_config) {
                Ok(client) => Some(client),
                Err(e) => {
                    warn!("Failed to create Claude MCP client: {:?}", e);
                    None
                }
            }
        } else {
            None
        };
        
        // Create default models
        let default_models = vec![
            Model {
                id: "claude-3-opus-20240229".to_string(),
                provider: "anthropic".to_string(),
                name: "Claude 3 Opus".to_string(),
                version: "20240229".to_string(),
                capabilities: crate::models::ModelCapabilities {
                    vision: true,
                    max_context_length: 200_000,
                    functions: true,
                    streaming: true,
                },
            },
            Model {
                id: "claude-3-sonnet-20240229".to_string(),
                provider: "anthropic".to_string(),
                name: "Claude 3 Sonnet".to_string(),
                version: "20240229".to_string(),
                capabilities: crate::models::ModelCapabilities {
                    vision: true,
                    max_context_length: 180_000,
                    functions: true,
                    streaming: true,
                },
            },
            Model {
                id: "claude-3-haiku-20240307".to_string(),
                provider: "anthropic".to_string(),
                name: "Claude 3 Haiku".to_string(),
                version: "20240307".to_string(),
                capabilities: crate::models::ModelCapabilities {
                    vision: true,
                    max_context_length: 150_000,
                    functions: true,
                    streaming: true,
                },
            },
        ];
        
        Ok(Self {
            config: provider_config,
            api_client,
            mcp_client,
            models: Arc::new(RwLock::new(default_models)),
            model_status: Arc::new(RwLock::new(HashMap::new())),
            active_streams: Arc::new(Mutex::new(HashMap::new())),
        })
    }
    
    /// Convert a Message to Claude API format
    fn convert_to_claude_format(&self, message: &Message) -> serde_json::Value {
        // Convert role
        let role = match message.role {
            MessageRole::User => "user",
            MessageRole::Assistant => "assistant",
            MessageRole::System => "system",
            MessageRole::Tool => "tool",
        };
        
        // Build content
        let mut contents = Vec::new();
        for part in &message.content.parts {
            match part {
                ContentType::Text { text } => {
                    contents.push(serde_json::json!({
                        "type": "text",
                        "text": text
                    }));
                }
                ContentType::Image { url, media_type } => {
                    contents.push(serde_json::json!({
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": media_type,
                            "data": url.trim_start_matches("data:").to_string()
                        }
                    }));
                }
                ContentType::ToolCall { id, name, arguments } => {
                    contents.push(serde_json::json!({
                        "type": "tool_call",
                        "id": id,
                        "name": name,
                        "arguments": arguments
                    }));
                }
                ContentType::ToolResult { tool_call_id, result } => {
                    contents.push(serde_json::json!({
                        "type": "tool_result",
                        "tool_call_id": tool_call_id,
                        "result": result
                    }));
                }
            }
        }
        
        serde_json::json!({
            "role": role,
            "content": contents
        })
    }
    
    /// Convert Claude API response to Message
    fn convert_from_claude_response(&self, response: &ClaudeResponse) -> Message {
        // Extract text content from response
        let text = response.content.iter()
            .filter_map(|content| {
                if let Some(content_type) = content.get("type") {
                    if content_type == "text" {
                        content.get("text").and_then(|text| text.as_str())
                    } else {
                        None
                    }
                } else {
                    None
                }
            })
            .collect::<Vec<&str>>()
            .join("");
        
        Message {
            id: response.id.clone(),
            role: MessageRole::Assistant,
            content: MessageContent {
                parts: vec![ContentType::Text { text: text.to_string() }],
            },
            metadata: Some(HashMap::from([
                ("model".to_string(), serde_json::to_value(&response.model).unwrap()),
                ("stop_reason".to_string(), serde_json::to_value(&response.stop_reason).unwrap()),
                ("usage".to_string(), serde_json::to_value(&response.usage).unwrap()),
            ])),
            created_at: SystemTime::now(),
        }
    }
}

#[async_trait]
impl ModelProvider for ClaudeProvider {
    fn provider_type(&self) -> ProviderType {
        ProviderType::Claude
    }
    
    fn name(&self) -> &str {
        &self.config.name
    }
    
    fn config(&self) -> &ModelProviderConfig {
        &self.config
    }
    
    async fn available_models(&self) -> Result<Vec<Model>, ModelError> {
        // Return cached models
        let models = self.models.read().unwrap().clone();
        
        // In a real implementation, we would fetch models from the API
        // For now, return the default models
        
        Ok(models)
    }
    
    async fn is_available(&self, model_id: &str) -> bool {
        // Check if model exists in available models
        let models = self.models.read().unwrap();
        models.iter().any(|m| m.id == model_id)
    }
    
    async fn model_status(&self, model_id: &str) -> ModelStatus {
        // Check cached status
        {
            let statuses = self.model_status.read().unwrap();
            if let Some(status) = statuses.get(model_id) {
                return *status;
            }
        }
        
        // Check if model exists
        if self.is_available(model_id).await {
            // Update cache
            {
                let mut statuses = self.model_status.write().unwrap();
                statuses.insert(model_id.to_string(), ModelStatus::Available);
            }
            
            ModelStatus::Available
        } else {
            // Update cache
            {
                let mut statuses = self.model_status.write().unwrap();
                statuses.insert(model_id.to_string(), ModelStatus::Unavailable);
            }
            
            ModelStatus::Unavailable
        }
    }
    
    async fn complete(&self, model_id: &str, message: Message) -> Result<Message, MessageError> {
        // Check if model is available
        if !self.is_available(model_id).await {
            return Err(MessageError::ProtocolError(format!(
                "Model {} is not available",
                model_id
            )));
        }
        
        // If MCP client is available and enabled, use it
        if let Some(mcp_client) = &self.mcp_client {
            if self.config.enable_mcp {
                return mcp_client.complete(model_id, message).await;
            }
        }
        
        // Otherwise use REST API
        let claude_message = self.convert_to_claude_format(&message);
        
        // Create request body
        let request_body = serde_json::json!({
            "model": model_id,
            "messages": [claude_message],
            "max_tokens": 4096,
            "temperature": 0.7,
            "system": "You are Claude, an AI assistant created by Anthropic. You are helpful, harmless, and honest."
        });
        
        // Send request
        match self.api_client.create_message(&request_body).await {
            Ok(response) => Ok(self.convert_from_claude_response(&response)),
            Err(e) => Err(MessageError::NetworkError(e.to_string())),
        }
    }
    
    async fn stream(
        &self,
        model_id: &str,
        message: Message,
    ) -> Result<mpsc::Receiver<Result<Message, MessageError>>, MessageError> {
        // Check if model is available
        if !self.is_available(model_id).await {
            return Err(MessageError::ProtocolError(format!(
                "Model {} is not available",
                model_id
            )));
        }
        
        // Check if streaming is enabled
        if !self.config.enable_streaming {
            return Err(MessageError::ProtocolError(
                "Streaming is not enabled".to_string(),
            ));
        }
        
        // If MCP client is available and enabled, use it
        if let Some(mcp_client) = &self.mcp_client {
            if self.config.enable_mcp {
                return mcp_client.stream(model_id, message).await;
            }
        }
        
        // Otherwise use REST API with streaming
        let claude_message = self.convert_to_claude_format(&message);
        
        // Create request body
        let request_body = serde_json::json!({
            "model": model_id,
            "messages": [claude_message],
            "max_tokens": 4096,
            "temperature": 0.7,
            "system": "You are Claude, an AI assistant created by Anthropic. You are helpful, harmless, and honest.",
            "stream": true
        });
        
        // Create stream handler
        let stream_id = Uuid::new_v4().to_string();
        let (tx, rx) = mpsc::channel(32);
        
        let handler = ClaudeStreamHandler::new(
            stream_id.clone(),
            tx.clone(),
            message.id.clone(),
        );
        
        // Store stream handler
        {
            let mut streams = self.active_streams.lock().unwrap();
            streams.insert(stream_id.clone(), handler.clone());
        }
        
        // Start streaming in separate task
        let api_client = self.api_client.clone();
        let self_clone = self.clone();
        let message_id = message.id.clone();
        
        tokio::spawn(async move {
            match api_client.create_message_stream(&request_body).await {
                Ok(mut stream) => {
                    handler.handle_stream(&mut stream, &message_id).await;
                    
                    // Remove stream handler when done
                    let mut streams = self_clone.active_streams.lock().unwrap();
                    streams.remove(&stream_id);
                }
                Err(e) => {
                    // Send error and remove stream handler
                    let _ = tx
                        .send(Err(MessageError::NetworkError(e.to_string())))
                        .await;
                    
                    let mut streams = self_clone.active_streams.lock().unwrap();
                    streams.remove(&stream_id);
                }
            }
        });
        
        Ok(rx)
    }
    
    async fn cancel_stream(&self, stream_id: &str) -> Result<(), MessageError> {
        // Find stream handler
        let mut handler_opt = None;
        
        {
            let mut streams = self.active_streams.lock().unwrap();
            handler_opt = streams.remove(stream_id);
        }
        
        if let Some(handler) = handler_opt {
            // Cancel the stream
            handler.cancel().await;
            Ok(())
        } else {
            Err(MessageError::Unknown(format!(
                "Stream {} not found",
                stream_id
            )))
        }
    }
}

impl Clone for ClaudeProvider {
    fn clone(&self) -> Self {
        Self {
            config: self.config.clone(),
            api_client: self.api_client.clone(),
            mcp_client: self.mcp_client.clone(),
            models: self.models.clone(),
            model_status: self.model_status.clone(),
            active_streams: self.active_streams.clone(),
        }
    }
}
</file>

<file path="src/ai/claude/streaming.rs">
use super::api::ClaudeDeltaResponse;
use crate::models::messages::{ContentType, Message, MessageError, MessageRole};
use crate::utils::events::{events, get_event_system};
use futures_util::StreamExt;
use log::{debug, error, warn};
use std::sync::{Arc, Mutex};
use std::time::SystemTime;
use tokio::sync::{mpsc, oneshot};
use tokio_stream::Stream;

/// Claude streaming handler
#[derive(Clone)]
pub struct ClaudeStreamHandler {
    /// Stream ID
    stream_id: String,
    
    /// Message sender
    tx: mpsc::Sender<Result<Message, MessageError>>,
    
    /// Original message ID
    original_message_id: String,
    
    /// Cancel channel
    cancel: Arc<Mutex<Option<oneshot::Sender<()>>>>,
    
    /// Accumulated text
    accumulated_text: Arc<Mutex<String>>,
}

impl ClaudeStreamHandler {
    /// Create a new Claude streaming handler
    pub fn new(
        stream_id: String,
        tx: mpsc::Sender<Result<Message, MessageError>>,
        original_message_id: String,
    ) -> Self {
        Self {
            stream_id,
            tx,
            original_message_id,
            cancel: Arc::new(Mutex::new(None)),
            accumulated_text: Arc::new(Mutex::new(String::new())),
        }
    }
    
    /// Handle streaming response
    pub async fn handle_stream<S, E>(
        &self,
        stream: &mut S,
        message_id: &str,
    ) where
        S: Stream<Item = Result<ClaudeDeltaResponse, E>> + Unpin,
        E: std::error::Error + Send + Sync + 'static,
    {
        // Create cancel channel
        let (cancel_tx, mut cancel_rx) = oneshot::channel();
        
        {
            let mut cancel_guard = self.cancel.lock().unwrap();
            *cancel_guard = Some(cancel_tx);
        }
        
        let mut final_response = None;
        
        // Process stream
        loop {
            tokio::select! {
                // Check for cancellation
                _ = &mut cancel_rx => {
                    debug!("Stream cancelled: {}", self.stream_id);
                    break;
                }
                
                // Process next chunk
                chunk = stream.next() => {
                    match chunk {
                        Some(Ok(delta)) => {
                            // Process delta
                            self.process_delta(delta, message_id).await;
                        }
                        Some(Err(e)) => {
                            // Send error
                            let error_msg = format!("Stream error: {}", e);
                            error!("{}", error_msg);
                            
                            let _ = self.tx.send(Err(MessageError::NetworkError(error_msg))).await;
                            break;
                        }
                        None => {
                            // End of stream
                            debug!("End of stream: {}", self.stream_id);
                            
                            // Send final complete message
                            let text = {
                                let text_guard = self.accumulated_text.lock().unwrap();
                                text_guard.clone()
                            };
                            
                            let final_message = Message {
                                id: message_id.to_string(),
                                role: MessageRole::Assistant,
                                content: crate::models::messages::MessageContent {
                                    parts: vec![ContentType::Text { text }],
                                },
                                metadata: None,
                                created_at: SystemTime::now(),
                            };
                            
                            let _ = self.tx.send(Ok(final_message)).await;
                            break;
                        }
                    }
                }
            }
        }
    }
    
    /// Process delta response
    async fn process_delta(&self, delta: ClaudeDeltaResponse, message_id: &str) {
        match delta.response_type.as_str() {
            "message_delta" => {
                // Check if delta contains content
                if let Some(delta_content) = delta.delta.get("content") {
                    // Only process text content for now
                    if let Some(content) = delta_content.as_array() {
                        for part in content {
                            if let Some(part_type) = part.get("type") {
                                if part_type.as_str() == Some("text") {
                                    if let Some(text) = part.get("text") {
                                        if let Some(text_str) = text.as_str() {
                                            // Append to accumulated text
                                            {
                                                let mut text_guard = self.accumulated_text.lock().unwrap();
                                                text_guard.push_str(text_str);
                                            }
                                            
                                            // Send partial message
                                            let text = {
                                                let text_guard = self.accumulated_text.lock().unwrap();
                                                text_guard.clone()
                                            };
                                            
                                            let partial_message = Message {
                                                id: message_id.to_string(),
                                                role: MessageRole::Assistant,
                                                content: crate::models::messages::MessageContent {
                                                    parts: vec![ContentType::Text { text }],
                                                },
                                                metadata: None,
                                                created_at: SystemTime::now(),
                                            };
                                            
                                            let _ = self.tx.send(Ok(partial_message)).await;
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
            "message_stop" => {
                // Final message, check if we have usage info
                if let Some(usage) = delta.usage {
                    // In a real implementation, we would include usage in metadata
                    debug!("Stream stopped with usage: {:?}", usage);
                }
            }
            _ => {
                // Ignore other message types
                debug!("Unknown delta type: {}", delta.response_type);
            }
        }
    }
    
    /// Cancel the stream
    pub async fn cancel(&self) {
        // Send cancel signal
        let mut cancel_tx = None;
        
        {
            let mut cancel_guard = self.cancel.lock().unwrap();
            cancel_tx = cancel_guard.take();
        }
        
        if let Some(tx) = cancel_tx {
            let _ = tx.send(());
        }
    }
}
</file>

<file path="src/ai/local/inference.rs">
use super::models::LocalModelInfo;
use crate::ai::ModelError;
use log::{debug, error, info, warn};
use std::path::Path;
use std::sync::{Arc, Mutex};

/// Error type for inference operations
#[derive(Debug)]
pub enum InferenceError {
    /// Model loading error
    ModelLoadError(String),
    
    /// Inference runtime error
    RuntimeError(String),
    
    /// Out of memory error
    OutOfMemory,
    
    /// Invalid input
    InvalidInput(String),
    
    /// System error
    SystemError(String),
}

/// Simulated inference engine
/// 
/// Note: This is a placeholder for a real inference engine like llama.cpp
/// In a real implementation, this would use FFI bindings to a C/C++ inference library
pub struct InferenceEngine {
    /// Model directory
    model_dir: std::path::PathBuf,
    
    /// Currently loaded model
    current_model: Arc<Mutex<Option<LocalModelInfo>>>,
}

impl InferenceEngine {
    /// Create a new inference engine
    pub fn new(model_dir: &Path) -> Result<Self, InferenceError> {
        Ok(Self {
            model_dir: model_dir.to_path_buf(),
            current_model: Arc::new(Mutex::new(None)),
        })
    }
    
    /// Load a model
    pub fn load_model(&self, model_info: &LocalModelInfo) -> Result<(), InferenceError> {
        // Check if model file exists
        if !model_info.path.exists() {
            return Err(InferenceError::ModelLoadError(format!(
                "Model file not found: {}",
                model_info.path.display()
            )));
        }
        
        // Update current model
        let mut current_model = self.current_model.lock().unwrap();
        *current_model = Some(model_info.clone());
        
        // In a real implementation, this would load the model into memory
        // using the inference library's API
        
        info!("Loaded model {}", model_info.name);
        Ok(())
    }
    
    /// Generate text from a prompt
    pub fn generate(&self, prompt: &str) -> Result<String, InferenceError> {
        // Get current model
        let current_model = self.current_model.lock().unwrap();
        let model_info = current_model.as_ref().ok_or_else(|| {
            InferenceError::RuntimeError("No model loaded".to_string())
        })?;
        
        // In a real implementation, this would use the inference library's API
        // to generate text from the prompt
        
        // For now, return a simulated response
        let response = self.simulate_response(model_info, prompt);
        
        Ok(response)
    }
    
    /// Generate text from a prompt with streaming
    pub fn generate_streaming<F>(
        &self,
        prompt: &str,
        max_tokens: usize,
        callback: F,
    ) -> Result<(), InferenceError>
    where
        F: FnMut(&str) -> bool,
    {
        // Get current model
        let current_model = self.current_model.lock().unwrap();
        let model_info = current_model.as_ref().ok_or_else(|| {
            InferenceError::RuntimeError("No model loaded".to_string())
        })?;
        
        // In a real implementation, this would use the inference library's API
        // to generate text from the prompt with streaming via callback
        
        // For now, return a simulated streaming response
        self.simulate_streaming_response(model_info, prompt, max_tokens, callback)
    }
    
    /// Simulate a response for testing
    fn simulate_response(&self, model_info: &LocalModelInfo, prompt: &str) -> String {
        // Generate a deterministic but somewhat random-looking response
        let hash = self.simple_hash(prompt);
        let response_template = match hash % 5 {
            0 => "I'm sorry, but I cannot provide a detailed response as I am a small local language model with limited capabilities. However, I can tell you that the topic you're asking about involves complex considerations that would require more context to address properly.",
            1 => "As a compact local AI model, I have a basic understanding of this topic. From what I know, this relates to technology and systems that help process information efficiently. Would you like me to elaborate on any specific aspect?",
            2 => "That's an interesting question. While I'm just a small local model, I can tell you that this area has seen significant developments in recent years. The fundamental principles involve data analysis and pattern recognition.",
            3 => "Based on my limited local knowledge base, I understand this relates to human-computer interaction and information processing. These concepts are central to modern computing paradigms and continue to evolve.",
            _ => "Thank you for your question. As a small local language model, I have limited knowledge, but I believe this topic involves communication systems and information exchange. These are important concepts in our increasingly connected world.",
        };
        
        format!("{}\n\nNote: This response was generated by the local {} model running on your device.", response_template, model_info.name)
    }
    
    /// Simulate a streaming response for testing
    fn simulate_streaming_response<F>(
        &self,
        model_info: &LocalModelInfo,
        prompt: &str,
        max_tokens: usize,
        mut callback: F,
    ) -> Result<(), InferenceError>
    where
        F: FnMut(&str) -> bool,
    {
        // Generate full response
        let full_response = self.simulate_response(model_info, prompt);
        
        // Split into tokens (words) and limit to max_tokens
        let words: Vec<&str> = full_response.split_whitespace().collect();
        let num_tokens = std::cmp::min(words.len(), max_tokens);
        
        // Stream each token with a small delay
        for i in 0..num_tokens {
            // Add a space before each word except the first one
            let token = if i == 0 {
                words[i].to_string()
            } else {
                format!(" {}", words[i])
            };
            
            // Call the callback with current token
            let should_continue = callback(&token);
            if !should_continue {
                break;
            }
            
            // Add a small delay to simulate generation time
            std::thread::sleep(std::time::Duration::from_millis(50));
        }
        
        Ok(())
    }
    
    /// Simple hash function for deterministic but varied responses
    fn simple_hash(&self, s: &str) -> usize {
        let mut hash = 0;
        for c in s.chars() {
            hash = ((hash << 5) + hash) + c as usize;
        }
        hash
    }
}
</file>

<file path="src/ai/local/mod.rs">
mod inference;
mod models;

use self::inference::InferenceEngine;
use self::models::LocalModelInfo;
use crate::ai::{ModelError, ModelProvider, ModelProviderConfig, ModelStatus, ProviderType};
use crate::models::messages::{ContentType, Message, MessageContent, MessageError, MessageRole};
use crate::models::Model;
use crate::utils::config;
use crate::utils::events::{events, get_event_system};
use async_trait::async_trait;
use log::{debug, error, info, warn};
use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, SystemTime};
use tokio::sync::mpsc;
use uuid::Uuid;

/// Local model provider for offline operations
pub struct LocalProvider {
    /// Provider configuration
    config: ModelProviderConfig,
    
    /// Inference engine
    inference_engine: Arc<Mutex<Option<InferenceEngine>>>,
    
    /// Available models
    models: Arc<RwLock<Vec<LocalModelInfo>>>,
    
    /// Active model status
    model_status: Arc<RwLock<HashMap<String, ModelStatus>>>,
    
    /// Active streaming sessions
    active_streams: Arc<Mutex<HashMap<String, mpsc::Sender<Result<Message, MessageError>>>>>,
    
    /// Model download directory
    model_dir: PathBuf,
}

impl LocalProvider {
    /// Create a new local provider
    pub fn new() -> Result<Self, ModelError> {
        // Load configuration
        let config = config::get_config();
        let config_guard = config.lock().unwrap();
        
        // Get model directory
        let model_dir = if let Some(dir) = config_guard.get_string("ai.local.model_dir") {
            PathBuf::from(dir)
        } else {
            // Use default location
            if let Some(proj_dirs) = directories::ProjectDirs::from("com", "claude", "mcp") {
                proj_dirs.data_dir().join("models")
            } else {
                PathBuf::from("models")
            }
        };
        
        // Create model directory if it doesn't exist
        if !model_dir.exists() {
            if let Err(e) = std::fs::create_dir_all(&model_dir) {
                warn!("Failed to create model directory: {}", e);
            }
        }
        
        let provider_config = ModelProviderConfig {
            provider_type: ProviderType::Local,
            name: "Local Models".to_string(),
            base_url: "".to_string(),
            api_key: "".to_string(),
            organization_id: None,
            timeout: Duration::from_secs(60),
            default_model: "tinyllama".to_string(),
            fallback_model: None,
            enable_mcp: false,
            enable_streaming: true,
            settings: serde_json::Map::new(),
        };
        
        // Try to initialize inference engine
        let inference_engine = match InferenceEngine::new(&model_dir) {
            Ok(engine) => Some(engine),
            Err(e) => {
                warn!("Failed to initialize inference engine: {:?}", e);
                None
            }
        };
        
        // Discover available local models
        let default_models = vec![
            LocalModelInfo {
                id: "tinyllama".to_string(),
                name: "TinyLlama".to_string(),
                path: model_dir.join("tinyllama.bin"),
                parameters: 1_000_000_000,
                quantization: "q4_0".to_string(),
                context_size: 2048,
                is_downloaded: false,
                download_url: Some("https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/ggml-model-q4_0.gguf".to_string()),
                model: Model {
                    id: "tinyllama".to_string(),
                    provider: "local".to_string(),
                    name: "TinyLlama 1.1B".to_string(),
                    version: "1.1".to_string(),
                    capabilities: crate::models::ModelCapabilities {
                        vision: false,
                        max_context_length: 2048,
                        functions: false,
                        streaming: true,
                    },
                },
            },
            LocalModelInfo {
                id: "redpajama-mini".to_string(),
                name: "RedPajama Mini".to_string(),
                path: model_dir.join("redpajama-mini.bin"),
                parameters: 1_400_000_000,
                quantization: "q4_0".to_string(),
                context_size: 2048,
                is_downloaded: false,
                download_url: Some("https://huggingface.co/weyaxi/redpajama.cpp/resolve/main/redpajama-mini-q4_0.bin".to_string()),
                model: Model {
                    id: "redpajama-mini".to_string(),
                    provider: "local".to_string(),
                    name: "RedPajama Mini".to_string(),
                    version: "1.0".to_string(),
                    capabilities: crate::models::ModelCapabilities {
                        vision: false,
                        max_context_length: 2048,
                        functions: false,
                        streaming: true,
                    },
                },
            },
        ];
        
        let provider = Self {
            config: provider_config,
            inference_engine: Arc::new(Mutex::new(inference_engine)),
            models: Arc::new(RwLock::new(default_models)),
            model_status: Arc::new(RwLock::new(HashMap::new())),
            active_streams: Arc::new(Mutex::new(HashMap::new())),
            model_dir,
        };
        
        // Update model download status
        provider.update_model_download_status();
        
        Ok(provider)
    }
    
    /// Update the download status of all models
    fn update_model_download_status(&self) {
        let mut models = self.models.write().unwrap();
        for model in models.iter_mut() {
            model.is_downloaded = model.path.exists();
        }
    }
    
    /// Download a model
    pub async fn download_model(&self, model_id: &str) -> Result<(), ModelError> {
        // Find model info
        let model_info = {
            let models = self.models.read().unwrap();
            models
                .iter()
                .find(|m| m.id == model_id)
                .cloned()
                .ok_or(ModelError::InvalidRequest)?
        };
        
        // Check if already downloaded
        if model_info.is_downloaded {
            return Ok(());
        }
        
        // Check if download URL is available
        let download_url = model_info
            .download_url
            .clone()
            .ok_or(ModelError::InvalidRequest)?;
        
        // Update model status
        {
            let mut statuses = self.model_status.write().unwrap();
            statuses.insert(model_id.to_string(), ModelStatus::Loading);
        }
        
        // Create temporary file
        let temp_path = model_info.path.with_extension("download");
        
        // Download model
        let client = reqwest::Client::new();
        let response = client
            .get(&download_url)
            .send()
            .await
            .map_err(|_| ModelError::NetworkError)?;
        
        if !response.status().is_success() {
            return Err(ModelError::NetworkError);
        }
        
        // Create parent directory if it doesn't exist
        if let Some(parent) = model_info.path.parent() {
            if !parent.exists() {
                if let Err(e) = std::fs::create_dir_all(parent) {
                    error!("Failed to create model directory: {}", e);
                    return Err(ModelError::SystemError);
                }
            }
        }
        
        // Save to file
        let mut file = tokio::fs::File::create(&temp_path)
            .await
            .map_err(|_| ModelError::SystemError)?;
        
        let mut stream = response.bytes_stream();
        while let Some(item) = stream.next().await {
            let chunk = item.map_err(|_| ModelError::NetworkError)?;
            tokio::io::AsyncWriteExt::write_all(&mut file, &chunk)
                .await
                .map_err(|_| ModelError::SystemError)?;
        }
        
        // Rename temp file to final file
        tokio::fs::rename(&temp_path, &model_info.path)
            .await
            .map_err(|_| ModelError::SystemError)?;
        
        // Update model download status
        self.update_model_download_status();
        
        // Update model status
        {
            let mut statuses = self.model_status.write().unwrap();
            statuses.insert(model_id.to_string(), ModelStatus::Available);
        }
        
        Ok(())
    }
    
    /// Load a model into the inference engine
    async fn load_model(&self, model_id: &str) -> Result<(), ModelError> {
        // Find model info
        let model_info = {
            let models = self.models.read().unwrap();
            models
                .iter()
                .find(|m| m.id == model_id)
                .cloned()
                .ok_or(ModelError::InvalidRequest)?
        };
        
        // Check if model is downloaded
        if !model_info.is_downloaded {
            // Try to download the model
            self.download_model(model_id).await?;
        }
        
        // Check if inference engine is initialized
        let mut engine_guard = self.inference_engine.lock().unwrap();
        if engine_guard.is_none() {
            *engine_guard = match InferenceEngine::new(&self.model_dir) {
                Ok(engine) => Some(engine),
                Err(e) => {
                    error!("Failed to initialize inference engine: {:?}", e);
                    return Err(ModelError::SystemError);
                }
            };
        }
        
        // Load model into inference engine
        if let Some(engine) = engine_guard.as_mut() {
            if let Err(e) = engine.load_model(&model_info) {
                error!("Failed to load model {}: {:?}", model_id, e);
                return Err(ModelError::SystemError);
            }
        } else {
            return Err(ModelError::SystemError);
        }
        
        // Update model status
        {
            let mut statuses = self.model_status.write().unwrap();
            statuses.insert(model_id.to_string(), ModelStatus::Available);
        }
        
        Ok(())
    }
    
    /// Process a message with the local model
    async fn process_message(&self, model_id: &str, message: &Message) -> Result<String, ModelError> {
        // Load model if needed
        self.load_model(model_id).await?;
        
        // Extract text from message
        let mut prompt = String::new();
        
        // Process message content
        for part in &message.content.parts {
            match part {
                ContentType::Text { text } => {
                    prompt.push_str(text);
                }
                _ => {
                    // Local models only support text input for now
                    warn!("Local model only supports text input");
                }
            }
        }
        
        // Generate response using inference engine
        let engine_guard = self.inference_engine.lock().unwrap();
        if let Some(engine) = engine_guard.as_ref() {
            match engine.generate(&prompt) {
                Ok(response) => Ok(response),
                Err(e) => {
                    error!("Inference error: {:?}", e);
                    Err(ModelError::SystemError)
                }
            }
        } else {
            Err(ModelError::SystemError)
        }
    }
    
    /// Process a streaming message with the local model
    async fn process_streaming(
        &self,
        model_id: &str,
        message: &Message,
        tx: mpsc::Sender<Result<Message, MessageError>>,
    ) -> Result<(), ModelError> {
        // Load model if needed
        self.load_model(model_id).await?;
        
        // Extract text from message
        let mut prompt = String::new();
        
        // Process message content
        for part in &message.content.parts {
            match part {
                ContentType::Text { text } => {
                    prompt.push_str(text);
                }
                _ => {
                    // Local models only support text input for now
                    warn!("Local model only supports text input");
                }
            }
        }
        
        // Generate streaming response using inference engine
        let engine_guard = self.inference_engine.lock().unwrap();
        if let Some(engine) = engine_guard.as_ref() {
            let mut accumulated_text = String::new();
            let response_id = Uuid::new_v4().to_string();
            
            match engine.generate_streaming(&prompt, 512, |token| {
                // Accumulate text
                accumulated_text.push_str(token);
                
                // Send partial message
                let message = Message {
                    id: response_id.clone(),
                    role: MessageRole::Assistant,
                    content: MessageContent {
                        parts: vec![ContentType::Text {
                            text: accumulated_text.clone(),
                        }],
                    },
                    metadata: Some(HashMap::from([(
                        "model".to_string(),
                        serde_json::to_value(model_id).unwrap(),
                    )])),
                    created_at: SystemTime::now(),
                };
                
                // Send token - ignore errors as the receiver might be dropped
                let _ = tx.blocking_send(Ok(message));
                
                // Continue generating
                true
            }) {
                Ok(_) => {
                    // Send final message with complete text
                    let final_message = Message {
                        id: response_id,
                        role: MessageRole::Assistant,
                        content: MessageContent {
                            parts: vec![ContentType::Text {
                                text: accumulated_text,
                            }],
                        },
                        metadata: Some(HashMap::from([(
                            "model".to_string(),
                            serde_json::to_value(model_id).unwrap(),
                        )])),
                        created_at: SystemTime::now(),
                    };
                    
                    let _ = tx.blocking_send(Ok(final_message));
                    Ok(())
                }
                Err(e) => {
                    error!("Inference error: {:?}", e);
                    Err(ModelError::SystemError)
                }
            }
        } else {
            Err(ModelError::SystemError)
        }
    }
}

#[async_trait]
impl ModelProvider for LocalProvider {
    fn provider_type(&self) -> ProviderType {
        ProviderType::Local
    }
    
    fn name(&self) -> &str {
        &self.config.name
    }
    
    fn config(&self) -> &ModelProviderConfig {
        &self.config
    }
    
    async fn available_models(&self) -> Result<Vec<Model>, ModelError> {
        // Return cached model list
        let models = self.models.read().unwrap();
        let result = models.iter().map(|m| m.model.clone()).collect();
        Ok(result)
    }
    
    async fn is_available(&self, model_id: &str) -> bool {
        // Check if model exists in available models
        let models = self.models.read().unwrap();
        let model = models.iter().find(|m| m.id == model_id);
        
        match model {
            Some(model_info) => model_info.is_downloaded,
            None => false,
        }
    }
    
    async fn model_status(&self, model_id: &str) -> ModelStatus {
        // Check cached status
        {
            let statuses = self.model_status.read().unwrap();
            if let Some(status) = statuses.get(model_id) {
                return *status;
            }
        }
        
        // Find model
        let models = self.models.read().unwrap();
        let model = models.iter().find(|m| m.id == model_id);
        
        match model {
            Some(model_info) => {
                let status = if model_info.is_downloaded {
                    ModelStatus::Available
                } else {
                    ModelStatus::Unavailable
                };
                
                // Update cache
                {
                    let mut statuses = self.model_status.write().unwrap();
                    statuses.insert(model_id.to_string(), status);
                }
                
                status
            }
            None => {
                // Update cache
                {
                    let mut statuses = self.model_status.write().unwrap();
                    statuses.insert(model_id.to_string(), ModelStatus::Unavailable);
                }
                
                ModelStatus::Unavailable
            }
        }
    }
    
    async fn complete(&self, model_id: &str, message: Message) -> Result<Message, MessageError> {
        // Check if model is available
        if !self.is_available(model_id).await {
            let downloadable = {
                let models = self.models.read().unwrap();
                models
                    .iter()
                    .find(|m| m.id == model_id)
                    .map(|m| m.download_url.is_some())
                    .unwrap_or(false)
            };
            
            if downloadable {
                // Try to download the model
                match self.download_model(model_id).await {
                    Ok(_) => {
                        // Continue with completion
                    }
                    Err(e) => {
                        return Err(MessageError::ProtocolError(format!(
                            "Failed to download model {}: {:?}",
                            model_id, e
                        )));
                    }
                }
            } else {
                return Err(MessageError::ProtocolError(format!(
                    "Model {} is not available",
                    model_id
                )));
            }
        }
        
        // Process message
        match self.process_message(model_id, &message).await {
            Ok(response_text) => {
                // Create response message
                let response = Message {
                    id: Uuid::new_v4().to_string(),
                    role: MessageRole::Assistant,
                    content: MessageContent {
                        parts: vec![ContentType::Text {
                            text: response_text,
                        }],
                    },
                    metadata: Some(HashMap::from([(
                        "model".to_string(),
                        serde_json::to_value(model_id).unwrap(),
                    )])),
                    created_at: SystemTime::now(),
                };
                
                Ok(response)
            }
            Err(e) => Err(MessageError::ProtocolError(format!(
                "Failed to process message: {:?}",
                e
            ))),
        }
    }
    
    async fn stream(
        &self,
        model_id: &str,
        message: Message,
    ) -> Result<mpsc::Receiver<Result<Message, MessageError>>, MessageError> {
        // Check if model is available
        if !self.is_available(model_id).await {
            let downloadable = {
                let models = self.models.read().unwrap();
                models
                    .iter()
                    .find(|m| m.id == model_id)
                    .map(|m| m.download_url.is_some())
                    .unwrap_or(false)
            };
            
            if downloadable {
                // Try to download the model
                match self.download_model(model_id).await {
                    Ok(_) => {
                        // Continue with streaming
                    }
                    Err(e) => {
                        return Err(MessageError::ProtocolError(format!(
                            "Failed to download model {}: {:?}",
                            model_id, e
                        )));
                    }
                }
            } else {
                return Err(MessageError::ProtocolError(format!(
                    "Model {} is not available",
                    model_id
                )));
            }
        }
        
        // Create streaming channel
        let (tx, rx) = mpsc::channel(32);
        let stream_id = Uuid::new_v4().to_string();
        
        // Store streaming channel
        {
            let mut streams = self.active_streams.lock().unwrap();
            streams.insert(stream_id.clone(), tx.clone());
        }
        
        // Start streaming in background
        let self_clone = self.clone();
        let message_clone = message.clone();
        let model_id_clone = model_id.to_string();
        
        tokio::spawn(async move {
            if let Err(e) = self_clone.process_streaming(&model_id_clone, &message_clone, tx.clone()).await {
                // Send error
                let _ = tx
                    .send(Err(MessageError::ProtocolError(format!(
                        "Streaming error: {:?}",
                        e
                    ))))
                    .await;
            }
            
            // Remove streaming channel when done
            let mut streams = self_clone.active_streams.lock().unwrap();
            streams.remove(&stream_id);
        });
        
        Ok(rx)
    }
    
    async fn cancel_stream(&self, stream_id: &str) -> Result<(), MessageError> {
        // Find streaming channel
        let tx_opt = {
            let mut streams = self.active_streams.lock().unwrap();
            streams.remove(stream_id)
        };
        
        if tx_opt.is_some() {
            // Channel is dropped, stream will be cancelled when the task notices
            Ok(())
        } else {
            Err(MessageError::Unknown(format!(
                "Stream {} not found",
                stream_id
            )))
        }
    }
}

impl Clone for LocalProvider {
    fn clone(&self) -> Self {
        Self {
            config: self.config.clone(),
            inference_engine: self.inference_engine.clone(),
            models: self.models.clone(),
            model_status: self.model_status.clone(),
            active_streams: self.active_streams.clone(),
            model_dir: self.model_dir.clone(),
        }
    }
}
</file>

<file path="src/ai/local/models.rs">
use crate::models::Model;
use std::path::PathBuf;

/// Information about a local model
#[derive(Debug, Clone)]
pub struct LocalModelInfo {
    /// Model ID
    pub id: String,
    
    /// Model name
    pub name: String,
    
    /// Path to model file
    pub path: PathBuf,
    
    /// Number of parameters
    pub parameters: u64,
    
    /// Quantization type
    pub quantization: String,
    
    /// Context size (max tokens)
    pub context_size: usize,
    
    /// Whether the model is downloaded
    pub is_downloaded: bool,
    
    /// URL to download the model
    pub download_url: Option<String>,
    
    /// Model metadata
    pub model: Model,
}
</file>

<file path="src/ai/mod.rs">
pub mod claude;
pub mod local;
pub mod router;

use crate::models::messages::{Message, MessageError};
use crate::models::Model;
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::fmt;
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::mpsc;

/// AI provider type
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum ProviderType {
    /// Claude AI (Anthropic)
    Claude,
    
    /// Local model
    Local,
    
    /// Custom provider
    Custom,
}

impl fmt::Display for ProviderType {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        match self {
            ProviderType::Claude => write!(f, "Claude"),
            ProviderType::Local => write!(f, "Local"),
            ProviderType::Custom => write!(f, "Custom"),
        }
    }
}

/// AI model status
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ModelStatus {
    /// Model is available
    Available,
    
    /// Model is being loaded
    Loading,
    
    /// Model is not available
    Unavailable,
    
    /// Error occurred while loading or using the model
    Error(ModelError),
}

/// Model error type
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ModelError {
    /// Network error
    NetworkError,
    
    /// Authentication error
    AuthError,
    
    /// Rate limit error
    RateLimitError,
    
    /// Model overloaded
    ModelOverloaded,
    
    /// Context length exceeded
    ContextLengthExceeded,
    
    /// Content filtered
    ContentFiltered,
    
    /// Invalid request
    InvalidRequest,
    
    /// System error
    SystemError,
    
    /// Not implemented
    NotImplemented,
    
    /// Unknown error
    Unknown,
}

/// Model provider configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelProviderConfig {
    /// Provider type
    pub provider_type: ProviderType,
    
    /// Provider name
    pub name: String,
    
    /// API base URL
    pub base_url: String,
    
    /// API key
    pub api_key: String,
    
    /// Organization ID
    pub organization_id: Option<String>,
    
    /// Request timeout
    pub timeout: Duration,
    
    /// Default model
    pub default_model: String,
    
    /// Fallback model
    pub fallback_model: Option<String>,
    
    /// Enable MCP protocol
    pub enable_mcp: bool,
    
    /// Enable streaming
    pub enable_streaming: bool,
    
    /// Additional settings
    pub settings: serde_json::Map<String, serde_json::Value>,
}

impl Default for ModelProviderConfig {
    fn default() -> Self {
        Self {
            provider_type: ProviderType::Claude,
            name: "Claude".to_string(),
            base_url: "https://api.anthropic.com".to_string(),
            api_key: String::new(),
            organization_id: None,
            timeout: Duration::from_secs(120),
            default_model: "claude-3-opus-20240229".to_string(),
            fallback_model: Some("claude-3-haiku-20240307".to_string()),
            enable_mcp: true,
            enable_streaming: true,
            settings: serde_json::Map::new(),
        }
    }
}

/// Model callback for streaming responses
pub type ModelCallback = Box<dyn Fn(Message) -> () + Send + Sync>;

/// Base trait for AI model providers
#[async_trait]
pub trait ModelProvider: Send + Sync {
    /// Get provider type
    fn provider_type(&self) -> ProviderType;
    
    /// Get provider name
    fn name(&self) -> &str;
    
    /// Get provider configuration
    fn config(&self) -> &ModelProviderConfig;
    
    /// Get available models
    async fn available_models(&self) -> Result<Vec<Model>, ModelError>;
    
    /// Check if model is available
    async fn is_available(&self, model_id: &str) -> bool;
    
    /// Get model status
    async fn model_status(&self, model_id: &str) -> ModelStatus;
    
    /// Complete a message (synchronous)
    async fn complete(&self, model_id: &str, message: Message) -> Result<Message, MessageError>;
    
    /// Stream a message (asynchronous)
    async fn stream(&self, model_id: &str, message: Message) 
        -> Result<mpsc::Receiver<Result<Message, MessageError>>, MessageError>;
    
    /// Cancel a streaming message
    async fn cancel_stream(&self, stream_id: &str) -> Result<(), MessageError>;
    
    /// Check if provider supports a feature
    fn supports_feature(&self, feature: &str) -> bool {
        match feature {
            "streaming" => self.config().enable_streaming,
            "mcp" => self.config().enable_mcp,
            _ => false,
        }
    }
}

/// Get all available model providers
pub fn get_all_providers() -> Vec<Arc<dyn ModelProvider>> {
    let mut providers = Vec::new();
    
    // Claude provider
    if let Ok(claude_provider) = claude::ClaudeProvider::new() {
        providers.push(Arc::new(claude_provider) as Arc<dyn ModelProvider>);
    }
    
    // Local provider
    if let Ok(local_provider) = local::LocalProvider::new() {
        providers.push(Arc::new(local_provider) as Arc<dyn ModelProvider>);
    }
    
    providers
}
</file>

<file path="src/ai/router/mod.rs">
use crate::ai::{get_all_providers, ModelError, ModelProvider, ModelProviderConfig, ModelStatus, ProviderType};
use crate::models::messages::{Message, MessageError};
use crate::models::Model;
use crate::utils::config;
use crate::utils::events::{events, get_event_system};
use async_trait::async_trait;
use log::{debug, error, info, warn};
use std::collections::HashMap;
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, SystemTime};
use tokio::sync::mpsc;

/// Model router for switching between providers
pub struct ModelRouter {
    /// Available providers
    providers: RwLock<Vec<Arc<dyn ModelProvider>>>,
    
    /// Provider selection strategy
    strategy: RouterStrategy,
    
    /// Network status
    network_status: Arc<RwLock<NetworkStatus>>,
    
    /// Model routing rules
    routing_rules: Arc<RwLock<HashMap<String, RoutingRule>>>,
    
    /// Default provider
    default_provider: Arc<RwLock<Option<Arc<dyn ModelProvider>>>>,
    
    /// Fallback provider
    fallback_provider: Arc<RwLock<Option<Arc<dyn ModelProvider>>>>,
}

/// Network status for determining connection availability
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum NetworkStatus {
    /// Network is connected
    Connected,
    
    /// Network is disconnected
    Disconnected,
    
    /// Network connection is unstable
    Unstable,
    
    /// Network status is unknown
    Unknown,
}

/// Provider selection strategy
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum RouterStrategy {
    /// Prefer online providers (default)
    PreferOnline,
    
    /// Prefer local providers
    PreferLocal,
    
    /// Use only online providers
    OnlineOnly,
    
    /// Use only local providers
    LocalOnly,
    
    /// Round-robin between providers
    RoundRobin,
    
    /// Use specific routing rules
    RulesBased,
}

/// Routing rule for model selection
#[derive(Debug, Clone)]
pub struct RoutingRule {
    /// Model ID pattern to match
    pub model_pattern: String,
    
    /// Provider type to use
    pub provider_type: ProviderType,
    
    /// Fallback provider type
    pub fallback_provider_type: Option<ProviderType>,
    
    /// Fallback model ID
    pub fallback_model_id: Option<String>,
    
    /// Timeout before falling back
    pub timeout: Duration,
    
    /// Whether to fail if provider is unavailable
    pub fail_if_unavailable: bool,
}

impl ModelRouter {
    /// Create a new model router
    pub fn new() -> Self {
        // Get all available providers
        let providers = get_all_providers();
        
        // Set default and fallback providers
        let default_provider = providers
            .iter()
            .find(|p| p.provider_type() == ProviderType::Claude)
            .cloned();
        
        let fallback_provider = providers
            .iter()
            .find(|p| p.provider_type() == ProviderType::Local)
            .cloned();
        
        Self {
            providers: RwLock::new(providers),
            strategy: RouterStrategy::PreferOnline,
            network_status: Arc::new(RwLock::new(NetworkStatus::Unknown)),
            routing_rules: Arc::new(RwLock::new(HashMap::new())),
            default_provider: Arc::new(RwLock::new(default_provider)),
            fallback_provider: Arc::new(RwLock::new(fallback_provider)),
        }
    }
    
    /// Set router strategy
    pub fn set_strategy(&self, strategy: RouterStrategy) {
        self.strategy = strategy;
    }
    
    /// Set network status
    pub fn set_network_status(&self, status: NetworkStatus) {
        let mut network_status = self.network_status.write().unwrap();
        *network_status = status;
        
        // Notify status change
        get_event_system().emit(
            events::NETWORK_STATUS_CHANGED,
            serde_json::json!({
                "status": match status {
                    NetworkStatus::Connected => "connected",
                    NetworkStatus::Disconnected => "disconnected",
                    NetworkStatus::Unstable => "unstable",
                    NetworkStatus::Unknown => "unknown",
                }
            }),
        );
    }
    
    /// Add a routing rule
    pub fn add_routing_rule(&self, model_pattern: &str, rule: RoutingRule) {
        let mut rules = self.routing_rules.write().unwrap();
        rules.insert(model_pattern.to_string(), rule);
    }
    
    /// Remove a routing rule
    pub fn remove_routing_rule(&self, model_pattern: &str) {
        let mut rules = self.routing_rules.write().unwrap();
        rules.remove(model_pattern);
    }
    
    /// Get all available providers
    pub fn get_providers(&self) -> Vec<Arc<dyn ModelProvider>> {
        self.providers.read().unwrap().clone()
    }
    
    /// Get provider by type
    pub fn get_provider_by_type(&self, provider_type: ProviderType) -> Option<Arc<dyn ModelProvider>> {
        self.providers
            .read()
            .unwrap()
            .iter()
            .find(|p| p.provider_type() == provider_type)
            .cloned()
    }
    
    /// Check if network is available
    pub fn is_network_available(&self) -> bool {
        let status = self.network_status.read().unwrap();
        *status == NetworkStatus::Connected || *status == NetworkStatus::Unstable
    }
    
    /// Select provider for a model
    pub fn select_provider_for_model(&self, model_id: &str) -> Option<(Arc<dyn ModelProvider>, String)> {
        // Check routing rules first
        let rules = self.routing_rules.read().unwrap();
        for (pattern, rule) in rules.iter() {
            if model_id.starts_with(pattern) {
                // Find provider of the specified type
                if let Some(provider) = self.get_provider_by_type(rule.provider_type) {
                    return Some((provider, model_id.to_string()));
                }
                
                // If provider not found and fallback is specified, try fallback
                if let Some(fallback_type) = rule.fallback_provider_type {
                    if let Some(fallback_provider) = self.get_provider_by_type(fallback_type) {
                        // Use fallback model ID if specified, otherwise use original
                        let fallback_model_id = rule.fallback_model_id.clone().unwrap_or_else(|| model_id.to_string());
                        return Some((fallback_provider, fallback_model_id));
                    }
                }
                
                // No provider found, return None
                return None;
            }
        }
        
        // If no rule matched, use strategy-based selection
        match self.strategy {
            RouterStrategy::PreferOnline => {
                // Check if network is available
                if self.is_network_available() {
                    // Try to find a cloud provider that supports this model
                    for provider in self.providers.read().unwrap().iter() {
                        if provider.provider_type() != ProviderType::Local {
                            return Some((provider.clone(), model_id.to_string()));
                        }
                    }
                }
                
                // Fallback to local provider
                if let Some(fallback) = self.fallback_provider.read().unwrap().clone() {
                    return Some((fallback, model_id.to_string()));
                }
            }
            RouterStrategy::PreferLocal => {
                // Try to find a local provider first
                for provider in self.providers.read().unwrap().iter() {
                    if provider.provider_type() == ProviderType::Local {
                        return Some((provider.clone(), model_id.to_string()));
                    }
                }
                
                // Fallback to cloud provider if network is available
                if self.is_network_available() {
                    if let Some(default) = self.default_provider.read().unwrap().clone() {
                        return Some((default, model_id.to_string()));
                    }
                }
            }
            RouterStrategy::OnlineOnly => {
                // Only use cloud providers
                if self.is_network_available() {
                    for provider in self.providers.read().unwrap().iter() {
                        if provider.provider_type() != ProviderType::Local {
                            return Some((provider.clone(), model_id.to_string()));
                        }
                    }
                }
            }
            RouterStrategy::LocalOnly => {
                // Only use local providers
                for provider in self.providers.read().unwrap().iter() {
                    if provider.provider_type() == ProviderType::Local {
                        return Some((provider.clone(), model_id.to_string()));
                    }
                }
            }
            RouterStrategy::RoundRobin => {
                // Simple round-robin: just use the default provider for now
                // In a real implementation, this would rotate between providers
                if let Some(default) = self.default_provider.read().unwrap().clone() {
                    return Some((default, model_id.to_string()));
                }
            }
            RouterStrategy::RulesBased => {
                // We already checked rules, so this is a fallback
                if let Some(default) = self.default_provider.read().unwrap().clone() {
                    return Some((default, model_id.to_string()));
                }
            }
        }
        
        // No suitable provider found
        None
    }
    
    /// Get available models from all providers
    pub async fn get_available_models(&self) -> Vec<Model> {
        let mut models = Vec::new();
        
        for provider in self.providers.read().unwrap().iter() {
            match provider.available_models().await {
                Ok(provider_models) => {
                    models.extend(provider_models);
                }
                Err(e) => {
                    warn!("Failed to get models from provider {}: {:?}", provider.name(), e);
                }
            }
        }
        
        models
    }
    
    /// Complete a message with the appropriate model
    pub async fn complete(&self, model_id: &str, message: Message) -> Result<Message, MessageError> {
        // Select provider
        let (provider, final_model_id) = self
            .select_provider_for_model(model_id)
            .ok_or_else(|| MessageError::ProtocolError(format!("No provider found for model {}", model_id)))?;
        
        // Complete with selected provider
        provider.complete(&final_model_id, message).await
    }
    
    /// Stream a message with the appropriate model
    pub async fn stream(
        &self,
        model_id: &str,
        message: Message,
    ) -> Result<mpsc::Receiver<Result<Message, MessageError>>, MessageError> {
        // Select provider
        let (provider, final_model_id) = self
            .select_provider_for_model(model_id)
            .ok_or_else(|| MessageError::ProtocolError(format!("No provider found for model {}", model_id)))?;
        
        // Stream with selected provider
        provider.stream(&final_model_id, message).await
    }
    
    /// Cancel a streaming message
    pub async fn cancel_stream(&self, stream_id: &str) -> Result<(), MessageError> {
        // Try cancelling with all providers
        // In a real implementation, we would track which provider is handling which stream
        for provider in self.providers.read().unwrap().iter() {
            let _ = provider.cancel_stream(stream_id).await;
        }
        
        Ok(())
    }
}

/// Global model router instance
static MODEL_ROUTER: once_cell::sync::OnceCell<ModelRouter> = once_cell::sync::OnceCell::new();

/// Get the global model router instance
pub fn get_model_router() -> &'static ModelRouter {
    MODEL_ROUTER.get_or_init(|| ModelRouter::new())
}
</file>

<file path="src/auto_update/mod.rs">
use serde::{Deserialize, Serialize};
use tauri::{AppHandle, Manager, Wry};
use std::time::{Duration, SystemTime};
use reqwest::Client;
use log::{info, error, warn};
use std::sync::{Arc, Mutex};
use std::path::PathBuf;

/// Configuration for the auto-updater
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UpdaterConfig {
    /// Whether auto-updates are enabled
    pub enabled: bool,
    /// How often to check for updates (in hours)
    pub check_interval: u64,
    /// When the last check was performed
    pub last_check: Option<SystemTime>,
    /// Whether to download updates automatically
    pub auto_download: bool,
    /// Whether to install updates automatically
    pub auto_install: bool,
}

impl Default for UpdaterConfig {
    fn default() -> Self {
        Self {
            enabled: true,
            check_interval: 24, // Check once per day by default
            last_check: None,
            auto_download: true,
            auto_install: false,
        }
    }
}

/// Update information returned from the server
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UpdateInfo {
    pub version: String,
    pub notes: String,
    pub pub_date: String,
    pub url: String,
    pub signature: String,
}

/// Manager for handling application updates
pub struct UpdateManager {
    config: Arc<Mutex<UpdaterConfig>>,
    client: Client,
    app: AppHandle<Wry>,
    config_path: PathBuf,
}

impl UpdateManager {
    /// Create a new update manager
    pub fn new(app: AppHandle<Wry>) -> Self {
        let config_path = app
            .path_resolver()
            .app_config_dir()
            .unwrap()
            .join("updater_config.json");
        
        let config = if config_path.exists() {
            match std::fs::read_to_string(&config_path) {
                Ok(content) => match serde_json::from_str(&content) {
                    Ok(config) => config,
                    Err(e) => {
                        error!("Failed to parse updater config: {}", e);
                        UpdaterConfig::default()
                    }
                },
                Err(e) => {
                    error!("Failed to read updater config: {}", e);
                    UpdaterConfig::default()
                }
            }
        } else {
            UpdaterConfig::default()
        };
        
        let client = Client::builder()
            .timeout(Duration::from_secs(30))
            .build()
            .unwrap();

        Self {
            config: Arc::new(Mutex::new(config)),
            client,
            app,
            config_path,
        }
    }

    /// Start the update checker
    pub async fn start(&self) {
        // Save default config if it doesn't exist
        if !self.config_path.exists() {
            self.save_config().await;
        }

        let config = self.config.lock().unwrap().clone();
        if !config.enabled {
            info!("Auto-updates are disabled");
            return;
        }

        // Check if it's time to check for updates
        let should_check = match config.last_check {
            Some(last_check) => {
                match SystemTime::now().duration_since(last_check) {
                    Ok(duration) => duration.as_secs() >= config.check_interval * 3600,
                    Err(_) => true, // System time error, check anyway
                }
            }
            None => true, // First time checking
        };

        if should_check {
            self.check_for_updates().await;
        }
    }

    /// Check for available updates
    pub async fn check_for_updates(&self) {
        info!("Checking for updates...");
        
        // Update last check time
        {
            let mut config = self.config.lock().unwrap();
            config.last_check = Some(SystemTime::now());
        }
        self.save_config().await;

        // Use built-in Tauri updater to check for updates
        match tauri::updater::builder(self.app.clone()).check().await {
            Ok(update) => {
                if update.is_update_available() {
                    info!("Update available: {}", update.latest_version());
                    
                    let config = self.config.lock().unwrap().clone();
                    if config.auto_download {
                        // Emit event to notify frontend about available update
                        self.app.emit_all("update-available", update.latest_version()).unwrap();
                        
                        if config.auto_install {
                            info!("Auto-installing update...");
                            self.app.updater().unwrap().install().await.unwrap();
                        } else {
                            // Show update dialog
                            self.app.updater().unwrap().show().await.unwrap();
                        }
                    } else {
                        // Just notify about available update
                        self.app.emit_all("update-available", update.latest_version()).unwrap();
                    }
                } else {
                    info!("No updates available");
                }
            }
            Err(e) => {
                error!("Failed to check for updates: {}", e);
                self.app.emit_all("update-check-error", e.to_string()).unwrap();
            }
        }
    }

    /// Save the current updater configuration to disk
    async fn save_config(&self) {
        let config = self.config.lock().unwrap().clone();
        
        // Create parent directories if they don't exist
        if let Some(parent) = self.config_path.parent() {
            if !parent.exists() {
                std::fs::create_dir_all(parent).unwrap();
            }
        }
        
        match serde_json::to_string_pretty(&config) {
            Ok(json) => {
                if let Err(e) = std::fs::write(&self.config_path, json) {
                    error!("Failed to save updater config: {}", e);
                }
            }
            Err(e) => {
                error!("Failed to serialize updater config: {}", e);
            }
        }
    }

    /// Update the updater configuration
    pub async fn update_config(&self, new_config: UpdaterConfig) {
        {
            let mut config = self.config.lock().unwrap();
            *config = new_config;
        }
        self.save_config().await;
    }

    /// Get the current updater configuration
    pub fn get_config(&self) -> UpdaterConfig {
        self.config.lock().unwrap().clone()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::time::{Duration, SystemTime};

    #[test]
    fn test_updater_config_default() {
        let config = UpdaterConfig::default();
        assert!(config.enabled);
        assert_eq!(config.check_interval, 24);
        assert!(config.auto_download);
        assert!(!config.auto_install);
    }

    #[test]
    fn test_updater_config_serialization() {
        let config = UpdaterConfig {
            enabled: true,
            check_interval: 12,
            last_check: Some(SystemTime::now()),
            auto_download: false,
            auto_install: true,
        };
        
        let json = serde_json::to_string(&config).unwrap();
        let deserialized: UpdaterConfig = serde_json::from_str(&json).unwrap();
        
        assert_eq!(config.enabled, deserialized.enabled);
        assert_eq!(config.check_interval, deserialized.check_interval);
        assert_eq!(config.auto_download, deserialized.auto_download);
        assert_eq!(config.auto_install, deserialized.auto_install);
    }
}
</file>

<file path="src/collaboration/mod.rs">
// Collaboration System for MCP Client
//
// This module provides real-time collaboration features including:
// - Real-time synchronization of conversation data
// - Cursor presence and user awareness
// - Session management for multi-device usage
// - Cross-device synchronization
// - Infrastructure for audio/video communication

pub mod presence;
pub mod rtc;
pub mod sessions;
pub mod sync;

use std::collections::HashMap;
use std::sync::{Arc, RwLock};
use std::time::{Duration, Instant, SystemTime};

use log::{debug, info, warn, error};
use serde::{Serialize, Deserialize};
use uuid::Uuid;

use crate::error::Result;
use crate::models::messages::{Conversation, Message};
use crate::observability::metrics::{record_counter, record_gauge};
use crate::security::permissions;

/// Collaboration configuration options
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CollaborationConfig {
    /// Whether collaboration features are enabled
    pub enabled: bool,
    
    /// Maximum number of users per collaboration session
    pub max_users_per_session: usize,
    
    /// Whether to automatically discover other devices
    pub auto_discover: bool,
    
    /// Whether to show presence information (cursors, etc.)
    pub show_presence: bool,
    
    /// Whether to enable audio/video capabilities
    pub enable_av: bool,
    
    /// Sync interval in milliseconds
    pub sync_interval_ms: u64,
    
    /// P2P mode (direct connections between clients when possible)
    pub p2p_enabled: bool,
    
    /// Server URLs for signaling and STUN/TURN
    pub server_urls: Vec<String>,
    
    /// Custom username for collaboration
    pub username: Option<String>,
    
    /// Custom user avatar (base64 encoded)
    pub user_avatar: Option<String>,
}

impl Default for CollaborationConfig {
    fn default() -> Self {
        Self {
            enabled: true,
            max_users_per_session: 10,
            auto_discover: true,
            show_presence: true,
            enable_av: false, // Disabled by default
            sync_interval_ms: 1000, // 1 second
            p2p_enabled: true,
            server_urls: vec![
                "https://signaling.mcp-client.com".to_string(),
                "stun:stun.mcp-client.com:19302".to_string(),
                "turn:turn.mcp-client.com:3478".to_string(),
            ],
            username: None,
            user_avatar: None,
        }
    }
}

/// User role in a collaboration session
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum UserRole {
    /// Owner of the session (has all privileges)
    Owner,
    
    /// Co-owner with administrative privileges
    CoOwner,
    
    /// Editor can make changes but not manage users
    Editor,
    
    /// Commentator can only add comments
    Commentator,
    
    /// Viewer can only view but not edit
    Viewer,
}

/// User information for collaboration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct User {
    /// Unique ID for the user
    pub id: String,
    
    /// Display name
    pub name: String,
    
    /// User role
    pub role: UserRole,
    
    /// User avatar (base64 encoded)
    pub avatar: Option<String>,
    
    /// User's color for cursor and other UI elements
    pub color: String,
    
    /// Online status
    pub online: bool,
    
    /// Last active timestamp
    pub last_active: SystemTime,
    
    /// Current device ID
    pub device_id: String,
    
    /// Custom user metadata
    pub metadata: HashMap<String, String>,
}

/// A collaborative session
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Session {
    /// Unique ID for the session
    pub id: String,
    
    /// Session name
    pub name: String,
    
    /// Whether the session is active
    pub active: bool,
    
    /// Session creation time
    pub created_at: SystemTime,
    
    /// Last update time
    pub updated_at: SystemTime,
    
    /// Conversation ID associated with this session
    pub conversation_id: String,
    
    /// Users in the session
    pub users: HashMap<String, User>,
    
    /// Custom session metadata
    pub metadata: HashMap<String, String>,
}

/// Connection status for collaboration
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum ConnectionStatus {
    /// Not connected to collaboration services
    Disconnected,
    
    /// Currently connecting
    Connecting,
    
    /// Connected and ready
    Connected,
    
    /// Connected with limited functionality
    Limited,
    
    /// Connection error
    Error,
}

/// Collaboration manager
pub struct CollaborationManager {
    /// Configuration
    config: Arc<RwLock<CollaborationConfig>>,
    
    /// Current user
    current_user: Arc<RwLock<User>>,
    
    /// Active sessions
    sessions: Arc<RwLock<HashMap<String, Session>>>,
    
    /// Current session ID
    current_session_id: Arc<RwLock<Option<String>>>,
    
    /// Connection status
    connection_status: Arc<RwLock<ConnectionStatus>>,
    
    /// Session manager
    session_manager: Arc<RwLock<sessions::SessionManager>>,
    
    /// Presence manager
    presence_manager: Arc<RwLock<presence::PresenceManager>>,
    
    /// Sync manager
    sync_manager: Arc<RwLock<sync::SyncManager>>,
    
    /// RTC manager for audio/video
    rtc_manager: Arc<RwLock<rtc::RTCManager>>,
}

impl CollaborationManager {
    /// Create a new collaboration manager
    pub fn new(config: CollaborationConfig) -> Result<Self> {
        // Generate a unique user ID if none exists yet
        let user_id = Uuid::new_v4().to_string();
        
        // Generate a unique device ID
        let device_id = format!("{}-{}", whoami::hostname(), Uuid::new_v4().to_string());
        
        // Create default user
        let username = config.username.clone().unwrap_or_else(|| {
            whoami::username()
        });
        
        // Create current user
        let current_user = User {
            id: user_id.clone(),
            name: username,
            role: UserRole::Owner, // Default to owner for the local user
            avatar: config.user_avatar.clone(),
            color: generate_user_color(&user_id),
            online: true,
            last_active: SystemTime::now(),
            device_id: device_id.clone(),
            metadata: HashMap::new(),
        };
        
        // Create managers
        let session_manager = sessions::SessionManager::new(
            user_id.clone(), 
            device_id.clone(),
            config.server_urls.clone(),
        )?;
        
        let presence_manager = presence::PresenceManager::new(
            user_id.clone(),
            device_id.clone(),
            config.show_presence,
        )?;
        
        let sync_manager = sync::SyncManager::new(
            user_id.clone(),
            device_id.clone(),
            config.sync_interval_ms,
        )?;
        
        let rtc_manager = rtc::RTCManager::new(
            user_id,
            device_id,
            config.enable_av,
            config.server_urls.clone(),
        )?;
        
        Ok(Self {
            config: Arc::new(RwLock::new(config)),
            current_user: Arc::new(RwLock::new(current_user)),
            sessions: Arc::new(RwLock::new(HashMap::new())),
            current_session_id: Arc::new(RwLock::new(None)),
            connection_status: Arc::new(RwLock::new(ConnectionStatus::Disconnected)),
            session_manager: Arc::new(RwLock::new(session_manager)),
            presence_manager: Arc::new(RwLock::new(presence_manager)),
            sync_manager: Arc::new(RwLock::new(sync_manager)),
            rtc_manager: Arc::new(RwLock::new(rtc_manager)),
        })
    }
    
    /// Initialize the collaboration system and start services
    pub fn start(&self) -> Result<()> {
        // Check if collaboration is enabled
        if !self.config.read().unwrap().enabled {
            info!("Collaboration system is disabled");
            return Ok(());
        }
        
        // Check for permissions
        let permission_granted = permissions::check_permission("collaboration")?;
        if !permission_granted {
            let permission_granted = permissions::request_permission(
                "collaboration", 
                "Enable real-time collaboration with other users",
            )?;
            
            if !permission_granted {
                warn!("Collaboration permission denied");
                return Ok(());
            }
        }
        
        // Update connection status
        *self.connection_status.write().unwrap() = ConnectionStatus::Connecting;
        
        // Start session manager
        self.session_manager.read().unwrap().start()?;
        
        // Start presence manager
        self.presence_manager.read().unwrap().start()?;
        
        // Start sync manager
        self.sync_manager.read().unwrap().start()?;
        
        // Start RTC manager if enabled
        if self.config.read().unwrap().enable_av {
            self.rtc_manager.read().unwrap().start()?;
        }
        
        // Update connection status
        *self.connection_status.write().unwrap() = ConnectionStatus::Connected;
        
        info!("Collaboration system started");
        record_counter("collaboration.system_started", 1.0, None);
        
        Ok(())
    }
    
    /// Stop collaboration services
    pub fn stop(&self) -> Result<()> {
        // Update connection status
        *self.connection_status.write().unwrap() = ConnectionStatus::Disconnected;
        
        // Stop session manager
        self.session_manager.read().unwrap().stop()?;
        
        // Stop presence manager
        self.presence_manager.read().unwrap().stop()?;
        
        // Stop sync manager
        self.sync_manager.read().unwrap().stop()?;
        
        // Stop RTC manager
        self.rtc_manager.read().unwrap().stop()?;
        
        info!("Collaboration system stopped");
        record_counter("collaboration.system_stopped", 1.0, None);
        
        Ok(())
    }
    
    /// Create a new collaborative session
    pub fn create_session(&self, name: &str, conversation_id: &str) -> Result<Session> {
        // Generate a new session ID
        let session_id = Uuid::new_v4().to_string();
        
        // Create a new session
        let mut users = HashMap::new();
        users.insert(
            self.current_user.read().unwrap().id.clone(),
            self.current_user.read().unwrap().clone(),
        );
        
        let session = Session {
            id: session_id.clone(),
            name: name.to_string(),
            active: true,
            created_at: SystemTime::now(),
            updated_at: SystemTime::now(),
            conversation_id: conversation_id.to_string(),
            users,
            metadata: HashMap::new(),
        };
        
        // Store the session
        self.sessions.write().unwrap().insert(session_id.clone(), session.clone());
        
        // Set as current session
        *self.current_session_id.write().unwrap() = Some(session_id.clone());
        
        // Initialize session in session manager
        self.session_manager.write().unwrap().create_session(&session_id, &session.name, conversation_id)?;
        
        // Initialize presence for this session
        self.presence_manager.write().unwrap().join_session(&session_id)?;
        
        // Initialize sync for this session
        self.sync_manager.write().unwrap().init_session(&session_id, conversation_id)?;
        
        info!("Created collaboration session: {}", session_id);
        record_counter("collaboration.session_created", 1.0, None);
        
        Ok(session)
    }
    
    /// Join an existing collaborative session
    pub fn join_session(&self, session_id: &str) -> Result<Session> {
        // Get session information from session manager
        let session_info = self.session_manager.write().unwrap().join_session(session_id)?;
        
        // Initialize presence for this session
        self.presence_manager.write().unwrap().join_session(session_id)?;
        
        // Initialize sync for this session
        self.sync_manager.write().unwrap().join_session(session_id, &session_info.conversation_id)?;
        
        // Update current session
        *self.current_session_id.write().unwrap() = Some(session_id.to_string());
        
        // Store session locally
        self.sessions.write().unwrap().insert(session_id.to_string(), session_info.clone());
        
        info!("Joined collaboration session: {}", session_id);
        record_counter("collaboration.session_joined", 1.0, None);
        
        Ok(session_info)
    }
    
    /// Leave the current collaborative session
    pub fn leave_session(&self) -> Result<()> {
        // Get current session ID
        let session_id = match *self.current_session_id.read().unwrap() {
            Some(ref id) => id.clone(),
            None => return Ok(()),  // No active session
        };
        
        // Leave session in managers
        self.session_manager.write().unwrap().leave_session(&session_id)?;
        self.presence_manager.write().unwrap().leave_session(&session_id)?;
        self.sync_manager.write().unwrap().leave_session(&session_id)?;
        
        // Clear current session
        *self.current_session_id.write().unwrap() = None;
        
        info!("Left collaboration session: {}", session_id);
        record_counter("collaboration.session_left", 1.0, None);
        
        Ok(())
    }
    
    /// Invite a user to the current session
    pub fn invite_user(&self, email: &str, role: UserRole) -> Result<()> {
        // Get current session ID
        let session_id = match *self.current_session_id.read().unwrap() {
            Some(ref id) => id.clone(),
            None => return Err("No active collaboration session".into()),
        };
        
        // Check if user has permission to invite
        let session = match self.sessions.read().unwrap().get(&session_id) {
            Some(session) => session.clone(),
            None => return Err("Session not found".into()),
        };
        
        let current_user_id = self.current_user.read().unwrap().id.clone();
        let current_user = match session.users.get(&current_user_id) {
            Some(user) => user,
            None => return Err("Current user not in session".into()),
        };
        
        // Only owners and co-owners can invite
        if current_user.role != UserRole::Owner && current_user.role != UserRole::CoOwner {
            return Err("You don't have permission to invite users".into());
        }
        
        // Invite the user via session manager
        self.session_manager.write().unwrap().invite_user(&session_id, email, role)?;
        
        info!("Invited user {} to session {}", email, session_id);
        record_counter("collaboration.user_invited", 1.0, None);
        
        Ok(())
    }
    
    /// Remove a user from the current session
    pub fn remove_user(&self, user_id: &str) -> Result<()> {
        // Get current session ID
        let session_id = match *self.current_session_id.read().unwrap() {
            Some(ref id) => id.clone(),
            None => return Err("No active collaboration session".into()),
        };
        
        // Check if user has permission to remove
        let session = match self.sessions.read().unwrap().get(&session_id) {
            Some(session) => session.clone(),
            None => return Err("Session not found".into()),
        };
        
        let current_user_id = self.current_user.read().unwrap().id.clone();
        let current_user = match session.users.get(&current_user_id) {
            Some(user) => user,
            None => return Err("Current user not in session".into()),
        };
        
        // Only owners and co-owners can remove users
        if current_user.role != UserRole::Owner && current_user.role != UserRole::CoOwner {
            return Err("You don't have permission to remove users".into());
        }
        
        // Can't remove yourself this way
        if user_id == &current_user_id {
            return Err("Can't remove yourself from session. Use leave_session instead.".into());
        }
        
        // Remove the user via session manager
        self.session_manager.write().unwrap().remove_user(&session_id, user_id)?;
        
        info!("Removed user {} from session {}", user_id, session_id);
        record_counter("collaboration.user_removed", 1.0, None);
        
        Ok(())
    }
    
    /// Change a user's role in the current session
    pub fn change_user_role(&self, user_id: &str, role: UserRole) -> Result<()> {
        // Get current session ID
        let session_id = match *self.current_session_id.read().unwrap() {
            Some(ref id) => id.clone(),
            None => return Err("No active collaboration session".into()),
        };
        
        // Check if user has permission to change roles
        let session = match self.sessions.read().unwrap().get(&session_id) {
            Some(session) => session.clone(),
            None => return Err("Session not found".into()),
        };
        
        let current_user_id = self.current_user.read().unwrap().id.clone();
        let current_user = match session.users.get(&current_user_id) {
            Some(user) => user,
            None => return Err("Current user not in session".into()),
        };
        
        // Only owners and co-owners can change roles
        if current_user.role != UserRole::Owner && current_user.role != UserRole::CoOwner {
            return Err("You don't have permission to change user roles".into());
        }
        
        // Change the user's role via session manager
        self.session_manager.write().unwrap().change_user_role(&session_id, user_id, role)?;
        
        // Update local session data
        if let Some(session) = self.sessions.write().unwrap().get_mut(&session_id) {
            if let Some(user) = session.users.get_mut(user_id) {
                user.role = role;
            }
        }
        
        info!("Changed role for user {} in session {}", user_id, session_id);
        record_counter("collaboration.role_changed", 1.0, None);
        
        Ok(())
    }
    
    /// Update user cursor position
    pub fn update_cursor_position(&self, x: f32, y: f32, element_id: Option<&str>) -> Result<()> {
        // Get current session ID
        let session_id = match *self.current_session_id.read().unwrap() {
            Some(ref id) => id.clone(),
            None => return Ok(()),  // No active session
        };
        
        // Update cursor via presence manager
        self.presence_manager.write().unwrap().update_cursor_position(&session_id, x, y, element_id)?;
        
        // Update last active time
        if let Some(session) = self.sessions.write().unwrap().get_mut(&session_id) {
            let current_user_id = self.current_user.read().unwrap().id.clone();
            if let Some(user) = session.users.get_mut(&current_user_id) {
                user.last_active = SystemTime::now();
            }
        }
        
        Ok(())
    }
    
    /// Update user selection
    pub fn update_selection(&self, start_id: &str, end_id: &str, start_offset: usize, end_offset: usize) -> Result<()> {
        // Get current session ID
        let session_id = match *self.current_session_id.read().unwrap() {
            Some(ref id) => id.clone(),
            None => return Ok(()),  // No active session
        };
        
        // Update selection via presence manager
        self.presence_manager.write().unwrap().update_selection(
            &session_id, 
            start_id, 
            end_id, 
            start_offset, 
            end_offset
        )?;
        
        Ok(())
    }
    
    /// Get all users in the current session
    pub fn get_session_users(&self) -> Result<Vec<User>> {
        // Get current session ID
        let session_id = match *self.current_session_id.read().unwrap() {
            Some(ref id) => id.clone(),
            None => return Ok(Vec::new()),  // No active session
        };
        
        // Get session
        let session = match self.sessions.read().unwrap().get(&session_id) {
            Some(session) => session.clone(),
            None => return Ok(Vec::new()), // Session not found
        };
        
        // Return all users
        Ok(session.users.values().cloned().collect())
    }
    
    /// Get all cursors in the current session
    pub fn get_cursors(&self) -> Result<HashMap<String, presence::CursorPosition>> {
        // Get current session ID
        let session_id = match *self.current_session_id.read().unwrap() {
            Some(ref id) => id.clone(),
            None => return Ok(HashMap::new()),  // No active session
        };
        
        // Get cursors from presence manager
        self.presence_manager.read().unwrap().get_cursors(&session_id)
    }
    
    /// Get all selections in the current session
    pub fn get_selections(&self) -> Result<HashMap<String, presence::Selection>> {
        // Get current session ID
        let session_id = match *self.current_session_id.read().unwrap() {
            Some(ref id) => id.clone(),
            None => return Ok(HashMap::new()),  // No active session
        };
        
        // Get selections from presence manager
        self.presence_manager.read().unwrap().get_selections(&session_id)
    }
    
    /// Synchronize a conversation
    pub fn sync_conversation(&self, conversation: &Conversation) -> Result<()> {
        // Get current session ID
        let session_id = match *self.current_session_id.read().unwrap() {
            Some(ref id) => id.clone(),
            None => return Ok(()),  // No active session
        };
        
        // Sync via sync manager
        self.sync_manager.write().unwrap().sync_conversation(&session_id, conversation)?;
        
        Ok(())
    }
    
    /// Send a message in the collaborative session
    pub fn send_message(&self, message: &Message) -> Result<()> {
        // Get current session ID
        let session_id = match *self.current_session_id.read().unwrap() {
            Some(ref id) => id.clone(),
            None => return Err("No active collaboration session".into()),
        };
        
        // Send message via sync manager
        self.sync_manager.write().unwrap().send_message(&session_id, message)?;
        
        // Update last active time
        if let Some(session) = self.sessions.write().unwrap().get_mut(&session_id) {
            let current_user_id = self.current_user.read().unwrap().id.clone();
            if let Some(user) = session.users.get_mut(&current_user_id) {
                user.last_active = SystemTime::now();
            }
        }
        
        record_counter("collaboration.message_sent", 1.0, None);
        
        Ok(())
    }
    
    /// Start an audio call in the current session
    pub fn start_audio_call(&self) -> Result<()> {
        // Check if audio is enabled
        if !self.config.read().unwrap().enable_av {
            return Err("Audio/video features are not enabled".into());
        }
        
        // Get current session ID
        let session_id = match *self.current_session_id.read().unwrap() {
            Some(ref id) => id.clone(),
            None => return Err("No active collaboration session".into()),
        };
        
        // Start call via RTC manager
        self.rtc_manager.write().unwrap().start_audio_call(&session_id)?;
        
        info!("Started audio call in session {}", session_id);
        record_counter("collaboration.audio_call_started", 1.0, None);
        
        Ok(())
    }
    
    /// Start a video call in the current session
    pub fn start_video_call(&self) -> Result<()> {
        // Check if video is enabled
        if !self.config.read().unwrap().enable_av {
            return Err("Audio/video features are not enabled".into());
        }
        
        // Get current session ID
        let session_id = match *self.current_session_id.read().unwrap() {
            Some(ref id) => id.clone(),
            None => return Err("No active collaboration session".into()),
        };
        
        // Start call via RTC manager
        self.rtc_manager.write().unwrap().start_video_call(&session_id)?;
        
        info!("Started video call in session {}", session_id);
        record_counter("collaboration.video_call_started", 1.0, None);
        
        Ok(())
    }
    
    /// End the current call
    pub fn end_call(&self) -> Result<()> {
        // Get current session ID
        let session_id = match *self.current_session_id.read().unwrap() {
            Some(ref id) => id.clone(),
            None => return Err("No active collaboration session".into()),
        };
        
        // End call via RTC manager
        self.rtc_manager.write().unwrap().end_call(&session_id)?;
        
        info!("Ended call in session {}", session_id);
        record_counter("collaboration.call_ended", 1.0, None);
        
        Ok(())
    }
    
    /// Update collaboration configuration
    pub fn update_config(&self, config: CollaborationConfig) -> Result<()> {
        let old_config = self.config.read().unwrap().clone();
        
        // Update config
        *self.config.write().unwrap() = config.clone();
        
        // Update presence if needed
        if old_config.show_presence != config.show_presence {
            self.presence_manager.write().unwrap().set_enabled(config.show_presence)?;
        }
        
        // Update sync interval if needed
        if old_config.sync_interval_ms != config.sync_interval_ms {
            self.sync_manager.write().unwrap().set_sync_interval(config.sync_interval_ms)?;
        }
        
        // Update A/V if needed
        if old_config.enable_av != config.enable_av {
            self.rtc_manager.write().unwrap().set_enabled(config.enable_av)?;
        }
        
        // Update server URLs if needed
        if old_config.server_urls != config.server_urls {
            self.session_manager.write().unwrap().update_server_urls(config.server_urls.clone())?;
            self.rtc_manager.write().unwrap().update_server_urls(config.server_urls.clone())?;
        }
        
        info!("Updated collaboration config");
        
        Ok(())
    }
    
    /// Get the current configuration
    pub fn get_config(&self) -> CollaborationConfig {
        self.config.read().unwrap().clone()
    }
    
    /// Get the current connection status
    pub fn get_connection_status(&self) -> ConnectionStatus {
        *self.connection_status.read().unwrap()
    }
    
    /// Get the current user
    pub fn get_current_user(&self) -> User {
        self.current_user.read().unwrap().clone()
    }
    
    /// Update the current user's name
    pub fn update_username(&self, name: &str) -> Result<()> {
        // Update local user
        self.current_user.write().unwrap().name = name.to_string();
        
        // Get current session ID
        if let Some(session_id) = self.current_session_id.read().unwrap().clone() {
            // Update user in session manager
            self.session_manager.write().unwrap().update_username(&session_id, name)?;
            
            // Update in current session
            if let Some(session) = self.sessions.write().unwrap().get_mut(&session_id) {
                let user_id = self.current_user.read().unwrap().id.clone();
                if let Some(user) = session.users.get_mut(&user_id) {
                    user.name = name.to_string();
                }
            }
        }
        
        Ok(())
    }
    
    /// Update the current user's avatar
    pub fn update_avatar(&self, avatar: Option<&str>) -> Result<()> {
        // Update local user
        self.current_user.write().unwrap().avatar = avatar.map(|s| s.to_string());
        
        // Get current session ID
        if let Some(session_id) = self.current_session_id.read().unwrap().clone() {
            // Update user in session manager
            self.session_manager.write().unwrap().update_avatar(&session_id, avatar)?;
            
            // Update in current session
            if let Some(session) = self.sessions.write().unwrap().get_mut(&session_id) {
                let user_id = self.current_user.read().unwrap().id.clone();
                if let Some(user) = session.users.get_mut(&user_id) {
                    user.avatar = avatar.map(|s| s.to_string());
                }
            }
        }
        
        Ok(())
    }
    
    /// Get statistics about collaboration
    pub fn get_statistics(&self) -> Result<CollaborationStatistics> {
        // Get session stats
        let session_stats = self.session_manager.read().unwrap().get_statistics()?;
        
        // Get presence stats
        let presence_stats = self.presence_manager.read().unwrap().get_statistics()?;
        
        // Get sync stats
        let sync_stats = self.sync_manager.read().unwrap().get_statistics()?;
        
        // Get RTC stats
        let rtc_stats = self.rtc_manager.read().unwrap().get_statistics()?;
        
        // Combine stats
        let stats = CollaborationStatistics {
            session_count: session_stats.session_count,
            total_users: session_stats.total_users,
            active_sessions: session_stats.active_sessions,
            cursor_updates: presence_stats.cursor_updates,
            selection_updates: presence_stats.selection_updates,
            messages_sent: sync_stats.messages_sent,
            messages_received: sync_stats.messages_received,
            sync_operations: sync_stats.sync_operations,
            conflicts_resolved: sync_stats.conflicts_resolved,
            calls_initiated: rtc_stats.calls_initiated,
            call_duration_seconds: rtc_stats.call_duration_seconds,
            current_session_id: self.current_session_id.read().unwrap().clone(),
            connection_status: *self.connection_status.read().unwrap(),
        };
        
        Ok(stats)
    }
}

/// Statistics about collaboration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CollaborationStatistics {
    /// Number of sessions the user has participated in
    pub session_count: usize,
    
    /// Total users collaborated with
    pub total_users: usize,
    
    /// Number of currently active sessions
    pub active_sessions: usize,
    
    /// Number of cursor updates sent/received
    pub cursor_updates: usize,
    
    /// Number of selection updates sent/received
    pub selection_updates: usize,
    
    /// Number of messages sent
    pub messages_sent: usize,
    
    /// Number of messages received
    pub messages_received: usize,
    
    /// Number of sync operations
    pub sync_operations: usize,
    
    /// Number of conflicts resolved
    pub conflicts_resolved: usize,
    
    /// Number of calls initiated
    pub calls_initiated: usize,
    
    /// Total duration of calls in seconds
    pub call_duration_seconds: u64,
    
    /// Current session ID
    pub current_session_id: Option<String>,
    
    /// Current connection status
    pub connection_status: ConnectionStatus,
}

// Global collaboration manager instance
lazy_static::lazy_static! {
    static ref COLLABORATION_MANAGER: Arc<RwLock<Option<CollaborationManager>>> = Arc::new(RwLock::new(None));
}

/// Initialize the collaboration system
pub fn init_collaboration(config: Option<CollaborationConfig>) -> Result<()> {
    let config = config.unwrap_or_default();
    
    // Create manager
    let manager = CollaborationManager::new(config)?;
    
    // Start services if enabled
    if manager.get_config().enabled {
        manager.start()?;
    }
    
    // Store globally
    *COLLABORATION_MANAGER.write().unwrap() = Some(manager);
    
    info!("Collaboration system initialized");
    
    Ok(())
}

/// Get a reference to the collaboration manager
pub fn get_collaboration_manager() -> Result<Arc<CollaborationManager>> {
    match COLLABORATION_MANAGER.read().unwrap().as_ref() {
        Some(manager) => Ok(Arc::new(manager.clone())),
        None => Err("Collaboration system not initialized".into()),
    }
}

// Helper functions for common operations

/// Generate a user color based on user ID
fn generate_user_color(user_id: &str) -> String {
    // Use a simple hash of the user ID to generate a hue value
    let hash: u32 = user_id.bytes().fold(0, |acc, byte| acc.wrapping_add(byte as u32));
    let hue = hash % 360;
    
    // Use HSL with high saturation and medium lightness for vibrant, distinguishable colors
    format!("hsl({}, 70%, 50%)", hue)
}
</file>

<file path="src/collaboration/presence.rs">
// Presence Management System
//
// This module handles user presence information including:
// - Cursor positions
// - Text selections
// - Active element tracking
// - User online status

use std::collections::HashMap;
use std::sync::{Arc, Mutex, RwLock};
use std::thread;
use std::time::{Duration, Instant, SystemTime};

use log::{debug, info, warn, error};
use serde::{Serialize, Deserialize};

use crate::error::Result;
use crate::observability::metrics::{record_counter, record_gauge};

/// Cursor position information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CursorPosition {
    /// User ID
    pub user_id: String,
    
    /// Device ID
    pub device_id: String,
    
    /// X coordinate (normalized 0.0-1.0)
    pub x: f32,
    
    /// Y coordinate (normalized 0.0-1.0)
    pub y: f32,
    
    /// Element ID the cursor is over
    pub element_id: Option<String>,
    
    /// Timestamp of the update
    pub timestamp: SystemTime,
}

/// Text selection information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Selection {
    /// User ID
    pub user_id: String,
    
    /// Device ID
    pub device_id: String,
    
    /// Start element ID
    pub start_id: String,
    
    /// End element ID
    pub end_id: String,
    
    /// Start offset within element
    pub start_offset: usize,
    
    /// End offset within element
    pub end_offset: usize,
    
    /// Timestamp of the update
    pub timestamp: SystemTime,
}

/// Active element information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ActiveElement {
    /// User ID
    pub user_id: String,
    
    /// Device ID
    pub device_id: String,
    
    /// Element ID
    pub element_id: String,
    
    /// Timestamp of the update
    pub timestamp: SystemTime,
}

/// User presence update message for network communication
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum PresenceUpdate {
    /// Cursor position update
    Cursor(CursorPosition),
    
    /// Selection update
    Selection(Selection),
    
    /// Active element update
    ActiveElement(ActiveElement),
    
    /// User joined
    UserJoined {
        user_id: String,
        device_id: String,
        timestamp: SystemTime,
    },
    
    /// User left
    UserLeft {
        user_id: String,
        device_id: String,
        timestamp: SystemTime,
    },
}

/// Presence tracking for a session
struct SessionPresence {
    /// Session ID
    session_id: String,
    
    /// Cursor positions by user ID
    cursors: HashMap<String, CursorPosition>,
    
    /// Selections by user ID
    selections: HashMap<String, Selection>,
    
    /// Active elements by user ID
    active_elements: HashMap<String, ActiveElement>,
    
    /// Last update time
    last_update: Instant,
}

impl SessionPresence {
    fn new(session_id: &str) -> Self {
        Self {
            session_id: session_id.to_string(),
            cursors: HashMap::new(),
            selections: HashMap::new(),
            active_elements: HashMap::new(),
            last_update: Instant::now(),
        }
    }
}

/// Presence manager for keeping track of users' presence
pub struct PresenceManager {
    /// User ID
    user_id: String,
    
    /// Device ID
    device_id: String,
    
    /// Whether presence features are enabled
    enabled: bool,
    
    /// Active sessions by session ID
    sessions: HashMap<String, SessionPresence>,
    
    /// Update queue for sending to other clients
    update_queue: Arc<Mutex<Vec<(String, PresenceUpdate)>>>,
    
    /// Running flag
    running: Arc<RwLock<bool>>,
    
    /// Statistics
    cursor_updates: Arc<RwLock<usize>>,
    selection_updates: Arc<RwLock<usize>>,
}

impl PresenceManager {
    /// Create a new presence manager
    pub fn new(user_id: String, device_id: String, enabled: bool) -> Result<Self> {
        Ok(Self {
            user_id,
            device_id,
            enabled,
            sessions: HashMap::new(),
            update_queue: Arc::new(Mutex::new(Vec::new())),
            running: Arc::new(RwLock::new(false)),
            cursor_updates: Arc::new(RwLock::new(0)),
            selection_updates: Arc::new(RwLock::new(0)),
        })
    }
    
    /// Start the presence service
    pub fn start(&self) -> Result<()> {
        if !self.enabled {
            info!("Presence service is disabled");
            return Ok(());
        }
        
        // Mark as running
        *self.running.write().unwrap() = true;
        
        // Start the update thread
        let running = self.running.clone();
        let update_queue = self.update_queue.clone();
        
        thread::spawn(move || {
            while *running.read().unwrap() {
                // Process presence updates
                let updates = {
                    let mut queue = update_queue.lock().unwrap();
                    let updates = queue.clone();
                    queue.clear();
                    updates
                };
                
                if !updates.is_empty() {
                    // In a real implementation, we would send these updates to other clients
                    // For now, just log them
                    debug!("Sending {} presence updates", updates.len());
                    
                    // In a real implementation, we'd use WebSockets or similar to send updates
                    // self.send_presence_updates(&updates);
                }
                
                // Sleep briefly
                thread::sleep(Duration::from_millis(50));
            }
        });
        
        info!("Presence service started");
        
        Ok(())
    }
    
    /// Stop the presence service
    pub fn stop(&self) -> Result<()> {
        *self.running.write().unwrap() = false;
        
        info!("Presence service stopped");
        
        Ok(())
    }
    
    /// Enable or disable presence tracking
    pub fn set_enabled(&self, enabled: bool) -> Result<()> {
        let mut this = unsafe { &mut *(self as *const Self as *mut Self) };
        
        if this.enabled == enabled {
            return Ok(());
        }
        
        this.enabled = enabled;
        
        if enabled {
            // Start service if not running
            if !*this.running.read().unwrap() {
                this.start()?;
            }
        } else {
            // Stop service if running
            if *this.running.read().unwrap() {
                this.stop()?;
            }
        }
        
        Ok(())
    }
    
    /// Join a session for presence tracking
    pub fn join_session(&mut self, session_id: &str) -> Result<()> {
        if !self.enabled {
            return Ok(());
        }
        
        // Create a new session presence
        let session = SessionPresence::new(session_id);
        self.sessions.insert(session_id.to_string(), session);
        
        // Send join notification
        let update = PresenceUpdate::UserJoined {
            user_id: self.user_id.clone(),
            device_id: self.device_id.clone(),
            timestamp: SystemTime::now(),
        };
        
        self.queue_update(session_id, update)?;
        
        info!("Joined presence tracking for session {}", session_id);
        
        Ok(())
    }
    
    /// Leave a session
    pub fn leave_session(&mut self, session_id: &str) -> Result<()> {
        if !self.enabled {
            return Ok(());
        }
        
        // Remove session
        self.sessions.remove(session_id);
        
        // Send leave notification
        let update = PresenceUpdate::UserLeft {
            user_id: self.user_id.clone(),
            device_id: self.device_id.clone(),
            timestamp: SystemTime::now(),
        };
        
        self.queue_update(session_id, update)?;
        
        info!("Left presence tracking for session {}", session_id);
        
        Ok(())
    }
    
    /// Update cursor position
    pub fn update_cursor_position(
        &mut self, 
        session_id: &str, 
        x: f32, 
        y: f32, 
        element_id: Option<&str>
    ) -> Result<()> {
        if !self.enabled {
            return Ok(());
        }
        
        let now = SystemTime::now();
        
        // Create cursor position
        let cursor = CursorPosition {
            user_id: self.user_id.clone(),
            device_id: self.device_id.clone(),
            x,
            y,
            element_id: element_id.map(|s| s.to_string()),
            timestamp: now,
        };
        
        // Update local state
        if let Some(session) = self.sessions.get_mut(session_id) {
            session.cursors.insert(self.user_id.clone(), cursor.clone());
            session.last_update = Instant::now();
        } else {
            return Err(format!("Session {} not found", session_id).into());
        }
        
        // Send update
        let update = PresenceUpdate::Cursor(cursor);
        self.queue_update(session_id, update)?;
        
        // Update stats
        *self.cursor_updates.write().unwrap() += 1;
        record_counter("collaboration.cursor_updates", 1.0, None);
        
        Ok(())
    }
    
    /// Update selection
    pub fn update_selection(
        &mut self,
        session_id: &str,
        start_id: &str,
        end_id: &str,
        start_offset: usize,
        end_offset: usize,
    ) -> Result<()> {
        if !self.enabled {
            return Ok(());
        }
        
        let now = SystemTime::now();
        
        // Create selection
        let selection = Selection {
            user_id: self.user_id.clone(),
            device_id: self.device_id.clone(),
            start_id: start_id.to_string(),
            end_id: end_id.to_string(),
            start_offset,
            end_offset,
            timestamp: now,
        };
        
        // Update local state
        if let Some(session) = self.sessions.get_mut(session_id) {
            session.selections.insert(self.user_id.clone(), selection.clone());
            session.last_update = Instant::now();
        } else {
            return Err(format!("Session {} not found", session_id).into());
        }
        
        // Send update
        let update = PresenceUpdate::Selection(selection);
        self.queue_update(session_id, update)?;
        
        // Update stats
        *self.selection_updates.write().unwrap() += 1;
        record_counter("collaboration.selection_updates", 1.0, None);
        
        Ok(())
    }
    
    /// Get all cursor positions for a session
    pub fn get_cursors(&self, session_id: &str) -> Result<HashMap<String, CursorPosition>> {
        if !self.enabled {
            return Ok(HashMap::new());
        }
        
        if let Some(session) = self.sessions.get(session_id) {
            // Return all cursors except our own
            let cursors = session.cursors.iter()
                .filter(|(id, _)| id != &&self.user_id)
                .map(|(id, cursor)| (id.clone(), cursor.clone()))
                .collect();
                
            return Ok(cursors);
        }
        
        Err(format!("Session {} not found", session_id).into())
    }
    
    /// Get all selections for a session
    pub fn get_selections(&self, session_id: &str) -> Result<HashMap<String, Selection>> {
        if !self.enabled {
            return Ok(HashMap::new());
        }
        
        if let Some(session) = self.sessions.get(session_id) {
            // Return all selections except our own
            let selections = session.selections.iter()
                .filter(|(id, _)| id != &&self.user_id)
                .map(|(id, selection)| (id.clone(), selection.clone()))
                .collect();
                
            return Ok(selections);
        }
        
        Err(format!("Session {} not found", session_id).into())
    }
    
    /// Handle a presence update from another user
    pub fn handle_update(&mut self, session_id: &str, update: PresenceUpdate) -> Result<()> {
        if !self.enabled {
            return Ok(());
        }
        
        let session = match self.sessions.get_mut(session_id) {
            Some(session) => session,
            None => return Err(format!("Session {} not found", session_id).into()),
        };
        
        match update {
            PresenceUpdate::Cursor(cursor) => {
                // Ignore our own cursor
                if cursor.user_id != self.user_id {
                    session.cursors.insert(cursor.user_id.clone(), cursor);
                    *self.cursor_updates.write().unwrap() += 1;
                }
            },
            PresenceUpdate::Selection(selection) => {
                // Ignore our own selection
                if selection.user_id != self.user_id {
                    session.selections.insert(selection.user_id.clone(), selection);
                    *self.selection_updates.write().unwrap() += 1;
                }
            },
            PresenceUpdate::ActiveElement(element) => {
                // Ignore our own active element
                if element.user_id != self.user_id {
                    session.active_elements.insert(element.user_id.clone(), element);
                }
            },
            PresenceUpdate::UserJoined { user_id, .. } => {
                info!("User {} joined session {}", user_id, session_id);
            },
            PresenceUpdate::UserLeft { user_id, .. } => {
                info!("User {} left session {}", user_id, session_id);
                
                // Remove user's presence data
                session.cursors.remove(&user_id);
                session.selections.remove(&user_id);
                session.active_elements.remove(&user_id);
            },
        }
        
        session.last_update = Instant::now();
        
        Ok(())
    }
    
    /// Queue an update to be sent to other clients
    fn queue_update(&self, session_id: &str, update: PresenceUpdate) -> Result<()> {
        if !self.enabled {
            return Ok(());
        }
        
        let mut queue = self.update_queue.lock().unwrap();
        queue.push((session_id.to_string(), update));
        
        Ok(())
    }
    
    /// Get statistics about presence
    pub fn get_statistics(&self) -> Result<PresenceStatistics> {
        Ok(PresenceStatistics {
            cursor_updates: *self.cursor_updates.read().unwrap(),
            selection_updates: *self.selection_updates.read().unwrap(),
            active_sessions: self.sessions.len(),
        })
    }
}

/// Statistics about presence
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PresenceStatistics {
    /// Number of cursor updates
    pub cursor_updates: usize,
    
    /// Number of selection updates
    pub selection_updates: usize,
    
    /// Number of active sessions
    pub active_sessions: usize,
}
</file>

<file path="src/collaboration/rtc.rs">
// Real-Time Communication System
//
// This module provides infrastructure for real-time audio/video communication:
// - WebRTC-based audio and video calls
// - Signaling for connection establishment
// - ICE servers for NAT traversal
// - Connection management

use std::collections::HashMap;
use std::sync::{Arc, Mutex, RwLock};
use std::thread;
use std::time::{Duration, Instant, SystemTime};

use log::{debug, info, warn, error};
use serde::{Serialize, Deserialize};

use crate::error::Result;
use crate::observability::metrics::{record_counter, record_gauge, record_histogram};

/// Call participant information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Participant {
    /// Participant user ID
    pub user_id: String,
    
    /// Participant name
    pub name: String,
    
    /// Device ID
    pub device_id: String,
    
    /// Whether audio is enabled
    pub audio_enabled: bool,
    
    /// Whether video is enabled
    pub video_enabled: bool,
    
    /// Whether the participant is speaking
    pub is_speaking: bool,
    
    /// Audio level (0.0-1.0)
    pub audio_level: f32,
    
    /// Network quality (0-5, 5 being best)
    pub network_quality: u8,
    
    /// Time when participant joined
    pub joined_at: SystemTime,
}

/// Call information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Call {
    /// Call ID
    pub id: String,
    
    /// Session ID
    pub session_id: String,
    
    /// Whether the call includes audio
    pub has_audio: bool,
    
    /// Whether the call includes video
    pub has_video: bool,
    
    /// Call start time
    pub start_time: SystemTime,
    
    /// Call participants
    pub participants: HashMap<String, Participant>,
}

/// Audio/video device information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MediaDevice {
    /// Device ID
    pub id: String,
    
    /// Device name
    pub name: String,
    
    /// Device kind (audioinput, videoinput, audiooutput)
    pub kind: String,
}

/// RTC manager for audio/video calls
pub struct RTCManager {
    /// User ID
    user_id: String,
    
    /// Device ID
    device_id: String,
    
    /// Whether A/V features are enabled
    enabled: bool,
    
    /// Active calls
    calls: HashMap<String, Call>,
    
    /// Available media devices
    media_devices: Vec<MediaDevice>,
    
    /// Server URLs for ICE and signaling
    server_urls: Vec<String>,
    
    /// Running flag
    running: Arc<RwLock<bool>>,
    
    /// Statistics
    statistics: Arc<RwLock<RTCStatistics>>,
}

impl RTCManager {
    /// Create a new RTC manager
    pub fn new(user_id: String, device_id: String, enabled: bool, server_urls: Vec<String>) -> Result<Self> {
        Ok(Self {
            user_id,
            device_id,
            enabled,
            calls: HashMap::new(),
            media_devices: Vec::new(),
            server_urls,
            running: Arc::new(RwLock::new(false)),
            statistics: Arc::new(RwLock::new(RTCStatistics {
                calls_initiated: 0,
                calls_received: 0,
                call_duration_seconds: 0,
                total_participants: 0,
                peak_participants: 0,
                audio_minutes: 0,
                video_minutes: 0,
            })),
        })
    }
    
    /// Start the RTC service
    pub fn start(&self) -> Result<()> {
        if !self.enabled {
            info!("RTC service is disabled");
            return Ok(());
        }
        
        // Mark as running
        *self.running.write().unwrap() = true;
        
        // Initialize media devices - this would be done in a real implementation
        let mut this = unsafe { &mut *(self as *const Self as *mut Self) };
        this.init_media_devices()?;
        
        // Start the background thread for call management
        let running = self.running.clone();
        let statistics = self.statistics.clone();
        
        thread::spawn(move || {
            while *running.read().unwrap() {
                // In a real implementation, we would:
                // 1. Process signaling messages
                // 2. Monitor call quality
                // 3. Update participants status
                
                // For now, just update statistics for active calls
                let mut stats = statistics.write().unwrap();
                
                // Sleep for a bit
                thread::sleep(Duration::from_secs(1));
            }
        });
        
        info!("RTC service started");
        
        Ok(())
    }
    
    /// Stop the RTC service
    pub fn stop(&self) -> Result<()> {
        *self.running.write().unwrap() = false;
        
        info!("RTC service stopped");
        
        Ok(())
    }
    
    /// Enable or disable RTC features
    pub fn set_enabled(&mut self, enabled: bool) -> Result<()> {
        if self.enabled == enabled {
            return Ok(());
        }
        
        self.enabled = enabled;
        
        if enabled {
            // Start service if not running
            if !*self.running.read().unwrap() {
                self.start()?;
            }
        } else {
            // Stop service if running
            if *self.running.read().unwrap() {
                self.stop()?;
            }
            
            // End any active calls
            for session_id in self.calls.keys().cloned().collect::<Vec<_>>() {
                self.end_call(&session_id)?;
            }
        }
        
        Ok(())
    }
    
    /// Initialize media devices
    fn init_media_devices(&mut self) -> Result<()> {
        // In a real implementation, we would detect available devices
        // For now, just create some dummy devices
        
        self.media_devices = vec![
            MediaDevice {
                id: "default-audio-in".to_string(),
                name: "Default Microphone".to_string(),
                kind: "audioinput".to_string(),
            },
            MediaDevice {
                id: "default-audio-out".to_string(),
                name: "Default Speakers".to_string(),
                kind: "audiooutput".to_string(),
            },
            MediaDevice {
                id: "default-video-in".to_string(),
                name: "Default Camera".to_string(),
                kind: "videoinput".to_string(),
            },
        ];
        
        info!("Initialized {} media devices", self.media_devices.len());
        
        Ok(())
    }
    
    /// Start an audio call in a session
    pub fn start_audio_call(&mut self, session_id: &str) -> Result<()> {
        if !self.enabled {
            return Err("RTC features are not enabled".into());
        }
        
        // Check if a call is already in progress
        if self.calls.values().any(|call| call.session_id == session_id) {
            return Err(format!("A call is already in progress in session {}", session_id).into());
        }
        
        // Create a new call
        let call_id = uuid::Uuid::new_v4().to_string();
        
        // Create our participant
        let participant = Participant {
            user_id: self.user_id.clone(),
            name: whoami::username(),
            device_id: self.device_id.clone(),
            audio_enabled: true,
            video_enabled: false,
            is_speaking: false,
            audio_level: 0.0,
            network_quality: 5,
            joined_at: SystemTime::now(),
        };
        
        let mut participants = HashMap::new();
        participants.insert(self.user_id.clone(), participant);
        
        let call = Call {
            id: call_id.clone(),
            session_id: session_id.to_string(),
            has_audio: true,
            has_video: false,
            start_time: SystemTime::now(),
            participants,
        };
        
        // Store call
        self.calls.insert(call_id, call);
        
        // Update statistics
        let mut stats = self.statistics.write().unwrap();
        stats.calls_initiated += 1;
        stats.total_participants += 1;
        stats.peak_participants = stats.peak_participants.max(1);
        
        record_counter("collaboration.audio_call_started", 1.0, None);
        
        info!("Started audio call in session {}", session_id);
        
        // In a real implementation, we would set up the WebRTC connection here
        
        Ok(())
    }
    
    /// Start a video call in a session
    pub fn start_video_call(&mut self, session_id: &str) -> Result<()> {
        if !self.enabled {
            return Err("RTC features are not enabled".into());
        }
        
        // Check if a call is already in progress
        if let Some(existing_call) = self.calls.values_mut().find(|call| call.session_id == session_id) {
            // If an audio call is in progress, upgrade to video
            if !existing_call.has_video {
                existing_call.has_video = true;
                
                // Update our participant
                if let Some(participant) = existing_call.participants.get_mut(&self.user_id) {
                    participant.video_enabled = true;
                }
                
                info!("Upgraded audio call to video in session {}", session_id);
                record_counter("collaboration.call_upgraded_to_video", 1.0, None);
                
                return Ok(());
            }
            
            return Err(format!("A call is already in progress in session {}", session_id).into());
        }
        
        // Create a new call
        let call_id = uuid::Uuid::new_v4().to_string();
        
        // Create our participant
        let participant = Participant {
            user_id: self.user_id.clone(),
            name: whoami::username(),
            device_id: self.device_id.clone(),
            audio_enabled: true,
            video_enabled: true,
            is_speaking: false,
            audio_level: 0.0,
            network_quality: 5,
            joined_at: SystemTime::now(),
        };
        
        let mut participants = HashMap::new();
        participants.insert(self.user_id.clone(), participant);
        
        let call = Call {
            id: call_id.clone(),
            session_id: session_id.to_string(),
            has_audio: true,
            has_video: true,
            start_time: SystemTime::now(),
            participants,
        };
        
        // Store call
        self.calls.insert(call_id, call);
        
        // Update statistics
        let mut stats = self.statistics.write().unwrap();
        stats.calls_initiated += 1;
        stats.total_participants += 1;
        stats.peak_participants = stats.peak_participants.max(1);
        
        record_counter("collaboration.video_call_started", 1.0, None);
        
        info!("Started video call in session {}", session_id);
        
        // In a real implementation, we would set up the WebRTC connection here
        
        Ok(())
    }
    
    /// Join an existing call
    pub fn join_call(&mut self, call_id: &str) -> Result<()> {
        if !self.enabled {
            return Err("RTC features are not enabled".into());
        }
        
        // Get call
        let call = match self.calls.get_mut(call_id) {
            Some(call) => call,
            None => return Err(format!("Call {} not found", call_id).into()),
        };
        
        // Create our participant
        let participant = Participant {
            user_id: self.user_id.clone(),
            name: whoami::username(),
            device_id: self.device_id.clone(),
            audio_enabled: true,
            video_enabled: call.has_video,
            is_speaking: false,
            audio_level: 0.0,
            network_quality: 5,
            joined_at: SystemTime::now(),
        };
        
        // Add to call
        call.participants.insert(self.user_id.clone(), participant);
        
        // Update statistics
        let mut stats = self.statistics.write().unwrap();
        stats.calls_received += 1;
        stats.total_participants += 1;
        stats.peak_participants = stats.peak_participants.max(call.participants.len());
        
        record_counter("collaboration.call_joined", 1.0, None);
        
        info!("Joined call {}", call_id);
        
        // In a real implementation, we would set up the WebRTC connection here
        
        Ok(())
    }
    
    /// End a call
    pub fn end_call(&mut self, session_id: &str) -> Result<()> {
        // Find call for this session
        let call_id = match self.calls.iter()
            .find(|(_, call)| call.session_id == session_id)
            .map(|(id, _)| id.clone()) {
            Some(id) => id,
            None => return Err(format!("No active call in session {}", session_id).into()),
        };
        
        // Get call duration
        let duration_seconds = if let Some(call) = self.calls.get(&call_id) {
            SystemTime::now().duration_since(call.start_time)
                .unwrap_or_else(|_| Duration::from_secs(0))
                .as_secs()
        } else {
            0
        };
        
        // Remove call
        if let Some(call) = self.calls.remove(&call_id) {
            // Update statistics
            let mut stats = self.statistics.write().unwrap();
            stats.call_duration_seconds += duration_seconds;
            
            // Update audio/video minutes
            let minutes = (duration_seconds + 59) / 60; // Round up
            stats.audio_minutes += minutes;
            
            if call.has_video {
                stats.video_minutes += minutes;
            }
        }
        
        record_counter("collaboration.call_ended", 1.0, None);
        
        info!("Ended call in session {}", session_id);
        
        Ok(())
    }
    
    /// Toggle mute status
    pub fn toggle_mute(&mut self, session_id: &str) -> Result<bool> {
        if !self.enabled {
            return Err("RTC features are not enabled".into());
        }
        
        // Find call for this session
        let call = match self.calls.values_mut()
            .find(|call| call.session_id == session_id) {
            Some(call) => call,
            None => return Err(format!("No active call in session {}", session_id).into()),
        };
        
        // Get our participant
        let participant = match call.participants.get_mut(&self.user_id) {
            Some(p) => p,
            None => return Err("User not in call".into()),
        };
        
        // Toggle audio
        participant.audio_enabled = !participant.audio_enabled;
        
        info!("Toggled mute to {} in session {}", !participant.audio_enabled, session_id);
        
        Ok(participant.audio_enabled)
    }
    
    /// Toggle video status
    pub fn toggle_video(&mut self, session_id: &str) -> Result<bool> {
        if !self.enabled {
            return Err("RTC features are not enabled".into());
        }
        
        // Find call for this session
        let call = match self.calls.values_mut()
            .find(|call| call.session_id == session_id) {
            Some(call) => call,
            None => return Err(format!("No active call in session {}", session_id).into()),
        };
        
        // Check if call supports video
        if !call.has_video {
            return Err("Call does not support video".into());
        }
        
        // Get our participant
        let participant = match call.participants.get_mut(&self.user_id) {
            Some(p) => p,
            None => return Err("User not in call".into()),
        };
        
        // Toggle video
        participant.video_enabled = !participant.video_enabled;
        
        info!("Toggled video to {} in session {}", participant.video_enabled, session_id);
        
        Ok(participant.video_enabled)
    }
    
    /// Get active call in a session
    pub fn get_active_call(&self, session_id: &str) -> Result<Option<Call>> {
        let call = self.calls.values()
            .find(|call| call.session_id == session_id)
            .cloned();
            
        Ok(call)
    }
    
    /// Get available media devices
    pub fn get_media_devices(&self) -> Result<Vec<MediaDevice>> {
        if !self.enabled {
            return Err("RTC features are not enabled".into());
        }
        
        Ok(self.media_devices.clone())
    }
    
    /// Update server URLs
    pub fn update_server_urls(&mut self, server_urls: Vec<String>) -> Result<()> {
        self.server_urls = server_urls;
        Ok(())
    }
    
    /// Get statistics about RTC
    pub fn get_statistics(&self) -> Result<RTCStatistics> {
        Ok(self.statistics.read().unwrap().clone())
    }
}

/// Statistics about real-time communication
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RTCStatistics {
    /// Number of calls initiated
    pub calls_initiated: usize,
    
    /// Number of calls received
    pub calls_received: usize,
    
    /// Total call duration in seconds
    pub call_duration_seconds: u64,
    
    /// Total number of participants across all calls
    pub total_participants: usize,
    
    /// Peak number of participants in a single call
    pub peak_participants: usize,
    
    /// Total audio minutes
    pub audio_minutes: u64,
    
    /// Total video minutes
    pub video_minutes: u64,
}
</file>

<file path="src/collaboration/sessions.rs">
// Session Management System
//
// This module manages collaborative sessions, including:
// - Session creation and joining
// - User management and permissions
// - Session discovery
// - Cross-device session coordination

use std::collections::HashMap;
use std::sync::{Arc, Mutex, RwLock};
use std::thread;
use std::time::{Duration, Instant, SystemTime};

use log::{debug, info, warn, error};
use serde::{Serialize, Deserialize};

use crate::collaboration::{Session, User, UserRole};
use crate::error::Result;
use crate::observability::metrics::{record_counter, record_gauge};

/// Session invitation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SessionInvitation {
    /// Invitation ID
    pub id: String,
    
    /// Session ID
    pub session_id: String,
    
    /// Session name
    pub session_name: String,
    
    /// Inviter user ID
    pub inviter_id: String,
    
    /// Inviter name
    pub inviter_name: String,
    
    /// Invitee email
    pub invitee_email: String,
    
    /// Assigned role
    pub role: UserRole,
    
    /// Invitation creation time
    pub created_at: SystemTime,
    
    /// Invitation expiration time
    pub expires_at: SystemTime,
    
    /// Whether the invitation has been accepted
    pub accepted: bool,
}

/// Session manager for handling session lifecycle
pub struct SessionManager {
    /// User ID
    user_id: String,
    
    /// Device ID
    device_id: String,
    
    /// Active sessions
    sessions: HashMap<String, Session>,
    
    /// Session invitations
    invitations: Vec<SessionInvitation>,
    
    /// Server URLs for signaling
    server_urls: Vec<String>,
    
    /// Running flag
    running: Arc<RwLock<bool>>,
    
    /// Last server ping time
    last_ping: Arc<Mutex<Instant>>,
    
    /// Session statistics
    statistics: Arc<RwLock<SessionStatistics>>,
}

impl SessionManager {
    /// Create a new session manager
    pub fn new(user_id: String, device_id: String, server_urls: Vec<String>) -> Result<Self> {
        Ok(Self {
            user_id,
            device_id,
            sessions: HashMap::new(),
            invitations: Vec::new(),
            server_urls,
            running: Arc::new(RwLock::new(false)),
            last_ping: Arc::new(Mutex::new(Instant::now())),
            statistics: Arc::new(RwLock::new(SessionStatistics {
                session_count: 0,
                total_users: 0,
                active_sessions: 0,
                invitations_sent: 0,
                invitations_received: 0,
            })),
        })
    }
    
    /// Start the session management service
    pub fn start(&self) -> Result<()> {
        // Mark as running
        *self.running.write().unwrap() = true;
        
        // Start the background thread for session maintenance
        let running = self.running.clone();
        let last_ping = self.last_ping.clone();
        let server_urls = self.server_urls.clone();
        
        thread::spawn(move || {
            while *running.read().unwrap() {
                // In a real implementation, we would:
                // 1. Ping the server to maintain connection
                // 2. Check for session updates
                // 3. Check for new invitations
                // 4. Clean up expired sessions
                
                // For now, just update the last ping time
                *last_ping.lock().unwrap() = Instant::now();
                
                // Sleep for a bit
                thread::sleep(Duration::from_secs(5));
            }
        });
        
        info!("Session management service started");
        
        Ok(())
    }
    
    /// Stop the session management service
    pub fn stop(&self) -> Result<()> {
        *self.running.write().unwrap() = false;
        
        info!("Session management service stopped");
        
        Ok(())
    }
    
    /// Create a new session
    pub fn create_session(&mut self, session_id: &str, name: &str, conversation_id: &str) -> Result<()> {
        // Create a new session
        let mut users = HashMap::new();
        
        // Add the current user as owner
        let user = User {
            id: self.user_id.clone(),
            name: whoami::username(),
            role: UserRole::Owner,
            avatar: None,
            color: self.generate_user_color(),
            online: true,
            last_active: SystemTime::now(),
            device_id: self.device_id.clone(),
            metadata: HashMap::new(),
        };
        
        users.insert(self.user_id.clone(), user);
        
        let session = Session {
            id: session_id.to_string(),
            name: name.to_string(),
            active: true,
            created_at: SystemTime::now(),
            updated_at: SystemTime::now(),
            conversation_id: conversation_id.to_string(),
            users,
            metadata: HashMap::new(),
        };
        
        // Store session
        self.sessions.insert(session_id.to_string(), session);
        
        // Update statistics
        let mut stats = self.statistics.write().unwrap();
        stats.session_count += 1;
        stats.active_sessions += 1;
        
        record_counter("collaboration.session_created", 1.0, None);
        
        Ok(())
    }
    
    /// Join an existing session
    pub fn join_session(&mut self, session_id: &str) -> Result<Session> {
        // In a real implementation, we would fetch session details from server
        // For now, simulate joining by creating a session if it doesn't exist
        
        if let Some(session) = self.sessions.get(session_id) {
            return Ok(session.clone());
        }
        
        // Simulate session details from server
        let mut users = HashMap::new();
        
        // Add ourselves as a participant
        let user = User {
            id: self.user_id.clone(),
            name: whoami::username(),
            role: UserRole::Editor, // Default role when joining
            avatar: None,
            color: self.generate_user_color(),
            online: true,
            last_active: SystemTime::now(),
            device_id: self.device_id.clone(),
            metadata: HashMap::new(),
        };
        
        users.insert(self.user_id.clone(), user);
        
        // Add a simulated owner
        let owner_id = format!("owner-{}", uuid::Uuid::new_v4());
        let owner = User {
            id: owner_id.clone(),
            name: "Session Owner".to_string(),
            role: UserRole::Owner,
            avatar: None,
            color: "#ff0000".to_string(),
            online: true,
            last_active: SystemTime::now(),
            device_id: "owner-device".to_string(),
            metadata: HashMap::new(),
        };
        
        users.insert(owner_id, owner);
        
        let session = Session {
            id: session_id.to_string(),
            name: format!("Session {}", session_id),
            active: true,
            created_at: SystemTime::now(),
            updated_at: SystemTime::now(),
            conversation_id: format!("conversation-{}", uuid::Uuid::new_v4()),
            users,
            metadata: HashMap::new(),
        };
        
        // Store session
        self.sessions.insert(session_id.to_string(), session.clone());
        
        // Update statistics
        let mut stats = self.statistics.write().unwrap();
        stats.session_count += 1;
        stats.active_sessions += 1;
        stats.total_users += 1; // For the simulated owner
        
        record_counter("collaboration.session_joined", 1.0, None);
        
        Ok(session)
    }
    
    /// Leave a session
    pub fn leave_session(&mut self, session_id: &str) -> Result<()> {
        if self.sessions.remove(session_id).is_some() {
            // Update statistics
            let mut stats = self.statistics.write().unwrap();
            stats.active_sessions -= 1;
            
            record_counter("collaboration.session_left", 1.0, None);
        }
        
        Ok(())
    }
    
    /// Invite a user to a session
    pub fn invite_user(&mut self, session_id: &str, email: &str, role: UserRole) -> Result<()> {
        // Validate session
        let session = match self.sessions.get(session_id) {
            Some(session) => session,
            None => return Err(format!("Session {} not found", session_id).into()),
        };
        
        // Create an invitation
        let invitation = SessionInvitation {
            id: uuid::Uuid::new_v4().to_string(),
            session_id: session_id.to_string(),
            session_name: session.name.clone(),
            inviter_id: self.user_id.clone(),
            inviter_name: whoami::username(),
            invitee_email: email.to_string(),
            role,
            created_at: SystemTime::now(),
            expires_at: SystemTime::now() + Duration::from_secs(86400 * 7), // 7 days
            accepted: false,
        };
        
        // Store invitation
        self.invitations.push(invitation);
        
        // Update statistics
        let mut stats = self.statistics.write().unwrap();
        stats.invitations_sent += 1;
        
        record_counter("collaboration.invitation_sent", 1.0, None);
        
        // In a real implementation, we would send the invitation to the server
        info!("Invited {} to session {}", email, session_id);
        
        Ok(())
    }
    
    /// Remove a user from a session
    pub fn remove_user(&mut self, session_id: &str, user_id: &str) -> Result<()> {
        let session = match self.sessions.get_mut(session_id) {
            Some(session) => session,
            None => return Err(format!("Session {} not found", session_id).into()),
        };
        
        if session.users.remove(user_id).is_some() {
            // Update session timestamp
            session.updated_at = SystemTime::now();
            
            // Update statistics
            let mut stats = self.statistics.write().unwrap();
            stats.total_users -= 1;
            
            record_counter("collaboration.user_removed", 1.0, None);
            
            info!("Removed user {} from session {}", user_id, session_id);
        }
        
        Ok(())
    }
    
    /// Change a user's role in a session
    pub fn change_user_role(&mut self, session_id: &str, user_id: &str, role: UserRole) -> Result<()> {
        let session = match self.sessions.get_mut(session_id) {
            Some(session) => session,
            None => return Err(format!("Session {} not found", session_id).into()),
        };
        
        if let Some(user) = session.users.get_mut(user_id) {
            // Update role
            user.role = role;
            
            // Update session timestamp
            session.updated_at = SystemTime::now();
            
            record_counter("collaboration.role_changed", 1.0, None);
            
            info!("Changed role for user {} to {:?} in session {}", user_id, role, session_id);
        }
        
        Ok(())
    }
    
    /// Update username
    pub fn update_username(&mut self, session_id: &str, name: &str) -> Result<()> {
        let session = match self.sessions.get_mut(session_id) {
            Some(session) => session,
            None => return Err(format!("Session {} not found", session_id).into()),
        };
        
        if let Some(user) = session.users.get_mut(&self.user_id) {
            // Update name
            user.name = name.to_string();
            
            // Update session timestamp
            session.updated_at = SystemTime::now();
            
            info!("Updated username to {} in session {}", name, session_id);
        }
        
        Ok(())
    }
    
    /// Update avatar
    pub fn update_avatar(&mut self, session_id: &str, avatar: Option<&str>) -> Result<()> {
        let session = match self.sessions.get_mut(session_id) {
            Some(session) => session,
            None => return Err(format!("Session {} not found", session_id).into()),
        };
        
        if let Some(user) = session.users.get_mut(&self.user_id) {
            // Update avatar
            user.avatar = avatar.map(|s| s.to_string());
            
            // Update session timestamp
            session.updated_at = SystemTime::now();
            
            info!("Updated avatar in session {}", session_id);
        }
        
        Ok(())
    }
    
    /// Get session statistics
    pub fn get_statistics(&self) -> Result<SessionStatistics> {
        Ok(self.statistics.read().unwrap().clone())
    }
    
    /// Update server URLs
    pub fn update_server_urls(&mut self, server_urls: Vec<String>) -> Result<()> {
        self.server_urls = server_urls;
        Ok(())
    }
    
    /// Generate a user color based on user ID
    fn generate_user_color(&self) -> String {
        // Use a simple hash of the user ID to generate a hue value
        let hash: u32 = self.user_id.bytes().fold(0, |acc, byte| acc.wrapping_add(byte as u32));
        let hue = hash % 360;
        
        // Use HSL with high saturation and medium lightness for vibrant, distinguishable colors
        format!("hsl({}, 70%, 50%)", hue)
    }
}

/// Session statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SessionStatistics {
    /// Number of sessions the user has participated in
    pub session_count: usize,
    
    /// Total users collaborated with
    pub total_users: usize,
    
    /// Number of currently active sessions
    pub active_sessions: usize,
    
    /// Number of invitations sent
    pub invitations_sent: usize,
    
    /// Number of invitations received
    pub invitations_received: usize,
}
</file>

<file path="src/collaboration/sync.rs">
// Synchronization System
//
// This module handles synchronization of conversation data between devices and users:
// - Message synchronization
// - Conflict resolution
// - Cross-device state persistence
// - Operational transformation for concurrent edits

use std::collections::{HashMap, VecDeque};
use std::sync::{Arc, Mutex, RwLock};
use std::thread;
use std::time::{Duration, Instant, SystemTime};

use log::{debug, info, warn, error};
use serde::{Serialize, Deserialize};

use crate::error::Result;
use crate::models::messages::{Conversation, Message};
use crate::observability::metrics::{record_counter, record_gauge, record_histogram};

/// Operation type for synchronization
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub enum Operation {
    /// Add a message
    AddMessage(Message),
    
    /// Update a message
    UpdateMessage {
        id: String,
        content: String,
    },
    
    /// Delete a message
    DeleteMessage(String),
    
    /// Update conversation metadata
    UpdateMetadata {
        key: String,
        value: String,
    },
    
    /// Set conversation title
    SetTitle(String),
}

/// Change record for syncing
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Change {
    /// Change ID
    pub id: String,
    
    /// User ID who made the change
    pub user_id: String,
    
    /// Device ID where the change originated
    pub device_id: String,
    
    /// Session ID
    pub session_id: String,
    
    /// Conversation ID
    pub conversation_id: String,
    
    /// Operation to apply
    pub operation: Operation,
    
    /// Timestamp when the change was created
    pub timestamp: SystemTime,
    
    /// Vector clock for causality tracking
    pub vector_clock: HashMap<String, u64>,
}

/// Status of a sync operation
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum SyncStatus {
    /// Sync successful
    Success,
    
    /// Sync in progress
    InProgress,
    
    /// Sync failed
    Failed,
    
    /// Sync conflict detected
    Conflict,
}

/// Active conversation being synchronized
struct SyncedConversation {
    /// Conversation ID
    id: String,
    
    /// Session ID
    session_id: String,
    
    /// Last synchronized time
    last_sync: Instant,
    
    /// Vector clock tracking causality
    vector_clock: HashMap<String, u64>,
    
    /// Pending changes to be applied
    pending_changes: VecDeque<Change>,
    
    /// Applied changes
    applied_changes: Vec<Change>,
    
    /// Last sync status
    last_status: SyncStatus,
}

/// Synchronization manager for cross-device data sync
pub struct SyncManager {
    /// User ID
    user_id: String,
    
    /// Device ID
    device_id: String,
    
    /// Sync interval in milliseconds
    sync_interval_ms: u64,
    
    /// Active conversations being synced
    conversations: HashMap<String, SyncedConversation>,
    
    /// Outgoing changes queue
    outgoing_changes: Arc<Mutex<VecDeque<Change>>>,
    
    /// Incoming changes queue
    incoming_changes: Arc<Mutex<VecDeque<Change>>>,
    
    /// Running flag
    running: Arc<RwLock<bool>>,
    
    /// Statistics
    statistics: Arc<RwLock<SyncStatistics>>,
}

impl SyncManager {
    /// Create a new sync manager
    pub fn new(user_id: String, device_id: String, sync_interval_ms: u64) -> Result<Self> {
        Ok(Self {
            user_id,
            device_id,
            sync_interval_ms,
            conversations: HashMap::new(),
            outgoing_changes: Arc::new(Mutex::new(VecDeque::new())),
            incoming_changes: Arc::new(Mutex::new(VecDeque::new())),
            running: Arc::new(RwLock::new(false)),
            statistics: Arc::new(RwLock::new(SyncStatistics {
                messages_sent: 0,
                messages_received: 0,
                sync_operations: 0,
                conflicts_resolved: 0,
                bytes_sent: 0,
                bytes_received: 0,
                last_sync_time: None,
            })),
        })
    }
    
    /// Start the sync service
    pub fn start(&self) -> Result<()> {
        // Mark as running
        *self.running.write().unwrap() = true;
        
        // Start the sync thread
        let running = self.running.clone();
        let incoming_changes = self.incoming_changes.clone();
        let outgoing_changes = self.outgoing_changes.clone();
        let statistics = self.statistics.clone();
        let sync_interval = self.sync_interval_ms;
        
        thread::spawn(move || {
            while *running.read().unwrap() {
                // Process incoming changes
                let mut incoming = incoming_changes.lock().unwrap();
                if !incoming.is_empty() {
                    let count = incoming.len();
                    
                    // In a real implementation, we would apply these changes
                    debug!("Processing {} incoming changes", count);
                    
                    // Update statistics
                    let mut stats = statistics.write().unwrap();
                    stats.messages_received += count;
                    stats.sync_operations += count;
                    
                    // Clear processed changes
                    incoming.clear();
                }
                
                // Process outgoing changes
                let mut outgoing = outgoing_changes.lock().unwrap();
                if !outgoing.is_empty() {
                    let count = outgoing.len();
                    
                    // In a real implementation, we would send these changes to other clients
                    debug!("Sending {} outgoing changes", count);
                    
                    // Update statistics
                    let mut stats = statistics.write().unwrap();
                    stats.messages_sent += count;
                    stats.sync_operations += count;
                    stats.last_sync_time = Some(SystemTime::now());
                    
                    // Clear processed changes
                    outgoing.clear();
                }
                
                // Sleep for sync interval
                thread::sleep(Duration::from_millis(sync_interval));
            }
        });
        
        info!("Sync service started with interval {}ms", self.sync_interval_ms);
        
        Ok(())
    }
    
    /// Stop the sync service
    pub fn stop(&self) -> Result<()> {
        *self.running.write().unwrap() = false;
        
        info!("Sync service stopped");
        
        Ok(())
    }
    
    /// Set sync interval
    pub fn set_sync_interval(&mut self, interval_ms: u64) -> Result<()> {
        self.sync_interval_ms = interval_ms;
        
        info!("Updated sync interval to {}ms", interval_ms);
        
        Ok(())
    }
    
    /// Initialize synchronization for a session
    pub fn init_session(&mut self, session_id: &str, conversation_id: &str) -> Result<()> {
        // Create a synced conversation
        let conversation = SyncedConversation {
            id: conversation_id.to_string(),
            session_id: session_id.to_string(),
            last_sync: Instant::now(),
            vector_clock: HashMap::new(),
            pending_changes: VecDeque::new(),
            applied_changes: Vec::new(),
            last_status: SyncStatus::Success,
        };
        
        // Store it
        self.conversations.insert(conversation_id.to_string(), conversation);
        
        info!("Initialized sync for conversation {} in session {}", conversation_id, session_id);
        
        Ok(())
    }
    
    /// Join an existing sync session
    pub fn join_session(&mut self, session_id: &str, conversation_id: &str) -> Result<()> {
        // First check if we already have this conversation
        if self.conversations.contains_key(conversation_id) {
            return Ok(());
        }
        
        // Create a synced conversation
        let conversation = SyncedConversation {
            id: conversation_id.to_string(),
            session_id: session_id.to_string(),
            last_sync: Instant::now(),
            vector_clock: HashMap::new(),
            pending_changes: VecDeque::new(),
            applied_changes: Vec::new(),
            last_status: SyncStatus::Success,
        };
        
        // Store it
        self.conversations.insert(conversation_id.to_string(), conversation);
        
        info!("Joined sync for conversation {} in session {}", conversation_id, session_id);
        
        Ok(())
    }
    
    /// Leave a sync session
    pub fn leave_session(&mut self, session_id: &str) -> Result<()> {
        // Find conversation for this session
        let conversation_ids: Vec<String> = self.conversations.iter()
            .filter(|(_, conv)| conv.session_id == session_id)
            .map(|(id, _)| id.clone())
            .collect();
            
        // Remove conversations
        for id in conversation_ids {
            self.conversations.remove(&id);
        }
        
        info!("Left sync for session {}", session_id);
        
        Ok(())
    }
    
    /// Synchronize a conversation
    pub fn sync_conversation(&mut self, session_id: &str, conversation: &Conversation) -> Result<()> {
        let conversation_id = &conversation.id;
        
        // Get synced conversation
        let synced = match self.conversations.get_mut(conversation_id) {
            Some(conv) => conv,
            None => {
                // Initialize new sync
                self.init_session(session_id, conversation_id)?;
                self.conversations.get_mut(conversation_id).unwrap()
            }
        };
        
        // Update last sync time
        synced.last_sync = Instant::now();
        
        // Update statistics
        let mut stats = self.statistics.write().unwrap();
        stats.sync_operations += 1;
        stats.last_sync_time = Some(SystemTime::now());
        
        record_counter("collaboration.sync_operation", 1.0, None);
        
        Ok(())
    }
    
    /// Send a message through sync
    pub fn send_message(&mut self, session_id: &str, message: &Message) -> Result<()> {
        let conversation_id = &message.conversation_id;
        
        // Get synced conversation
        let synced = match self.conversations.get_mut(conversation_id) {
            Some(conv) => conv,
            None => {
                // Initialize new sync
                self.init_session(session_id, conversation_id)?;
                self.conversations.get_mut(conversation_id).unwrap()
            }
        };
        
        // Increment vector clock for this user
        let user_count = synced.vector_clock.entry(self.user_id.clone()).or_insert(0);
        *user_count += 1;
        
        // Create change record
        let change = Change {
            id: uuid::Uuid::new_v4().to_string(),
            user_id: self.user_id.clone(),
            device_id: self.device_id.clone(),
            session_id: session_id.to_string(),
            conversation_id: conversation_id.to_string(),
            operation: Operation::AddMessage(message.clone()),
            timestamp: SystemTime::now(),
            vector_clock: synced.vector_clock.clone(),
        };
        
        // Add to outgoing changes
        self.outgoing_changes.lock().unwrap().push_back(change.clone());
        
        // Add to applied changes
        synced.applied_changes.push(change);
        
        // Update last sync time
        synced.last_sync = Instant::now();
        
        // Update statistics
        let mut stats = self.statistics.write().unwrap();
        stats.messages_sent += 1;
        stats.last_sync_time = Some(SystemTime::now());
        
        record_counter("collaboration.message_sent", 1.0, None);
        
        Ok(())
    }
    
    /// Process an incoming change
    pub fn process_change(&mut self, change: Change) -> Result<SyncStatus> {
        let conversation_id = &change.conversation_id;
        
        // Get synced conversation
        let synced = match self.conversations.get_mut(conversation_id) {
            Some(conv) => conv,
            None => {
                // Initialize new sync if session exists
                if let Some((session_id, _)) = self.conversations.iter()
                    .find(|(_, conv)| conv.session_id == change.session_id) {
                    self.init_session(&session_id, conversation_id)?;
                    self.conversations.get_mut(conversation_id).unwrap()
                } else {
                    return Err(format!("No active session for change in conversation {}", conversation_id).into());
                }
            }
        };
        
        // Check for conflicts
        let has_conflict = self.detect_conflict(&synced.vector_clock, &change.vector_clock);
        
        if has_conflict {
            // Handle conflict based on operation type
            info!("Conflict detected for change in conversation {}", conversation_id);
            
            // In a real implementation, we would resolve the conflict
            // For now, just accept the incoming change
            
            // Update statistics
            let mut stats = self.statistics.write().unwrap();
            stats.conflicts_resolved += 1;
            
            record_counter("collaboration.conflict_resolved", 1.0, None);
        }
        
        // Merge vector clocks
        self.merge_vector_clocks(&mut synced.vector_clock, &change.vector_clock);
        
        // Apply the change
        // In a real implementation, we would apply the change to the conversation
        
        // Add to applied changes
        synced.applied_changes.push(change);
        
        // Update last sync time
        synced.last_sync = Instant::now();
        
        // Update statistics
        let mut stats = self.statistics.write().unwrap();
        stats.messages_received += 1;
        stats.sync_operations += 1;
        stats.last_sync_time = Some(SystemTime::now());
        
        record_counter("collaboration.change_processed", 1.0, None);
        
        Ok(if has_conflict { SyncStatus::Conflict } else { SyncStatus::Success })
    }
    
    /// Detect conflicts between vector clocks
    fn detect_conflict(&self, local: &HashMap<String, u64>, remote: &HashMap<String, u64>) -> bool {
        // Check if either clock has events the other doesn't know about
        let mut local_ahead = false;
        let mut remote_ahead = false;
        
        // Check all keys in local clock
        for (user, local_count) in local {
            let remote_count = remote.get(user).unwrap_or(&0);
            
            if local_count > remote_count {
                local_ahead = true;
            }
        }
        
        // Check all keys in remote clock
        for (user, remote_count) in remote {
            let local_count = local.get(user).unwrap_or(&0);
            
            if remote_count > local_count {
                remote_ahead = true;
            }
        }
        
        // Conflict if both clocks have events the other doesn't know about
        local_ahead && remote_ahead
    }
    
    /// Merge vector clocks
    fn merge_vector_clocks(&self, local: &mut HashMap<String, u64>, remote: &HashMap<String, u64>) {
        for (user, remote_count) in remote {
            let local_count = local.entry(user.clone()).or_insert(0);
            *local_count = (*local_count).max(*remote_count);
        }
    }
    
    /// Get statistics about sync
    pub fn get_statistics(&self) -> Result<SyncStatistics> {
        Ok(self.statistics.read().unwrap().clone())
    }
}

/// Statistics about synchronization
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncStatistics {
    /// Number of messages sent
    pub messages_sent: usize,
    
    /// Number of messages received
    pub messages_received: usize,
    
    /// Number of sync operations
    pub sync_operations: usize,
    
    /// Number of conflicts resolved
    pub conflicts_resolved: usize,
    
    /// Bytes sent
    pub bytes_sent: usize,
    
    /// Bytes received
    pub bytes_received: usize,
    
    /// Last sync time
    pub last_sync_time: Option<SystemTime>,
}
</file>

<file path="src/commands/ai.rs">
use crate::ai::router::NetworkStatus;
use crate::models::messages::{Message, MessageError};
use crate::models::Model;
use crate::services::ai::get_ai_service;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tauri::State;

/// Get available models
#[tauri::command]
pub async fn get_available_models() -> Result<Vec<Model>, String> {
    Ok(get_ai_service().available_models().await)
}

/// Set network status
#[tauri::command]
pub fn set_network_status(status: String) -> Result<(), String> {
    let network_status = match status.as_str() {
        "connected" => NetworkStatus::Connected,
        "disconnected" => NetworkStatus::Disconnected,
        "unstable" => NetworkStatus::Unstable,
        _ => NetworkStatus::Unknown,
    };
    
    get_ai_service().set_network_status(network_status);
    Ok(())
}

/// Send a message to a model
#[tauri::command]
pub async fn send_message(
    conversation_id: String,
    model_id: String,
    content: String,
) -> Result<serde_json::Value, String> {
    // Create a message
    let message = Message::new_user_text(content);
    
    // Send message
    match get_ai_service()
        .send_message(&conversation_id, &model_id, message)
        .await
    {
        Ok(response) => {
            // Convert to json
            let mut map = serde_json::Map::new();
            
            // Convert message
            map.insert(
                "message".to_string(),
                serde_json::to_value(&response.message).unwrap(),
            );
            
            // Convert parent IDs
            map.insert(
                "parent_ids".to_string(),
                serde_json::to_value(&response.parent_ids).unwrap(),
            );
            
            // Convert completed_at
            map.insert(
                "completed_at".to_string(),
                if let Some(time) = response.completed_at {
                    serde_json::to_value(time).unwrap()
                } else {
                    serde_json::Value::Null
                },
            );
            
            // Convert partial_content
            map.insert(
                "partial_content".to_string(),
                if let Some(content) = response.partial_content {
                    serde_json::to_value(content).unwrap()
                } else {
                    serde_json::Value::Null
                },
            );
            
            // Convert status
            map.insert(
                "status".to_string(),
                serde_json::to_value(match response.status {
                    crate::models::messages::MessageStatus::Sending => "sending",
                    crate::models::messages::MessageStatus::Streaming => "streaming",
                    crate::models::messages::MessageStatus::Complete => "complete",
                    crate::models::messages::MessageStatus::Failed => "failed",
                    crate::models::messages::MessageStatus::Cancelled => "cancelled",
                })
                .unwrap(),
            );
            
            Ok(serde_json::Value::Object(map))
        }
        Err(e) => Err(format!("Failed to send message: {}", e)),
    }
}

/// Stream a message to a model
#[tauri::command]
pub async fn stream_message(
    window: tauri::Window,
    conversation_id: String,
    model_id: String,
    content: String,
) -> Result<String, String> {
    // Create a message
    let message = Message::new_user_text(content);
    
    // Generate a unique stream ID
    let stream_id = uuid::Uuid::new_v4().to_string();
    
    // Start streaming
    match get_ai_service()
        .stream_message(&conversation_id, &model_id, message)
        .await
    {
        Ok(mut stream) => {
            // Process stream in a separate task
            let window_clone = window.clone();
            let stream_id_clone = stream_id.clone();
            
            tauri::async_runtime::spawn(async move {
                while let Some(response) = stream.recv().await {
                    // Convert to json
                    let mut map = serde_json::Map::new();
                    
                    // Convert message
                    map.insert(
                        "message".to_string(),
                        serde_json::to_value(&response.message).unwrap(),
                    );
                    
                    // Convert parent IDs
                    map.insert(
                        "parent_ids".to_string(),
                        serde_json::to_value(&response.parent_ids).unwrap(),
                    );
                    
                    // Convert completed_at
                    map.insert(
                        "completed_at".to_string(),
                        if let Some(time) = response.completed_at {
                            serde_json::to_value(time).unwrap()
                        } else {
                            serde_json::Value::Null
                        },
                    );
                    
                    // Convert partial_content
                    map.insert(
                        "partial_content".to_string(),
                        if let Some(content) = response.partial_content {
                            serde_json::to_value(content).unwrap()
                        } else {
                            serde_json::Value::Null
                        },
                    );
                    
                    // Convert status
                    map.insert(
                        "status".to_string(),
                        serde_json::to_value(match response.status {
                            crate::models::messages::MessageStatus::Sending => "sending",
                            crate::models::messages::MessageStatus::Streaming => "streaming",
                            crate::models::messages::MessageStatus::Complete => "complete",
                            crate::models::messages::MessageStatus::Failed => "failed",
                            crate::models::messages::MessageStatus::Cancelled => "cancelled",
                        })
                        .unwrap(),
                    );
                    
                    // Add stream ID
                    map.insert(
                        "stream_id".to_string(),
                        serde_json::to_value(&stream_id_clone).unwrap(),
                    );
                    
                    // Emit event to window
                    let _ = window_clone.emit("stream-update", serde_json::Value::Object(map));
                }
                
                // Emit stream end event
                let _ = window_clone.emit(
                    "stream-end",
                    serde_json::json!({
                        "stream_id": stream_id_clone
                    }),
                );
            });
            
            Ok(stream_id)
        }
        Err(e) => Err(format!("Failed to start streaming: {}", e)),
    }
}

/// Cancel a streaming message
#[tauri::command]
pub async fn cancel_streaming(
    conversation_id: String,
    message_id: String,
) -> Result<(), String> {
    match get_ai_service()
        .cancel_streaming(&conversation_id, &message_id)
        .await
    {
        Ok(_) => Ok(()),
        Err(e) => Err(format!("Failed to cancel streaming: {}", e)),
    }
}

/// Get conversation messages
#[tauri::command]
pub fn get_messages(conversation_id: String) -> Result<Vec<serde_json::Value>, String> {
    let messages = get_ai_service().get_messages(&conversation_id);
    
    // Convert to serde_json::Value for serialization
    let messages_json = messages
        .into_iter()
        .map(|msg| {
            let mut map = serde_json::Map::new();
            
            // Convert message
            map.insert(
                "message".to_string(),
                serde_json::to_value(&msg.message).unwrap(),
            );
            
            // Convert parent IDs
            map.insert(
                "parent_ids".to_string(),
                serde_json::to_value(&msg.parent_ids).unwrap(),
            );
            
            // Convert completed_at
            map.insert(
                "completed_at".to_string(),
                if let Some(time) = msg.completed_at {
                    serde_json::to_value(time).unwrap()
                } else {
                    serde_json::Value::Null
                },
            );
            
            // Convert partial_content
            map.insert(
                "partial_content".to_string(),
                if let Some(content) = msg.partial_content {
                    serde_json::to_value(content).unwrap()
                } else {
                    serde_json::Value::Null
                },
            );
            
            // Convert status
            map.insert(
                "status".to_string(),
                serde_json::to_value(match msg.status {
                    crate::models::messages::MessageStatus::Sending => "sending",
                    crate::models::messages::MessageStatus::Streaming => "streaming",
                    crate::models::messages::MessageStatus::Complete => "complete",
                    crate::models::messages::MessageStatus::Failed => "failed",
                    crate::models::messages::MessageStatus::Cancelled => "cancelled",
                })
                .unwrap(),
            );
            
            serde_json::Value::Object(map)
        })
        .collect();
    
    Ok(messages_json)
}

/// Create a conversation
#[tauri::command]
pub fn create_conversation(title: String, model_id: String) -> Result<serde_json::Value, String> {
    // Find model by ID
    let models = get_ai_service().available_models().await;
    let model = models
        .into_iter()
        .find(|m| m.id == model_id)
        .ok_or_else(|| format!("Model with ID {} not found", model_id))?;
    
    // Create conversation
    let conversation = get_ai_service().create_conversation(&title, model);
    
    Ok(serde_json::to_value(conversation).unwrap())
}

/// Delete a conversation
#[tauri::command]
pub fn delete_conversation(id: String) -> Result<(), String> {
    get_ai_service().delete_conversation(&id)
}
</file>

<file path="src/commands/auth.rs">
use crate::services::auth::get_auth_service;
use tauri::State;

/// Set API key
#[tauri::command]
pub async fn set_api_key(api_key: String) -> Result<(), String> {
    get_auth_service().set_api_key(api_key)
}

/// Validate API key
#[tauri::command]
pub async fn validate_api_key() -> Result<bool, String> {
    get_auth_service().validate_api_key().await
}

/// Get organization ID
#[tauri::command]
pub fn get_organization_id() -> Option<String> {
    get_auth_service().get_organization_id()
}

/// Logout
#[tauri::command]
pub fn logout() -> Result<(), String> {
    get_auth_service().logout()
}
</file>

<file path="src/commands/chat.rs">
use crate::models::messages::{Message, MessageError};
use crate::models::{Conversation, Model};
use crate::services::chat::get_chat_service;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tauri::State;

/// Get available models
#[tauri::command]
pub fn get_available_models() -> Vec<Model> {
    get_chat_service().available_models()
}

/// Create a new conversation
#[tauri::command]
pub fn create_conversation(title: String, model_id: String) -> Result<Conversation, String> {
    // Find model by ID
    let model = get_chat_service()
        .available_models()
        .into_iter()
        .find(|m| m.id == model_id)
        .ok_or_else(|| format!("Model with ID {} not found", model_id))?;
    
    // Create conversation
    let conversation = get_chat_service().create_conversation(&title, model);
    Ok(conversation)
}

/// Get a conversation by ID
#[tauri::command]
pub fn get_conversation(id: String) -> Result<Conversation, String> {
    get_chat_service()
        .get_conversation(&id)
        .ok_or_else(|| format!("Conversation with ID {} not found", id))
}

/// Get all conversations
#[tauri::command]
pub fn get_conversations() -> Vec<Conversation> {
    let service = get_chat_service();
    
    // In a real implementation, we would fetch all conversations
    // For now, we'll return an empty list
    Vec::new()
}

/// Delete a conversation
#[tauri::command]
pub fn delete_conversation(id: String) -> Result<(), String> {
    get_chat_service().delete_conversation(&id)
}

/// Get conversation message history
#[tauri::command]
pub fn get_messages(conversation_id: String) -> Result<Vec<serde_json::Value>, String> {
    let messages = get_chat_service().get_messages(&conversation_id);
    
    // Convert to serde_json::Value for serialization
    let messages_json = messages
        .into_iter()
        .map(|msg| {
            let mut map = serde_json::Map::new();
            
            // Convert message
            map.insert(
                "message".to_string(),
                serde_json::to_value(&msg.message).unwrap(),
            );
            
            // Convert parent IDs
            map.insert(
                "parent_ids".to_string(),
                serde_json::to_value(&msg.parent_ids).unwrap(),
            );
            
            // Convert completed_at
            map.insert(
                "completed_at".to_string(),
                if let Some(time) = msg.completed_at {
                    serde_json::to_value(time).unwrap()
                } else {
                    serde_json::Value::Null
                },
            );
            
            // Convert partial_content
            map.insert(
                "partial_content".to_string(),
                if let Some(content) = msg.partial_content {
                    serde_json::to_value(content).unwrap()
                } else {
                    serde_json::Value::Null
                },
            );
            
            // Convert status
            map.insert(
                "status".to_string(),
                serde_json::to_value(match msg.status {
                    crate::models::messages::MessageStatus::Sending => "sending",
                    crate::models::messages::MessageStatus::Streaming => "streaming",
                    crate::models::messages::MessageStatus::Complete => "complete",
                    crate::models::messages::MessageStatus::Failed => "failed",
                    crate::models::messages::MessageStatus::Cancelled => "cancelled",
                })
                .unwrap(),
            );
            
            serde_json::Value::Object(map)
        })
        .collect();
    
    Ok(messages_json)
}

/// Send a message in a conversation
#[tauri::command]
pub async fn send_message(
    conversation_id: String,
    content: String,
) -> Result<serde_json::Value, String> {
    // Create a message
    let message = Message::new_user_text(content);
    
    // Send message
    match get_chat_service()
        .send_message(&conversation_id, message)
        .await
    {
        Ok(response) => {
            // Convert to json
            let mut map = serde_json::Map::new();
            
            // Convert message
            map.insert(
                "message".to_string(),
                serde_json::to_value(&response.message).unwrap(),
            );
            
            // Convert parent IDs
            map.insert(
                "parent_ids".to_string(),
                serde_json::to_value(&response.parent_ids).unwrap(),
            );
            
            // Convert completed_at
            map.insert(
                "completed_at".to_string(),
                if let Some(time) = response.completed_at {
                    serde_json::to_value(time).unwrap()
                } else {
                    serde_json::Value::Null
                },
            );
            
            // Convert partial_content
            map.insert(
                "partial_content".to_string(),
                if let Some(content) = response.partial_content {
                    serde_json::to_value(content).unwrap()
                } else {
                    serde_json::Value::Null
                },
            );
            
            // Convert status
            map.insert(
                "status".to_string(),
                serde_json::to_value(match response.status {
                    crate::models::messages::MessageStatus::Sending => "sending",
                    crate::models::messages::MessageStatus::Streaming => "streaming",
                    crate::models::messages::MessageStatus::Complete => "complete",
                    crate::models::messages::MessageStatus::Failed => "failed",
                    crate::models::messages::MessageStatus::Cancelled => "cancelled",
                })
                .unwrap(),
            );
            
            Ok(serde_json::Value::Object(map))
        }
        Err(e) => Err(format!("Failed to send message: {}", e)),
    }
}
</file>

<file path="src/commands/collaboration/mod.rs">
use serde::{Serialize, Deserialize};
use tauri::Wry;
use std::collections::HashMap;

pub mod whiteboard;

use crate::collaboration::{
    CollaborationConfig, 
    ConnectionStatus, 
    UserRole,
    Session,
    User,
    init_collaboration,
    get_collaboration_manager
};
use crate::collaboration::presence::{CursorPosition, Selection};
use crate::error::Result;
use crate::models::messages::{Conversation, Message};

/// Register collaboration commands with Tauri
pub fn register_commands(builder: tauri::Builder<Wry>) -> tauri::Builder<Wry> {
    builder.invoke_handler(tauri::generate_handler![
        // General commands
        init_collaboration_system,
        get_collaboration_config,
        update_collaboration_config,
        get_connection_status,
        
        // Session commands
        create_session,
        join_session,
        leave_session,
        invite_user,
        remove_user,
        change_user_role,
        get_session_users,
        
        // Presence commands
        update_cursor_position,
        update_selection,
        get_cursors,
        get_selections,
        
        // Sync commands
        sync_conversation,
        send_message,
        
        // AV commands
        start_audio_call,
        start_video_call,
        end_call,
        toggle_mute,
        toggle_video,
        get_active_call,
        get_media_devices,
        
        // User commands
        update_username,
        update_avatar,
        
        // Statistics
        get_collaboration_statistics,
        
        // Whiteboard commands
        whiteboard::send_whiteboard_operation,
        whiteboard::get_whiteboard_state,
        whiteboard::clear_whiteboard,
        whiteboard::save_whiteboard_image,
    ])
}

/// Initialize the collaboration system
#[tauri::command]
pub async fn init_collaboration_system(config: Option<CollaborationConfig>) -> Result<()> {
    init_collaboration(config)
}

/// Get the current collaboration configuration
#[tauri::command]
pub async fn get_collaboration_config() -> Result<CollaborationConfig> {
    let manager = get_collaboration_manager()?;
    Ok(manager.get_config())
}

/// Update the collaboration configuration
#[tauri::command]
pub async fn update_collaboration_config(config: CollaborationConfig) -> Result<()> {
    let manager = get_collaboration_manager()?;
    manager.update_config(config)
}

/// Get the current connection status
#[tauri::command]
pub async fn get_connection_status() -> Result<ConnectionStatus> {
    let manager = get_collaboration_manager()?;
    Ok(manager.get_connection_status())
}

/// Create a new collaboration session
#[tauri::command]
pub async fn create_session(name: String, conversation_id: String) -> Result<Session> {
    let manager = get_collaboration_manager()?;
    manager.create_session(&name, &conversation_id)
}

/// Join an existing collaboration session
#[tauri::command]
pub async fn join_session(session_id: String) -> Result<Session> {
    let manager = get_collaboration_manager()?;
    manager.join_session(&session_id)
}

/// Leave the current collaboration session
#[tauri::command]
pub async fn leave_session() -> Result<()> {
    let manager = get_collaboration_manager()?;
    manager.leave_session()
}

/// Invite a user to the current session
#[tauri::command]
pub async fn invite_user(email: String, role: UserRole) -> Result<()> {
    let manager = get_collaboration_manager()?;
    manager.invite_user(&email, role)
}

/// Remove a user from the current session
#[tauri::command]
pub async fn remove_user(user_id: String) -> Result<()> {
    let manager = get_collaboration_manager()?;
    manager.remove_user(&user_id)
}

/// Change a user's role in the current session
#[tauri::command]
pub async fn change_user_role(user_id: String, role: UserRole) -> Result<()> {
    let manager = get_collaboration_manager()?;
    manager.change_user_role(&user_id, role)
}

/// Get all users in the current session
#[tauri::command]
pub async fn get_session_users() -> Result<Vec<User>> {
    let manager = get_collaboration_manager()?;
    manager.get_session_users()
}

/// Update cursor position
#[tauri::command]
pub async fn update_cursor_position(x: f32, y: f32, element_id: Option<String>) -> Result<()> {
    let manager = get_collaboration_manager()?;
    manager.update_cursor_position(x, y, element_id.as_deref())
}

/// Update selection
#[tauri::command]
pub async fn update_selection(
    start_id: String, 
    end_id: String, 
    start_offset: usize, 
    end_offset: usize
) -> Result<()> {
    let manager = get_collaboration_manager()?;
    manager.update_selection(&start_id, &end_id, start_offset, end_offset)
}

/// Get all cursors in the current session
#[tauri::command]
pub async fn get_cursors() -> Result<HashMap<String, CursorPosition>> {
    let manager = get_collaboration_manager()?;
    manager.get_cursors()
}

/// Get all selections in the current session
#[tauri::command]
pub async fn get_selections() -> Result<HashMap<String, Selection>> {
    let manager = get_collaboration_manager()?;
    manager.get_selections()
}

/// Synchronize a conversation
#[tauri::command]
pub async fn sync_conversation(conversation: Conversation) -> Result<()> {
    let manager = get_collaboration_manager()?;
    manager.sync_conversation(&conversation)
}

/// Send a message in the collaborative session
#[tauri::command]
pub async fn send_message(message: Message) -> Result<()> {
    let manager = get_collaboration_manager()?;
    manager.send_message(&message)
}

/// Start an audio call in the current session
#[tauri::command]
pub async fn start_audio_call() -> Result<()> {
    let manager = get_collaboration_manager()?;
    manager.start_audio_call()
}

/// Start a video call in the current session
#[tauri::command]
pub async fn start_video_call() -> Result<()> {
    let manager = get_collaboration_manager()?;
    manager.start_video_call()
}

/// End the current call
#[tauri::command]
pub async fn end_call() -> Result<()> {
    let manager = get_collaboration_manager()?;
    manager.end_call()
}

/// Toggle mute status
#[tauri::command]
pub async fn toggle_mute() -> Result<bool> {
    let manager = get_collaboration_manager()?;
    
    // Get current session ID
    let session_id = match manager.get_statistics()?.current_session_id {
        Some(id) => id,
        None => return Err("No active session".into()),
    };
    
    // Get RTC manager and toggle mute
    let rtc_manager = manager.rtc_manager.write().unwrap();
    rtc_manager.toggle_mute(&session_id)
}

/// Toggle video status
#[tauri::command]
pub async fn toggle_video() -> Result<bool> {
    let manager = get_collaboration_manager()?;
    
    // Get current session ID
    let session_id = match manager.get_statistics()?.current_session_id {
        Some(id) => id,
        None => return Err("No active session".into()),
    };
    
    // Get RTC manager and toggle video
    let rtc_manager = manager.rtc_manager.write().unwrap();
    rtc_manager.toggle_video(&session_id)
}

/// Get the active call in the current session
#[tauri::command]
pub async fn get_active_call() -> Result<Option<crate::collaboration::rtc::Call>> {
    let manager = get_collaboration_manager()?;
    
    // Get current session ID
    let session_id = match manager.get_statistics()?.current_session_id {
        Some(id) => id,
        None => return Ok(None),
    };
    
    // Get RTC manager and get active call
    let rtc_manager = manager.rtc_manager.read().unwrap();
    rtc_manager.get_active_call(&session_id)
}

/// Get available media devices
#[tauri::command]
pub async fn get_media_devices() -> Result<Vec<crate::collaboration::rtc::MediaDevice>> {
    let manager = get_collaboration_manager()?;
    
    // Get RTC manager and get devices
    let rtc_manager = manager.rtc_manager.read().unwrap();
    rtc_manager.get_media_devices()
}

/// Update username
#[tauri::command]
pub async fn update_username(name: String) -> Result<()> {
    let manager = get_collaboration_manager()?;
    manager.update_username(&name)
}

/// Update avatar
#[tauri::command]
pub async fn update_avatar(avatar: Option<String>) -> Result<()> {
    let manager = get_collaboration_manager()?;
    manager.update_avatar(avatar.as_deref())
}

/// Get collaboration statistics
#[tauri::command]
pub async fn get_collaboration_statistics() -> Result<crate::collaboration::CollaborationStatistics> {
    let manager = get_collaboration_manager()?;
    manager.get_statistics()
}
</file>

<file path="src/commands/collaboration/whiteboard.rs">
// Whiteboard command handlers for the collaborative whiteboard feature

use std::collections::HashMap;
use std::sync::{Arc, RwLock};
use std::time::{Duration, SystemTime};

use log::{debug, info, warn, error};
use serde::{Serialize, Deserialize};
use serde_json::Value;

use crate::error::Result;
use crate::collaboration::get_collaboration_manager;

/// Point coordinates for whiteboard operations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Point {
    pub x: f32,
    pub y: f32,
}

/// Type of drawing operation
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub enum OperationType {
    #[serde(rename = "pencil")]
    Pencil,
    
    #[serde(rename = "line")]
    Line,
    
    #[serde(rename = "rectangle")]
    Rectangle,
    
    #[serde(rename = "circle")]
    Circle,
    
    #[serde(rename = "text")]
    Text,
    
    #[serde(rename = "eraser")]
    Eraser,
}

/// Whiteboard drawing operation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DrawOperation {
    #[serde(rename = "type")]
    pub operation_type: OperationType,
    
    pub points: Vec<Point>,
    
    pub color: String,
    
    pub size: u32,
    
    #[serde(skip_serializing_if = "Option::is_none")]
    pub text: Option<String>,
    
    pub user_id: String,
    
    pub timestamp: u64,
}

/// Whiteboard state for a session
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WhiteboardState {
    pub operations: Vec<DrawOperation>,
    pub version: u64,
}

// In-memory storage for whiteboard state by session ID
lazy_static::lazy_static! {
    static ref WHITEBOARDS: Arc<RwLock<HashMap<String, WhiteboardState>>> = Arc::new(RwLock::new(HashMap::new()));
}

/// Send a whiteboard operation to all users in a session
#[tauri::command]
pub async fn send_whiteboard_operation(session_id: String, operation: String) -> Result<()> {
    // Parse the operation JSON
    let operation: DrawOperation = serde_json::from_str(&operation)?;
    
    // Get the collaboration manager
    let manager = get_collaboration_manager()?;
    
    // Add operation to whiteboard state
    {
        let mut whiteboards = WHITEBOARDS.write().unwrap();
        let whiteboard = whiteboards.entry(session_id.clone()).or_insert(WhiteboardState {
            operations: Vec::new(),
            version: 0,
        });
        
        whiteboard.operations.push(operation.clone());
        whiteboard.version += 1;
    }
    
    // Create a message with the operation
    let message_content = serde_json::to_string(&operation)?;
    let message = crate::models::messages::Message {
        id: uuid::Uuid::new_v4().to_string(),
        conversation_id: session_id.clone(),
        content: message_content,
        timestamp: SystemTime::now(),
        sender: "system".to_string(),
        sender_name: "Whiteboard".to_string(),
        message_type: "whiteboard_operation".to_string(),
        metadata: HashMap::new(),
    };
    
    // Send the message through the collaboration system
    manager.send_message(&message)?;
    
    info!("Sent whiteboard operation in session {}", session_id);
    
    Ok(())
}

/// Get the current whiteboard state for a session
#[tauri::command]
pub async fn get_whiteboard_state(session_id: String) -> Result<WhiteboardState> {
    let whiteboards = WHITEBOARDS.read().unwrap();
    
    // Return the whiteboard state or an empty one if it doesn't exist
    Ok(whiteboards.get(&session_id).cloned().unwrap_or(WhiteboardState {
        operations: Vec::new(),
        version: 0,
    }))
}

/// Clear the whiteboard for a session
#[tauri::command]
pub async fn clear_whiteboard(session_id: String) -> Result<()> {
    // Get the collaboration manager
    let manager = get_collaboration_manager()?;
    
    // Clear the whiteboard state
    {
        let mut whiteboards = WHITEBOARDS.write().unwrap();
        whiteboards.insert(session_id.clone(), WhiteboardState {
            operations: Vec::new(),
            version: 0,
        });
    }
    
    // Create a clear message
    let message = crate::models::messages::Message {
        id: uuid::Uuid::new_v4().to_string(),
        conversation_id: session_id.clone(),
        content: "clear".to_string(),
        timestamp: SystemTime::now(),
        sender: "system".to_string(),
        sender_name: "Whiteboard".to_string(),
        message_type: "whiteboard_clear".to_string(),
        metadata: HashMap::new(),
    };
    
    // Send the message through the collaboration system
    manager.send_message(&message)?;
    
    info!("Cleared whiteboard in session {}", session_id);
    
    Ok(())
}

/// Save the whiteboard as an image
#[tauri::command]
pub async fn save_whiteboard_image(session_id: String, image_data: String) -> Result<String> {
    // In a real implementation, we would save the image to disk or cloud storage
    // For now, we'll just return a mock file path
    
    let file_path = format!("whiteboard_{}.png", session_id);
    
    info!("Saved whiteboard image for session {}", session_id);
    
    Ok(file_path)
}
</file>

<file path="src/commands/llm_metrics.rs">
use serde::{Serialize, Deserialize};
use std::collections::HashMap;
use log::{debug, error};

use crate::observability::metrics::llm::{
    ProviderPerformanceMetrics, ModelPerformanceMetrics,
    get_provider_metrics, get_model_metrics, LLMMetricsConfig,
    get_llm_metrics_manager
};
use crate::error::Result;
use crate::commands::offline::llm::get_active_provider_type;

// Make provider and model metrics serializable for Tauri
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SerializableProviderMetrics(pub HashMap<String, ProviderPerformanceMetrics>);

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SerializableModelMetrics(pub HashMap<String, ModelPerformanceMetrics>);

/// Get LLM provider metrics
#[tauri::command]
pub fn get_llm_provider_metrics() -> Result<HashMap<String, ProviderPerformanceMetrics>> {
    debug!("Getting LLM provider metrics");
    
    match get_provider_metrics() {
        Some(metrics) => Ok(metrics),
        None => Ok(HashMap::new()),
    }
}

/// Get LLM model metrics
#[tauri::command]
pub fn get_llm_model_metrics() -> Result<HashMap<String, ModelPerformanceMetrics>> {
    debug!("Getting LLM model metrics");
    
    match get_model_metrics() {
        Some(metrics) => Ok(metrics),
        None => Ok(HashMap::new()),
    }
}

/// Get active LLM provider
#[tauri::command]
pub fn get_active_llm_provider() -> Result<String> {
    debug!("Getting active LLM provider");
    
    match get_active_provider_type() {
        Ok(provider) => Ok(provider),
        Err(_) => Ok("None".to_string()),
    }
}

/// Get default LLM model
#[tauri::command]
pub fn get_default_llm_model() -> Result<String> {
    debug!("Getting default LLM model");
    
    // Get from the offline config
    match crate::commands::offline::get_offline_config() {
        Ok(config) => {
            if let Some(default_model) = config.llm_config.default_model {
                Ok(default_model)
            } else {
                Ok("None".to_string())
            }
        },
        Err(_) => Ok("None".to_string()),
    }
}

/// Check if LLM metrics collection is enabled
#[tauri::command]
pub fn get_llm_metrics_enabled() -> Result<bool> {
    debug!("Checking if LLM metrics collection is enabled");
    
    match get_llm_metrics_manager() {
        Some(manager) => Ok(manager.is_enabled()),
        None => Ok(false),
    }
}

/// Get current LLM metrics configuration
#[tauri::command]
pub fn get_llm_metrics_config() -> Result<LLMMetricsConfig> {
    debug!("Getting LLM metrics configuration");
    
    match get_llm_metrics_manager() {
        Some(manager) => Ok(manager.get_config()),
        None => Ok(LLMMetricsConfig::default()),
    }
}

/// Update LLM metrics configuration
#[tauri::command]
pub fn update_llm_metrics_config(config: LLMMetricsConfig) -> Result<bool> {
    debug!("Updating LLM metrics configuration");
    
    match get_llm_metrics_manager() {
        Some(manager) => {
            manager.update_config(config);
            Ok(true)
        },
        None => {
            error!("Cannot update LLM metrics config: manager not initialized");
            Ok(false)
        },
    }
}

/// Accept privacy notice for metrics collection
#[tauri::command]
pub fn accept_llm_metrics_privacy_notice(version: String) -> Result<bool> {
    debug!("Accepting LLM metrics privacy notice: {}", version);
    
    match get_llm_metrics_manager() {
        Some(manager) => {
            manager.accept_privacy_notice(&version);
            Ok(true)
        },
        None => {
            error!("Cannot accept privacy notice: manager not initialized");
            Ok(false)
        },
    }
}

/// Reset all LLM metrics (for testing/debugging)
#[tauri::command]
pub fn reset_llm_metrics() -> Result<bool> {
    debug!("Resetting LLM metrics");
    
    // Simply reinitialize the metrics manager
    match crate::src_common::observability::telemetry::get_telemetry_client() {
        Some(client) => {
            let _ = crate::observability::metrics::llm::init_llm_metrics(Some(client));
            Ok(true)
        },
        None => {
            let _ = crate::observability::metrics::llm::init_llm_metrics(None);
            Ok(true)
        },
    }
}
</file>

<file path="src/commands/mcp.rs">
use crate::protocols::ConnectionStatus;
use crate::services::mcp::get_mcp_service;

/// Connect to MCP server
#[tauri::command]
pub async fn connect() -> Result<(), String> {
    get_mcp_service().connect().await
}

/// Disconnect from MCP server
#[tauri::command]
pub async fn disconnect() -> Result<(), String> {
    get_mcp_service().disconnect().await
}

/// Get connection status
#[tauri::command]
pub fn get_connection_status() -> String {
    match get_mcp_service().connection_status() {
        ConnectionStatus::Disconnected => "disconnected".to_string(),
        ConnectionStatus::Connecting => "connecting".to_string(),
        ConnectionStatus::Connected => "connected".to_string(),
        ConnectionStatus::AuthFailed => "auth_failed".to_string(),
        ConnectionStatus::ConnectionError(e) => format!("error: {}", e),
    }
}
</file>

<file path="src/commands/offline.rs">
use tauri::{command, AppHandle, Manager, State, Window};
use serde::{Serialize, Deserialize};
use log::{debug, info, warn, error};
use std::sync::Arc;

use crate::offline::{self, ConnectivityStatus, OfflineConfig, OfflineStats};
use crate::models::messages::{Message, Conversation};
use crate::error::Result;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OfflineResponse {
    pub success: bool,
    pub message: String,
    pub data: Option<serde_json::Value>,
}

impl OfflineResponse {
    pub fn success(message: &str, data: Option<serde_json::Value>) -> Self {
        Self {
            success: true,
            message: message.to_string(),
            data,
        }
    }
    
    pub fn error(message: &str) -> Self {
        Self {
            success: false,
            message: message.to_string(),
            data: None,
        }
    }
}

/// Check if offline mode is active
#[command]
pub async fn is_offline_mode_active() -> Result<bool> {
    Ok(offline::is_offline())
}

/// Get the current connectivity status
#[command]
pub async fn get_connectivity_status() -> Result<ConnectivityStatus> {
    Ok(offline::get_connectivity_status())
}

/// Enable offline mode
#[command]
pub async fn enable_offline_mode() -> Result<OfflineResponse> {
    match offline::enable_offline_mode() {
        Ok(_) => Ok(OfflineResponse::success("Offline mode enabled", None)),
        Err(e) => Ok(OfflineResponse::error(&format!("Failed to enable offline mode: {}", e))),
    }
}

/// Disable offline mode
#[command]
pub async fn disable_offline_mode() -> Result<OfflineResponse> {
    match offline::disable_offline_mode() {
        Ok(_) => Ok(OfflineResponse::success("Offline mode disabled", None)),
        Err(e) => Ok(OfflineResponse::error(&format!("Failed to disable offline mode: {}", e))),
    }
}

/// Process a message in offline mode
#[command]
pub async fn process_message_offline(message: Message, conversation: Conversation) -> Result<Message> {
    offline::process_message(&message, &conversation)
}

/// Sync offline changes to the cloud
#[command]
pub async fn sync_offline_changes() -> Result<OfflineResponse> {
    match offline::sync_changes() {
        Ok(_) => Ok(OfflineResponse::success("Changes synced successfully", None)),
        Err(e) => Ok(OfflineResponse::error(&format!("Failed to sync changes: {}", e))),
    }
}

/// Get the number of pending sync items
#[command]
pub async fn get_pending_sync_count() -> Result<usize> {
    offline::get_pending_sync_count()
}

/// Get the offline configuration
#[command]
pub async fn get_offline_config() -> Result<OfflineConfig> {
    offline::get_config()
}

/// Update the offline configuration
#[command]
pub async fn update_offline_config(config: OfflineConfig) -> Result<OfflineResponse> {
    match offline::update_config(config) {
        Ok(_) => Ok(OfflineResponse::success("Configuration updated", None)),
        Err(e) => Ok(OfflineResponse::error(&format!("Failed to update configuration: {}", e))),
    }
}

/// Get stats about offline mode
#[command]
pub async fn get_offline_stats() -> Result<OfflineStats> {
    offline::get_stats()
}

/// Get available local models
#[command]
pub async fn get_available_local_models() -> Result<Vec<String>> {
    match offline::get_config() {
        Ok(config) => {
            let llm_engine = crate::offline::llm::LLMEngine::new(
                &config.models_directory,
                &config.default_model,
            )?;
            llm_engine.get_available_models()
        },
        Err(e) => Err(e),
    }
}

/// Register all offline commands with Tauri
pub fn register_offline_commands(builder: tauri::Builder<tauri::Wry>) -> tauri::Builder<tauri::Wry> {
    builder.invoke_handler(tauri::generate_handler![
        is_offline_mode_active,
        get_connectivity_status,
        enable_offline_mode,
        disable_offline_mode,
        process_message_offline,
        sync_offline_changes,
        get_pending_sync_count,
        get_offline_config,
        update_offline_config,
        get_offline_stats,
        get_available_local_models,
    ])
}
</file>

<file path="src/commands/offline/llm.rs">
use tauri::{command, AppHandle, Manager, State, Window};
use serde::{Serialize, Deserialize};
use log::{debug, info, warn, error};
use std::sync::{Arc, Mutex};
use std::collections::HashMap;
use std::path::PathBuf;
use anyhow::anyhow;

use crate::offline::llm::{LocalLLM, ModelInfo, LLMConfig, LLMParameters, DownloadStatus};
use crate::error::{Result, Error};

// --------------------------
// Provider-Related Structs
// --------------------------

/// Provider type enum
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub enum ProviderType {
    Ollama,
    LocalAI,
    LlamaCpp,
    Custom(String),
}

impl ProviderType {
    pub fn to_string(&self) -> String {
        match self {
            ProviderType::Ollama => "Ollama".to_string(),
            ProviderType::LocalAI => "LocalAI".to_string(),
            ProviderType::LlamaCpp => "LlamaCpp".to_string(),
            ProviderType::Custom(name) => format!("Custom({})", name),
        }
    }

    pub fn from_string(s: &str) -> Result<Self> {
        match s {
            "Ollama" => Ok(ProviderType::Ollama),
            "LocalAI" => Ok(ProviderType::LocalAI),
            "LlamaCpp" => Ok(ProviderType::LlamaCpp),
            s if s.starts_with("Custom(") && s.ends_with(")") => {
                let name = s[7..s.len()-1].to_string();
                Ok(ProviderType::Custom(name))
            },
            _ => Err(Error::InvalidInput(format!("Invalid provider type: {}", s))),
        }
    }
}

/// Provider information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProviderInfo {
    /// Provider type identifier
    pub provider_type: String,
    /// Display name
    pub name: String,
    /// Provider description
    pub description: String,
    /// Provider version
    pub version: String,
    /// Default endpoint URL
    pub default_endpoint: String,
    /// Whether the provider supports text generation
    pub supports_text_generation: bool,
    /// Whether the provider supports chat
    pub supports_chat: bool,
    /// Whether the provider supports embeddings
    pub supports_embeddings: bool,
    /// Whether the provider requires an API key
    pub requires_api_key: bool,
}

/// Provider availability status
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AvailabilityResult {
    /// Whether the provider is available
    pub available: bool,
    /// Provider version if available
    pub version: Option<String>,
    /// Error message if not available
    pub error: Option<String>,
    /// Response time in milliseconds
    pub response_time_ms: Option<u64>,
}

/// Provider configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProviderConfig {
    /// Provider type identifier
    pub provider_type: String,
    /// Endpoint URL for the provider
    pub endpoint_url: String,
    /// API key for the provider (if required)
    pub api_key: Option<String>,
    /// Default model to use
    pub default_model: Option<String>,
    /// Whether to enable advanced configuration
    pub enable_advanced_config: bool,
    /// Advanced configuration options (provider-specific)
    pub advanced_config: HashMap<String, serde_json::Value>,
}

/// Enhanced model information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EnhancedModelInfo {
    /// Model identifier
    pub id: String,
    /// Model name for display
    pub name: String,
    /// Model description
    pub description: String,
    /// Model size in bytes
    pub size_bytes: usize,
    /// Whether the model is downloaded
    pub is_downloaded: bool,
    /// Provider-specific metadata
    pub provider_metadata: HashMap<String, serde_json::Value>,
    /// Provider type
    pub provider: String,
    /// Whether the model supports text generation
    pub supports_text_generation: bool,
    /// Whether the model supports completion
    pub supports_completion: bool,
    /// Whether the model supports chat
    pub supports_chat: bool,
    /// Whether the model supports embeddings
    pub supports_embeddings: bool,
    /// Whether the model supports image generation
    pub supports_image_generation: bool,
    /// Quantization level (if applicable)
    pub quantization: Option<String>,
    /// Parameter count in billions
    pub parameter_count_b: Option<f32>,
    /// Context length in tokens
    pub context_length: Option<usize>,
    /// Model family/architecture
    pub model_family: Option<String>,
    /// When the model was created
    pub created_at: Option<String>,
    /// Model tags
    pub tags: Vec<String>,
    /// Model license
    pub license: Option<String>,
}

impl From<ModelInfo> for EnhancedModelInfo {
    fn from(info: ModelInfo) -> Self {
        Self {
            id: info.id,
            name: info.name,
            description: info.description,
            size_bytes: info.size_mb * 1024 * 1024,
            is_downloaded: info.installed,
            provider_metadata: HashMap::new(),
            provider: "LlamaCpp".to_string(), // Default to LlamaCpp for basic ModelInfo
            supports_text_generation: true,
            supports_completion: true,
            supports_chat: true,
            supports_embeddings: false,
            supports_image_generation: false,
            quantization: None,
            parameter_count_b: None,
            context_length: Some(info.context_size),
            model_family: None,
            created_at: None,
            tags: Vec::new(),
            license: None,
        }
    }
}

/// Enhanced download status
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EnhancedDownloadStatus {
    /// Status type 
    pub status: String,
    /// Not started status (empty object)
    pub NotStarted: Option<HashMap<String, serde_json::Value>>,
    /// In progress status
    pub InProgress: Option<InProgressStatus>,
    /// Completed status
    pub Completed: Option<CompletedStatus>,
    /// Failed status
    pub Failed: Option<FailedStatus>,
    /// Cancelled status
    pub Cancelled: Option<CancelledStatus>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InProgressStatus {
    /// Download progress as percentage
    pub percent: f32,
    /// Bytes downloaded
    pub bytes_downloaded: Option<usize>,
    /// Total bytes to download
    pub total_bytes: Option<usize>,
    /// Estimated time remaining in seconds
    pub eta_seconds: Option<u64>,
    /// Download speed in bytes per second
    pub bytes_per_second: Option<usize>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CompletedStatus {
    /// When the download completed
    pub completed_at: Option<String>,
    /// Total download duration in seconds
    pub duration_seconds: Option<u64>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FailedStatus {
    /// Reason for failure
    pub reason: String,
    /// Error code if available
    pub error_code: Option<String>,
    /// When the download failed
    pub failed_at: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CancelledStatus {
    /// When the download was cancelled
    pub cancelled_at: Option<String>,
}

impl From<DownloadStatus> for EnhancedDownloadStatus {
    fn from(status: DownloadStatus) -> Self {
        if status.complete {
            Self {
                status: "Completed".to_string(),
                NotStarted: None,
                InProgress: None,
                Completed: Some(CompletedStatus {
                    completed_at: Some(chrono::Utc::now().to_rfc3339()),
                    duration_seconds: None,
                }),
                Failed: None,
                Cancelled: None,
            }
        } else if let Some(error) = status.error {
            Self {
                status: "Failed".to_string(),
                NotStarted: None,
                InProgress: None,
                Completed: None,
                Failed: Some(FailedStatus {
                    reason: error,
                    error_code: None,
                    failed_at: Some(chrono::Utc::now().to_rfc3339()),
                }),
                Cancelled: None,
            }
        } else {
            Self {
                status: "InProgress".to_string(),
                NotStarted: None,
                InProgress: Some(InProgressStatus {
                    percent: status.progress * 100.0,
                    bytes_downloaded: Some(status.bytes_downloaded),
                    total_bytes: Some(status.total_bytes),
                    eta_seconds: Some(status.eta_seconds),
                    bytes_per_second: Some(status.speed_bps),
                }),
                Completed: None,
                Failed: None,
                Cancelled: None,
            }
        }
    }
}

/// Command response wrapper
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CommandResponse<T> {
    /// Whether the command was successful
    pub success: bool,
    /// Error message if unsuccessful
    pub error: Option<String>,
    /// Response data if successful
    pub data: Option<T>,
}

impl<T> CommandResponse<T> {
    pub fn success(data: T) -> Self {
        Self {
            success: true,
            error: None,
            data: Some(data),
        }
    }
    
    pub fn error(message: &str) -> Self {
        Self {
            success: false,
            error: Some(message.to_string()),
            data: None,
        }
    }
}

// --------------------------
// Provider Manager
// --------------------------

/// Provider manager state
#[derive(Debug)]
pub struct ProviderManager {
    /// Available providers
    providers: Mutex<HashMap<String, ProviderInfo>>,
    /// Provider availability status
    availability: Mutex<HashMap<String, AvailabilityResult>>,
    /// Active provider
    active_provider: Mutex<Option<ProviderType>>,
    /// Provider configurations
    configs: Mutex<HashMap<String, ProviderConfig>>,
    /// LLM instances for each provider
    llm_instances: Mutex<HashMap<String, Arc<LocalLLM>>>,
}

impl Default for ProviderManager {
    fn default() -> Self {
        let mut providers = HashMap::new();
        
        // Add default providers
        providers.insert(
            "Ollama".to_string(),
            ProviderInfo {
                provider_type: "Ollama".to_string(),
                name: "Ollama".to_string(),
                description: "Local model runner for LLama and other models".to_string(),
                version: "1.0.0".to_string(),
                default_endpoint: "http://localhost:11434".to_string(),
                supports_text_generation: true,
                supports_chat: true,
                supports_embeddings: true,
                requires_api_key: false,
            },
        );
        
        providers.insert(
            "LocalAI".to_string(),
            ProviderInfo {
                provider_type: "LocalAI".to_string(),
                name: "LocalAI".to_string(),
                description: "Self-hosted OpenAI API compatible server".to_string(),
                version: "1.0.0".to_string(),
                default_endpoint: "http://localhost:8080".to_string(),
                supports_text_generation: true,
                supports_chat: true,
                supports_embeddings: true,
                requires_api_key: false,
            },
        );
        
        providers.insert(
            "LlamaCpp".to_string(),
            ProviderInfo {
                provider_type: "LlamaCpp".to_string(),
                name: "llama.cpp".to_string(),
                description: "Embedded llama.cpp integration for efficient local inference".to_string(),
                version: "1.0.0".to_string(),
                default_endpoint: "local://models".to_string(),
                supports_text_generation: true,
                supports_chat: true,
                supports_embeddings: false,
                requires_api_key: false,
            },
        );
        
        // Create default configs for each provider
        let mut configs = HashMap::new();
        for (provider_type, info) in &providers {
            configs.insert(
                provider_type.clone(),
                ProviderConfig {
                    provider_type: provider_type.clone(),
                    endpoint_url: info.default_endpoint.clone(),
                    api_key: None,
                    default_model: None,
                    enable_advanced_config: false,
                    advanced_config: HashMap::new(),
                },
            );
        }
        
        // Create LLM instances
        let mut llm_instances = HashMap::new();
        llm_instances.insert("LlamaCpp".to_string(), Arc::new(LocalLLM::new_manager()));
        
        Self {
            providers: Mutex::new(providers),
            availability: Mutex::new(HashMap::new()),
            active_provider: Mutex::new(Some(ProviderType::LlamaCpp)),
            configs: Mutex::new(configs),
            llm_instances: Mutex::new(llm_instances),
        }
    }
}

impl ProviderManager {
    /// Get all available providers
    pub fn get_all_providers(&self) -> Result<Vec<ProviderInfo>> {
        let providers = self.providers.lock().unwrap();
        Ok(providers.values().cloned().collect())
    }
    
    /// Get a specific provider
    pub fn get_provider(&self, provider_type: &str) -> Result<ProviderInfo> {
        let providers = self.providers.lock().unwrap();
        
        providers.get(provider_type)
            .cloned()
            .ok_or_else(|| Error::NotFound(format!("Provider {} not found", provider_type)))
    }
    
    /// Add a custom provider
    pub fn add_provider(&self, provider: ProviderInfo) -> Result<()> {
        let mut providers = self.providers.lock().unwrap();
        
        // Check if provider type is valid
        if provider.provider_type.starts_with("Custom(") && provider.provider_type.ends_with(")") {
            providers.insert(provider.provider_type.clone(), provider.clone());
            
            // Add default config
            let mut configs = self.configs.lock().unwrap();
            configs.insert(
                provider.provider_type.clone(),
                ProviderConfig {
                    provider_type: provider.provider_type.clone(),
                    endpoint_url: provider.default_endpoint.clone(),
                    api_key: None,
                    default_model: None,
                    enable_advanced_config: false,
                    advanced_config: HashMap::new(),
                },
            );
            
            Ok(())
        } else {
            Err(Error::InvalidInput("Custom provider type must start with 'Custom(' and end with ')'".to_string()))
        }
    }
    
    /// Remove a custom provider
    pub fn remove_provider(&self, provider_type: &str) -> Result<()> {
        let mut providers = self.providers.lock().unwrap();
        
        // Only allow removing custom providers
        if provider_type.starts_with("Custom(") && provider_type.ends_with(")") {
            if providers.remove(provider_type).is_some() {
                // Remove config
                let mut configs = self.configs.lock().unwrap();
                configs.remove(provider_type);
                
                // Remove LLM instance if exists
                let mut llm_instances = self.llm_instances.lock().unwrap();
                llm_instances.remove(provider_type);
                
                Ok(())
            } else {
                Err(Error::NotFound(format!("Provider {} not found", provider_type)))
            }
        } else {
            Err(Error::InvalidInput("Only custom providers can be removed".to_string()))
        }
    }
    
    /// Check availability of all providers
    pub fn check_all_providers(&self) -> Result<HashMap<String, AvailabilityResult>> {
        let providers = self.providers.lock().unwrap();
        let configs = self.configs.lock().unwrap();
        let mut availability = self.availability.lock().unwrap();
        
        for (provider_type, provider) in providers.iter() {
            if let Some(config) = configs.get(provider_type) {
                // Check provider availability
                let start_time = std::time::Instant::now();
                let available = match provider_type.as_str() {
                    "Ollama" => self.check_ollama(&config.endpoint_url),
                    "LocalAI" => self.check_localai(&config.endpoint_url),
                    "LlamaCpp" => self.check_llamacpp(),
                    _ if provider_type.starts_with("Custom(") => self.check_custom(&config.endpoint_url),
                    _ => Err(anyhow!("Unsupported provider type")),
                };
                
                let elapsed = start_time.elapsed();
                
                // Update availability
                match available {
                    Ok(version) => {
                        availability.insert(
                            provider_type.clone(),
                            AvailabilityResult {
                                available: true,
                                version: Some(version),
                                error: None,
                                response_time_ms: Some(elapsed.as_millis() as u64),
                            },
                        );
                    }
                    Err(e) => {
                        availability.insert(
                            provider_type.clone(),
                            AvailabilityResult {
                                available: false,
                                version: None,
                                error: Some(e.to_string()),
                                response_time_ms: Some(elapsed.as_millis() as u64),
                            },
                        );
                    }
                }
            }
        }
        
        Ok(availability.clone())
    }
    
    /// Check availability of a specific provider
    pub fn check_provider(&self, provider_type: &str) -> Result<AvailabilityResult> {
        let configs = self.configs.lock().unwrap();
        let mut availability = self.availability.lock().unwrap();
        
        let config = configs.get(provider_type)
            .ok_or_else(|| Error::NotFound(format!("Provider config for {} not found", provider_type)))?;
        
        // Check provider availability
        let start_time = std::time::Instant::now();
        let available = match provider_type {
            "Ollama" => self.check_ollama(&config.endpoint_url),
            "LocalAI" => self.check_localai(&config.endpoint_url),
            "LlamaCpp" => self.check_llamacpp(),
            _ if provider_type.starts_with("Custom(") => self.check_custom(&config.endpoint_url),
            _ => Err(anyhow!("Unsupported provider type")),
        };
        
        let elapsed = start_time.elapsed();
        
        // Update availability
        let result = match available {
            Ok(version) => {
                AvailabilityResult {
                    available: true,
                    version: Some(version),
                    error: None,
                    response_time_ms: Some(elapsed.as_millis() as u64),
                }
            }
            Err(e) => {
                AvailabilityResult {
                    available: false,
                    version: None,
                    error: Some(e.to_string()),
                    response_time_ms: Some(elapsed.as_millis() as u64),
                }
            }
        };
        
        // Store result
        availability.insert(provider_type.to_string(), result.clone());
        
        Ok(result)
    }
    
    // Check Ollama availability
    fn check_ollama(&self, endpoint: &str) -> anyhow::Result<String> {
        // This is a simplified check - in a real implementation,
        // you would make an HTTP request to the Ollama API
        if endpoint.starts_with("http://") || endpoint.starts_with("https://") {
            // Simulate HTTP request
            std::thread::sleep(std::time::Duration::from_millis(100));
            
            if endpoint.contains("localhost") || endpoint.contains("127.0.0.1") {
                Ok("0.1.0".to_string())
            } else {
                Err(anyhow!("Could not connect to Ollama endpoint"))
            }
        } else {
            Err(anyhow!("Invalid Ollama endpoint URL"))
        }
    }
    
    // Check LocalAI availability
    fn check_localai(&self, endpoint: &str) -> anyhow::Result<String> {
        // Similar to check_ollama, but for LocalAI
        if endpoint.starts_with("http://") || endpoint.starts_with("https://") {
            // Simulate HTTP request
            std::thread::sleep(std::time::Duration::from_millis(100));
            
            if endpoint.contains("localhost") || endpoint.contains("127.0.0.1") {
                Ok("1.0.0".to_string())
            } else {
                Err(anyhow!("Could not connect to LocalAI endpoint"))
            }
        } else {
            Err(anyhow!("Invalid LocalAI endpoint URL"))
        }
    }
    
    // Check LlamaCpp availability
    fn check_llamacpp(&self) -> anyhow::Result<String> {
        // Check if any models are available
        let llm_instances = self.llm_instances.lock().unwrap();
        
        if let Some(llm) = llm_instances.get("LlamaCpp") {
            if llm.list_models().is_empty() {
                Err(anyhow!("No models available for llama.cpp"))
            } else {
                Ok("0.2.0".to_string())
            }
        } else {
            Err(anyhow!("llama.cpp instance not found"))
        }
    }
    
    // Check custom provider availability
    fn check_custom(&self, endpoint: &str) -> anyhow::Result<String> {
        // Basic check for custom providers
        if endpoint.starts_with("http://") || endpoint.starts_with("https://") {
            // Simulate HTTP request
            std::thread::sleep(std::time::Duration::from_millis(150));
            
            if endpoint.contains("localhost") || endpoint.contains("127.0.0.1") {
                Ok("1.0.0".to_string())
            } else {
                Err(anyhow!("Could not connect to custom provider endpoint"))
            }
        } else {
            Err(anyhow!("Invalid custom provider endpoint URL"))
        }
    }
    
    /// Get the active provider type
    pub fn get_active_provider(&self) -> Result<ProviderType> {
        let active_provider = self.active_provider.lock().unwrap();
        
        active_provider.clone()
            .ok_or_else(|| Error::NotInitialized("No active provider set".to_string()))
    }
    
    /// Set the active provider
    pub fn set_active_provider(&self, provider_type: &str) -> Result<()> {
        let providers = self.providers.lock().unwrap();
        
        if providers.contains_key(provider_type) {
            let provider_type = ProviderType::from_string(provider_type)?;
            *self.active_provider.lock().unwrap() = Some(provider_type);
            Ok(())
        } else {
            Err(Error::NotFound(format!("Provider {} not found", provider_type)))
        }
    }
    
    /// Get the configuration for a provider
    pub fn get_provider_config(&self, provider_type: &str) -> Result<ProviderConfig> {
        let configs = self.configs.lock().unwrap();
        
        configs.get(provider_type)
            .cloned()
            .ok_or_else(|| Error::NotFound(format!("Provider config for {} not found", provider_type)))
    }
    
    /// Update the configuration for a provider
    pub fn update_provider_config(&self, config: ProviderConfig) -> Result<()> {
        let mut configs = self.configs.lock().unwrap();
        
        if configs.contains_key(&config.provider_type) {
            configs.insert(config.provider_type.clone(), config);
            Ok(())
        } else {
            Err(Error::NotFound(format!("Provider {} not found", config.provider_type)))
        }
    }
    
    /// List all available models for a provider
    pub fn list_available_models(&self, provider_type: &str) -> Result<Vec<EnhancedModelInfo>> {
        match provider_type {
            "LlamaCpp" => {
                let llm_instances = self.llm_instances.lock().unwrap();
                
                if let Some(llm) = llm_instances.get("LlamaCpp") {
                    let models = llm.list_models();
                    Ok(models.into_iter().map(EnhancedModelInfo::from).collect())
                } else {
                    Err(Error::NotInitialized("LlamaCpp instance not found".to_string()))
                }
            },
            "Ollama" => {
                // Simulate fetching models from Ollama API
                self.simulate_ollama_models()
            },
            "LocalAI" => {
                // Simulate fetching models from LocalAI API
                self.simulate_localai_models()
            },
            _ if provider_type.starts_with("Custom(") => {
                // Simulate fetching models from custom provider
                self.simulate_custom_models(provider_type)
            },
            _ => Err(Error::InvalidInput(format!("Unsupported provider type: {}", provider_type))),
        }
    }
    
    // Simulate fetching models from Ollama
    fn simulate_ollama_models(&self) -> Result<Vec<EnhancedModelInfo>> {
        // This would be an API call in a real implementation
        let models = vec![
            EnhancedModelInfo {
                id: "llama2".to_string(),
                name: "Llama 2".to_string(),
                description: "Open-source LLM with 7B parameters".to_string(),
                size_bytes: 4 * 1024 * 1024 * 1024, // 4GB
                is_downloaded: true,
                provider_metadata: HashMap::new(),
                provider: "Ollama".to_string(),
                supports_text_generation: true,
                supports_completion: true,
                supports_chat: true,
                supports_embeddings: false,
                supports_image_generation: false,
                quantization: Some("Q4_K_M".to_string()),
                parameter_count_b: Some(7.0),
                context_length: Some(4096),
                model_family: Some("Llama".to_string()),
                created_at: Some("2023-07-18T00:00:00Z".to_string()),
                tags: vec!["llama".to_string(), "meta".to_string()],
                license: Some("Meta License".to_string()),
            },
            EnhancedModelInfo {
                id: "mistral".to_string(),
                name: "Mistral".to_string(),
                description: "Mistral 7B model with excellent performance".to_string(),
                size_bytes: 5 * 1024 * 1024 * 1024, // 5GB
                is_downloaded: false,
                provider_metadata: HashMap::new(),
                provider: "Ollama".to_string(),
                supports_text_generation: true,
                supports_completion: true,
                supports_chat: true,
                supports_embeddings: false,
                supports_image_generation: false,
                quantization: Some("Q4_K_M".to_string()),
                parameter_count_b: Some(7.0),
                context_length: Some(8192),
                model_family: Some("Mistral".to_string()),
                created_at: Some("2023-09-15T00:00:00Z".to_string()),
                tags: vec!["mistral".to_string(), "mistral-ai".to_string()],
                license: Some("Apache 2.0".to_string()),
            },
        ];
        
        Ok(models)
    }
    
    // Simulate fetching models from LocalAI
    fn simulate_localai_models(&self) -> Result<Vec<EnhancedModelInfo>> {
        // This would be an API call in a real implementation
        let models = vec![
            EnhancedModelInfo {
                id: "ggml-gpt4all-j".to_string(),
                name: "GPT4All-J".to_string(),
                description: "Compact and efficient GPT4All-J model".to_string(),
                size_bytes: 2 * 1024 * 1024 * 1024, // 2GB
                is_downloaded: true,
                provider_metadata: HashMap::new(),
                provider: "LocalAI".to_string(),
                supports_text_generation: true,
                supports_completion: true,
                supports_chat: true,
                supports_embeddings: false,
                supports_image_generation: false,
                quantization: Some("Q4_0".to_string()),
                parameter_count_b: Some(6.0),
                context_length: Some(2048),
                model_family: Some("GPT".to_string()),
                created_at: Some("2023-04-28T00:00:00Z".to_string()),
                tags: vec!["gpt".to_string(), "compact".to_string()],
                license: Some("MIT".to_string()),
            },
            EnhancedModelInfo {
                id: "ggml-vicuna-13b-1.1".to_string(),
                name: "Vicuna 13B".to_string(),
                description: "Vicuna 13B fine-tuned model".to_string(),
                size_bytes: 8 * 1024 * 1024 * 1024, // 8GB
                is_downloaded: false,
                provider_metadata: HashMap::new(),
                provider: "LocalAI".to_string(),
                supports_text_generation: true,
                supports_completion: true,
                supports_chat: true,
                supports_embeddings: false,
                supports_image_generation: false,
                quantization: Some("Q4_0".to_string()),
                parameter_count_b: Some(13.0),
                context_length: Some(4096),
                model_family: Some("Vicuna".to_string()),
                created_at: Some("2023-05-10T00:00:00Z".to_string()),
                tags: vec!["vicuna".to_string(), "large".to_string()],
                license: Some("License-to-use".to_string()),
            },
        ];
        
        Ok(models)
    }
    
    // Simulate fetching models from custom provider
    fn simulate_custom_models(&self, provider_type: &str) -> Result<Vec<EnhancedModelInfo>> {
        // This would be an API call in a real implementation
        let models = vec![
            EnhancedModelInfo {
                id: format!("{}-model1", provider_type),
                name: "Custom Model 1".to_string(),
                description: "A custom model for demonstration".to_string(),
                size_bytes: 1 * 1024 * 1024 * 1024, // 1GB
                is_downloaded: true,
                provider_metadata: HashMap::new(),
                provider: provider_type.to_string(),
                supports_text_generation: true,
                supports_completion: true,
                supports_chat: true,
                supports_embeddings: true,
                supports_image_generation: false,
                quantization: None,
                parameter_count_b: Some(2.0),
                context_length: Some(4096),
                model_family: Some("Custom".to_string()),
                created_at: Some("2023-12-01T00:00:00Z".to_string()),
                tags: vec!["custom".to_string(), "demo".to_string()],
                license: Some("Proprietary".to_string()),
            },
        ];
        
        Ok(models)
    }
    
    /// List all downloaded models for a provider
    pub fn list_downloaded_models(&self, provider_type: &str) -> Result<Vec<EnhancedModelInfo>> {
        // Get all models and filter for downloaded ones
        let all_models = self.list_available_models(provider_type)?;
        let downloaded = all_models.into_iter().filter(|m| m.is_downloaded).collect();
        Ok(downloaded)
    }
    
    /// Get the download status for a model
    pub fn get_download_status(&self, model_id: &str) -> Result<EnhancedDownloadStatus> {
        // For simplicity, we'll just use the LlamaCpp provider here
        let llm_instances = self.llm_instances.lock().unwrap();
        
        if let Some(llm) = llm_instances.get("LlamaCpp") {
            let download_id = format!("download_{}", model_id);
            
            if let Some(status) = llm.get_download_status(&download_id) {
                Ok(EnhancedDownloadStatus::from(status))
            } else {
                // If no download status, assume not started
                Ok(EnhancedDownloadStatus {
                    status: "NotStarted".to_string(),
                    NotStarted: Some(HashMap::new()),
                    InProgress: None,
                    Completed: None,
                    Failed: None,
                    Cancelled: None,
                })
            }
        } else {
            Err(Error::NotInitialized("LlamaCpp instance not found".to_string()))
        }
    }
    
    /// Download a model
    pub fn download_model(&self, provider_type: &str, model_id: &str) -> Result<EnhancedDownloadStatus> {
        match provider_type {
            "LlamaCpp" => {
                let llm_instances = self.llm_instances.lock().unwrap();
                
                if let Some(llm) = llm_instances.get("LlamaCpp") {
                    match llm.download_model(model_id) {
                        Ok(download_id) => {
                            if let Some(status) = llm.get_download_status(&download_id) {
                                Ok(EnhancedDownloadStatus::from(status))
                            } else {
                                Err(Error::Internal("Failed to get download status".to_string()))
                            }
                        },
                        Err(e) => Err(Error::Internal(e)),
                    }
                } else {
                    Err(Error::NotInitialized("LlamaCpp instance not found".to_string()))
                }
            },
            "Ollama" | "LocalAI" | _ if provider_type.starts_with("Custom(") => {
                // Simulate starting a download
                Ok(EnhancedDownloadStatus {
                    status: "InProgress".to_string(),
                    NotStarted: None,
                    InProgress: Some(InProgressStatus {
                        percent: 0.0,
                        bytes_downloaded: Some(0),
                        total_bytes: Some(1 * 1024 * 1024 * 1024), // 1GB
                        eta_seconds: Some(600), // 10 minutes
                        bytes_per_second: Some(2 * 1024 * 1024), // 2MB/s
                    }),
                    Completed: None,
                    Failed: None,
                    Cancelled: None,
                })
            },
            _ => Err(Error::InvalidInput(format!("Unsupported provider type: {}", provider_type))),
        }
    }
    
    /// Cancel a model download
    pub fn cancel_download(&self, provider_type: &str, model_id: &str) -> Result<bool> {
        match provider_type {
            "LlamaCpp" => {
                let llm_instances = self.llm_instances.lock().unwrap();
                
                if let Some(llm) = llm_instances.get("LlamaCpp") {
                    let download_id = format!("download_{}", model_id);
                    match llm.cancel_download(&download_id) {
                        Ok(_) => Ok(true),
                        Err(e) => Err(Error::Internal(e)),
                    }
                } else {
                    Err(Error::NotInitialized("LlamaCpp instance not found".to_string()))
                }
            },
            "Ollama" | "LocalAI" | _ if provider_type.starts_with("Custom(") => {
                // Simulate cancelling a download
                Ok(true)
            },
            _ => Err(Error::InvalidInput(format!("Unsupported provider type: {}", provider_type))),
        }
    }
    
    /// Delete a model
    pub fn delete_model(&self, provider_type: &str, model_id: &str) -> Result<bool> {
        // In a real implementation, you would call the appropriate provider API
        // For now, just return success
        Ok(true)
    }
    
    /// Generate text using the active provider
    pub fn generate_text(&self, prompt: &str, max_tokens: usize) -> Result<String> {
        let active_provider = self.get_active_provider()?;
        
        match active_provider {
            ProviderType::LlamaCpp => {
                let llm_instances = self.llm_instances.lock().unwrap();
                
                if let Some(llm) = llm_instances.get("LlamaCpp") {
                    Ok(llm.generate(prompt, max_tokens))
                } else {
                    Err(Error::NotInitialized("LlamaCpp instance not found".to_string()))
                }
            },
            ProviderType::Ollama => {
                // Simulate Ollama text generation
                std::thread::sleep(std::time::Duration::from_millis(500));
                Ok(format!("Generated text from Ollama: {}", prompt))
            },
            ProviderType::LocalAI => {
                // Simulate LocalAI text generation
                std::thread::sleep(std::time::Duration::from_millis(700));
                Ok(format!("Generated text from LocalAI: {}", prompt))
            },
            ProviderType::Custom(name) => {
                // Simulate custom provider text generation
                std::thread::sleep(std::time::Duration::from_millis(600));
                Ok(format!("Generated text from custom provider {}: {}", name, prompt))
            },
        }
    }
}

// --------------------------
// Tauri Commands
// --------------------------

/// Get all available providers
#[command]
pub async fn get_all_providers(
    state: State<'_, ProviderManager>,
) -> CommandResponse<Vec<ProviderInfo>> {
    match state.get_all_providers() {
        Ok(providers) => CommandResponse::success(providers),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// Check availability of all providers
#[command]
pub async fn get_all_provider_availability(
    state: State<'_, ProviderManager>,
) -> CommandResponse<HashMap<String, AvailabilityResult>> {
    match state.check_all_providers() {
        Ok(availability) => CommandResponse::success(availability),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// Check availability of a specific provider
#[command]
pub async fn check_provider_availability(
    provider_type: String,
    state: State<'_, ProviderManager>,
) -> CommandResponse<AvailabilityResult> {
    match state.check_provider(&provider_type) {
        Ok(availability) => CommandResponse::success(availability),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// Add a custom provider
#[command]
pub async fn add_custom_provider(
    provider: ProviderInfo,
    state: State<'_, ProviderManager>,
) -> CommandResponse<bool> {
    match state.add_provider(provider) {
        Ok(_) => CommandResponse::success(true),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// Remove a custom provider
#[command]
pub async fn remove_custom_provider(
    provider_type: String,
    state: State<'_, ProviderManager>,
) -> CommandResponse<bool> {
    match state.remove_provider(&provider_type) {
        Ok(_) => CommandResponse::success(true),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// Get the active provider
#[command]
pub async fn get_active_provider(
    state: State<'_, ProviderManager>,
) -> CommandResponse<String> {
    match state.get_active_provider() {
        Ok(provider_type) => CommandResponse::success(provider_type.to_string()),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// Set the active provider
#[command]
pub async fn set_active_provider(
    provider_type: String,
    state: State<'_, ProviderManager>,
) -> CommandResponse<bool> {
    match state.set_active_provider(&provider_type) {
        Ok(_) => CommandResponse::success(true),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// Get the configuration for a provider
#[command]
pub async fn get_provider_config(
    provider_type: String,
    state: State<'_, ProviderManager>,
) -> CommandResponse<ProviderConfig> {
    match state.get_provider_config(&provider_type) {
        Ok(config) => CommandResponse::success(config),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// Update the configuration for a provider
#[command]
pub async fn update_provider_config(
    config: ProviderConfig,
    state: State<'_, ProviderManager>,
) -> CommandResponse<bool> {
    match state.update_provider_config(config) {
        Ok(_) => CommandResponse::success(true),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// List available models for a provider
#[command]
pub async fn list_available_models(
    provider_type: Option<String>,
    state: State<'_, ProviderManager>,
) -> CommandResponse<Vec<EnhancedModelInfo>> {
    // Use active provider if none specified
    let provider_type = match provider_type {
        Some(pt) => pt,
        None => match state.get_active_provider() {
            Ok(pt) => pt.to_string(),
            Err(e) => return CommandResponse::error(&e.to_string()),
        },
    };
    
    match state.list_available_models(&provider_type) {
        Ok(models) => CommandResponse::success(models),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// List downloaded models for a provider
#[command]
pub async fn list_downloaded_models(
    provider_type: Option<String>,
    state: State<'_, ProviderManager>,
) -> CommandResponse<Vec<EnhancedModelInfo>> {
    // Use active provider if none specified
    let provider_type = match provider_type {
        Some(pt) => pt,
        None => match state.get_active_provider() {
            Ok(pt) => pt.to_string(),
            Err(e) => return CommandResponse::error(&e.to_string()),
        },
    };
    
    match state.list_downloaded_models(&provider_type) {
        Ok(models) => CommandResponse::success(models),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// Get download status for a model
#[command]
pub async fn get_download_status(
    model_id: String,
    provider_type: Option<String>,
    state: State<'_, ProviderManager>,
) -> CommandResponse<EnhancedDownloadStatus> {
    match state.get_download_status(&model_id) {
        Ok(status) => CommandResponse::success(status),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// Download a model
#[command]
pub async fn download_model(
    model_id: String,
    provider_type: Option<String>,
    state: State<'_, ProviderManager>,
) -> CommandResponse<EnhancedDownloadStatus> {
    // Use active provider if none specified
    let provider_type = match provider_type {
        Some(pt) => pt,
        None => match state.get_active_provider() {
            Ok(pt) => pt.to_string(),
            Err(e) => return CommandResponse::error(&e.to_string()),
        },
    };
    
    match state.download_model(&provider_type, &model_id) {
        Ok(status) => CommandResponse::success(status),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// Cancel a model download
#[command]
pub async fn cancel_download(
    model_id: String,
    provider_type: Option<String>,
    state: State<'_, ProviderManager>,
) -> CommandResponse<bool> {
    // Use active provider if none specified
    let provider_type = match provider_type {
        Some(pt) => pt,
        None => match state.get_active_provider() {
            Ok(pt) => pt.to_string(),
            Err(e) => return CommandResponse::error(&e.to_string()),
        },
    };
    
    match state.cancel_download(&provider_type, &model_id) {
        Ok(success) => CommandResponse::success(success),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// Delete a model
#[command]
pub async fn delete_model(
    model_id: String,
    provider_type: Option<String>,
    state: State<'_, ProviderManager>,
) -> CommandResponse<bool> {
    // Use active provider if none specified
    let provider_type = match provider_type {
        Some(pt) => pt,
        None => match state.get_active_provider() {
            Ok(pt) => pt.to_string(),
            Err(e) => return CommandResponse::error(&e.to_string()),
        },
    };
    
    match state.delete_model(&provider_type, &model_id) {
        Ok(success) => CommandResponse::success(success),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// Generate text using a model
#[command]
pub async fn generate_text(
    prompt: String,
    max_tokens: Option<usize>,
    model_id: Option<String>,
    provider_type: Option<String>,
    state: State<'_, ProviderManager>,
) -> CommandResponse<String> {
    // Default max tokens if not specified
    let max_tokens = max_tokens.unwrap_or(1024);
    
    match state.generate_text(&prompt, max_tokens) {
        Ok(text) => CommandResponse::success(text),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// Register all LLM commands
pub fn register_commands(app: &mut tauri::App) -> Result<()> {
    // Create and manage the provider manager
    let provider_manager = ProviderManager::default();
    app.manage(provider_manager);
    
    Ok(())
}

// --------------------------
// Provider Discovery Commands
// --------------------------

/// Scan for LLM providers
#[command]
pub async fn scan_for_providers(
    app_handle: AppHandle,
) -> CommandResponse<bool> {
    let offline_manager = app_handle.state::<Arc<crate::offline::OfflineManager>>();
    
    match offline_manager.scan_for_llm_providers().await {
        Ok(_) => CommandResponse::success(true),
        Err(e) => CommandResponse::error(&e),
    }
}

/// Get LLM provider discovery status
#[command]
pub async fn get_discovery_status(
    app_handle: AppHandle,
) -> CommandResponse<HashMap<String, crate::offline::llm::discovery::InstallationInfo>> {
    let offline_manager = app_handle.state::<Arc<crate::offline::OfflineManager>>();
    let discovery = offline_manager.get_llm_discovery();
    
    let installations = discovery.get_installations();
    CommandResponse::success(installations)
}

/// Get LLM provider suggestions
#[command]
pub async fn get_provider_suggestions(
    app_handle: AppHandle,
) -> CommandResponse<Vec<crate::offline::llm::discovery::ProviderSuggestion>> {
    let offline_manager = app_handle.state::<Arc<crate::offline::OfflineManager>>();
    let suggestions = offline_manager.get_provider_suggestions();
    
    CommandResponse::success(suggestions)
}

/// Get LLM provider discovery configuration
#[command]
pub async fn get_discovery_config(
    app_handle: AppHandle,
) -> CommandResponse<crate::offline::llm::discovery::DiscoveryConfig> {
    let offline_manager = app_handle.state::<Arc<crate::offline::OfflineManager>>();
    let discovery = offline_manager.get_llm_discovery();
    
    let config = discovery.get_config();
    CommandResponse::success(config)
}

/// Update LLM provider discovery configuration
#[command]
pub async fn update_discovery_config(
    config: crate::offline::llm::discovery::DiscoveryConfig,
    app_handle: AppHandle,
) -> CommandResponse<bool> {
    let offline_manager = app_handle.state::<Arc<crate::offline::OfflineManager>>();
    let discovery = offline_manager.get_llm_discovery();
    
    discovery.update_config(config);
    CommandResponse::success(true)
}

/// Auto-configure providers based on discovery
#[command]
pub async fn auto_configure_providers(
    app_handle: AppHandle,
    state: State<'_, ProviderManager>,
) -> CommandResponse<Vec<ProviderConfig>> {
    let offline_manager = app_handle.state::<Arc<crate::offline::OfflineManager>>();
    
    // Get provider configs from discovery
    let configs = offline_manager.get_provider_configs();
    
    // Update provider manager with discovered configs
    for config in &configs {
        if let Err(e) = state.update_provider_config(config.clone()) {
            warn!("Failed to configure provider {}: {}", config.provider_type, e);
        }
    }
    
    CommandResponse::success(configs)
}

// --------------------------
// Migration Commands
// --------------------------

/// Check for legacy LLM system
#[command]
pub async fn check_legacy_system(
    app_handle: AppHandle,
) -> CommandResponse<bool> {
    let offline_manager = app_handle.state::<Arc<crate::offline::OfflineManager>>();
    let migration = offline_manager.get_llm_migration();
    
    match migration.detect_legacy_system().await {
        Ok(detected) => CommandResponse::success(detected),
        Err(e) => CommandResponse::error(&e.to_string()),
    }
}

/// Get migration status
#[command]
pub async fn get_migration_status(
    app_handle: AppHandle,
) -> CommandResponse<crate::offline::llm::migration::MigrationStatus> {
    let offline_manager = app_handle.state::<Arc<crate::offline::OfflineManager>>();
    let migration = offline_manager.get_llm_migration();
    
    let status = migration.get_status();
    CommandResponse::success(status)
}

/// Run migration
#[command]
pub async fn run_migration(
    options: crate::offline::llm::migration::MigrationOptions,
    app_handle: AppHandle,
) -> CommandResponse<crate::offline::llm::migration::MigrationNotification> {
    let offline_manager = app_handle.state::<Arc<crate::offline::OfflineManager>>();
    
    match offline_manager.run_llm_migration(options).await {
        Ok(notification) => CommandResponse::success(notification),
        Err(e) => CommandResponse::error(&e),
    }
}

/// Get migration configuration
#[command]
pub async fn get_migration_config(
    app_handle: AppHandle,
) -> CommandResponse<crate::offline::llm::migration::MigrationConfig> {
    let offline_manager = app_handle.state::<Arc<crate::offline::OfflineManager>>();
    let migration = offline_manager.get_llm_migration();
    
    let config = migration.get_config();
    CommandResponse::success(config)
}

/// Update migration configuration
#[command]
pub async fn update_migration_config(
    config: crate::offline::llm::migration::MigrationConfig,
    app_handle: AppHandle,
) -> CommandResponse<bool> {
    let offline_manager = app_handle.state::<Arc<crate::offline::OfflineManager>>();
    let migration = offline_manager.get_llm_migration();
    
    migration.update_config(config);
    CommandResponse::success(true)
}

/// Opt out of migration
#[command]
pub async fn opt_out_of_migration(
    app_handle: AppHandle,
) -> CommandResponse<bool> {
    let offline_manager = app_handle.state::<Arc<crate::offline::OfflineManager>>();
    let migration = offline_manager.get_llm_migration();
    
    migration.opt_out();
    CommandResponse::success(true)
}

/// Get model mappings
#[command]
pub async fn get_model_mappings(
    app_handle: AppHandle,
) -> CommandResponse<Vec<crate::offline::llm::migration::LegacyModelMapping>> {
    let offline_manager = app_handle.state::<Arc<crate::offline::OfflineManager>>();
    let migration = offline_manager.get_llm_migration();
    
    let mappings = migration.get_model_mappings();
    CommandResponse::success(mappings)
}

/// Get provider mappings
#[command]
pub async fn get_provider_mappings(
    app_handle: AppHandle,
) -> CommandResponse<Vec<crate::offline::llm::migration::ProviderMapping>> {
    let offline_manager = app_handle.state::<Arc<crate::offline::OfflineManager>>();
    let migration = offline_manager.get_llm_migration();
    
    let mappings = migration.get_provider_mappings();
    CommandResponse::success(mappings)
}

// Generate Tauri command handler
pub fn init_commands() -> Vec<(&'static str, Box<dyn Fn() + Send + Sync + 'static>)> {
    vec![
        ("get_all_providers", Box::new(|| {})),
        ("get_all_provider_availability", Box::new(|| {})),
        ("check_provider_availability", Box::new(|| {})),
        ("add_custom_provider", Box::new(|| {})),
        ("remove_custom_provider", Box::new(|| {})),
        ("get_active_provider", Box::new(|| {})),
        ("set_active_provider", Box::new(|| {})),
        ("get_provider_config", Box::new(|| {})),
        ("update_provider_config", Box::new(|| {})),
        ("list_available_models", Box::new(|| {})),
        ("list_downloaded_models", Box::new(|| {})),
        ("get_download_status", Box::new(|| {})),
        ("download_model", Box::new(|| {})),
        ("cancel_download", Box::new(|| {})),
        ("delete_model", Box::new(|| {})),
        ("generate_text", Box::new(|| {})),
        ("scan_for_providers", Box::new(|| {})),
        ("get_discovery_status", Box::new(|| {})),
        ("get_provider_suggestions", Box::new(|| {})),
        ("get_discovery_config", Box::new(|| {})),
        ("update_discovery_config", Box::new(|| {})),
        ("auto_configure_providers", Box::new(|| {})),
        ("check_legacy_system", Box::new(|| {})),
        ("get_migration_status", Box::new(|| {})),
        ("run_migration", Box::new(|| {})),
        ("get_migration_config", Box::new(|| {})),
        ("update_migration_config", Box::new(|| {})),
        ("opt_out_of_migration", Box::new(|| {})),
        ("get_model_mappings", Box::new(|| {})),
        ("get_provider_mappings", Box::new(|| {})),
    ]
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_provider_type_conversion() {
        let ollama = ProviderType::Ollama;
        let str_value = ollama.to_string();
        assert_eq!(str_value, "Ollama");
        
        let parsed = ProviderType::from_string(&str_value).unwrap();
        assert!(matches!(parsed, ProviderType::Ollama));
        
        let custom = ProviderType::Custom("MyProvider".to_string());
        let str_value = custom.to_string();
        assert_eq!(str_value, "Custom(MyProvider)");
        
        let parsed = ProviderType::from_string(&str_value).unwrap();
        assert!(matches!(parsed, ProviderType::Custom(name) if name == "MyProvider"));
    }
    
    #[test]
    fn test_provider_manager() {
        let manager = ProviderManager::default();
        
        // Check initial providers
        let providers = manager.get_all_providers().unwrap();
        assert_eq!(providers.len(), 3);
        
        // Check provider types
        let provider_types: Vec<String> = providers.iter().map(|p| p.provider_type.clone()).collect();
        assert!(provider_types.contains(&"Ollama".to_string()));
        assert!(provider_types.contains(&"LocalAI".to_string()));
        assert!(provider_types.contains(&"LlamaCpp".to_string()));
        
        // Check active provider
        let active = manager.get_active_provider().unwrap();
        assert!(matches!(active, ProviderType::LlamaCpp));
    }
    
    #[test]
    fn test_download_status_conversion() {
        let status = DownloadStatus {
            model_id: "test".to_string(),
            progress: 0.5,
            bytes_downloaded: 500,
            total_bytes: 1000,
            speed_bps: 100,
            eta_seconds: 5,
            complete: false,
            error: None,
        };
        
        let enhanced = EnhancedDownloadStatus::from(status);
        assert_eq!(enhanced.status, "InProgress");
        assert!(enhanced.InProgress.is_some());
        
        let in_progress = enhanced.InProgress.unwrap();
        assert_eq!(in_progress.percent, 50.0);
        assert_eq!(in_progress.bytes_downloaded, Some(500));
        assert_eq!(in_progress.total_bytes, Some(1000));
        assert_eq!(in_progress.eta_seconds, Some(5));
        assert_eq!(in_progress.bytes_per_second, Some(100));
    }
}
</file>

<file path="src/commands/offline/mod.rs">
pub mod llm;

// Re-export the contents
pub use self::llm::*;
</file>

<file path="src/commands/optimization.rs">
use crate::optimization::{OptimizationManager, MemoryLimits, CacheConfig, MemoryStats, CacheStats};
use std::sync::{Arc, Mutex};
use tauri::{command, State};

/// State for the optimization manager
pub struct OptimizationState {
    manager: Arc<Mutex<Option<OptimizationManager>>>,
}

impl OptimizationState {
    pub fn new() -> Self {
        Self {
            manager: Arc::new(Mutex::new(None)),
        }
    }
    
    pub fn initialize(&self) {
        let manager = OptimizationManager::new();
        manager.start();
        *self.manager.lock().unwrap() = Some(manager);
    }
    
    pub fn get_manager(&self) -> Option<OptimizationManager> {
        self.manager.lock().unwrap().as_ref().cloned()
    }
}

/// Initialize the optimization manager
#[command]
pub fn init_optimizations(state: State<'_, OptimizationState>) -> Result<String, String> {
    state.initialize();
    Ok("Optimization manager initialized".into())
}

/// Get memory statistics
#[command]
pub fn get_memory_stats(state: State<'_, OptimizationState>) -> Result<MemoryStats, String> {
    match state.get_manager() {
        Some(manager) => Ok(manager.memory_manager().get_stats()),
        None => Err("Optimization manager not initialized".into()),
    }
}

/// Get memory limits
#[command]
pub fn get_memory_limits(state: State<'_, OptimizationState>) -> Result<MemoryLimits, String> {
    match state.get_manager() {
        Some(manager) => Ok(manager.memory_manager().get_limits()),
        None => Err("Optimization manager not initialized".into()),
    }
}

/// Update memory limits
#[command]
pub fn update_memory_limits(
    limits: MemoryLimits,
    state: State<'_, OptimizationState>
) -> Result<String, String> {
    match state.get_manager() {
        Some(manager) => {
            manager.memory_manager().update_limits(limits);
            Ok("Memory limits updated".into())
        }
        None => Err("Optimization manager not initialized".into()),
    }
}

/// Force garbage collection
#[command]
pub fn force_gc(
    aggressive: bool,
    state: State<'_, OptimizationState>
) -> Result<String, String> {
    match state.get_manager() {
        Some(manager) => {
            manager.memory_manager().force_gc(aggressive);
            Ok("Garbage collection performed".into())
        }
        None => Err("Optimization manager not initialized".into()),
    }
}

/// Get API cache statistics
#[command]
pub fn get_api_cache_stats(state: State<'_, OptimizationState>) -> Result<CacheStats, String> {
    match state.get_manager() {
        Some(manager) => Ok(manager.api_cache().get_stats()),
        None => Err("Optimization manager not initialized".into()),
    }
}

/// Get API cache configuration
#[command]
pub fn get_api_cache_config(state: State<'_, OptimizationState>) -> Result<CacheConfig, String> {
    match state.get_manager() {
        Some(manager) => Ok(manager.api_cache().get_config()),
        None => Err("Optimization manager not initialized".into()),
    }
}

/// Update API cache configuration
#[command]
pub fn update_api_cache_config(
    config: CacheConfig,
    state: State<'_, OptimizationState>
) -> Result<String, String> {
    match state.get_manager() {
        Some(manager) => {
            manager.api_cache().update_config(config);
            Ok("API cache configuration updated".into())
        }
        None => Err("Optimization manager not initialized".into()),
    }
}

/// Clear API cache
#[command]
pub fn clear_api_cache(state: State<'_, OptimizationState>) -> Result<String, String> {
    match state.get_manager() {
        Some(manager) => {
            manager.api_cache().clear();
            Ok("API cache cleared".into())
        }
        None => Err("Optimization manager not initialized".into()),
    }
}

/// Get resource cache statistics
#[command]
pub fn get_resource_cache_stats(state: State<'_, OptimizationState>) -> Result<CacheStats, String> {
    match state.get_manager() {
        Some(manager) => Ok(manager.resource_cache().get_stats()),
        None => Err("Optimization manager not initialized".into()),
    }
}

/// Get resource cache configuration
#[command]
pub fn get_resource_cache_config(state: State<'_, OptimizationState>) -> Result<CacheConfig, String> {
    match state.get_manager() {
        Some(manager) => Ok(manager.resource_cache().get_config()),
        None => Err("Optimization manager not initialized".into()),
    }
}

/// Update resource cache configuration
#[command]
pub fn update_resource_cache_config(
    config: CacheConfig,
    state: State<'_, OptimizationState>
) -> Result<String, String> {
    match state.get_manager() {
        Some(manager) => {
            manager.resource_cache().update_config(config);
            Ok("Resource cache configuration updated".into())
        }
        None => Err("Optimization manager not initialized".into()),
    }
}

/// Clear resource cache
#[command]
pub fn clear_resource_cache(state: State<'_, OptimizationState>) -> Result<String, String> {
    match state.get_manager() {
        Some(manager) => {
            manager.resource_cache().clear();
            Ok("Resource cache cleared".into())
        }
        None => Err("Optimization manager not initialized".into()),
    }
}

/// Register optimization commands with Tauri
pub fn register_commands(app: &mut tauri::App) -> Result<(), Box<dyn std::error::Error>> {
    app.manage(OptimizationState::new());
    
    Ok(())
}
</file>

<file path="src/commands/security.rs">
// Security Commands Module
//
// This module provides Tauri commands for interacting with the security system

use serde::{Deserialize, Serialize};
use std::collections::HashMap;

use crate::error::Result;
use crate::security::{
    SecurityConfig,
    DataClassification,
    PermissionLevel,
    init_security_manager,
    get_security_manager,
    check_permission,
    request_permission,
    store_credential,
    get_credential,
    encrypt_for_sync,
    decrypt_from_sync,
};
use crate::security::permissions::PermissionStatistics;
use crate::security::data_flow::{DataFlowGraph, DataFlowEvent, DataFlowStatistics};

// Security system configuration
#[tauri::command]
pub async fn init_security(config: Option<SecurityConfig>) -> Result<()> {
    init_security_manager(config)
}

#[tauri::command]
pub async fn get_security_config() -> Result<SecurityConfig> {
    let manager = get_security_manager()?;
    Ok(manager.get_config())
}

#[tauri::command]
pub async fn update_security_config(config: SecurityConfig) -> Result<()> {
    let manager = get_security_manager()?;
    manager.update_config(config)
}

// Credentials management commands

#[tauri::command]
pub async fn store_secure_credential(key: String, value: String) -> Result<()> {
    store_credential(&key, &value)
}

#[tauri::command]
pub async fn get_secure_credential(key: String) -> Result<String> {
    get_credential(&key)
}

#[tauri::command]
pub async fn delete_secure_credential(key: String) -> Result<()> {
    let manager = get_security_manager()?;
    manager.get_credential_manager().read().unwrap().delete_credential(&key)
}

#[tauri::command]
pub async fn list_secure_credentials() -> Result<Vec<String>> {
    let manager = get_security_manager()?;
    manager.get_credential_manager().read().unwrap().list_credential_keys()
}

// End-to-end encryption commands

#[tauri::command]
pub async fn encrypt_data(data: Vec<u8>) -> Result<Vec<u8>> {
    encrypt_for_sync(&data)
}

#[tauri::command]
pub async fn decrypt_data(data: Vec<u8>) -> Result<Vec<u8>> {
    decrypt_from_sync(&data)
}

#[tauri::command]
pub async fn rotate_encryption_keys() -> Result<()> {
    let manager = get_security_manager()?;
    manager.get_e2ee_manager().read().unwrap().rotate_keys()
}

// Permission management commands

#[tauri::command]
pub async fn check_permission_granted(permission: String) -> Result<bool> {
    check_permission(&permission)
}

#[tauri::command]
pub async fn request_app_permission(permission: String, reason: String) -> Result<bool> {
    request_permission(&permission, &reason)
}

#[tauri::command]
pub async fn get_all_permissions() -> Result<Vec<serde_json::Value>> {
    let manager = get_security_manager()?;
    let permissions = manager.get_permission_manager().read().unwrap().get_all_permissions()?;
    
    // Convert to generic JSON values for frontend
    let json_permissions = permissions.into_iter()
        .map(|p| serde_json::to_value(p).unwrap())
        .collect();
        
    Ok(json_permissions)
}

#[tauri::command]
pub async fn set_permission_level(id: String, level: PermissionLevel) -> Result<()> {
    let manager = get_security_manager()?;
    manager.get_permission_manager().read().unwrap().set_permission_level(&id, level)
}

#[tauri::command]
pub async fn reset_permission(id: String) -> Result<()> {
    let manager = get_security_manager()?;
    manager.get_permission_manager().read().unwrap().reset_permission(&id)
}

#[tauri::command]
pub async fn reset_all_permissions() -> Result<()> {
    let manager = get_security_manager()?;
    manager.get_permission_manager().read().unwrap().reset_all_permissions()
}

#[tauri::command]
pub async fn get_permission_statistics() -> Result<PermissionStatistics> {
    let manager = get_security_manager()?;
    manager.get_permission_manager().read().unwrap().get_statistics()
}

// Data flow tracking commands

#[tauri::command]
pub async fn get_data_flow_graph() -> Result<DataFlowGraph> {
    let manager = get_security_manager()?;
    manager.get_data_flow_manager().read().unwrap().get_data_flow_graph()
}

#[tauri::command]
pub async fn get_recent_data_flow_events(limit: Option<usize>) -> Result<Vec<DataFlowEvent>> {
    let manager = get_security_manager()?;
    manager.get_data_flow_manager().read().unwrap().get_recent_events(limit)
}

#[tauri::command]
pub async fn track_data_flow(
    operation: String,
    data_item: String,
    classification: DataClassification,
    destination: String,
) -> Result<()> {
    let manager = get_security_manager()?;
    manager.get_data_flow_manager().read().unwrap().track_data_flow(
        &operation,
        &data_item,
        classification,
        &destination,
    )
}

#[tauri::command]
pub async fn clear_data_flow_events() -> Result<()> {
    let manager = get_security_manager()?;
    manager.get_data_flow_manager().read().unwrap().clear_events()
}

#[tauri::command]
pub async fn get_data_flow_statistics() -> Result<DataFlowStatistics> {
    let manager = get_security_manager()?;
    manager.get_data_flow_manager().read().unwrap().get_statistics()
}

#[tauri::command]
pub async fn search_data_flow_events(
    data_item: Option<String>,
    classification: Option<DataClassification>,
    source: Option<String>,
    destination: Option<String>,
    operation: Option<String>,
) -> Result<Vec<DataFlowEvent>> {
    let manager = get_security_manager()?;
    manager.get_data_flow_manager().read().unwrap().search_events(
        data_item.as_deref(),
        classification,
        source.as_deref(),
        destination.as_deref(),
        operation.as_deref(),
        None,
        None,
    )
}
</file>

<file path="src/commands/update.rs">
use crate::auto_update::{UpdateManager, UpdaterConfig};
use tauri::{AppHandle, command, State, Manager};
use std::sync::{Arc, Mutex};

/// State for the updater
pub struct UpdaterState {
    manager: Arc<Mutex<Option<UpdateManager>>>,
}

impl UpdaterState {
    pub fn new() -> Self {
        Self {
            manager: Arc::new(Mutex::new(None)),
        }
    }

    pub fn initialize(&self, app: AppHandle) {
        let manager = UpdateManager::new(app);
        *self.manager.lock().unwrap() = Some(manager);
    }

    pub fn get_manager(&self) -> Option<UpdateManager> {
        self.manager.lock().unwrap().clone()
    }
}

/// Initialize the update manager
#[command]
pub async fn init_updater(app_handle: AppHandle, state: State<'_, UpdaterState>) {
    state.initialize(app_handle);
    if let Some(manager) = state.get_manager() {
        manager.start().await;
    }
}

/// Check for updates manually
#[command]
pub async fn check_for_updates(state: State<'_, UpdaterState>) -> Result<String, String> {
    match state.get_manager() {
        Some(manager) => {
            manager.check_for_updates().await;
            Ok("Update check initiated".into())
        }
        None => Err("Update manager not initialized".into()),
    }
}

/// Get the current updater configuration
#[command]
pub fn get_updater_config(state: State<'_, UpdaterState>) -> Result<UpdaterConfig, String> {
    match state.get_manager() {
        Some(manager) => Ok(manager.get_config()),
        None => Err("Update manager not initialized".into()),
    }
}

/// Update the updater configuration
#[command]
pub async fn update_updater_config(
    config: UpdaterConfig, 
    state: State<'_, UpdaterState>
) -> Result<String, String> {
    match state.get_manager() {
        Some(manager) => {
            manager.update_config(config).await;
            Ok("Configuration updated".into())
        }
        None => Err("Update manager not initialized".into()),
    }
}

/// Register updater commands with Tauri
pub fn register_commands(app: &mut tauri::App) -> Result<(), Box<dyn std::error::Error>> {
    app.manage(UpdaterState::new());
    
    Ok(())
}
</file>

<file path="src/models/messages.rs">
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, SystemTime};
use uuid::Uuid;

/// Error type for message-related operations
#[derive(Debug, thiserror::Error)]
pub enum MessageError {
    #[error("Network error: {0}")]
    NetworkError(String),
    
    #[error("Protocol error: {0}")]
    ProtocolError(String),
    
    #[error("Authentication error: {0}")]
    AuthError(String),
    
    #[error("Serialization error: {0}")]
    SerializationError(String),
    
    #[error("Message timeout after {0:?}")]
    Timeout(Duration),
    
    #[error("Connection closed")]
    ConnectionClosed,
    
    #[error("Unknown error: {0}")]
    Unknown(String),
}

/// Message role (user, assistant, system)
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum MessageRole {
    User,
    Assistant,
    System,
    Tool,
}

/// Message content type
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "lowercase", tag = "type")]
pub enum ContentType {
    #[serde(rename = "text")]
    Text { text: String },
    
    #[serde(rename = "image")]
    Image { url: String, media_type: String },
    
    #[serde(rename = "tool_call")]
    ToolCall {
        id: String,
        name: String,
        arguments: String,
    },
    
    #[serde(rename = "tool_result")]
    ToolResult {
        tool_call_id: String,
        result: String,
    },
}

/// Message content that can contain multiple parts (text, images, etc.)
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub struct MessageContent {
    pub parts: Vec<ContentType>,
}

/// Base message structure used for MCP communication
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Message {
    /// Unique message identifier
    pub id: String,
    
    /// Message role (user, assistant, etc.)
    pub role: MessageRole,
    
    /// Message content (can be multipart)
    pub content: MessageContent,
    
    /// Optional metadata key-value pairs
    #[serde(skip_serializing_if = "Option::is_none")]
    pub metadata: Option<HashMap<String, serde_json::Value>>,
    
    /// Message creation timestamp
    #[serde(with = "time_serde")]
    pub created_at: SystemTime,
}

/// Conversation message for tracking conversation history
#[derive(Debug, Clone)]
pub struct ConversationMessage {
    /// The underlying message
    pub message: Message,
    
    /// References to any parent messages (for threading)
    pub parent_ids: Vec<String>,
    
    /// If message was streamed, time when streaming completed
    pub completed_at: Option<SystemTime>,
    
    /// If message is being streamed, partial content to display
    pub partial_content: Option<String>,
    
    /// Message status
    pub status: MessageStatus,
}

/// Status of a message in the conversation
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum MessageStatus {
    /// Message has been sent and we're waiting for response
    Sending,
    
    /// Message is currently being streamed (receiving)
    Streaming,
    
    /// Message has been sent and received completely
    Complete,
    
    /// Message sending or receiving failed
    Failed,
    
    /// Message is canceled
    Cancelled,
}

/// Implementation for Message
impl Message {
    /// Create a new user message with text content
    pub fn new_user_text(text: impl Into<String>) -> Self {
        Self {
            id: Uuid::new_v4().to_string(),
            role: MessageRole::User,
            content: MessageContent {
                parts: vec![ContentType::Text { text: text.into() }],
            },
            metadata: None,
            created_at: SystemTime::now(),
        }
    }
    
    /// Create a new system message with text content
    pub fn new_system_text(text: impl Into<String>) -> Self {
        Self {
            id: Uuid::new_v4().to_string(),
            role: MessageRole::System,
            content: MessageContent {
                parts: vec![ContentType::Text { text: text.into() }],
            },
            metadata: None,
            created_at: SystemTime::now(),
        }
    }
    
    /// Add metadata to a message
    pub fn with_metadata(mut self, key: impl Into<String>, value: impl Into<serde_json::Value>) -> Self {
        let metadata = self.metadata.get_or_insert_with(HashMap::new);
        metadata.insert(key.into(), value.into());
        self
    }
    
    /// Get text content if message contains only text
    pub fn text_content(&self) -> Option<&str> {
        if self.content.parts.len() == 1 {
            match &self.content.parts[0] {
                ContentType::Text { text } => Some(text),
                _ => None,
            }
        } else {
            None
        }
    }
}

/// Time serialization helpers for serde
mod time_serde {
    use serde::{Deserialize, Deserializer, Serializer};
    use std::time::{Duration, SystemTime, UNIX_EPOCH};

    pub fn serialize<S>(time: &SystemTime, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        let timestamp = time
            .duration_since(UNIX_EPOCH)
            .map_err(|e| serde::ser::Error::custom(e.to_string()))?
            .as_secs();
        serializer.serialize_u64(timestamp)
    }

    pub fn deserialize<'de, D>(deserializer: D) -> Result<SystemTime, D::Error>
    where
        D: Deserializer<'de>,
    {
        let timestamp = u64::deserialize(deserializer)?;
        Ok(UNIX_EPOCH + Duration::from_secs(timestamp))
    }
}
</file>

<file path="src/models/mod.rs">
pub mod messages;

use serde::{Deserialize, Serialize};
use std::time::{Duration, SystemTime};
use uuid::Uuid;

/// Represents a conversation with a model
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Conversation {
    /// Unique conversation identifier
    pub id: String,
    
    /// User-friendly title
    pub title: String,
    
    /// When the conversation was created
    pub created_at: SystemTime,
    
    /// When the conversation was last modified
    pub updated_at: SystemTime,
    
    /// Model used for this conversation
    pub model: Model,
    
    /// Conversation metadata
    pub metadata: serde_json::Value,
}

/// Information about a model
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub struct Model {
    /// Model identifier (e.g., "claude-3-opus-20240229")
    pub id: String,
    
    /// Provider name (e.g., "anthropic")
    pub provider: String,
    
    /// User-friendly name (e.g., "Claude 3 Opus")
    pub name: String,
    
    /// Model version
    pub version: String,
    
    /// Model capabilities
    pub capabilities: ModelCapabilities,
}

/// Model capabilities
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub struct ModelCapabilities {
    /// Can process images
    pub vision: bool,
    
    /// Maximum context length
    pub max_context_length: usize,
    
    /// Supports functions/tools
    pub functions: bool,
    
    /// Supports streamed responses
    pub streaming: bool,
}

/// Implementation for Conversation
impl Conversation {
    /// Create a new conversation
    pub fn new(title: impl Into<String>, model: Model) -> Self {
        let now = SystemTime::now();
        Self {
            id: Uuid::new_v4().to_string(),
            title: title.into(),
            created_at: now,
            updated_at: now,
            model,
            metadata: serde_json::Value::Object(serde_json::Map::new()),
        }
    }
    
    /// Set conversation title
    pub fn set_title(&mut self, title: impl Into<String>) {
        self.title = title.into();
        self.updated_at = SystemTime::now();
    }
    
    /// Calculate conversation age
    pub fn age(&self) -> Duration {
        SystemTime::now()
            .duration_since(self.created_at)
            .unwrap_or(Duration::from_secs(0))
    }
}

/// Implementation for Model
impl Model {
    /// Create a new Claude model
    pub fn claude(variant: &str, version: &str) -> Self {
        let capabilities = match variant {
            "opus" => ModelCapabilities {
                vision: true,
                max_context_length: 200_000,
                functions: true,
                streaming: true,
            },
            "sonnet" => ModelCapabilities {
                vision: true,
                max_context_length: 180_000,
                functions: true,
                streaming: true,
            },
            "haiku" => ModelCapabilities {
                vision: true,
                max_context_length: 150_000,
                functions: true,
                streaming: true,
            },
            _ => ModelCapabilities {
                vision: false,
                max_context_length: 100_000,
                functions: false,
                streaming: true,
            },
        };
        
        Self {
            id: format!("claude-3-{}-{}", variant, version),
            provider: "anthropic".to_string(),
            name: format!("Claude 3 {}", variant.to_string()),
            version: version.to_string(),
            capabilities,
        }
    }
}
</file>

<file path="src/observability/metrics/llm.rs">
use std::collections::HashMap;
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, Instant, SystemTime};
use serde::{Serialize, Deserialize};
use log::{debug, info, warn, error};

use crate::commands::offline::llm::{ProviderType, ProviderInfo, ProviderConfig};
use crate::offline::llm::discovery::{DiscoveryService, InstallationInfo};
use crate::offline::llm::migration::{MigrationService, MigrationStatus};
use crate::error::Result;

// Reexport metrics types from src-common for convenience
pub use crate::src_common::observability::metrics::{
    Metric, MetricType, TimerStats, HistogramStats, METRICS_COLLECTOR,
    record_counter, increment_counter, record_gauge, record_histogram, time_operation
};

// Import telemetry helpers
use crate::src_common::observability::telemetry::{
    track_feature_usage, track_error, track_performance, track_model_usage,
    TelemetryClient, TelemetryEventType, PrivacyLevel
};

/// Configuration for LLM metrics collection
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMMetricsConfig {
    /// Whether to collect LLM metrics
    pub enabled: bool,
    /// Whether to collect detailed model performance metrics
    pub collect_performance_metrics: bool,
    /// Whether to collect model usage statistics
    pub collect_usage_metrics: bool,
    /// Whether to collect error metrics
    pub collect_error_metrics: bool,
    /// Level of anonymization for metrics
    pub anonymization_level: AnonymizationLevel,
    /// Sample rate for performance metrics (0.0 to 1.0)
    pub performance_sampling_rate: f64,
    /// Whether to automatically track provider changes
    pub track_provider_changes: bool,
    /// Whether to track model loading/unloading
    pub track_model_events: bool,
    /// Privacy notice version accepted by the user
    pub privacy_notice_version: String,
    /// Whether the privacy notice has been accepted
    pub privacy_notice_accepted: bool,
}

impl Default for LLMMetricsConfig {
    fn default() -> Self {
        Self {
            enabled: false, // Disabled by default, requires explicit opt-in
            collect_performance_metrics: true,
            collect_usage_metrics: true,
            collect_error_metrics: true,
            anonymization_level: AnonymizationLevel::Full,
            performance_sampling_rate: 0.1, // 10% sampling by default
            track_provider_changes: true,
            track_model_events: true,
            privacy_notice_version: "1.0.0".to_string(),
            privacy_notice_accepted: false,
        }
    }
}

/// Level of anonymization for metrics
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq)]
pub enum AnonymizationLevel {
    /// No anonymization - collect all metrics with full details (not recommended)
    None,
    /// Partial anonymization - collect metrics with limited identifiable information
    Partial,
    /// Full anonymization - collect metrics with no identifiable information
    Full,
}

/// LLM provider event type
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ProviderEventType {
    /// Provider discovered
    Discovered,
    /// Provider configuration changed
    ConfigChanged,
    /// Provider became active
    BecameActive,
    /// Provider became inactive
    BecameInactive,
    /// Provider not available (e.g., process not running)
    Unavailable,
    /// Provider error
    Error,
}

/// LLM model event type
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ModelEventType {
    /// Model loaded
    Loaded,
    /// Model unloaded
    Unloaded,
    /// Model download started
    DownloadStarted,
    /// Model download completed
    DownloadCompleted,
    /// Model download failed
    DownloadFailed,
    /// Model download canceled
    DownloadCanceled,
    /// Error loading model
    LoadError,
}

/// Generation status
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum GenerationStatus {
    /// Generation succeeded
    Success,
    /// Generation failed
    Failure,
    /// Generation canceled
    Canceled,
    /// Generation timeout
    Timeout,
}

/// Privacy-sensitive string - only included if privacy settings allow
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum PrivacySensitiveString {
    /// Value is included
    Included(String),
    /// Value is redacted
    Redacted,
}

/// Provider-specific performance metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProviderPerformanceMetrics {
    /// Provider type
    pub provider_type: String,
    /// Number of generation requests
    pub generation_count: u64,
    /// Number of successful generations
    pub successful_generations: u64,
    /// Number of failed generations
    pub failed_generations: u64,
    /// Average tokens per second
    pub avg_tokens_per_second: f64,
    /// Average latency in ms
    pub avg_latency_ms: f64,
    /// P90 latency in ms
    pub p90_latency_ms: f64,
    /// P99 latency in ms
    pub p99_latency_ms: f64,
    /// System information - CPU usage during generation
    pub avg_cpu_usage: f64,
    /// System information - Memory usage during generation
    pub avg_memory_usage: f64,
    /// Timestamp of the last update
    pub last_updated: SystemTime,
}

impl Default for ProviderPerformanceMetrics {
    fn default() -> Self {
        Self {
            provider_type: "unknown".to_string(),
            generation_count: 0,
            successful_generations: 0,
            failed_generations: 0,
            avg_tokens_per_second: 0.0,
            avg_latency_ms: 0.0,
            p90_latency_ms: 0.0,
            p99_latency_ms: 0.0,
            avg_cpu_usage: 0.0,
            avg_memory_usage: 0.0,
            last_updated: SystemTime::now(),
        }
    }
}

/// Model-specific performance metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelPerformanceMetrics {
    /// Model ID
    pub model_id: String,
    /// Provider type
    pub provider_type: String,
    /// Number of generation requests
    pub generation_count: u64,
    /// Number of tokens generated
    pub tokens_generated: u64,
    /// Number of successful generations
    pub successful_generations: u64,
    /// Number of failed generations
    pub failed_generations: u64,
    /// Average tokens per second
    pub avg_tokens_per_second: f64,
    /// Average latency in ms
    pub avg_latency_ms: f64,
    /// P90 latency in ms
    pub p90_latency_ms: f64,
    /// P99 latency in ms
    pub p99_latency_ms: f64,
    /// Average time to first token in ms
    pub avg_time_to_first_token_ms: f64,
    /// Average tokens per request
    pub avg_tokens_per_request: f64,
    /// Timestamp of the last update
    pub last_updated: SystemTime,
}

impl Default for ModelPerformanceMetrics {
    fn default() -> Self {
        Self {
            model_id: "unknown".to_string(),
            provider_type: "unknown".to_string(),
            generation_count: 0,
            tokens_generated: 0,
            successful_generations: 0,
            failed_generations: 0,
            avg_tokens_per_second: 0.0,
            avg_latency_ms: 0.0,
            p90_latency_ms: 0.0,
            p99_latency_ms: 0.0,
            avg_time_to_first_token_ms: 0.0,
            avg_tokens_per_request: 0.0,
            last_updated: SystemTime::now(),
        }
    }
}

/// LLM Metrics Manager
pub struct LLMMetricsManager {
    /// Configuration
    config: RwLock<LLMMetricsConfig>,
    /// Provider performance metrics
    provider_metrics: RwLock<HashMap<String, ProviderPerformanceMetrics>>,
    /// Model performance metrics
    model_metrics: RwLock<HashMap<String, ModelPerformanceMetrics>>,
    /// Telemetry client
    telemetry_client: Option<Arc<TelemetryClient>>,
    /// Last active provider
    last_active_provider: RwLock<Option<String>>,
    /// Auto-detection of metrics changes
    active_monitoring: Mutex<bool>,
}

impl LLMMetricsManager {
    /// Create a new LLM metrics manager
    pub fn new(telemetry_client: Option<Arc<TelemetryClient>>) -> Self {
        Self {
            config: RwLock::new(LLMMetricsConfig::default()),
            provider_metrics: RwLock::new(HashMap::new()),
            model_metrics: RwLock::new(HashMap::new()),
            telemetry_client,
            last_active_provider: RwLock::new(None),
            active_monitoring: Mutex::new(false),
        }
    }
    
    /// Collect system resource usage metrics
    fn collect_system_resources(&self) -> (f64, f64) {
        // Try to get CPU and memory usage
        let cpu_usage = match sys_info::loadavg() {
            Ok(load) => load.one as f64 * 100.0, // Convert load average to percentage
            Err(_) => 0.0,
        };
        
        let memory_usage = match sys_info::mem_info() {
            Ok(mem) => {
                let used_mem = mem.total - mem.free - mem.buffers - mem.cached;
                (used_mem * 1024) as f64 // Convert to bytes
            },
            Err(_) => 0.0,
        };
        
        (cpu_usage, memory_usage)
    }

    /// Get current configuration
    pub fn get_config(&self) -> LLMMetricsConfig {
        self.config.read().unwrap().clone()
    }

    /// Update configuration
    pub fn update_config(&self, config: LLMMetricsConfig) {
        let mut current_config = self.config.write().unwrap();
        *current_config = config;
    }

    /// Accept privacy notice
    pub fn accept_privacy_notice(&self, version: &str) {
        let mut config = self.config.write().unwrap();
        config.privacy_notice_version = version.to_string();
        config.privacy_notice_accepted = true;
        config.enabled = true;
    }

    /// Check if metrics collection is enabled
    pub fn is_enabled(&self) -> bool {
        let config = self.config.read().unwrap();
        config.enabled && config.privacy_notice_accepted
    }

    /// Track provider event
    pub fn track_provider_event(
        &self,
        provider_type: &str,
        event_type: ProviderEventType,
        details: Option<HashMap<String, String>>,
    ) {
        // Check if enabled
        if !self.is_enabled() {
            return;
        }

        let config = self.config.read().unwrap();
        if !config.track_provider_changes {
            return;
        }

        // Create event name
        let event_name = match event_type {
            ProviderEventType::Discovered => "provider_discovered",
            ProviderEventType::ConfigChanged => "provider_config_changed",
            ProviderEventType::BecameActive => "provider_became_active",
            ProviderEventType::BecameInactive => "provider_became_inactive",
            ProviderEventType::Unavailable => "provider_unavailable",
            ProviderEventType::Error => "provider_error",
        };

        // Track as counter
        let mut tags = HashMap::new();
        tags.insert("provider_type".to_string(), provider_type.to_string());
        tags.insert("event_type".to_string(), event_name.to_string());

        if let Some(details) = &details {
            // Add details if allowed by privacy settings
            if config.anonymization_level != AnonymizationLevel::Full {
                for (key, value) in details {
                    // Skip sensitive fields in full anonymization mode
                    if !is_sensitive_field(key) || config.anonymization_level == AnonymizationLevel::None {
                        tags.insert(key.clone(), value.clone());
                    }
                }
            }
        }

        // Record metric
        increment_counter(&format!("llm.provider.{}", event_name), Some(tags.clone()));

        // Track in telemetry if available
        if let Some(client) = &self.telemetry_client {
            client.track_feature_usage(&format!("llm_provider_{}", event_name), Some(tags));
        }

        // Update last active provider if this is an "active" event
        if let ProviderEventType::BecameActive = event_type {
            let mut last_active = self.last_active_provider.write().unwrap();
            *last_active = Some(provider_type.to_string());
        }
    }

    /// Track model event
    pub fn track_model_event(
        &self,
        provider_type: &str,
        model_id: &str,
        event_type: ModelEventType,
        details: Option<HashMap<String, String>>,
    ) {
        // Check if enabled
        if !self.is_enabled() {
            return;
        }

        let config = self.config.read().unwrap();
        if !config.track_model_events {
            return;
        }

        // Create event name
        let event_name = match event_type {
            ModelEventType::Loaded => "model_loaded",
            ModelEventType::Unloaded => "model_unloaded",
            ModelEventType::DownloadStarted => "model_download_started",
            ModelEventType::DownloadCompleted => "model_download_completed",
            ModelEventType::DownloadFailed => "model_download_failed",
            ModelEventType::DownloadCanceled => "model_download_canceled",
            ModelEventType::LoadError => "model_load_error",
        };

        // Track as counter
        let mut tags = HashMap::new();
        tags.insert("provider_type".to_string(), provider_type.to_string());
        tags.insert("model_id".to_string(), model_id.to_string());
        tags.insert("event_type".to_string(), event_name.to_string());

        if let Some(details) = &details {
            // Add details if allowed by privacy settings
            if config.anonymization_level != AnonymizationLevel::Full {
                for (key, value) in details {
                    // Skip sensitive fields in full anonymization mode
                    if !is_sensitive_field(key) || config.anonymization_level == AnonymizationLevel::None {
                        tags.insert(key.clone(), value.clone());
                    }
                }
            }
        }

        // Record metric
        increment_counter(&format!("llm.model.{}", event_name), Some(tags.clone()));

        // Track in telemetry if available
        if let Some(client) = &self.telemetry_client {
            client.track_feature_usage(&format!("llm_model_{}", event_name), Some(tags));
        }
    }

    /// Track generation request
    pub fn track_generation_request(
        &self,
        provider_type: &str,
        model_id: &str,
        status: GenerationStatus,
        latency_ms: f64,
        tokens_generated: usize,
        tokens_per_second: f64,
        time_to_first_token_ms: f64,
        details: Option<HashMap<String, String>>,
    ) {
        // Check if enabled
        if !self.is_enabled() {
            return;
        }

        let config = self.config.read().unwrap();
        if !config.collect_usage_metrics {
            return;
        }

        // Check sampling rate for performance metrics
        if config.collect_performance_metrics {
            // Sample based on configuration
            let should_sample = rand::random::<f64>() <= config.performance_sampling_rate;
            
            if should_sample {
                // Record latency
                let mut perf_tags = HashMap::new();
                perf_tags.insert("provider_type".to_string(), provider_type.to_string());
                perf_tags.insert("model_id".to_string(), model_id.to_string());
                
                // Record performance metrics
                record_histogram("llm.generation.latency_ms", latency_ms, Some(perf_tags.clone()));
                record_histogram("llm.generation.tokens_per_second", tokens_per_second, Some(perf_tags.clone()));
                record_histogram("llm.generation.time_to_first_token_ms", time_to_first_token_ms, Some(perf_tags.clone()));
                record_histogram("llm.generation.tokens_generated", tokens_generated as f64, Some(perf_tags.clone()));
                
                // Update provider metrics
                self.update_provider_metrics(provider_type, &status, latency_ms, tokens_per_second);
                
                // Update model metrics
                self.update_model_metrics(
                    provider_type,
                    model_id,
                    &status,
                    latency_ms,
                    tokens_generated as u64,
                    tokens_per_second,
                    time_to_first_token_ms,
                );
            }
        }

        // Track usage metrics
        let mut tags = HashMap::new();
        tags.insert("provider_type".to_string(), provider_type.to_string());
        tags.insert("model_id".to_string(), model_id.to_string());
        tags.insert("status".to_string(), match status {
            GenerationStatus::Success => "success",
            GenerationStatus::Failure => "failure",
            GenerationStatus::Canceled => "canceled",
            GenerationStatus::Timeout => "timeout",
        }.to_string());

        if let Some(details) = &details {
            // Add details if allowed by privacy settings
            if config.anonymization_level != AnonymizationLevel::Full {
                for (key, value) in details {
                    // Skip sensitive fields
                    if !is_sensitive_field(key) || config.anonymization_level == AnonymizationLevel::None {
                        tags.insert(key.clone(), value.clone());
                    }
                }
            }
        }

        // Record metric
        increment_counter("llm.generation.count", Some(tags.clone()));
        
        // Count status-specific metrics
        match status {
            GenerationStatus::Success => {
                increment_counter("llm.generation.success", Some(tags.clone()));
            },
            GenerationStatus::Failure => {
                increment_counter("llm.generation.failure", Some(tags.clone()));
                
                // Track error if enabled
                if config.collect_error_metrics {
                    let error_message = details
                        .as_ref()
                        .and_then(|d| d.get("error_message"))
                        .unwrap_or(&"Unknown error".to_string())
                        .clone();
                    
                    if let Some(client) = &self.telemetry_client {
                        client.track_error("llm_generation_error", &error_message, Some(tags.clone()));
                    }
                }
            },
            GenerationStatus::Canceled => {
                increment_counter("llm.generation.canceled", Some(tags.clone()));
            },
            GenerationStatus::Timeout => {
                increment_counter("llm.generation.timeout", Some(tags.clone()));
            },
        }

        // Track in telemetry if available
        if let Some(client) = &self.telemetry_client {
            client.track_model_usage(model_id, tokens_generated as u32, Some(tags));
        }
    }

    /// Get performance metrics for all providers
    pub fn get_provider_metrics(&self) -> HashMap<String, ProviderPerformanceMetrics> {
        self.provider_metrics.read().unwrap().clone()
    }

    /// Get performance metrics for a specific provider
    pub fn get_provider_metric(&self, provider_type: &str) -> Option<ProviderPerformanceMetrics> {
        self.provider_metrics.read().unwrap().get(provider_type).cloned()
    }

    /// Get performance metrics for all models
    pub fn get_model_metrics(&self) -> HashMap<String, ModelPerformanceMetrics> {
        self.model_metrics.read().unwrap().clone()
    }

    /// Get performance metrics for a specific model
    pub fn get_model_metric(&self, model_id: &str) -> Option<ModelPerformanceMetrics> {
        self.model_metrics.read().unwrap().get(model_id).cloned()
    }

    /// Start monitoring for active provider changes
    pub fn start_monitoring(&self, discovery_service: Arc<DiscoveryService>) {
        let mut monitoring = self.active_monitoring.lock().unwrap();
        if *monitoring {
            return;
        }
        
        *monitoring = true;
        
        // Clone Arc for thread
        let metrics_manager = Arc::new(self.clone());
        
        // Start monitoring thread
        std::thread::spawn(move || {
            debug!("LLM provider metrics monitoring started");
            let check_interval = Duration::from_secs(30);
            
            let mut last_active: Option<String> = None;
            
            loop {
                // Check if monitoring should stop
                {
                    let monitoring = metrics_manager.active_monitoring.lock().unwrap();
                    if !*monitoring {
                        break;
                    }
                }
                
                // Check for metric changes
                if metrics_manager.is_enabled() {
                    // Check for provider changes
                    let installations = discovery_service.get_installations();
                    for (provider_type, info) in &installations {
                        use crate::offline::llm::discovery::InstallationStatus;
                        
                        match &info.status {
                            InstallationStatus::Installed { location, version } => {
                                // Track installed provider if not already tracked
                                let mut provider_metrics = metrics_manager.provider_metrics.write().unwrap();
                                if !provider_metrics.contains_key(provider_type) {
                                    let mut details = HashMap::new();
                                    details.insert("location".to_string(), location.to_string_lossy().to_string());
                                    details.insert("version".to_string(), version.clone());
                                    
                                    // Drop the lock before calling track method to avoid deadlock
                                    drop(provider_metrics);
                                    
                                    metrics_manager.track_provider_event(
                                        provider_type,
                                        ProviderEventType::Discovered,
                                        Some(details),
                                    );
                                }
                            },
                            InstallationStatus::PartiallyInstalled { reason, location } => {
                                // Track partially installed provider
                                let mut details = HashMap::new();
                                details.insert("reason".to_string(), reason.clone());
                                if let Some(loc) = location {
                                    details.insert("location".to_string(), loc.to_string_lossy().to_string());
                                }
                                
                                metrics_manager.track_provider_event(
                                    provider_type,
                                    ProviderEventType::Unavailable,
                                    Some(details),
                                );
                            },
                            _ => {},
                        }
                    }
                }
                
                // Sleep for the check interval
                std::thread::sleep(check_interval);
            }
            
            debug!("LLM provider metrics monitoring stopped");
        });
    }

    /// Stop monitoring for active provider changes
    pub fn stop_monitoring(&self) {
        let mut monitoring = self.active_monitoring.lock().unwrap();
        *monitoring = false;
    }

    // Internal method to update provider metrics
    fn update_provider_metrics(
        &self,
        provider_type: &str,
        status: &GenerationStatus,
        latency_ms: f64,
        tokens_per_second: f64,
    ) {
        // Collect system resource metrics if available
        let (cpu_usage, memory_usage) = self.collect_system_resources();
        let mut provider_metrics = self.provider_metrics.write().unwrap();
        
        // Get or create provider metrics
        let metrics = provider_metrics
            .entry(provider_type.to_string())
            .or_insert_with(|| {
                let mut default = ProviderPerformanceMetrics::default();
                default.provider_type = provider_type.to_string();
                default
            });
        
        // Update metrics
        metrics.generation_count += 1;
        metrics.last_updated = SystemTime::now();
        
        match status {
            GenerationStatus::Success => {
                metrics.successful_generations += 1;
                
                // Update latency and tokens per second metrics
                // Use exponential moving average to avoid sudden jumps
                metrics.avg_latency_ms = if metrics.avg_latency_ms == 0.0 {
                    latency_ms
                } else {
                    metrics.avg_latency_ms * 0.9 + latency_ms * 0.1
                };
                
                metrics.avg_tokens_per_second = if metrics.avg_tokens_per_second == 0.0 {
                    tokens_per_second
                } else {
                    metrics.avg_tokens_per_second * 0.9 + tokens_per_second * 0.1
                };
                
                // Update percentiles (simplistic approach)
                metrics.p90_latency_ms = metrics.avg_latency_ms * 1.5;
                metrics.p99_latency_ms = metrics.avg_latency_ms * 2.5;
                
                // Update CPU and memory usage if available
                if cpu_usage > 0.0 {
                    metrics.avg_cpu_usage = if metrics.avg_cpu_usage == 0.0 {
                        cpu_usage
                    } else {
                        metrics.avg_cpu_usage * 0.9 + cpu_usage * 0.1
                    };
                }
                
                if memory_usage > 0.0 {
                    metrics.avg_memory_usage = if metrics.avg_memory_usage == 0.0 {
                        memory_usage
                    } else {
                        metrics.avg_memory_usage * 0.9 + memory_usage * 0.1
                    };
                }
            },
            GenerationStatus::Failure | GenerationStatus::Timeout => {
                metrics.failed_generations += 1;
            },
            _ => {},
        }
    }

    // Internal method to update model metrics
    fn update_model_metrics(
        &self,
        provider_type: &str,
        model_id: &str,
        status: &GenerationStatus,
        latency_ms: f64,
        tokens_generated: u64,
        tokens_per_second: f64,
        time_to_first_token_ms: f64,
    ) {
        let mut model_metrics = self.model_metrics.write().unwrap();
        
        // Get or create model metrics
        let key = format!("{}:{}", provider_type, model_id);
        let metrics = model_metrics
            .entry(key)
            .or_insert_with(|| {
                let mut default = ModelPerformanceMetrics::default();
                default.provider_type = provider_type.to_string();
                default.model_id = model_id.to_string();
                default
            });
        
        // Update metrics
        metrics.generation_count += 1;
        metrics.last_updated = SystemTime::now();
        
        match status {
            GenerationStatus::Success => {
                metrics.successful_generations += 1;
                metrics.tokens_generated += tokens_generated;
                
                // Update latency and tokens per second metrics
                // Use exponential moving average to avoid sudden jumps
                metrics.avg_latency_ms = if metrics.avg_latency_ms == 0.0 {
                    latency_ms
                } else {
                    metrics.avg_latency_ms * 0.9 + latency_ms * 0.1
                };
                
                metrics.avg_tokens_per_second = if metrics.avg_tokens_per_second == 0.0 {
                    tokens_per_second
                } else {
                    metrics.avg_tokens_per_second * 0.9 + tokens_per_second * 0.1
                };
                
                metrics.avg_time_to_first_token_ms = if metrics.avg_time_to_first_token_ms == 0.0 {
                    time_to_first_token_ms
                } else {
                    metrics.avg_time_to_first_token_ms * 0.9 + time_to_first_token_ms * 0.1
                };
                
                metrics.avg_tokens_per_request = metrics.tokens_generated as f64 / metrics.successful_generations as f64;
                
                // Update percentiles (simplistic approach)
                metrics.p90_latency_ms = metrics.avg_latency_ms * 1.5;
                metrics.p99_latency_ms = metrics.avg_latency_ms * 2.5;
            },
            GenerationStatus::Failure | GenerationStatus::Timeout => {
                metrics.failed_generations += 1;
            },
            _ => {},
        }
    }
}

impl Clone for LLMMetricsManager {
    fn clone(&self) -> Self {
        Self {
            config: RwLock::new(self.config.read().unwrap().clone()),
            provider_metrics: RwLock::new(self.provider_metrics.read().unwrap().clone()),
            model_metrics: RwLock::new(self.model_metrics.read().unwrap().clone()),
            telemetry_client: self.telemetry_client.clone(),
            last_active_provider: RwLock::new(self.last_active_provider.read().unwrap().clone()),
            active_monitoring: Mutex::new(*self.active_monitoring.lock().unwrap()),
        }
    }
}

// Helper function to check if a field is privacy-sensitive
fn is_sensitive_field(field: &str) -> bool {
    matches!(field,
        "prompt" | "input" | "query" | "api_key" | "auth_token" | "user_id" | 
        "username" | "email" | "file_path" | "model_path" | "context" | "message" |
        "ip_address" | "location" | "device_id"
    )
}

// Create a global LLM metrics manager
lazy_static::lazy_static! {
    pub static ref LLM_METRICS_MANAGER: Arc<RwLock<Option<LLMMetricsManager>>> = Arc::new(RwLock::new(None));
}

/// Initialize LLM metrics manager
pub fn init_llm_metrics(telemetry_client: Option<Arc<TelemetryClient>>) -> Arc<LLMMetricsManager> {
    let manager = Arc::new(LLMMetricsManager::new(telemetry_client));
    
    // Store in global variable
    let mut global_manager = LLM_METRICS_MANAGER.write().unwrap();
    *global_manager = Some(manager.clone());
    
    debug!("LLM metrics manager initialized");
    
    manager
}

/// Get LLM metrics manager
pub fn get_llm_metrics_manager() -> Option<Arc<LLMMetricsManager>> {
    LLM_METRICS_MANAGER.read().unwrap().clone()
}

// --------------------------
// Utility Functions
// --------------------------

/// Time an LLM generation operation and record metrics
pub fn time_generation<F, R>(
    provider_type: &str,
    model_id: &str,
    input_tokens: usize,
    f: F
) -> R 
where
    F: FnOnce() -> (R, usize, Option<String>),  // Returns result, output tokens, error message
{
    // Check if metrics manager is available
    let metrics_manager = get_llm_metrics_manager();
    
    if metrics_manager.is_none() || 
       !metrics_manager.as_ref().unwrap().is_enabled() {
        // If metrics are disabled, just run the function without tracking
        let (result, _, _) = f();
        return result;
    }
    
    // Start timing
    let start_time = Instant::now();
    let time_to_first_token_marker = Arc::new(Mutex::new(None::<Instant>));
    let time_to_first_token_marker_clone = time_to_first_token_marker.clone();
    
    // Run the function
    let (result, output_tokens, error) = f();
    
    // Calculate metrics
    let end_time = Instant::now();
    let total_time = end_time.duration_since(start_time);
    let total_time_ms = total_time.as_secs_f64() * 1000.0;
    
    // Get time to first token if set
    let time_to_first_token_ms = time_to_first_token_marker
        .lock()
        .unwrap()
        .map(|t| t.duration_since(start_time).as_secs_f64() * 1000.0)
        .unwrap_or(0.0);
    
    // Calculate tokens per second if successful
    let tokens_per_second = if output_tokens > 0 && error.is_none() {
        output_tokens as f64 / total_time.as_secs_f64()
    } else {
        0.0
    };
    
    // Determine status
    let status = if error.is_some() {
        GenerationStatus::Failure
    } else {
        GenerationStatus::Success
    };
    
    // Create details map
    let mut details = HashMap::new();
    details.insert("input_tokens".to_string(), input_tokens.to_string());
    details.insert("output_tokens".to_string(), output_tokens.to_string());
    details.insert("total_tokens".to_string(), (input_tokens + output_tokens).to_string());
    
    if let Some(error_msg) = error {
        details.insert("error_message".to_string(), error_msg);
    }
    
    // Track metrics
    if let Some(manager) = metrics_manager {
        manager.track_generation_request(
            provider_type,
            model_id,
            status,
            total_time_ms,
            output_tokens,
            tokens_per_second,
            time_to_first_token_ms,
            Some(details),
        );
    }
    
    result
}

/// Mark time to first token during generation
pub fn mark_first_token(time_marker: &Arc<Mutex<Option<Instant>>>) {
    let mut marker = time_marker.lock().unwrap();
    if marker.is_none() {
        *marker = Some(Instant::now());
    }
}

/// Track provider event
pub fn track_provider_event(
    provider_type: &str,
    event_type: ProviderEventType,
    details: Option<HashMap<String, String>>,
) {
    if let Some(manager) = get_llm_metrics_manager() {
        manager.track_provider_event(provider_type, event_type, details);
    }
}

/// Track model event
pub fn track_model_event(
    provider_type: &str,
    model_id: &str,
    event_type: ModelEventType,
    details: Option<HashMap<String, String>>,
) {
    if let Some(manager) = get_llm_metrics_manager() {
        manager.track_model_event(provider_type, model_id, event_type, details);
    }
}

/// Get provider performance metrics
pub fn get_provider_metrics() -> Option<HashMap<String, ProviderPerformanceMetrics>> {
    get_llm_metrics_manager().map(|manager| manager.get_provider_metrics())
}

/// Get model performance metrics
pub fn get_model_metrics() -> Option<HashMap<String, ModelPerformanceMetrics>> {
    get_llm_metrics_manager().map(|manager| manager.get_model_metrics())
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::Arc;
    
    #[test]
    fn test_config_defaults() {
        let config = LLMMetricsConfig::default();
        assert_eq!(config.enabled, false); // Disabled by default
        assert_eq!(config.privacy_notice_accepted, false); // Requires explicit acceptance
    }
    
    #[test]
    fn test_sensitive_field_detection() {
        assert!(is_sensitive_field("prompt"));
        assert!(is_sensitive_field("api_key"));
        assert!(is_sensitive_field("user_id"));
        assert!(!is_sensitive_field("model_name"));
        assert!(!is_sensitive_field("tokens_generated"));
    }
    
    #[test]
    fn test_metrics_manager_creation() {
        let manager = LLMMetricsManager::new(None);
        assert!(!manager.is_enabled());
        
        let config = manager.get_config();
        assert_eq!(config.enabled, false);
        
        // Accept privacy notice
        manager.accept_privacy_notice("1.0.0");
        assert!(manager.is_enabled());
    }
    
    #[test]
    fn test_metrics_collection() {
        let manager = Arc::new(LLMMetricsManager::new(None));
        
        // Enable metrics
        let mut config = manager.get_config();
        config.enabled = true;
        config.privacy_notice_accepted = true;
        manager.update_config(config);
        
        // Track a model event
        manager.track_model_event(
            "LlamaCpp",
            "llama-7b",
            ModelEventType::Loaded,
            None,
        );
        
        // Track a generation request
        manager.track_generation_request(
            "LlamaCpp",
            "llama-7b",
            GenerationStatus::Success,
            500.0,
            100,
            20.0,
            100.0,
            None,
        );
        
        // Verify provider metrics were updated
        let provider_metrics = manager.get_provider_metrics();
        assert!(provider_metrics.contains_key("LlamaCpp"));
        let llama_metrics = provider_metrics.get("LlamaCpp").unwrap();
        assert_eq!(llama_metrics.provider_type, "LlamaCpp");
        assert_eq!(llama_metrics.generation_count, 1);
        assert_eq!(llama_metrics.successful_generations, 1);
        
        // Verify model metrics were updated
        let model_metrics = manager.get_model_metrics();
        let key = "LlamaCpp:llama-7b";
        assert!(model_metrics.contains_key(key));
        let model = model_metrics.get(key).unwrap();
        assert_eq!(model.provider_type, "LlamaCpp");
        assert_eq!(model.model_id, "llama-7b");
        assert_eq!(model.generation_count, 1);
        assert_eq!(model.successful_generations, 1);
    }
}
</file>

<file path="src/observability/metrics/mod.rs">
// LLM provider metrics module
pub mod llm;

// Re-export common types
pub use crate::src_common::observability::metrics::{
    Metric, MetricType, TimerStats, HistogramStats, MetricsHistory, 
    record_counter, increment_counter, record_gauge, record_histogram, time_operation,
    get_timers_report, get_counters_report, get_gauges_report, get_histograms_report
};

// Re-export LLM metrics
pub use self::llm::{
    LLMMetricsManager, LLMMetricsConfig, ProviderPerformanceMetrics, ModelPerformanceMetrics,
    AnonymizationLevel, ProviderEventType, ModelEventType, GenerationStatus,
    time_generation, track_provider_event, track_model_event,
    get_provider_metrics, get_model_metrics, init_llm_metrics
};
</file>

<file path="src/observability/mod.rs">
// Observability module for provider systems
pub mod metrics;

use std::sync::Arc;
use log::{debug, info, warn, error};

use crate::src_common::observability::telemetry::TelemetryClient;
use crate::src_common::observability::ObservabilityConfig;
use crate::offline::llm::discovery::DiscoveryService;

/// Initialize observability for the provider system
pub fn init_provider_observability(
    config: &ObservabilityConfig,
    discovery_service: Option<Arc<DiscoveryService>>,
) -> Result<(), String> {
    info!("Initializing provider observability systems");
    
    // Get telemetry client
    let telemetry_client = match TelemetryClient::get_instance() {
        Ok(client) => Some(client),
        Err(e) => {
            warn!("Failed to get telemetry client: {}", e);
            None
        }
    };
    
    // Initialize LLM metrics
    let llm_metrics = metrics::init_llm_metrics(telemetry_client);
    
    // Start monitoring if discovery service is provided
    if let Some(discovery) = discovery_service {
        llm_metrics.start_monitoring(discovery);
    }
    
    info!("Provider observability systems initialized");
    
    Ok(())
}

/// Shutdown observability for the provider system
pub fn shutdown_provider_observability() -> Result<(), String> {
    info!("Shutting down provider observability systems");
    
    // Get LLM metrics manager
    if let Some(manager) = metrics::llm::get_llm_metrics_manager() {
        // Stop monitoring
        manager.stop_monitoring();
    }
    
    info!("Provider observability systems shut down");
    
    Ok(())
}

/// Get the privacy notice HTML for the LLM metrics
pub fn get_llm_metrics_privacy_notice() -> String {
    r#"
    <h1>LLM Metrics Collection Privacy Notice</h1>
    
    <p>To improve your experience with local LLM providers, we collect anonymous metrics about
    performance and usage. This data helps us optimize the application and provide better
    recommendations for provider configuration.</p>
    
    <h2>What We Collect</h2>
    
    <ul>
        <li><strong>Performance metrics:</strong> Model latency, throughput, and resource usage</li>
        <li><strong>Usage statistics:</strong> Success/failure rates, model and provider preferences</li>
        <li><strong>Error information:</strong> Categories of errors to help us improve reliability</li>
    </ul>
    
    <h2>What We DON'T Collect</h2>
    
    <ul>
        <li><strong>Your prompts or inputs:</strong> The content you send to models is never collected</li>
        <li><strong>Generated outputs:</strong> The responses from models are never collected</li>
        <li><strong>Personal information:</strong> We don't collect names, addresses, or other PII</li>
        <li><strong>API keys or credentials:</strong> Your authentication information is never collected</li>
    </ul>
    
    <h2>How We Use This Data</h2>
    
    <ul>
        <li>Improve provider detection and configuration</li>
        <li>Optimize resource usage for different models</li>
        <li>Identify common issues and develop solutions</li>
        <li>Provide better recommendations for model selection</li>
    </ul>
    
    <h2>Your Choices</h2>
    
    <p>Metrics collection is <strong>completely optional and off by default</strong>. You can:</p>
    
    <ul>
        <li>Choose your desired anonymization level</li>
        <li>Enable or disable specific categories of metrics</li>
        <li>Turn off metrics collection at any time</li>
        <li>Request deletion of any data we've collected</li>
    </ul>
    
    <p>By accepting this notice, you allow us to collect the metrics described above according to
    your configuration choices.</p>
    "#.to_string()
}
</file>

<file path="src/offline/checkpointing/mod.rs">
use std::collections::HashMap;
use std::fs::{File, create_dir_all};
use std::io::{Read, Write};
use std::path::{Path, PathBuf};
use std::time::{Duration, SystemTime, UNIX_EPOCH};
use serde::{Serialize, Deserialize};
use log::{debug, info, warn, error};
use uuid::Uuid;
use chrono::{DateTime, Utc};

/// Checkpoint metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CheckpointMetadata {
    /// Checkpoint ID
    pub id: String,
    /// Checkpoint name
    pub name: String,
    /// Creation time
    pub created_at: DateTime<Utc>,
    /// Size in bytes
    pub size_bytes: usize,
    /// Compression ratio
    pub compression_ratio: f32,
    /// Number of items
    pub item_count: usize,
    /// Tags for organization
    pub tags: Vec<String>,
}

/// Checkpoint manager for saving and restoring conversation state
pub struct CheckpointManager {
    base_path: PathBuf,
    checkpoints: HashMap<String, CheckpointMetadata>,
    max_checkpoints: usize,
    compression_level: u32,
}

impl Default for CheckpointManager {
    fn default() -> Self {
        Self::new()
    }
}

impl CheckpointManager {
    /// Create a new checkpoint manager
    pub fn new() -> Self {
        Self {
            base_path: PathBuf::from("checkpoints"),
            checkpoints: HashMap::new(),
            max_checkpoints: 10,
            compression_level: 6,
        }
    }
    
    /// Set the base path for checkpoints
    pub fn with_base_path<P: AsRef<Path>>(mut self, path: P) -> Self {
        self.base_path = path.as_ref().to_path_buf();
        self
    }
    
    /// Set the maximum number of checkpoints to keep
    pub fn with_max_checkpoints(mut self, max: usize) -> Self {
        self.max_checkpoints = max;
        self
    }
    
    /// Set the compression level (0-9)
    pub fn with_compression_level(mut self, level: u32) -> Self {
        self.compression_level = level.min(9);
        self
    }
    
    /// Initialize the checkpoint manager
    pub fn initialize(&mut self) -> Result<(), String> {
        // Create base directory if it doesn't exist
        if !self.base_path.exists() {
            if let Err(e) = create_dir_all(&self.base_path) {
                return Err(format!("Failed to create checkpoint directory: {}", e));
            }
        }
        
        // Load existing checkpoints
        self.load_checkpoints()
    }
    
    /// Load existing checkpoints
    fn load_checkpoints(&mut self) -> Result<(), String> {
        if !self.base_path.exists() {
            return Ok(());
        }
        
        // Read directory contents
        let entries = match std::fs::read_dir(&self.base_path) {
            Ok(entries) => entries,
            Err(e) => return Err(format!("Failed to read checkpoint directory: {}", e)),
        };
        
        for entry in entries {
            if let Ok(entry) = entry {
                let path = entry.path();
                
                // Skip non-JSON files
                if !path.is_file() || path.extension().map_or(true, |ext| ext != "json") {
                    continue;
                }
                
                // Read metadata file
                if let Ok(file) = File::open(&path) {
                    let reader = std::io::BufReader::new(file);
                    
                    if let Ok(metadata) = serde_json::from_reader::<_, CheckpointMetadata>(reader) {
                        self.checkpoints.insert(metadata.id.clone(), metadata);
                    }
                }
            }
        }
        
        info!("Loaded {} checkpoints", self.checkpoints.len());
        Ok(())
    }
    
    /// Save a checkpoint
    pub fn save_checkpoint<T: Serialize>(
        &self,
        name: &str,
        data: T,
    ) -> String {
        debug!("Saving checkpoint: {}", name);
        
        // Generate checkpoint ID
        let id = Uuid::new_v4().to_string();
        
        // Create checkpoint directory if it doesn't exist
        if !self.base_path.exists() {
            if let Err(e) = create_dir_all(&self.base_path) {
                error!("Failed to create checkpoint directory: {}", e);
                return id;
            }
        }
        
        // Serialize data
        let serialized = match serde_json::to_vec(&data) {
            Ok(bytes) => bytes,
            Err(e) => {
                error!("Failed to serialize checkpoint data: {}", e);
                return id;
            }
        };
        
        // Compress data
        let uncompressed_size = serialized.len();
        let compressed = match compress(&serialized, self.compression_level) {
            Ok(bytes) => bytes,
            Err(e) => {
                error!("Failed to compress checkpoint data: {}", e);
                return id;
            }
        };
        
        let compressed_size = compressed.len();
        let compression_ratio = uncompressed_size as f32 / compressed_size as f32;
        
        debug!("Checkpoint compression: {} bytes -> {} bytes (ratio: {:.2})",
               uncompressed_size, compressed_size, compression_ratio);
        
        // Create metadata
        let metadata = CheckpointMetadata {
            id: id.clone(),
            name: name.to_string(),
            created_at: Utc::now(),
            size_bytes: compressed_size,
            compression_ratio,
            item_count: count_items(&data),
            tags: vec![],
        };
        
        // Save metadata
        let metadata_path = self.base_path.join(format!("{}.json", id));
        if let Err(e) = save_json(&metadata_path, &metadata) {
            error!("Failed to save checkpoint metadata: {}", e);
            return id;
        }
        
        // Save data
        let data_path = self.base_path.join(format!("{}.bin", id));
        if let Err(e) = save_binary(&data_path, &compressed) {
            error!("Failed to save checkpoint data: {}", e);
            return id;
        }
        
        info!("Checkpoint saved: {} ({})", name, id);
        id
    }
    
    /// Load a checkpoint
    pub fn load_checkpoint<T: for<'de> Deserialize<'de>>(
        &self,
        id: &str,
    ) -> Option<T> {
        debug!("Loading checkpoint: {}", id);
        
        // Check if checkpoint exists
        if !self.checkpoints.contains_key(id) {
            warn!("Checkpoint not found: {}", id);
            return None;
        }
        
        // Load compressed data
        let data_path = self.base_path.join(format!("{}.bin", id));
        let compressed = match load_binary(&data_path) {
            Ok(bytes) => bytes,
            Err(e) => {
                error!("Failed to load checkpoint data: {}", e);
                return None;
            }
        };
        
        // Decompress data
        let decompressed = match decompress(&compressed) {
            Ok(bytes) => bytes,
            Err(e) => {
                error!("Failed to decompress checkpoint data: {}", e);
                return None;
            }
        };
        
        // Deserialize data
        match serde_json::from_slice(&decompressed) {
            Ok(data) => {
                info!("Checkpoint loaded: {}", id);
                Some(data)
            }
            Err(e) => {
                error!("Failed to deserialize checkpoint data: {}", e);
                None
            }
        }
    }
    
    /// Delete a checkpoint
    pub fn delete_checkpoint(&mut self, id: &str) -> Result<(), String> {
        debug!("Deleting checkpoint: {}", id);
        
        // Check if checkpoint exists
        if !self.checkpoints.contains_key(id) {
            return Err(format!("Checkpoint not found: {}", id));
        }
        
        // Remove metadata file
        let metadata_path = self.base_path.join(format!("{}.json", id));
        if let Err(e) = std::fs::remove_file(&metadata_path) {
            warn!("Failed to delete checkpoint metadata file: {}", e);
        }
        
        // Remove data file
        let data_path = self.base_path.join(format!("{}.bin", id));
        if let Err(e) = std::fs::remove_file(&data_path) {
            warn!("Failed to delete checkpoint data file: {}", e);
        }
        
        // Remove from memory
        self.checkpoints.remove(id);
        
        info!("Checkpoint deleted: {}", id);
        Ok(())
    }
    
    /// List all checkpoints
    pub fn list_checkpoints(&self) -> Vec<CheckpointMetadata> {
        self.checkpoints.values().cloned().collect()
    }
    
    /// Get a specific checkpoint metadata
    pub fn get_checkpoint_metadata(&self, id: &str) -> Option<CheckpointMetadata> {
        self.checkpoints.get(id).cloned()
    }
    
    /// Clean up old checkpoints
    pub fn cleanup_old_checkpoints(&mut self) -> Result<usize, String> {
        if self.checkpoints.len() <= self.max_checkpoints {
            return Ok(0);
        }
        
        // Sort checkpoints by creation time
        let mut checkpoints: Vec<_> = self.checkpoints.values().cloned().collect();
        checkpoints.sort_by(|a, b| a.created_at.cmp(&b.created_at));
        
        // Delete oldest checkpoints
        let to_delete = checkpoints.len() - self.max_checkpoints;
        let mut deleted = 0;
        
        for i in 0..to_delete {
            if let Err(e) = self.delete_checkpoint(&checkpoints[i].id) {
                warn!("Failed to delete checkpoint {}: {}", checkpoints[i].id, e);
            } else {
                deleted += 1;
            }
        }
        
        info!("Cleaned up {} old checkpoints", deleted);
        Ok(deleted)
    }
}

// Helper functions

/// Count the number of items in serializable data
fn count_items<T: Serialize>(data: &T) -> usize {
    // For HashMap, count the number of entries
    if let Ok(map) = serde_json::to_value(data) {
        if map.is_object() {
            return map.as_object().unwrap().len();
        } else if map.is_array() {
            return map.as_array().unwrap().len();
        }
    }
    
    1
}

/// Compress data using zstd
fn compress(data: &[u8], level: u32) -> Result<Vec<u8>, String> {
    let mut encoder = match zstd::Encoder::new(Vec::new(), level as i32) {
        Ok(encoder) => encoder,
        Err(e) => return Err(format!("Failed to create zstd encoder: {}", e)),
    };
    
    // Write data
    if let Err(e) = encoder.write_all(data) {
        return Err(format!("Failed to compress data: {}", e));
    }
    
    // Finish encoding
    match encoder.finish() {
        Ok(compressed) => Ok(compressed),
        Err(e) => Err(format!("Failed to finalize compression: {}", e)),
    }
}

/// Decompress data using zstd
fn decompress(data: &[u8]) -> Result<Vec<u8>, String> {
    let mut decoder = match zstd::Decoder::new(data) {
        Ok(decoder) => decoder,
        Err(e) => return Err(format!("Failed to create zstd decoder: {}", e)),
    };
    
    let mut decompressed = Vec::new();
    
    // Read decompressed data
    if let Err(e) = decoder.read_to_end(&mut decompressed) {
        return Err(format!("Failed to decompress data: {}", e));
    }
    
    Ok(decompressed)
}

/// Save JSON to a file
fn save_json<T: Serialize>(path: &Path, data: &T) -> Result<(), String> {
    let file = match File::create(path) {
        Ok(file) => file,
        Err(e) => return Err(format!("Failed to create file: {}", e)),
    };
    
    let writer = std::io::BufWriter::new(file);
    
    match serde_json::to_writer_pretty(writer, data) {
        Ok(_) => Ok(()),
        Err(e) => Err(format!("Failed to write JSON: {}", e)),
    }
}

/// Save binary data to a file
fn save_binary(path: &Path, data: &[u8]) -> Result<(), String> {
    let mut file = match File::create(path) {
        Ok(file) => file,
        Err(e) => return Err(format!("Failed to create file: {}", e)),
    };
    
    match file.write_all(data) {
        Ok(_) => Ok(()),
        Err(e) => Err(format!("Failed to write binary data: {}", e)),
    }
}

/// Load binary data from a file
fn load_binary(path: &Path) -> Result<Vec<u8>, String> {
    let mut file = match File::open(path) {
        Ok(file) => file,
        Err(e) => return Err(format!("Failed to open file: {}", e)),
    };
    
    let mut data = Vec::new();
    
    match file.read_to_end(&mut data) {
        Ok(_) => Ok(data),
        Err(e) => Err(format!("Failed to read binary data: {}", e)),
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_save_load_checkpoint() {
        let temp_dir = tempfile::tempdir().unwrap();
        let manager = CheckpointManager::new()
            .with_base_path(temp_dir.path())
            .with_max_checkpoints(5)
            .with_compression_level(6);
        
        // Create test data
        let data: HashMap<String, String> = [
            ("key1".to_string(), "value1".to_string()),
            ("key2".to_string(), "value2".to_string()),
            ("key3".to_string(), "value3".to_string()),
        ].iter().cloned().collect();
        
        // Save checkpoint
        let id = manager.save_checkpoint("test", &data);
        
        // Load checkpoint
        let loaded: Option<HashMap<String, String>> = manager.load_checkpoint(&id);
        
        assert!(loaded.is_some());
        let loaded = loaded.unwrap();
        
        assert_eq!(loaded.len(), data.len());
        assert_eq!(loaded.get("key1"), Some(&"value1".to_string()));
        assert_eq!(loaded.get("key2"), Some(&"value2".to_string()));
        assert_eq!(loaded.get("key3"), Some(&"value3".to_string()));
    }
}
</file>

<file path="src/offline/llm/discovery.rs">
use std::collections::HashMap;
use std::path::{Path, PathBuf};
use std::process::Command;
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use log::{debug, info, warn, error};
use serde::{Serialize, Deserialize};
use tokio::time;
use reqwest;
use anyhow::{Result, anyhow, Context};

use crate::error::Error;
use crate::commands::offline::llm::{ProviderType, ProviderInfo, ProviderConfig};

/// Installation status of a provider
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum InstallationStatus {
    /// Provider is installed and available
    Installed {
        /// Where the provider was found
        location: PathBuf,
        /// Provider version
        version: String,
    },
    /// Provider is not installed
    NotInstalled,
    /// Provider is partially installed or in invalid state
    PartiallyInstalled {
        /// Reason for partial installation
        reason: String,
        /// Path to partial installation if available
        location: Option<PathBuf>,
    },
}

/// Installation information for a provider
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InstallationInfo {
    /// Provider type
    pub provider_type: ProviderType,
    /// Installation status
    pub status: InstallationStatus,
    /// Last checked timestamp
    pub last_checked: chrono::DateTime<chrono::Utc>,
    /// Is this provider auto-configured
    pub auto_configured: bool,
}

/// Suggestion for installing a provider
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProviderSuggestion {
    /// Provider type
    pub provider_type: ProviderType,
    /// Installation command or instructions
    pub install_command: String,
    /// URL to installation instructions
    pub instructions_url: String,
    /// Brief description of the provider
    pub description: String,
    /// Recommended for these use cases
    pub recommended_for: Vec<String>,
    /// Hardware requirements
    pub hardware_requirements: Option<HardwareRequirements>,
}

/// Hardware requirements for a provider
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HardwareRequirements {
    /// Minimum RAM in GB
    pub min_ram_gb: u32,
    /// Recommended RAM in GB
    pub recommended_ram_gb: u32,
    /// Minimum free disk space in GB
    pub min_disk_gb: u32,
    /// GPU required
    pub requires_gpu: bool,
    /// Recommended GPU with minimum VRAM in GB
    pub recommended_gpu: Option<String>,
    /// Minimum VRAM in GB
    pub min_vram_gb: Option<u32>,
}

/// Discovery service configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DiscoveryConfig {
    /// Whether to automatically detect providers
    pub auto_detect: bool,
    /// Whether to set detected providers as active
    pub auto_configure: bool,
    /// How often to scan for providers (in seconds)
    pub scan_interval_seconds: u64,
    /// Paths to scan for providers
    pub scan_paths: Vec<PathBuf>,
    /// Whether to show suggestions for missing providers
    pub show_suggestions: bool,
    /// Whether to check for new versions of providers
    pub check_for_updates: bool,
}

impl Default for DiscoveryConfig {
    fn default() -> Self {
        Self {
            auto_detect: true,
            auto_configure: true,
            scan_interval_seconds: 3600, // 1 hour
            scan_paths: vec![],
            show_suggestions: true,
            check_for_updates: true,
        }
    }
}

/// LLM Provider Discovery Service
pub struct DiscoveryService {
    /// Configuration for the discovery service
    config: Mutex<DiscoveryConfig>,
    /// Detected providers
    installations: Mutex<HashMap<String, InstallationInfo>>,
    /// Provider suggestions
    suggestions: Mutex<Vec<ProviderSuggestion>>,
    /// Whether a scan is currently running
    scanning: Mutex<bool>,
    /// Last scan timestamp
    last_scan: Mutex<Instant>,
    /// Is the background scanner running
    scanner_running: Mutex<bool>,
}

impl DiscoveryService {
    /// Create a new discovery service with default configuration
    pub fn new() -> Self {
        Self {
            config: Mutex::new(DiscoveryConfig::default()),
            installations: Mutex::new(HashMap::new()),
            suggestions: Mutex::new(Vec::new()),
            scanning: Mutex::new(false),
            last_scan: Mutex::new(Instant::now()),
            scanner_running: Mutex::new(false),
        }
    }

    /// Create a new discovery service with custom configuration
    pub fn with_config(config: DiscoveryConfig) -> Self {
        Self {
            config: Mutex::new(config),
            installations: Mutex::new(HashMap::new()),
            suggestions: Mutex::new(Vec::new()),
            scanning: Mutex::new(false),
            last_scan: Mutex::new(Instant::now()),
            scanner_running: Mutex::new(false),
        }
    }

    /// Get the current configuration
    pub fn get_config(&self) -> DiscoveryConfig {
        self.config.lock().unwrap().clone()
    }

    /// Update the configuration
    pub fn update_config(&self, config: DiscoveryConfig) {
        *self.config.lock().unwrap() = config;
    }

    /// Get all detected installations
    pub fn get_installations(&self) -> HashMap<String, InstallationInfo> {
        self.installations.lock().unwrap().clone()
    }

    /// Get a specific installation
    pub fn get_installation(&self, provider_type: &ProviderType) -> Option<InstallationInfo> {
        self.installations.lock().unwrap().get(&provider_type.to_string()).cloned()
    }

    /// Get all suggestions
    pub fn get_suggestions(&self) -> Vec<ProviderSuggestion> {
        self.suggestions.lock().unwrap().clone()
    }

    /// Start the background scanner
    pub async fn start_background_scanner(&self) -> Result<()> {
        let mut scanner_running = self.scanner_running.lock().unwrap();
        if *scanner_running {
            return Ok(());
        }
        
        *scanner_running = true;
        
        // Clone Arc for the task
        let service = Arc::new(self.clone());
        
        tokio::spawn(async move {
            loop {
                // Check if scanner should still be running
                {
                    let running = service.scanner_running.lock().unwrap();
                    if !*running {
                        break;
                    }
                }
                
                // Get scan interval from config
                let interval = {
                    let config = service.config.lock().unwrap();
                    Duration::from_secs(config.scan_interval_seconds)
                };
                
                // Scan for providers
                if let Err(e) = service.scan_for_providers().await {
                    error!("Error scanning for providers: {}", e);
                }
                
                // Update suggestions
                if let Err(e) = service.update_suggestions().await {
                    error!("Error updating suggestions: {}", e);
                }
                
                // Sleep until next scan
                time::sleep(interval).await;
            }
        });
        
        Ok(())
    }

    /// Stop the background scanner
    pub fn stop_background_scanner(&self) {
        let mut scanner_running = self.scanner_running.lock().unwrap();
        *scanner_running = false;
    }

    /// Scan for installed providers
    pub async fn scan_for_providers(&self) -> Result<()> {
        // Check if a scan is already running
        {
            let mut scanning = self.scanning.lock().unwrap();
            if *scanning {
                return Ok(());
            }
            *scanning = true;
        }
        
        info!("Scanning for LLM providers...");
        
        // Get a copy of the current config
        let config = self.get_config();
        
        // Skip if auto-detect is disabled
        if !config.auto_detect {
            let mut scanning = self.scanning.lock().unwrap();
            *scanning = false;
            return Ok(());
        }
        
        // Create a new installations map
        let mut new_installations = HashMap::new();
        
        // Scan for Ollama
        if let Ok(ollama_info) = self.detect_ollama().await {
            new_installations.insert(ProviderType::Ollama.to_string(), ollama_info);
        }
        
        // Scan for LocalAI
        if let Ok(localai_info) = self.detect_localai().await {
            new_installations.insert(ProviderType::LocalAI.to_string(), localai_info);
        }
        
        // Scan for LlamaCpp
        if let Ok(llamacpp_info) = self.detect_llamacpp().await {
            new_installations.insert(ProviderType::LlamaCpp.to_string(), llamacpp_info);
        }
        
        // Scan custom paths
        for path in &config.scan_paths {
            if let Ok(custom_providers) = self.scan_custom_path(path).await {
                for (provider_type, info) in custom_providers {
                    new_installations.insert(provider_type, info);
                }
            }
        }
        
        // Update installations
        *self.installations.lock().unwrap() = new_installations;
        
        // Update last scan time
        *self.last_scan.lock().unwrap() = Instant::now();
        
        // Reset scanning flag
        {
            let mut scanning = self.scanning.lock().unwrap();
            *scanning = false;
        }
        
        info!("Provider scan complete");
        Ok(())
    }

    /// Create provider configurations for installed providers
    pub fn create_provider_configs(&self) -> Vec<ProviderConfig> {
        let installations = self.get_installations();
        let mut configs = Vec::new();
        
        for (provider_type_str, info) in installations {
            if let InstallationStatus::Installed { location, version } = info.status {
                let provider_type = match ProviderType::from_string(&provider_type_str) {
                    Ok(pt) => pt,
                    Err(_) => continue,
                };
                
                let config = match provider_type {
                    ProviderType::Ollama => ProviderConfig {
                        provider_type: provider_type_str.clone(),
                        endpoint_url: "http://localhost:11434".to_string(),
                        api_key: None,
                        default_model: Some("llama2".to_string()),
                        enable_advanced_config: false,
                        advanced_config: HashMap::new(),
                    },
                    ProviderType::LocalAI => ProviderConfig {
                        provider_type: provider_type_str.clone(),
                        endpoint_url: "http://localhost:8080".to_string(),
                        api_key: None,
                        default_model: Some("ggml-gpt4all-j".to_string()),
                        enable_advanced_config: false,
                        advanced_config: HashMap::new(),
                    },
                    ProviderType::LlamaCpp => ProviderConfig {
                        provider_type: provider_type_str.clone(),
                        endpoint_url: format!("local://{}", location.to_string_lossy()),
                        api_key: None,
                        default_model: None,
                        enable_advanced_config: false,
                        advanced_config: HashMap::new(),
                    },
                    ProviderType::Custom(name) => ProviderConfig {
                        provider_type: provider_type_str.clone(),
                        endpoint_url: format!("http://localhost:8000/{}", name.to_lowercase()),
                        api_key: None,
                        default_model: None,
                        enable_advanced_config: false,
                        advanced_config: HashMap::new(),
                    },
                };
                
                configs.push(config);
            }
        }
        
        configs
    }

    /// Create provider information for installed providers
    pub fn create_provider_infos(&self) -> Vec<ProviderInfo> {
        let installations = self.get_installations();
        let mut infos = Vec::new();
        
        for (provider_type_str, info) in installations {
            if let InstallationStatus::Installed { location, version } = info.status {
                let provider_type = match ProviderType::from_string(&provider_type_str) {
                    Ok(pt) => pt,
                    Err(_) => continue,
                };
                
                let info = match provider_type {
                    ProviderType::Ollama => ProviderInfo {
                        provider_type: provider_type_str.clone(),
                        name: "Ollama".to_string(),
                        description: "Local model runner for LLama and other models".to_string(),
                        version,
                        default_endpoint: "http://localhost:11434".to_string(),
                        supports_text_generation: true,
                        supports_chat: true,
                        supports_embeddings: true,
                        requires_api_key: false,
                    },
                    ProviderType::LocalAI => ProviderInfo {
                        provider_type: provider_type_str.clone(),
                        name: "LocalAI".to_string(),
                        description: "Self-hosted OpenAI API compatible server".to_string(),
                        version,
                        default_endpoint: "http://localhost:8080".to_string(),
                        supports_text_generation: true,
                        supports_chat: true,
                        supports_embeddings: true,
                        requires_api_key: false,
                    },
                    ProviderType::LlamaCpp => ProviderInfo {
                        provider_type: provider_type_str.clone(),
                        name: "llama.cpp".to_string(),
                        description: "Embedded llama.cpp integration for efficient local inference".to_string(),
                        version,
                        default_endpoint: format!("local://{}", location.to_string_lossy()),
                        supports_text_generation: true,
                        supports_chat: true,
                        supports_embeddings: false,
                        requires_api_key: false,
                    },
                    ProviderType::Custom(name) => ProviderInfo {
                        provider_type: provider_type_str.clone(),
                        name: name.clone(),
                        description: format!("Custom provider: {}", name),
                        version,
                        default_endpoint: format!("http://localhost:8000/{}", name.to_lowercase()),
                        supports_text_generation: true,
                        supports_chat: true,
                        supports_embeddings: false,
                        requires_api_key: false,
                    },
                };
                
                infos.push(info);
            }
        }
        
        infos
    }

    /// Update suggestions for providers
    pub async fn update_suggestions(&self) -> Result<()> {
        let installations = self.get_installations();
        let config = self.get_config();
        
        // Skip if suggestions are disabled
        if !config.show_suggestions {
            return Ok(());
        }
        
        let mut suggestions = Vec::new();
        
        // Add suggestion for Ollama if not installed
        if !installations.contains_key(&ProviderType::Ollama.to_string()) {
            suggestions.push(ProviderSuggestion {
                provider_type: ProviderType::Ollama,
                install_command: if cfg!(target_os = "windows") {
                    "curl https://ollama.ai/download/ollama-windows-amd64.zip -o ollama.zip".to_string()
                } else if cfg!(target_os = "macos") {
                    "curl https://ollama.ai/download/ollama-darwin-amd64 -o ollama".to_string()
                } else {
                    "curl https://ollama.ai/install.sh | sh".to_string()
                },
                instructions_url: "https://ollama.ai/download".to_string(),
                description: "Run LLMs on your local machine. Ollama is a tool that allows you to run large language models locally.".to_string(),
                recommended_for: vec![
                    "Easy setup".to_string(),
                    "Familiar OpenAI-like API".to_string(),
                    "Wide model support".to_string(),
                ],
                hardware_requirements: Some(HardwareRequirements {
                    min_ram_gb: 8,
                    recommended_ram_gb: 16,
                    min_disk_gb: 10,
                    requires_gpu: false,
                    recommended_gpu: Some("NVIDIA with CUDA support".to_string()),
                    min_vram_gb: Some(4),
                }),
            });
        }
        
        // Add suggestion for LocalAI if not installed
        if !installations.contains_key(&ProviderType::LocalAI.to_string()) {
            suggestions.push(ProviderSuggestion {
                provider_type: ProviderType::LocalAI,
                install_command: if cfg!(target_os = "windows") {
                    "docker run -p 8080:8080 localai/localai:latest".to_string()
                } else if cfg!(target_os = "macos") {
                    "docker run -p 8080:8080 localai/localai:latest".to_string()
                } else {
                    "docker run -p 8080:8080 localai/localai:latest".to_string()
                },
                instructions_url: "https://localai.io/basics/getting_started/".to_string(),
                description: "Self-hosted, OpenAI API compatible server that can run with CPU, GPUs, M1/2, or even TPUs.".to_string(),
                recommended_for: vec![
                    "OpenAI API compatibility".to_string(),
                    "Multi-modal models".to_string(),
                    "Advanced customization".to_string(),
                ],
                hardware_requirements: Some(HardwareRequirements {
                    min_ram_gb: 8,
                    recommended_ram_gb: 16,
                    min_disk_gb: 10,
                    requires_gpu: false,
                    recommended_gpu: Some("NVIDIA with CUDA support".to_string()),
                    min_vram_gb: Some(4),
                }),
            });
        }
        
        // Update suggestions
        *self.suggestions.lock().unwrap() = suggestions;
        
        Ok(())
    }

    /// Detect Ollama installation
    async fn detect_ollama(&self) -> Result<InstallationInfo> {
        debug!("Detecting Ollama installation...");
        
        // Platform-specific detection
        let executable_name = if cfg!(target_os = "windows") {
            "ollama.exe"
        } else {
            "ollama"
        };
        
        // Try to find Ollama in PATH
        let output = match which::which(executable_name) {
            Ok(path) => {
                // Found ollama executable
                debug!("Found Ollama executable at {:?}", path);
                
                // Check if Ollama server is running
                match self.check_ollama_server().await {
                    Ok(version) => {
                        // Ollama server is running
                        InstallationInfo {
                            provider_type: ProviderType::Ollama,
                            status: InstallationStatus::Installed {
                                location: path.clone(),
                                version,
                            },
                            last_checked: chrono::Utc::now(),
                            auto_configured: true,
                        }
                    },
                    Err(_) => {
                        // Ollama server is not running
                        InstallationInfo {
                            provider_type: ProviderType::Ollama,
                            status: InstallationStatus::PartiallyInstalled {
                                reason: "Ollama executable found but server is not running".to_string(),
                                location: Some(path),
                            },
                            last_checked: chrono::Utc::now(),
                            auto_configured: false,
                        }
                    }
                }
            },
            Err(_) => {
                // Try additional platform-specific locations
                if cfg!(target_os = "windows") {
                    let program_files = std::env::var("ProgramFiles").unwrap_or("C:\\Program Files".to_string());
                    let ollama_path = Path::new(&program_files).join("Ollama").join(executable_name);
                    
                    if ollama_path.exists() {
                        match self.check_ollama_server().await {
                            Ok(version) => {
                                InstallationInfo {
                                    provider_type: ProviderType::Ollama,
                                    status: InstallationStatus::Installed {
                                        location: ollama_path.clone(),
                                        version,
                                    },
                                    last_checked: chrono::Utc::now(),
                                    auto_configured: true,
                                }
                            },
                            Err(_) => {
                                InstallationInfo {
                                    provider_type: ProviderType::Ollama,
                                    status: InstallationStatus::PartiallyInstalled {
                                        reason: "Ollama executable found but server is not running".to_string(),
                                        location: Some(ollama_path),
                                    },
                                    last_checked: chrono::Utc::now(),
                                    auto_configured: false,
                                }
                            }
                        }
                    } else {
                        // Not found
                        InstallationInfo {
                            provider_type: ProviderType::Ollama,
                            status: InstallationStatus::NotInstalled,
                            last_checked: chrono::Utc::now(),
                            auto_configured: false,
                        }
                    }
                } else if cfg!(target_os = "macos") {
                    let ollama_path = Path::new("/Applications/Ollama.app/Contents/MacOS").join(executable_name);
                    
                    if ollama_path.exists() {
                        match self.check_ollama_server().await {
                            Ok(version) => {
                                InstallationInfo {
                                    provider_type: ProviderType::Ollama,
                                    status: InstallationStatus::Installed {
                                        location: ollama_path.clone(),
                                        version,
                                    },
                                    last_checked: chrono::Utc::now(),
                                    auto_configured: true,
                                }
                            },
                            Err(_) => {
                                InstallationInfo {
                                    provider_type: ProviderType::Ollama,
                                    status: InstallationStatus::PartiallyInstalled {
                                        reason: "Ollama executable found but server is not running".to_string(),
                                        location: Some(ollama_path),
                                    },
                                    last_checked: chrono::Utc::now(),
                                    auto_configured: false,
                                }
                            }
                        }
                    } else {
                        // Not found
                        InstallationInfo {
                            provider_type: ProviderType::Ollama,
                            status: InstallationStatus::NotInstalled,
                            last_checked: chrono::Utc::now(),
                            auto_configured: false,
                        }
                    }
                } else {
                    // Linux - check /usr/local/bin and /usr/bin
                    let usr_local_bin = Path::new("/usr/local/bin").join(executable_name);
                    let usr_bin = Path::new("/usr/bin").join(executable_name);
                    
                    if usr_local_bin.exists() {
                        match self.check_ollama_server().await {
                            Ok(version) => {
                                InstallationInfo {
                                    provider_type: ProviderType::Ollama,
                                    status: InstallationStatus::Installed {
                                        location: usr_local_bin.clone(),
                                        version,
                                    },
                                    last_checked: chrono::Utc::now(),
                                    auto_configured: true,
                                }
                            },
                            Err(_) => {
                                InstallationInfo {
                                    provider_type: ProviderType::Ollama,
                                    status: InstallationStatus::PartiallyInstalled {
                                        reason: "Ollama executable found but server is not running".to_string(),
                                        location: Some(usr_local_bin),
                                    },
                                    last_checked: chrono::Utc::now(),
                                    auto_configured: false,
                                }
                            }
                        }
                    } else if usr_bin.exists() {
                        match self.check_ollama_server().await {
                            Ok(version) => {
                                InstallationInfo {
                                    provider_type: ProviderType::Ollama,
                                    status: InstallationStatus::Installed {
                                        location: usr_bin.clone(),
                                        version,
                                    },
                                    last_checked: chrono::Utc::now(),
                                    auto_configured: true,
                                }
                            },
                            Err(_) => {
                                InstallationInfo {
                                    provider_type: ProviderType::Ollama,
                                    status: InstallationStatus::PartiallyInstalled {
                                        reason: "Ollama executable found but server is not running".to_string(),
                                        location: Some(usr_bin),
                                    },
                                    last_checked: chrono::Utc::now(),
                                    auto_configured: false,
                                }
                            }
                        }
                    } else {
                        // Not found
                        InstallationInfo {
                            provider_type: ProviderType::Ollama,
                            status: InstallationStatus::NotInstalled,
                            last_checked: chrono::Utc::now(),
                            auto_configured: false,
                        }
                    }
                }
            }
        };
        
        Ok(output)
    }

    /// Check if Ollama server is running
    async fn check_ollama_server(&self) -> Result<String> {
        debug!("Checking Ollama server...");
        
        // Try to connect to Ollama server
        let client = reqwest::Client::builder()
            .timeout(Duration::from_secs(5))
            .build()?;
        
        let response = client
            .get("http://localhost:11434/api/version")
            .send()
            .await?;
        
        if response.status().is_success() {
            let json: serde_json::Value = response.json().await?;
            let version = json["version"].as_str().unwrap_or("unknown").to_string();
            Ok(version)
        } else {
            Err(anyhow!("Ollama server returned error: {}", response.status()))
        }
    }

    /// Detect LocalAI installation
    async fn detect_localai(&self) -> Result<InstallationInfo> {
        debug!("Detecting LocalAI installation...");
        
        // LocalAI is often run as a Docker container
        // Try to check if LocalAI is running by connecting to the API
        match self.check_localai_server().await {
            Ok(version) => {
                // LocalAI server is running
                Ok(InstallationInfo {
                    provider_type: ProviderType::LocalAI,
                    status: InstallationStatus::Installed {
                        location: PathBuf::from("/var/lib/docker"), // Approximate, not precise
                        version,
                    },
                    last_checked: chrono::Utc::now(),
                    auto_configured: true,
                })
            },
            Err(_) => {
                // Check if Docker is available and try to find LocalAI image
                if which::which("docker").is_ok() {
                    let output = Command::new("docker")
                        .args(["images", "localai/localai", "--format", "{{.Tag}}"])
                        .output();
                    
                    match output {
                        Ok(output) if !output.stdout.is_empty() => {
                            // LocalAI image found but not running
                            let tag = String::from_utf8_lossy(&output.stdout).trim().to_string();
                            Ok(InstallationInfo {
                                provider_type: ProviderType::LocalAI,
                                status: InstallationStatus::PartiallyInstalled {
                                    reason: "LocalAI Docker image found but server is not running".to_string(),
                                    location: Some(PathBuf::from("/var/lib/docker")),
                                },
                                last_checked: chrono::Utc::now(),
                                auto_configured: false,
                            })
                        },
                        _ => {
                            // LocalAI not found
                            Ok(InstallationInfo {
                                provider_type: ProviderType::LocalAI,
                                status: InstallationStatus::NotInstalled,
                                last_checked: chrono::Utc::now(),
                                auto_configured: false,
                            })
                        }
                    }
                } else {
                    // Docker not available
                    Ok(InstallationInfo {
                        provider_type: ProviderType::LocalAI,
                        status: InstallationStatus::NotInstalled,
                        last_checked: chrono::Utc::now(),
                        auto_configured: false,
                    })
                }
            }
        }
    }

    /// Check if LocalAI server is running
    async fn check_localai_server(&self) -> Result<String> {
        debug!("Checking LocalAI server...");
        
        // Try to connect to LocalAI server
        let client = reqwest::Client::builder()
            .timeout(Duration::from_secs(5))
            .build()?;
        
        let response = client
            .get("http://localhost:8080/version")
            .send()
            .await?;
        
        if response.status().is_success() {
            let json: serde_json::Value = response.json().await?;
            let version = json["version"].as_str().unwrap_or("unknown").to_string();
            Ok(version)
        } else {
            Err(anyhow!("LocalAI server returned error: {}", response.status()))
        }
    }

    /// Detect llama.cpp installation
    async fn detect_llamacpp(&self) -> Result<InstallationInfo> {
        debug!("Detecting llama.cpp installation...");
        
        // Look for llama.cpp in common locations
        let mut possible_locations = Vec::new();
        
        // Check PATH for main executable
        let executable_name = if cfg!(target_os = "windows") {
            "llama-main.exe"
        } else {
            "llama-main"
        };
        
        if let Ok(path) = which::which(executable_name) {
            possible_locations.push(path);
        }
        
        // Check other common names
        let alt_executable_names = if cfg!(target_os = "windows") {
            vec!["llama.exe", "llama-cli.exe", "llm.exe"]
        } else {
            vec!["llama", "llama-cli", "llm"]
        };
        
        for name in alt_executable_names {
            if let Ok(path) = which::which(name) {
                possible_locations.push(path);
            }
        }
        
        // Check common installation directories
        if cfg!(target_os = "windows") {
            let program_files = std::env::var("ProgramFiles").unwrap_or("C:\\Program Files".to_string());
            possible_locations.push(Path::new(&program_files).join("llama.cpp").join(executable_name));
            possible_locations.push(Path::new(&program_files).join("llama.cpp").join("bin").join(executable_name));
            
            // Check home directory
            if let Ok(home) = std::env::var("USERPROFILE") {
                possible_locations.push(Path::new(&home).join("llama.cpp").join(executable_name));
                possible_locations.push(Path::new(&home).join("Downloads").join("llama.cpp").join(executable_name));
                possible_locations.push(Path::new(&home).join("git").join("llama.cpp").join(executable_name));
            }
        } else if cfg!(target_os = "macos") {
            possible_locations.push(Path::new("/Applications/llama.cpp/bin").join(executable_name));
            possible_locations.push(Path::new("/usr/local/bin").join(executable_name));
            
            // Check home directory
            if let Ok(home) = std::env::var("HOME") {
                possible_locations.push(Path::new(&home).join("llama.cpp").join(executable_name));
                possible_locations.push(Path::new(&home).join("Downloads").join("llama.cpp").join(executable_name));
                possible_locations.push(Path::new(&home).join("git").join("llama.cpp").join(executable_name));
            }
        } else {
            // Linux
            possible_locations.push(Path::new("/usr/local/bin").join(executable_name));
            possible_locations.push(Path::new("/usr/bin").join(executable_name));
            possible_locations.push(Path::new("/opt/llama.cpp/bin").join(executable_name));
            
            // Check home directory
            if let Ok(home) = std::env::var("HOME") {
                possible_locations.push(Path::new(&home).join("llama.cpp").join(executable_name));
                possible_locations.push(Path::new(&home).join("Downloads").join("llama.cpp").join(executable_name));
                possible_locations.push(Path::new(&home).join("git").join("llama.cpp").join(executable_name));
            }
        }
        
        // Check if any location exists
        for location in possible_locations {
            if location.exists() {
                // Try to get version
                let version = self.get_llamacpp_version(&location).await?;
                
                return Ok(InstallationInfo {
                    provider_type: ProviderType::LlamaCpp,
                    status: InstallationStatus::Installed {
                        location: location.clone(),
                        version,
                    },
                    last_checked: chrono::Utc::now(),
                    auto_configured: true,
                });
            }
        }
        
        // Not found
        Ok(InstallationInfo {
            provider_type: ProviderType::LlamaCpp,
            status: InstallationStatus::NotInstalled,
            last_checked: chrono::Utc::now(),
            auto_configured: false,
        })
    }

    /// Get llama.cpp version
    async fn get_llamacpp_version(&self, path: &Path) -> Result<String> {
        debug!("Getting llama.cpp version from {:?}...", path);
        
        // Try to run the executable with --version
        let output = Command::new(path)
            .args(["--version"])
            .output();
        
        match output {
            Ok(output) if output.status.success() => {
                // Parse version from output
                let stdout = String::from_utf8_lossy(&output.stdout);
                let stderr = String::from_utf8_lossy(&output.stderr);
                
                // Look for version in stdout or stderr
                let combined = format!("{}{}", stdout, stderr);
                let version_regex = regex::Regex::new(r"version\s+([0-9]+\.[0-9]+\.[0-9]+)").unwrap();
                
                if let Some(captures) = version_regex.captures(&combined) {
                    Ok(captures[1].to_string())
                } else {
                    // Fallback - just return "unknown"
                    Ok("unknown".to_string())
                }
            },
            _ => {
                // Try with -v flag
                let output = Command::new(path)
                    .args(["-v"])
                    .output();
                
                match output {
                    Ok(output) if output.status.success() => {
                        // Parse version from output
                        let stdout = String::from_utf8_lossy(&output.stdout);
                        let stderr = String::from_utf8_lossy(&output.stderr);
                        
                        // Look for version in stdout or stderr
                        let combined = format!("{}{}", stdout, stderr);
                        let version_regex = regex::Regex::new(r"version\s+([0-9]+\.[0-9]+\.[0-9]+)").unwrap();
                        
                        if let Some(captures) = version_regex.captures(&combined) {
                            Ok(captures[1].to_string())
                        } else {
                            // Fallback - just return "unknown"
                            Ok("unknown".to_string())
                        }
                    },
                    _ => {
                        // Couldn't get version
                        Ok("unknown".to_string())
                    }
                }
            }
        }
    }

    /// Scan a custom path for possible providers
    async fn scan_custom_path(&self, path: &Path) -> Result<HashMap<String, InstallationInfo>> {
        debug!("Scanning custom path {:?} for providers...", path);
        
        let mut results = HashMap::new();
        
        // Skip if path doesn't exist
        if !path.exists() {
            return Ok(results);
        }
        
        // Check if path is a directory
        if !path.is_dir() {
            return Ok(results);
        }
        
        // List entries in directory
        let entries = std::fs::read_dir(path)
            .with_context(|| format!("Failed to read directory {:?}", path))?;
        
        // Look for executables and directories that might be providers
        for entry in entries {
            let entry = entry?;
            let entry_path = entry.path();
            
            if entry_path.is_dir() {
                // Check if directory name matches a known provider pattern
                let dir_name = entry_path.file_name()
                    .and_then(|name| name.to_str())
                    .unwrap_or("");
                
                if dir_name.to_lowercase().contains("llm") || 
                   dir_name.to_lowercase().contains("model") || 
                   dir_name.to_lowercase().contains("ai") {
                    // This might be a custom provider
                    // Look for executable files
                    if let Ok(subentries) = std::fs::read_dir(&entry_path) {
                        for subentry in subentries {
                            if let Ok(subentry) = subentry {
                                let subentry_path = subentry.path();
                                
                                if subentry_path.is_file() && self.is_executable(&subentry_path) {
                                    // This might be a provider executable
                                    let provider_name = dir_name.to_string();
                                    let provider_type = ProviderType::Custom(provider_name.clone());
                                    
                                    results.insert(
                                        provider_type.to_string(),
                                        InstallationInfo {
                                            provider_type: provider_type.clone(),
                                            status: InstallationStatus::Installed {
                                                location: entry_path.clone(),
                                                version: "custom".to_string(),
                                            },
                                            last_checked: chrono::Utc::now(),
                                            auto_configured: true,
                                        },
                                    );
                                    
                                    // Only add one provider per directory
                                    break;
                                }
                            }
                        }
                    }
                }
            } else if entry_path.is_file() && self.is_executable(&entry_path) {
                // This might be a provider executable
                let file_name = entry_path.file_name()
                    .and_then(|name| name.to_str())
                    .unwrap_or("");
                
                if file_name.to_lowercase().contains("llm") || 
                   file_name.to_lowercase().contains("model") || 
                   file_name.to_lowercase().contains("ai") {
                    // Strip extension to get provider name
                    let provider_name = entry_path.file_stem()
                        .and_then(|name| name.to_str())
                        .unwrap_or(file_name)
                        .to_string();
                    
                    let provider_type = ProviderType::Custom(provider_name.clone());
                    
                    results.insert(
                        provider_type.to_string(),
                        InstallationInfo {
                            provider_type: provider_type.clone(),
                            status: InstallationStatus::Installed {
                                location: entry_path.clone(),
                                version: "custom".to_string(),
                            },
                            last_checked: chrono::Utc::now(),
                            auto_configured: true,
                        },
                    );
                }
            }
        }
        
        Ok(results)
    }

    /// Check if a file is executable
    fn is_executable(&self, path: &Path) -> bool {
        if cfg!(unix) {
            // On Unix, check file permissions
            use std::os::unix::fs::PermissionsExt;
            
            if let Ok(metadata) = std::fs::metadata(path) {
                let permissions = metadata.permissions();
                return permissions.mode() & 0o111 != 0;
            }
            
            false
        } else {
            // On Windows, check file extension
            if let Some(extension) = path.extension() {
                let extension = extension.to_string_lossy().to_lowercase();
                return extension == "exe" || extension == "bat" || extension == "cmd";
            }
            
            false
        }
    }
}

impl Clone for DiscoveryService {
    fn clone(&self) -> Self {
        Self {
            config: Mutex::new(self.config.lock().unwrap().clone()),
            installations: Mutex::new(self.installations.lock().unwrap().clone()),
            suggestions: Mutex::new(self.suggestions.lock().unwrap().clone()),
            scanning: Mutex::new(*self.scanning.lock().unwrap()),
            last_scan: Mutex::new(*self.last_scan.lock().unwrap()),
            scanner_running: Mutex::new(*self.scanner_running.lock().unwrap()),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_discovery_service_creation() {
        let service = DiscoveryService::new();
        
        // Check default config
        let config = service.get_config();
        assert!(config.auto_detect);
        assert!(config.auto_configure);
        assert_eq!(config.scan_interval_seconds, 3600);
        
        // Initially no installations
        let installations = service.get_installations();
        assert!(installations.is_empty());
        
        // Initially no suggestions
        let suggestions = service.get_suggestions();
        assert!(suggestions.is_empty());
    }
    
    #[tokio::test]
    async fn test_custom_config() {
        let config = DiscoveryConfig {
            auto_detect: false,
            auto_configure: false,
            scan_interval_seconds: 120,
            scan_paths: vec![PathBuf::from("/tmp")],
            show_suggestions: false,
            check_for_updates: false,
        };
        
        let service = DiscoveryService::with_config(config.clone());
        
        // Check config values
        let retrieved_config = service.get_config();
        assert_eq!(retrieved_config.auto_detect, config.auto_detect);
        assert_eq!(retrieved_config.auto_configure, config.auto_configure);
        assert_eq!(retrieved_config.scan_interval_seconds, config.scan_interval_seconds);
        assert_eq!(retrieved_config.scan_paths, config.scan_paths);
        assert_eq!(retrieved_config.show_suggestions, config.show_suggestions);
        assert_eq!(retrieved_config.check_for_updates, config.check_for_updates);
    }
    
    #[tokio::test]
    async fn test_update_config() {
        let service = DiscoveryService::new();
        
        // Update config
        let new_config = DiscoveryConfig {
            auto_detect: false,
            auto_configure: false,
            scan_interval_seconds: 120,
            scan_paths: vec![PathBuf::from("/tmp")],
            show_suggestions: false,
            check_for_updates: false,
        };
        
        service.update_config(new_config.clone());
        
        // Check config values
        let retrieved_config = service.get_config();
        assert_eq!(retrieved_config.auto_detect, new_config.auto_detect);
        assert_eq!(retrieved_config.auto_configure, new_config.auto_configure);
        assert_eq!(retrieved_config.scan_interval_seconds, new_config.scan_interval_seconds);
        assert_eq!(retrieved_config.scan_paths, new_config.scan_paths);
        assert_eq!(retrieved_config.show_suggestions, new_config.show_suggestions);
        assert_eq!(retrieved_config.check_for_updates, new_config.check_for_updates);
    }
    
    // Note: More comprehensive tests would be added in a real implementation
    // to test the actual detection functionality, but those would require
    // mocking the file system and network requests.
}

// Public API functions

/// Initialize the discovery service
pub async fn init_discovery() -> Arc<DiscoveryService> {
    let service = Arc::new(DiscoveryService::new());
    
    // Perform initial scan
    if let Err(e) = service.scan_for_providers().await {
        error!("Error during initial provider scan: {}", e);
    }
    
    // Update suggestions
    if let Err(e) = service.update_suggestions().await {
        error!("Error updating provider suggestions: {}", e);
    }
    
    // Start background scanner
    if let Err(e) = service.start_background_scanner().await {
        error!("Error starting background scanner: {}", e);
    }
    
    service
}

/// Scan for providers
pub async fn scan_for_providers(service: &Arc<DiscoveryService>) -> Result<()> {
    service.scan_for_providers().await
}

/// Get provider configurations
pub fn get_provider_configs(service: &Arc<DiscoveryService>) -> Vec<ProviderConfig> {
    service.create_provider_configs()
}

/// Get provider information
pub fn get_provider_infos(service: &Arc<DiscoveryService>) -> Vec<ProviderInfo> {
    service.create_provider_infos()
}

/// Get provider suggestions
pub fn get_provider_suggestions(service: &Arc<DiscoveryService>) -> Vec<ProviderSuggestion> {
    service.get_suggestions()
}

/// Get provider installations
pub fn get_provider_installations(service: &Arc<DiscoveryService>) -> HashMap<String, InstallationInfo> {
    service.get_installations()
}

/// Get discovery service configuration
pub fn get_discovery_config(service: &Arc<DiscoveryService>) -> DiscoveryConfig {
    service.get_config()
}

/// Update discovery service configuration
pub fn update_discovery_config(service: &Arc<DiscoveryService>, config: DiscoveryConfig) {
    service.update_config(config);
}
</file>

<file path="src/offline/llm/migration.rs">
use std::collections::HashMap;
use std::path::{Path, PathBuf};
use std::sync::{Arc, Mutex};
use log::{debug, info, warn, error};
use serde::{Serialize, Deserialize};
use anyhow::{Result, anyhow, Context};
use tokio::fs;

use crate::error::Error;
use crate::commands::offline::llm::{ProviderType, ProviderConfig, ProviderInfo};
use crate::offline::llm::{LocalLLM, ModelInfo, LLMConfig, LLMParameters};
use crate::offline::llm::discovery::DiscoveryService;

/// Migration status enum
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum MigrationStatus {
    /// Migration has not been attempted
    NotMigrated,
    /// Migration is in progress
    InProgress,
    /// Migration completed successfully
    Completed {
        /// When the migration was completed
        timestamp: chrono::DateTime<chrono::Utc>,
        /// Total number of models migrated
        models_migrated: usize,
        /// Provider types that were migrated
        providers_configured: Vec<String>,
    },
    /// Migration failed
    Failed {
        /// When the migration failed
        timestamp: chrono::DateTime<chrono::Utc>,
        /// Reason for failure
        reason: String,
        /// Whether fallback to old system is active
        fallback_active: bool,
    },
    /// User opted out of migration
    OptedOut {
        /// When the user opted out
        timestamp: chrono::DateTime<chrono::Utc>,
    },
}

impl Default for MigrationStatus {
    fn default() -> Self {
        Self::NotMigrated
    }
}

/// Legacy model mapping
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LegacyModelMapping {
    /// Legacy model ID
    pub legacy_id: String,
    /// New provider type
    pub provider_type: String,
    /// New model ID
    pub new_id: String,
    /// File path to model
    pub model_path: PathBuf,
    /// Whether the model was successfully migrated
    pub migrated: bool,
}

/// Old to new provider mapping
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProviderMapping {
    /// Legacy configuration
    pub legacy_config: LLMConfig,
    /// New provider type
    pub provider_type: String,
    /// New provider configuration
    pub provider_config: ProviderConfig,
    /// Whether the provider was successfully migrated
    pub migrated: bool,
}

/// Migration configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MigrationConfig {
    /// Whether to auto-migrate on startup
    pub auto_migrate: bool,
    /// Whether to show migration UI
    pub show_migration_ui: bool,
    /// Whether to keep legacy files
    pub keep_legacy_files: bool,
    /// Extra paths to check for old models
    pub extra_model_paths: Vec<PathBuf>,
    /// Path to legacy config
    pub legacy_config_path: Option<PathBuf>,
}

impl Default for MigrationConfig {
    fn default() -> Self {
        Self {
            auto_migrate: true,
            show_migration_ui: true,
            keep_legacy_files: true,
            extra_model_paths: Vec::new(),
            legacy_config_path: None,
        }
    }
}

/// Migration notification info
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MigrationNotification {
    /// Migration status
    pub status: MigrationStatus,
    /// Number of models found
    pub models_found: usize,
    /// Number of models migrated successfully
    pub models_migrated: usize,
    /// Number of providers configured
    pub providers_configured: usize,
    /// Whether fallback is available
    pub fallback_available: bool,
    /// Duration of migration in seconds
    pub duration_seconds: f64,
    /// List of migrated model mappings
    pub model_mappings: Vec<LegacyModelMapping>,
    /// List of providers configured
    pub provider_mappings: Vec<ProviderMapping>,
}

/// Migration options
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MigrationOptions {
    /// Whether to migrate models
    pub migrate_models: bool,
    /// Whether to migrate configuration
    pub migrate_config: bool,
    /// Whether to delete legacy files
    pub delete_legacy_files: bool,
    /// Whether to enable fallback
    pub enable_fallback: bool,
}

impl Default for MigrationOptions {
    fn default() -> Self {
        Self {
            migrate_models: true,
            migrate_config: true,
            delete_legacy_files: false,
            enable_fallback: true,
        }
    }
}

/// Current legacy config store types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum LegacyStoreType {
    /// JSON file
    Json,
    /// TOML file
    Toml,
    /// YAML file
    Yaml,
    /// SQLite database
    Sqlite,
    /// Registry (Windows only)
    Registry,
}

/// Migration service
pub struct MigrationService {
    /// Current migration status
    status: Arc<Mutex<MigrationStatus>>,
    /// Migration configuration
    config: Arc<Mutex<MigrationConfig>>,
    /// Legacy models found
    legacy_models: Arc<Mutex<HashMap<String, ModelInfo>>>,
    /// Legacy config found
    legacy_config: Arc<Mutex<Option<LLMConfig>>>,
    /// Model mappings
    model_mappings: Arc<Mutex<Vec<LegacyModelMapping>>>,
    /// Provider mappings
    provider_mappings: Arc<Mutex<Vec<ProviderMapping>>>,
    /// Legacy store type
    store_type: Arc<Mutex<Option<LegacyStoreType>>>,
    /// Legacy fallback provider
    fallback_provider: Arc<Mutex<Option<LocalLLM>>>,
}

impl Default for MigrationService {
    fn default() -> Self {
        Self::new()
    }
}

impl MigrationService {
    /// Create a new migration service
    pub fn new() -> Self {
        Self {
            status: Arc::new(Mutex::new(MigrationStatus::NotMigrated)),
            config: Arc::new(Mutex::new(MigrationConfig::default())),
            legacy_models: Arc::new(Mutex::new(HashMap::new())),
            legacy_config: Arc::new(Mutex::new(None)),
            model_mappings: Arc::new(Mutex::new(Vec::new())),
            provider_mappings: Arc::new(Mutex::new(Vec::new())),
            store_type: Arc::new(Mutex::new(None)),
            fallback_provider: Arc::new(Mutex::new(None)),
        }
    }

    /// Get the current migration status
    pub fn get_status(&self) -> MigrationStatus {
        self.status.lock().unwrap().clone()
    }

    /// Get the current migration configuration
    pub fn get_config(&self) -> MigrationConfig {
        self.config.lock().unwrap().clone()
    }

    /// Update the migration configuration
    pub fn update_config(&self, config: MigrationConfig) {
        *self.config.lock().unwrap() = config;
    }

    /// Detect legacy LLM system
    pub async fn detect_legacy_system(&self) -> Result<bool> {
        info!("Detecting legacy LLM system...");
        
        // Check for legacy config
        let legacy_config_path = self.find_legacy_config().await?;
        
        if let Some(path) = &legacy_config_path {
            info!("Found legacy config at {:?}", path);
            
            // Try to load legacy config
            match self.load_legacy_config(path).await {
                Ok(config) => {
                    info!("Loaded legacy config: {:?}", config);
                    *self.legacy_config.lock().unwrap() = Some(config);
                },
                Err(e) => {
                    warn!("Failed to load legacy config: {}", e);
                }
            }
        }
        
        // Check for legacy models
        let legacy_models = self.find_legacy_models().await?;
        
        if !legacy_models.is_empty() {
            info!("Found {} legacy models", legacy_models.len());
            *self.legacy_models.lock().unwrap() = legacy_models;
        }
        
        // Legacy system detected if either config or models were found
        let legacy_detected = legacy_config_path.is_some() || !self.legacy_models.lock().unwrap().is_empty();
        
        info!("Legacy system detection complete: {}", legacy_detected);
        Ok(legacy_detected)
    }

    /// Find legacy config
    async fn find_legacy_config(&self) -> Result<Option<PathBuf>> {
        // Check custom path if specified
        let config = self.get_config();
        if let Some(path) = &config.legacy_config_path {
            if path.exists() {
                // Determine the store type
                let extension = path.extension().and_then(|e| e.to_str()).unwrap_or("");
                let store_type = match extension {
                    "json" => LegacyStoreType::Json,
                    "toml" => LegacyStoreType::Toml,
                    "yaml" | "yml" => LegacyStoreType::Yaml,
                    "db" | "sqlite" => LegacyStoreType::Sqlite,
                    _ => return Err(anyhow!("Unsupported legacy config format: {}", extension)),
                };
                
                *self.store_type.lock().unwrap() = Some(store_type);
                return Ok(Some(path.clone()));
            }
        }
        
        // Check common locations
        let mut possible_locations = Vec::new();
        
        // Add application data directory
        if let Some(app_data) = dirs::data_dir() {
            possible_locations.push(app_data.join("papin/llm_config.json"));
            possible_locations.push(app_data.join("papin/config/llm.json"));
            possible_locations.push(app_data.join("mcp-client/llm_config.json"));
            possible_locations.push(app_data.join("mcp-client/config/llm.json"));
        }
        
        // Add home directory
        if let Some(home) = dirs::home_dir() {
            possible_locations.push(home.join(".papin/llm_config.json"));
            possible_locations.push(home.join(".mcp-client/llm_config.json"));
            possible_locations.push(home.join(".config/papin/llm.json"));
            possible_locations.push(home.join(".config/mcp-client/llm.json"));
        }
        
        // Add current directory
        possible_locations.push(PathBuf::from("llm_config.json"));
        possible_locations.push(PathBuf::from("config/llm.json"));
        
        // Check Windows registry if on Windows
        #[cfg(target_os = "windows")]
        {
            use winreg::RegKey;
            use winreg::enums::*;
            
            // Try to open registry key
            let hklm = RegKey::predef(HKEY_LOCAL_MACHINE);
            if let Ok(key) = hklm.open_subkey("SOFTWARE\\Papin") {
                if let Ok(_) = key.get_value::<String, _>("LLMConfig") {
                    *self.store_type.lock().unwrap() = Some(LegacyStoreType::Registry);
                    return Ok(None); // No path for registry
                }
            }
            
            // Also check current user
            let hkcu = RegKey::predef(HKEY_CURRENT_USER);
            if let Ok(key) = hkcu.open_subkey("SOFTWARE\\Papin") {
                if let Ok(_) = key.get_value::<String, _>("LLMConfig") {
                    *self.store_type.lock().unwrap() = Some(LegacyStoreType::Registry);
                    return Ok(None); // No path for registry
                }
            }
        }
        
        // Check each possible location
        for path in possible_locations {
            if path.exists() {
                // JSON store type
                *self.store_type.lock().unwrap() = Some(LegacyStoreType::Json);
                return Ok(Some(path));
            }
            
            // Also check for TOML and YAML variants
            let toml_path = path.with_extension("toml");
            if toml_path.exists() {
                *self.store_type.lock().unwrap() = Some(LegacyStoreType::Toml);
                return Ok(Some(toml_path));
            }
            
            let yaml_path = path.with_extension("yaml");
            if yaml_path.exists() {
                *self.store_type.lock().unwrap() = Some(LegacyStoreType::Yaml);
                return Ok(Some(yaml_path));
            }
            
            let yml_path = path.with_extension("yml");
            if yml_path.exists() {
                *self.store_type.lock().unwrap() = Some(LegacyStoreType::Yaml);
                return Ok(Some(yml_path));
            }
            
            // Check for SQLite variant
            let sqlite_path = path.with_extension("db");
            if sqlite_path.exists() {
                *self.store_type.lock().unwrap() = Some(LegacyStoreType::Sqlite);
                return Ok(Some(sqlite_path));
            }
        }
        
        // No legacy config found
        Ok(None)
    }

    /// Load legacy config from file
    async fn load_legacy_config(&self, path: &Path) -> Result<LLMConfig> {
        // Read file content
        let content = fs::read_to_string(path).await
            .with_context(|| format!("Failed to read legacy config from {:?}", path))?;
        
        // Parse based on store type
        match *self.store_type.lock().unwrap() {
            Some(LegacyStoreType::Json) => {
                serde_json::from_str(&content)
                    .with_context(|| format!("Failed to parse legacy JSON config"))
            },
            Some(LegacyStoreType::Toml) => {
                toml::from_str(&content)
                    .with_context(|| format!("Failed to parse legacy TOML config"))
            },
            Some(LegacyStoreType::Yaml) => {
                serde_yaml::from_str(&content)
                    .with_context(|| format!("Failed to parse legacy YAML config"))
            },
            #[cfg(feature = "sqlite")]
            Some(LegacyStoreType::Sqlite) => {
                // Simplified SQLite handling - would be more complex in a real implementation
                Err(anyhow!("SQLite support not fully implemented"))
            },
            #[cfg(target_os = "windows")]
            Some(LegacyStoreType::Registry) => {
                use winreg::RegKey;
                use winreg::enums::*;
                
                // Try to open registry key
                let hklm = RegKey::predef(HKEY_LOCAL_MACHINE);
                if let Ok(key) = hklm.open_subkey("SOFTWARE\\Papin") {
                    if let Ok(config_json) = key.get_value::<String, _>("LLMConfig") {
                        return serde_json::from_str(&config_json)
                            .with_context(|| format!("Failed to parse legacy config from registry"));
                    }
                }
                
                // Also check current user
                let hkcu = RegKey::predef(HKEY_CURRENT_USER);
                if let Ok(key) = hkcu.open_subkey("SOFTWARE\\Papin") {
                    if let Ok(config_json) = key.get_value::<String, _>("LLMConfig") {
                        return serde_json::from_str(&config_json)
                            .with_context(|| format!("Failed to parse legacy config from registry"));
                    }
                }
                
                Err(anyhow!("Failed to load legacy config from registry"))
            },
            _ => Err(anyhow!("Unsupported legacy config store type")),
        }
    }

    /// Find legacy models
    async fn find_legacy_models(&self) -> Result<HashMap<String, ModelInfo>> {
        let mut legacy_models = HashMap::new();
        
        // Check standard model paths
        let mut model_paths = Vec::new();
        
        // Check for models in fixed app directory
        if let Some(app_data) = dirs::data_dir() {
            model_paths.push(app_data.join("papin/models"));
            model_paths.push(app_data.join("mcp-client/models"));
        }
        
        // Check config-defined model path
        if let Some(config) = &*self.legacy_config.lock().unwrap() {
            let model_dir = config.model_path.parent()
                .unwrap_or(&config.model_path)
                .to_path_buf();
            
            model_paths.push(model_dir);
        }
        
        // Add user-defined extra paths
        let config = self.get_config();
        model_paths.extend(config.extra_model_paths.clone());
        
        // Check all model paths
        for path in model_paths {
            if !path.exists() || !path.is_dir() {
                continue;
            }
            
            // Look for model files
            let entries = match fs::read_dir(&path).await {
                Ok(entries) => entries,
                Err(e) => {
                    warn!("Failed to read model directory {:?}: {}", path, e);
                    continue;
                }
            };
            
            // Process directory entries
            let mut entries_vec = Vec::new();
            let mut entry = entries.into_iter().next();
            while let Some(entry_result) = entry {
                match entry_result {
                    Ok(e) => entries_vec.push(e),
                    Err(e) => warn!("Failed to read directory entry: {}", e),
                }
                entry = entries.into_iter().next();
            }
            
            for entry in entries_vec {
                let entry_path = entry.path();
                
                // Skip directories and non-model files
                if entry_path.is_dir() {
                    continue;
                }
                
                let extension = entry_path.extension().and_then(|e| e.to_str()).unwrap_or("");
                if !Self::is_model_file(extension) {
                    continue;
                }
                
                // Get model ID from filename
                let model_id = entry_path.file_stem()
                    .and_then(|s| s.to_str())
                    .unwrap_or("unknown")
                    .to_string();
                
                // Get file size
                let metadata = match fs::metadata(&entry_path).await {
                    Ok(m) => m,
                    Err(e) => {
                        warn!("Failed to get metadata for {:?}: {}", entry_path, e);
                        continue;
                    }
                };
                
                let size_mb = metadata.len() / (1024 * 1024);
                
                // Create model info
                let model_info = ModelInfo {
                    id: model_id.clone(),
                    name: model_id.clone(),
                    size_mb: size_mb as usize,
                    context_size: 4096, // Default reasonable value
                    installed: true,
                    download_url: None,
                    description: format!("Legacy model found at {:?}", entry_path),
                };
                
                legacy_models.insert(model_id, model_info);
            }
        }
        
        Ok(legacy_models)
    }

    /// Check if a file extension is a model file
    fn is_model_file(extension: &str) -> bool {
        matches!(extension.to_lowercase().as_str(), 
            "bin" | "gguf" | "ggml" | "bin" | "weight" | "pt" | 
            "pth" | "model" | "onnx" | "safetensors" | "f16" | "f32")
    }

    /// Run the migration
    pub async fn run_migration(&self, options: MigrationOptions) -> Result<MigrationNotification> {
        info!("Starting legacy system migration...");
        
        // Update status
        *self.status.lock().unwrap() = MigrationStatus::InProgress;
        
        let start_time = std::time::Instant::now();
        
        // Detect legacy system if not already done
        if self.legacy_models.lock().unwrap().is_empty() && self.legacy_config.lock().unwrap().is_none() {
            self.detect_legacy_system().await?;
        }
        
        // Track migration results
        let mut models_found = 0;
        let mut models_migrated = 0;
        let mut providers_configured = 0;
        let mut model_mappings = Vec::new();
        let mut provider_mappings = Vec::new();
        
        // Migrate configuration if requested
        if options.migrate_config {
            match self.migrate_configuration().await {
                Ok(mappings) => {
                    providers_configured = mappings.len();
                    provider_mappings = mappings;
                },
                Err(e) => {
                    error!("Failed to migrate configuration: {}", e);
                    
                    // If fallback is enabled, set up fallback provider
                    if options.enable_fallback {
                        if let Some(config) = &*self.legacy_config.lock().unwrap() {
                            let llm = LocalLLM::new(
                                "legacy_fallback",
                                config.context_size,
                                1000, // Default speed
                            );
                            
                            *self.fallback_provider.lock().unwrap() = Some(llm);
                            info!("Fallback provider set up successfully");
                        }
                    }
                    
                    // Set failed status
                    *self.status.lock().unwrap() = MigrationStatus::Failed {
                        timestamp: chrono::Utc::now(),
                        reason: format!("Failed to migrate configuration: {}", e),
                        fallback_active: options.enable_fallback,
                    };
                    
                    let duration = start_time.elapsed().as_secs_f64();
                    
                    return Ok(MigrationNotification {
                        status: self.get_status(),
                        models_found,
                        models_migrated,
                        providers_configured,
                        fallback_available: options.enable_fallback,
                        duration_seconds: duration,
                        model_mappings,
                        provider_mappings,
                    });
                }
            }
        }
        
        // Migrate models if requested
        if options.migrate_models {
            let legacy_models = self.legacy_models.lock().unwrap().clone();
            models_found = legacy_models.len();
            
            match self.migrate_models(legacy_models, &provider_mappings).await {
                Ok(mappings) => {
                    models_migrated = mappings.iter().filter(|m| m.migrated).count();
                    model_mappings = mappings;
                },
                Err(e) => {
                    error!("Failed to migrate models: {}", e);
                    
                    // If fallback is enabled, set up fallback provider
                    if options.enable_fallback && self.fallback_provider.lock().unwrap().is_none() {
                        if let Some(config) = &*self.legacy_config.lock().unwrap() {
                            let llm = LocalLLM::new(
                                "legacy_fallback",
                                config.context_size,
                                1000, // Default speed
                            );
                            
                            *self.fallback_provider.lock().unwrap() = Some(llm);
                            info!("Fallback provider set up successfully");
                        }
                    }
                    
                    // Set failed status
                    *self.status.lock().unwrap() = MigrationStatus::Failed {
                        timestamp: chrono::Utc::now(),
                        reason: format!("Failed to migrate models: {}", e),
                        fallback_active: options.enable_fallback,
                    };
                    
                    let duration = start_time.elapsed().as_secs_f64();
                    
                    return Ok(MigrationNotification {
                        status: self.get_status(),
                        models_found,
                        models_migrated,
                        providers_configured,
                        fallback_available: options.enable_fallback,
                        duration_seconds: duration,
                        model_mappings,
                        provider_mappings,
                    });
                }
            }
        }
        
        // Delete legacy files if requested
        if options.delete_legacy_files && !options.enable_fallback {
            // This would be handled in a real implementation
            // For safety, we're not implementing actual file deletion here
            info!("Legacy file deletion requested but not implemented in this version");
        }
        
        // Save mappings
        *self.model_mappings.lock().unwrap() = model_mappings.clone();
        *self.provider_mappings.lock().unwrap() = provider_mappings.clone();
        
        // Update status to completed
        let provider_names = provider_mappings.iter()
            .map(|p| p.provider_type.clone())
            .collect();
        
        *self.status.lock().unwrap() = MigrationStatus::Completed {
            timestamp: chrono::Utc::now(),
            models_migrated,
            providers_configured: provider_names,
        };
        
        let duration = start_time.elapsed().as_secs_f64();
        
        info!("Migration completed successfully in {:.2} seconds", duration);
        info!("Migrated {}/{} models", models_migrated, models_found);
        info!("Configured {} providers", providers_configured);
        
        // Return migration notification
        Ok(MigrationNotification {
            status: self.get_status(),
            models_found,
            models_migrated,
            providers_configured,
            fallback_available: options.enable_fallback,
            duration_seconds: duration,
            model_mappings,
            provider_mappings,
        })
    }

    /// Migrate configuration
    async fn migrate_configuration(&self) -> Result<Vec<ProviderMapping>> {
        let mut mappings = Vec::new();
        
        // Check if we have legacy config
        let legacy_config = match &*self.legacy_config.lock().unwrap() {
            Some(config) => config.clone(),
            None => {
                // Create a default config if none found
                LLMConfig {
                    model_id: "default".to_string(),
                    model_path: PathBuf::from("models/default"),
                    context_size: 4096,
                    max_output_length: 2048,
                    parameters: LLMParameters::default(),
                    enabled: true,
                    memory_usage_mb: 512,
                }
            }
        };
        
        // Create provider configs based on legacy config
        
        // 1. Create LlamaCpp provider mapping
        let llamacpp_config = ProviderConfig {
            provider_type: ProviderType::LlamaCpp.to_string(),
            endpoint_url: format!("local://{}", legacy_config.model_path.to_string_lossy()),
            api_key: None,
            default_model: Some(legacy_config.model_id.clone()),
            enable_advanced_config: false,
            advanced_config: {
                let mut params = HashMap::new();
                
                // Map LLM parameters
                params.insert("temperature".to_string(), 
                    serde_json::to_value(legacy_config.parameters.temperature)?);
                params.insert("top_p".to_string(), 
                    serde_json::to_value(legacy_config.parameters.top_p)?);
                params.insert("top_k".to_string(), 
                    serde_json::to_value(legacy_config.parameters.top_k)?);
                params.insert("repeat_penalty".to_string(), 
                    serde_json::to_value(legacy_config.parameters.repetition_penalty)?);
                
                if legacy_config.parameters.use_mirostat {
                    params.insert("mirostat".to_string(), serde_json::to_value(true)?);
                    params.insert("mirostat_tau".to_string(), 
                        serde_json::to_value(legacy_config.parameters.mirostat_tau)?);
                    params.insert("mirostat_eta".to_string(), 
                        serde_json::to_value(legacy_config.parameters.mirostat_eta)?);
                }
                
                params
            },
        };
        
        mappings.push(ProviderMapping {
            legacy_config: legacy_config.clone(),
            provider_type: ProviderType::LlamaCpp.to_string(),
            provider_config: llamacpp_config,
            migrated: true,
        });
        
        // 2. Create Ollama provider mapping (if likely)
        // If the model path contains "ollama", it's likely an Ollama model
        if legacy_config.model_path.to_string_lossy().to_lowercase().contains("ollama") {
            let ollama_config = ProviderConfig {
                provider_type: ProviderType::Ollama.to_string(),
                endpoint_url: "http://localhost:11434".to_string(),
                api_key: None,
                default_model: Some(legacy_config.model_id.clone()),
                enable_advanced_config: false,
                advanced_config: {
                    let mut params = HashMap::new();
                    
                    // Map LLM parameters
                    params.insert("temperature".to_string(), 
                        serde_json::to_value(legacy_config.parameters.temperature)?);
                    params.insert("top_p".to_string(), 
                        serde_json::to_value(legacy_config.parameters.top_p)?);
                    params.insert("top_k".to_string(), 
                        serde_json::to_value(legacy_config.parameters.top_k)?);
                    
                    params
                },
            };
            
            mappings.push(ProviderMapping {
                legacy_config: legacy_config.clone(),
                provider_type: ProviderType::Ollama.to_string(),
                provider_config: ollama_config,
                migrated: true,
            });
        }
        
        // 3. Create LocalAI provider mapping
        let localai_config = ProviderConfig {
            provider_type: ProviderType::LocalAI.to_string(),
            endpoint_url: "http://localhost:8080".to_string(),
            api_key: None,
            default_model: None, // We don't know LocalAI model name
            enable_advanced_config: false,
            advanced_config: HashMap::new(),
        };
        
        mappings.push(ProviderMapping {
            legacy_config: legacy_config.clone(),
            provider_type: ProviderType::LocalAI.to_string(),
            provider_config: localai_config,
            migrated: false, // Mark as not migrated since it's speculative
        });
        
        Ok(mappings)
    }

    /// Migrate models
    async fn migrate_models(
        &self, 
        legacy_models: HashMap<String, ModelInfo>,
        provider_mappings: &[ProviderMapping]
    ) -> Result<Vec<LegacyModelMapping>> {
        let mut mappings = Vec::new();
        
        for (legacy_id, model_info) in legacy_models {
            // Determine best provider for this model
            let (provider_type, model_id) = self.determine_best_provider_for_model(&model_info, provider_mappings);
            
            // Create mapping
            mappings.push(LegacyModelMapping {
                legacy_id: legacy_id.clone(),
                provider_type: provider_type.clone(),
                new_id: model_id.clone(),
                model_path: if let Some(legacy_config) = &*self.legacy_config.lock().unwrap() {
                    if legacy_id == legacy_config.model_id {
                        legacy_config.model_path.clone()
                    } else {
                        PathBuf::new() // We don't know the exact path
                    }
                } else {
                    PathBuf::new() // We don't know the exact path
                },
                migrated: true, // Assume migration is successful
            });
        }
        
        Ok(mappings)
    }

    /// Determine the best provider for a model
    fn determine_best_provider_for_model(
        &self,
        model_info: &ModelInfo,
        provider_mappings: &[ProviderMapping]
    ) -> (String, String) {
        // Default to LlamaCpp as the most reliable option
        let mut best_provider = ProviderType::LlamaCpp.to_string();
        let mut best_model_id = model_info.id.clone();
        
        // If model ID contains "llama", it's likely a LlamaCpp model
        if model_info.id.to_lowercase().contains("llama") {
            best_provider = ProviderType::LlamaCpp.to_string();
            best_model_id = model_info.id.clone();
        }
        // If model ID contains "gpt4all", it's likely a LocalAI model
        else if model_info.id.to_lowercase().contains("gpt4all") {
            best_provider = ProviderType::LocalAI.to_string();
            best_model_id = format!("ggml-{}", model_info.id);
        }
        // If model ID contains "openai", it's likely an Ollama model
        else if model_info.id.to_lowercase().contains("openai") {
            best_provider = ProviderType::Ollama.to_string();
            best_model_id = model_info.id.clone();
        }
        // If model file is large (>4GB), it's likely an Ollama model
        else if model_info.size_mb > 4000 {
            best_provider = ProviderType::Ollama.to_string();
            best_model_id = model_info.id.clone();
        }
        
        // Check if we have a specific provider mapping that matches this model
        if let Some(legacy_config) = &*self.legacy_config.lock().unwrap() {
            if model_info.id == legacy_config.model_id {
                // This is the default model from config
                // Use the first successful mapping
                if let Some(mapping) = provider_mappings.iter().find(|m| m.migrated) {
                    best_provider = mapping.provider_type.clone();
                    best_model_id = model_info.id.clone();
                }
            }
        }
        
        (best_provider, best_model_id)
    }

    /// Get the fallback provider
    pub fn get_fallback_provider(&self) -> Option<Arc<LocalLLM>> {
        self.fallback_provider.lock().unwrap().as_ref().map(|p| Arc::new(p.clone()))
    }

    /// Opt out of migration
    pub fn opt_out(&self) {
        *self.status.lock().unwrap() = MigrationStatus::OptedOut {
            timestamp: chrono::Utc::now(),
        };
    }

    /// Get the model mappings
    pub fn get_model_mappings(&self) -> Vec<LegacyModelMapping> {
        self.model_mappings.lock().unwrap().clone()
    }

    /// Get the provider mappings
    pub fn get_provider_mappings(&self) -> Vec<ProviderMapping> {
        self.provider_mappings.lock().unwrap().clone()
    }
}

/// Initialize the migration service
pub async fn init_migration() -> Arc<MigrationService> {
    let service = Arc::new(MigrationService::new());
    
    // Detect legacy system
    if let Err(e) = service.detect_legacy_system().await {
        error!("Failed to detect legacy system: {}", e);
    }
    
    service
}

/// Run migration with default options
pub async fn run_migration(service: &Arc<MigrationService>) -> Result<MigrationNotification> {
    service.run_migration(MigrationOptions::default()).await
}

/// Get migration status
pub fn get_migration_status(service: &Arc<MigrationService>) -> MigrationStatus {
    service.get_status()
}

/// Get migration configuration
pub fn get_migration_config(service: &Arc<MigrationService>) -> MigrationConfig {
    service.get_config()
}

/// Update migration configuration
pub fn update_migration_config(service: &Arc<MigrationService>, config: MigrationConfig) {
    service.update_config(config);
}

/// Get model mappings
pub fn get_model_mappings(service: &Arc<MigrationService>) -> Vec<LegacyModelMapping> {
    service.get_model_mappings()
}

/// Get provider mappings
pub fn get_provider_mappings(service: &Arc<MigrationService>) -> Vec<ProviderMapping> {
    service.get_provider_mappings()
}

/// Get fallback provider
pub fn get_fallback_provider(service: &Arc<MigrationService>) -> Option<Arc<LocalLLM>> {
    service.get_fallback_provider()
}

/// Opt out of migration
pub fn opt_out_of_migration(service: &Arc<MigrationService>) {
    service.opt_out();
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_migration_status_defaults() {
        let status = MigrationStatus::default();
        assert!(matches!(status, MigrationStatus::NotMigrated));
    }
    
    #[test]
    fn test_migration_options_defaults() {
        let options = MigrationOptions::default();
        assert!(options.migrate_models);
        assert!(options.migrate_config);
        assert!(!options.delete_legacy_files);
        assert!(options.enable_fallback);
    }
    
    #[test]
    fn test_is_model_file() {
        assert!(MigrationService::is_model_file("bin"));
        assert!(MigrationService::is_model_file("gguf"));
        assert!(MigrationService::is_model_file("ggml"));
        assert!(MigrationService::is_model_file("safetensors"));
        assert!(!MigrationService::is_model_file("txt"));
        assert!(!MigrationService::is_model_file("json"));
        assert!(!MigrationService::is_model_file(""));
    }
}
</file>

<file path="src/offline/sync/mod.rs">
use std::collections::{HashMap, HashSet};
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant, SystemTime};
use serde::{Serialize, Deserialize};
use log::{debug, info, warn, error};
use chrono::{DateTime, Utc};

/// Sync operation type
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub enum SyncOperationType {
    /// Create a new item
    Create,
    /// Update an existing item
    Update,
    /// Delete an item
    Delete,
}

/// Sync operation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncOperation {
    /// Operation type
    pub operation_type: SyncOperationType,
    /// Key of the item
    pub key: String,
    /// New value of the item (null for delete)
    pub value: Option<String>,
    /// Timestamp of the operation
    pub timestamp: DateTime<Utc>,
    /// Device ID that created the operation
    pub device_id: String,
    /// Operation ID
    pub operation_id: String,
}

/// Sync conflict
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncConflict {
    /// Key of the conflicted item
    pub key: String,
    /// Local operation
    pub local_operation: SyncOperation,
    /// Remote operation
    pub remote_operation: SyncOperation,
    /// Resolution strategy
    pub resolution: SyncResolutionStrategy,
    /// Resolved value
    pub resolved_value: Option<String>,
}

/// Sync resolution strategy
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub enum SyncResolutionStrategy {
    /// Use local value
    UseLocal,
    /// Use remote value
    UseRemote,
    /// Merge values
    Merge,
    /// Manual resolution
    Manual,
}

/// Sync status
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncStatus {
    /// Last sync time
    pub last_sync: Option<DateTime<Utc>>,
    /// Number of local changes
    pub local_changes: usize,
    /// Number of remote changes
    pub remote_changes: usize,
    /// Number of conflicts
    pub conflicts: usize,
    /// Whether a sync is in progress
    pub syncing: bool,
    /// Current sync progress (0.0 to 1.0)
    pub progress: f32,
    /// Error message if sync failed
    pub error: Option<String>,
}

/// Sync configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncConfig {
    /// Whether sync is enabled
    pub enabled: bool,
    /// Sync interval in seconds
    pub interval_seconds: u64,
    /// Device ID
    pub device_id: String,
    /// Whether to sync automatically
    pub auto_sync: bool,
    /// Default conflict resolution strategy
    pub default_resolution: SyncResolutionStrategy,
    /// Whether to sync on startup
    pub sync_on_startup: bool,
    /// Whether to sync on shutdown
    pub sync_on_shutdown: bool,
}

impl Default for SyncConfig {
    fn default() -> Self {
        Self {
            enabled: true,
            interval_seconds: 300, // 5 minutes
            device_id: generate_device_id(),
            auto_sync: true,
            default_resolution: SyncResolutionStrategy::UseRemote,
            sync_on_startup: true,
            sync_on_shutdown: true,
        }
    }
}

/// Sync result
pub struct SyncResult {
    /// Whether the sync was successful
    pub success: bool,
    /// Number of local changes applied
    pub local_applied: usize,
    /// Number of remote changes applied
    pub remote_applied: usize,
    /// Conflicts that occurred during sync
    pub conflicts: Vec<SyncConflict>,
    /// Error message if sync failed
    pub error: Option<String>,
}

/// Synchronization manager for offline capabilities
pub struct SyncManager {
    config: Arc<Mutex<SyncConfig>>,
    status: Arc<Mutex<SyncStatus>>,
    pending_operations: Arc<Mutex<Vec<SyncOperation>>>,
    resolved_conflicts: Arc<Mutex<HashMap<String, SyncConflict>>>,
    running: Arc<Mutex<bool>>,
}

impl Default for SyncManager {
    fn default() -> Self {
        Self::new()
    }
}

impl SyncManager {
    /// Create a new sync manager
    pub fn new() -> Self {
        Self {
            config: Arc::new(Mutex::new(SyncConfig::default())),
            status: Arc::new(Mutex::new(SyncStatus {
                last_sync: None,
                local_changes: 0,
                remote_changes: 0,
                conflicts: 0,
                syncing: false,
                progress: 0.0,
                error: None,
            })),
            pending_operations: Arc::new(Mutex::new(Vec::new())),
            resolved_conflicts: Arc::new(Mutex::new(HashMap::new())),
            running: Arc::new(Mutex::new(false)),
        }
    }
    
    /// Start the sync manager
    pub fn start(&self) {
        let mut running = self.running.lock().unwrap();
        if *running {
            return;
        }
        *running = true;
        
        let config = self.config.clone();
        let status = self.status.clone();
        let pending_operations = self.pending_operations.clone();
        let resolved_conflicts = self.resolved_conflicts.clone();
        let running_clone = self.running.clone();
        
        // Start background sync task
        std::thread::spawn(move || {
            // Sync on startup if enabled
            {
                let cfg = config.lock().unwrap();
                if cfg.enabled && cfg.sync_on_startup {
                    drop(cfg);
                    let _ = Self::perform_sync(
                        &config,
                        &status,
                        &pending_operations,
                        &resolved_conflicts,
                    );
                }
            }
            
            // Background sync loop
            while *running_clone.lock().unwrap() {
                let interval = {
                    let cfg = config.lock().unwrap();
                    Duration::from_secs(cfg.interval_seconds)
                };
                
                // Sleep for the configured interval
                std::thread::sleep(interval);
                
                // Check if we're still running
                if !*running_clone.lock().unwrap() {
                    break;
                }
                
                // Check if auto-sync is enabled
                let should_sync = {
                    let cfg = config.lock().unwrap();
                    cfg.enabled && cfg.auto_sync
                };
                
                if should_sync {
                    let _ = Self::perform_sync(
                        &config,
                        &status,
                        &pending_operations,
                        &resolved_conflicts,
                    );
                }
            }
            
            // Sync on shutdown if enabled
            {
                let cfg = config.lock().unwrap();
                if cfg.enabled && cfg.sync_on_shutdown {
                    drop(cfg);
                    let _ = Self::perform_sync(
                        &config,
                        &status,
                        &pending_operations,
                        &resolved_conflicts,
                    );
                }
            }
        });
    }
    
    /// Stop the sync manager
    pub fn stop(&self) {
        let mut running = self.running.lock().unwrap();
        *running = false;
    }
    
    /// Perform a synchronization
    fn perform_sync(
        config: &Arc<Mutex<SyncConfig>>,
        status: &Arc<Mutex<SyncStatus>>,
        pending_operations: &Arc<Mutex<Vec<SyncOperation>>>,
        resolved_conflicts: &Arc<Mutex<HashMap<String, SyncConflict>>>,
    ) -> Result<SyncResult, String> {
        // Check if sync is enabled
        {
            let cfg = config.lock().unwrap();
            if !cfg.enabled {
                return Err("Sync is disabled".to_string());
            }
        }
        
        // Check if we're already syncing
        {
            let mut stat = status.lock().unwrap();
            if stat.syncing {
                return Err("Sync already in progress".to_string());
            }
            
            // Update status
            stat.syncing = true;
            stat.progress = 0.0;
            stat.error = None;
        }
        
        // Collect local changes
        let local_changes = {
            let operations = pending_operations.lock().unwrap();
            
            let mut changes = HashMap::new();
            for op in operations.iter() {
                changes.insert(op.key.clone(), op.value.clone().unwrap_or_default());
            }
            
            changes
        };
        
        // Update status
        {
            let mut stat = status.lock().unwrap();
            stat.local_changes = local_changes.len();
            stat.progress = 0.2;
        }
        
        // Simulate getting remote changes
        let remote_changes = generate_mock_remote_changes(&local_changes);
        
        // Update status
        {
            let mut stat = status.lock().unwrap();
            stat.remote_changes = remote_changes.len();
            stat.progress = 0.4;
        }
        
        // Perform sync (merging local and remote changes)
        let result = Self::sync(local_changes, remote_changes);
        
        // Update status
        {
            let mut stat = status.lock().unwrap();
            stat.conflicts = result.conflicts.len();
            stat.progress = 0.8;
        }
        
        // Store resolved conflicts
        {
            let mut conflicts = resolved_conflicts.lock().unwrap();
            for conflict in &result.conflicts {
                conflicts.insert(conflict.key.clone(), conflict.clone());
            }
        }
        
        // Clear pending operations if sync was successful
        if result.success {
            let mut operations = pending_operations.lock().unwrap();
            operations.clear();
        }
        
        // Update final status
        {
            let mut stat = status.lock().unwrap();
            stat.last_sync = Some(Utc::now());
            stat.progress = 1.0;
            stat.syncing = false;
            
            if !result.success {
                stat.error = result.error.clone();
            }
        }
        
        Ok(result)
    }
    
    /// Synchronize changes between local and remote
    pub fn sync(
        local_changes: HashMap<String, String>,
        remote_changes: HashMap<String, String>,
    ) -> SyncResult {
        let start = Instant::now();
        info!("Starting sync: {} local changes, {} remote changes",
              local_changes.len(), remote_changes.len());
        
        // Collect all keys
        let mut all_keys = HashSet::new();
        for key in local_changes.keys() {
            all_keys.insert(key.clone());
        }
        for key in remote_changes.keys() {
            all_keys.insert(key.clone());
        }
        
        let mut local_applied = 0;
        let mut remote_applied = 0;
        let mut conflicts = Vec::new();
        
        // Process each key
        for key in all_keys {
            let local_value = local_changes.get(&key);
            let remote_value = remote_changes.get(&key);
            
            match (local_value, remote_value) {
                // Both have changes
                (Some(local), Some(remote)) => {
                    if local == remote {
                        // Same changes, no conflict
                        debug!("Key '{}': Same changes in local and remote", key);
                    } else {
                        // Conflict
                        debug!("Key '{}': Conflict between local and remote", key);
                        
                        // Create conflict
                        let conflict = SyncConflict {
                            key: key.clone(),
                            local_operation: SyncOperation {
                                operation_type: SyncOperationType::Update,
                                key: key.clone(),
                                value: Some(local.clone()),
                                timestamp: Utc::now(),
                                device_id: "local".to_string(),
                                operation_id: generate_operation_id(),
                            },
                            remote_operation: SyncOperation {
                                operation_type: SyncOperationType::Update,
                                key: key.clone(),
                                value: Some(remote.clone()),
                                timestamp: Utc::now(),
                                device_id: "remote".to_string(),
                                operation_id: generate_operation_id(),
                            },
                            resolution: SyncResolutionStrategy::UseRemote,
                            resolved_value: Some(remote.clone()),
                        };
                        
                        conflicts.push(conflict);
                        remote_applied += 1;
                    }
                }
                // Only local has changes
                (Some(local), None) => {
                    debug!("Key '{}': Only local changes", key);
                    local_applied += 1;
                }
                // Only remote has changes
                (None, Some(remote)) => {
                    debug!("Key '{}': Only remote changes", key);
                    remote_applied += 1;
                }
                // Neither has changes (shouldn't happen)
                (None, None) => {
                    warn!("Key '{}': Neither local nor remote has changes", key);
                }
            }
        }
        
        let elapsed = start.elapsed();
        info!("Sync completed in {:?}: {} local applied, {} remote applied, {} conflicts",
              elapsed, local_applied, remote_applied, conflicts.len());
        
        SyncResult {
            success: true,
            local_applied,
            remote_applied,
            conflicts,
            error: None,
        }
    }
    
    /// Get current sync status
    pub fn get_status(&self) -> SyncStatus {
        self.status.lock().unwrap().clone()
    }
    
    /// Get sync configuration
    pub fn get_config(&self) -> SyncConfig {
        self.config.lock().unwrap().clone()
    }
    
    /// Update sync configuration
    pub fn update_config(&self, config: SyncConfig) {
        *self.config.lock().unwrap() = config;
    }
    
    /// Add a pending operation
    pub fn add_operation(&self, operation: SyncOperation) {
        let mut operations = self.pending_operations.lock().unwrap();
        operations.push(operation);
        
        // Update status
        let mut status = self.status.lock().unwrap();
        status.local_changes = operations.len();
    }
    
    /// Get all pending operations
    pub fn get_pending_operations(&self) -> Vec<SyncOperation> {
        self.pending_operations.lock().unwrap().clone()
    }
    
    /// Get all conflicts
    pub fn get_conflicts(&self) -> Vec<SyncConflict> {
        self.resolved_conflicts.lock().unwrap().values().cloned().collect()
    }
    
    /// Resolve a conflict
    pub fn resolve_conflict(&self, key: &str, resolution: SyncResolutionStrategy, value: Option<String>) -> Result<(), String> {
        let mut conflicts = self.resolved_conflicts.lock().unwrap();
        
        if let Some(conflict) = conflicts.get_mut(key) {
            conflict.resolution = resolution;
            conflict.resolved_value = value;
            Ok(())
        } else {
            Err(format!("Conflict for key '{}' not found", key))
        }
    }
    
    /// Manual sync
    pub fn manual_sync(&self) -> Result<SyncResult, String> {
        Self::perform_sync(
            &self.config,
            &self.status,
            &self.pending_operations,
            &self.resolved_conflicts,
        )
    }
}

/// Generate a unique device ID
fn generate_device_id() -> String {
    use uuid::Uuid;
    Uuid::new_v4().to_string()
}

/// Generate a unique operation ID
fn generate_operation_id() -> String {
    use uuid::Uuid;
    Uuid::new_v4().to_string()
}

/// Generate mock remote changes for testing
fn generate_mock_remote_changes(local_changes: &HashMap<String, String>) -> HashMap<String, String> {
    let mut remote_changes = HashMap::new();
    
    // Copy some local changes to simulate same changes
    for (key, value) in local_changes.iter() {
        if rand::random::<f32>() < 0.3 {
            remote_changes.insert(key.clone(), value.clone());
        }
    }
    
    // Add some conflicting changes
    for (key, _) in local_changes.iter() {
        if rand::random::<f32>() < 0.1 && !remote_changes.contains_key(key) {
            remote_changes.insert(key.clone(), format!("remote_{}", key));
        }
    }
    
    // Add some remote-only changes
    for i in 0..5 {
        let key = format!("remote_key_{}", i);
        if !local_changes.contains_key(&key) {
            remote_changes.insert(key, format!("remote_value_{}", i));
        }
    }
    
    remote_changes
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_sync_with_no_conflicts() {
        // Create test data
        let mut local_changes = HashMap::new();
        local_changes.insert("key1".to_string(), "value1".to_string());
        local_changes.insert("key2".to_string(), "value2".to_string());
        
        let mut remote_changes = HashMap::new();
        remote_changes.insert("key3".to_string(), "value3".to_string());
        remote_changes.insert("key4".to_string(), "value4".to_string());
        
        // Perform sync
        let result = SyncManager::sync(local_changes, remote_changes);
        
        // Verify result
        assert!(result.success);
        assert_eq!(result.local_applied, 2);
        assert_eq!(result.remote_applied, 2);
        assert_eq!(result.conflicts.len(), 0);
    }
    
    #[test]
    fn test_sync_with_conflicts() {
        // Create test data
        let mut local_changes = HashMap::new();
        local_changes.insert("key1".to_string(), "local_value1".to_string());
        local_changes.insert("key2".to_string(), "value2".to_string());
        
        let mut remote_changes = HashMap::new();
        remote_changes.insert("key1".to_string(), "remote_value1".to_string());
        remote_changes.insert("key3".to_string(), "value3".to_string());
        
        // Perform sync
        let result = SyncManager::sync(local_changes, remote_changes);
        
        // Verify result
        assert!(result.success);
        assert_eq!(result.local_applied, 1);
        assert_eq!(result.remote_applied, 2);
        assert_eq!(result.conflicts.len(), 1);
        
        // Verify conflict
        let conflict = &result.conflicts[0];
        assert_eq!(conflict.key, "key1");
        assert_eq!(conflict.local_operation.value, Some("local_value1".to_string()));
        assert_eq!(conflict.remote_operation.value, Some("remote_value1".to_string()));
        assert_eq!(conflict.resolution, SyncResolutionStrategy::UseRemote);
        assert_eq!(conflict.resolved_value, Some("remote_value1".to_string()));
    }
    
    #[test]
    fn test_sync_with_same_changes() {
        // Create test data
        let mut local_changes = HashMap::new();
        local_changes.insert("key1".to_string(), "value1".to_string());
        local_changes.insert("key2".to_string(), "value2".to_string());
        
        let mut remote_changes = HashMap::new();
        remote_changes.insert("key1".to_string(), "value1".to_string()); // Same as local
        remote_changes.insert("key3".to_string(), "value3".to_string());
        
        // Perform sync
        let result = SyncManager::sync(local_changes, remote_changes);
        
        // Verify result
        assert!(result.success);
        assert_eq!(result.local_applied, 1); // key2
        assert_eq!(result.remote_applied, 1); // key3
        assert_eq!(result.conflicts.len(), 0);
    }
}
</file>

<file path="src/optimization/cache/mod.rs">
use std::collections::HashMap;
use std::hash::{Hash, Hasher};
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, Instant};
use lru::LruCache;
use log::{debug, info, trace, warn};
use serde::{Serialize, Deserialize};
use tokio::time::{interval, sleep};
use std::future::Future;
use std::pin::Pin;
use std::task::{Context, Poll};

/// Cache configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CacheConfig {
    /// Maximum number of entries in the cache
    pub max_entries: usize,
    /// Time-to-live for cache entries in seconds
    pub ttl_seconds: u64,
    /// Whether to persist the cache to disk
    pub persist: bool,
    /// Path to the cache file
    pub cache_file: Option<String>,
    /// Whether the cache is enabled
    pub enabled: bool,
    /// Interval in seconds for cache cleanup
    pub cleanup_interval_secs: u64,
}

impl Default for CacheConfig {
    fn default() -> Self {
        Self {
            max_entries: 1000,
            ttl_seconds: 3600, // 1 hour
            persist: true,
            cache_file: None,
            enabled: true,
            cleanup_interval_secs: 300, // 5 minutes
        }
    }
}

/// Cache entry with metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
struct CacheEntry<T> {
    /// Cached value
    value: T,
    /// When the entry was created
    created_at: u64,
    /// When the entry expires
    expires_at: u64,
    /// Number of hits
    hits: u64,
}

/// Cache statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CacheStats {
    /// Total number of cache hits
    pub hits: u64,
    /// Total number of cache misses
    pub misses: u64,
    /// Total number of cache evictions
    pub evictions: u64,
    /// Total number of cache invalidations
    pub invalidations: u64,
    /// Current number of entries in the cache
    pub size: usize,
    /// Maximum capacity of the cache
    pub capacity: usize,
    /// Hit ratio (hits / (hits + misses))
    pub hit_ratio: f64,
}

impl Default for CacheStats {
    fn default() -> Self {
        Self {
            hits: 0,
            misses: 0,
            evictions: 0,
            invalidations: 0,
            size: 0,
            capacity: 0,
            hit_ratio: 0.0,
        }
    }
}

/// Cache with TTL and statistics
pub struct Cache<K, V>
where
    K: Hash + Eq + Clone + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    cache: Arc<RwLock<LruCache<K, CacheEntry<V>>>>,
    config: Arc<Mutex<CacheConfig>>,
    stats: Arc<Mutex<CacheStats>>,
    running: Arc<Mutex<bool>>,
}

impl<K, V> Cache<K, V>
where
    K: Hash + Eq + Clone + Send + Sync + 'static,
    V: Clone + Send + Sync + Serialize + for<'de> Deserialize<'de> + 'static,
{
    /// Create a new cache with the specified configuration
    pub fn new(config: CacheConfig) -> Self {
        let cache = Arc::new(RwLock::new(LruCache::new(config.max_entries)));
        let stats = Arc::new(Mutex::new(CacheStats {
            capacity: config.max_entries,
            ..Default::default()
        }));
        
        let instance = Self {
            cache,
            config: Arc::new(Mutex::new(config.clone())),
            stats,
            running: Arc::new(Mutex::new(false)),
        };
        
        // Load from disk if persistence is enabled
        if config.persist && config.cache_file.is_some() {
            instance.load_from_disk();
        }
        
        instance
    }
    
    /// Start the cache cleanup background task
    pub fn start_cleanup(&self) {
        let mut running = self.running.lock().unwrap();
        if *running {
            return;
        }
        *running = true;
        
        let cache = self.cache.clone();
        let config = self.config.clone();
        let stats = self.stats.clone();
        let running = self.running.clone();
        
        tokio::spawn(async move {
            let cleanup_interval = {
                let config = config.lock().unwrap();
                Duration::from_secs(config.cleanup_interval_secs)
            };
            
            let mut interval = interval(cleanup_interval);
            
            while *running.lock().unwrap() {
                interval.tick().await;
                
                // Skip if cache is disabled
                if !config.lock().unwrap().enabled {
                    continue;
                }
                
                // Perform cleanup
                Self::cleanup_expired_entries(&cache, &stats);
                
                // Save to disk if persistence is enabled
                let should_persist = {
                    let config = config.lock().unwrap();
                    config.persist && config.cache_file.is_some()
                };
                
                if should_persist {
                    Self::save_to_disk(&cache, &config);
                }
            }
        });
    }
    
    /// Stop the cache cleanup background task
    pub fn stop_cleanup(&self) {
        let mut running = self.running.lock().unwrap();
        *running = false;
    }
    
    /// Get a value from the cache
    pub fn get(&self, key: &K) -> Option<V> {
        if !self.config.lock().unwrap().enabled {
            let mut stats = self.stats.lock().unwrap();
            stats.misses += 1;
            Self::update_hit_ratio(&mut stats);
            return None;
        }
        
        let now = Self::now();
        let mut cache = self.cache.write().unwrap();
        
        if let Some(entry) = cache.get_mut(key) {
            if entry.expires_at > now {
                // Cache hit
                entry.hits += 1;
                
                let mut stats = self.stats.lock().unwrap();
                stats.hits += 1;
                Self::update_hit_ratio(&mut stats);
                
                return Some(entry.value.clone());
            } else {
                // Entry expired, remove it
                cache.pop(key);
                
                let mut stats = self.stats.lock().unwrap();
                stats.evictions += 1;
                stats.misses += 1;
                stats.size = cache.len();
                Self::update_hit_ratio(&mut stats);
            }
        } else {
            // Cache miss
            let mut stats = self.stats.lock().unwrap();
            stats.misses += 1;
            Self::update_hit_ratio(&mut stats);
        }
        
        None
    }
    
    /// Put a value in the cache
    pub fn put(&self, key: K, value: V) {
        if !self.config.lock().unwrap().enabled {
            return;
        }
        
        let now = Self::now();
        let config = self.config.lock().unwrap();
        let ttl = config.ttl_seconds;
        
        let entry = CacheEntry {
            value,
            created_at: now,
            expires_at: now + ttl,
            hits: 0,
        };
        
        let mut cache = self.cache.write().unwrap();
        let old_len = cache.len();
        cache.put(key, entry);
        
        if cache.len() <= old_len && old_len > 0 {
            // An entry was evicted
            let mut stats = self.stats.lock().unwrap();
            stats.evictions += 1;
        }
        
        // Update stats
        let mut stats = self.stats.lock().unwrap();
        stats.size = cache.len();
    }
    
    /// Put a value in the cache with a custom TTL
    pub fn put_with_ttl(&self, key: K, value: V, ttl_seconds: u64) {
        if !self.config.lock().unwrap().enabled {
            return;
        }
        
        let now = Self::now();
        
        let entry = CacheEntry {
            value,
            created_at: now,
            expires_at: now + ttl_seconds,
            hits: 0,
        };
        
        let mut cache = self.cache.write().unwrap();
        let old_len = cache.len();
        cache.put(key, entry);
        
        if cache.len() <= old_len && old_len > 0 {
            // An entry was evicted
            let mut stats = self.stats.lock().unwrap();
            stats.evictions += 1;
        }
        
        // Update stats
        let mut stats = self.stats.lock().unwrap();
        stats.size = cache.len();
    }
    
    /// Remove a value from the cache
    pub fn remove(&self, key: &K) -> Option<V> {
        let mut cache = self.cache.write().unwrap();
        let entry = cache.pop(key);
        
        if entry.is_some() {
            let mut stats = self.stats.lock().unwrap();
            stats.invalidations += 1;
            stats.size = cache.len();
        }
        
        entry.map(|e| e.value)
    }
    
    /// Clear the cache
    pub fn clear(&self) {
        let mut cache = self.cache.write().unwrap();
        let old_size = cache.len();
        cache.clear();
        
        let mut stats = self.stats.lock().unwrap();
        stats.invalidations += old_size as u64;
        stats.size = 0;
    }
    
    /// Check if the cache contains a key
    pub fn contains(&self, key: &K) -> bool {
        if !self.config.lock().unwrap().enabled {
            return false;
        }
        
        let now = Self::now();
        let cache = self.cache.read().unwrap();
        
        if let Some(entry) = cache.peek(key) {
            entry.expires_at > now
        } else {
            false
        }
    }
    
    /// Get cache statistics
    pub fn get_stats(&self) -> CacheStats {
        self.stats.lock().unwrap().clone()
    }
    
    /// Get cache configuration
    pub fn get_config(&self) -> CacheConfig {
        self.config.lock().unwrap().clone()
    }
    
    /// Update cache configuration
    pub fn update_config(&self, config: CacheConfig) {
        let mut current_config = self.config.lock().unwrap();
        let old_max_entries = current_config.max_entries;
        
        // Update config
        *current_config = config;
        
        // Resize cache if necessary
        if current_config.max_entries != old_max_entries {
            let mut cache = self.cache.write().unwrap();
            cache.resize(current_config.max_entries);
            
            let mut stats = self.stats.lock().unwrap();
            stats.capacity = current_config.max_entries;
        }
    }
    
    /// Asynchronously compute a value if not in cache
    pub async fn get_or_compute<F, Fut>(&self, key: K, compute_fn: F) -> V
    where
        F: FnOnce() -> Fut,
        Fut: Future<Output = V>,
    {
        // Try to get from cache
        if let Some(value) = self.get(&key) {
            return value;
        }
        
        // Compute the value
        let value = compute_fn().await;
        
        // Store in cache
        self.put(key, value.clone());
        
        value
    }
    
    /// Get the current time in seconds
    fn now() -> u64 {
        std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap_or_else(|_| Duration::from_secs(0))
            .as_secs()
    }
    
    /// Cleanup expired entries
    fn cleanup_expired_entries(
        cache: &Arc<RwLock<LruCache<K, CacheEntry<V>>>>,
        stats: &Arc<Mutex<CacheStats>>,
    ) {
        let now = Self::now();
        let mut expired_keys = Vec::new();
        
        // Find expired keys
        {
            let cache_read = cache.read().unwrap();
            for (key, entry) in cache_read.iter() {
                if entry.expires_at <= now {
                    expired_keys.push(key.clone());
                }
            }
        }
        
        // Remove expired keys
        if !expired_keys.is_empty() {
            let mut cache_write = cache.write().unwrap();
            for key in &expired_keys {
                cache_write.pop(key);
            }
            
            // Update stats
            let mut stats = stats.lock().unwrap();
            stats.evictions += expired_keys.len() as u64;
            stats.size = cache_write.len();
            
            debug!("Cleaned up {} expired cache entries", expired_keys.len());
        }
    }
    
    /// Save cache to disk
    fn save_to_disk(
        cache: &Arc<RwLock<LruCache<K, CacheEntry<V>>>>,
        config: &Arc<Mutex<CacheConfig>>,
    ) {
        let config = config.lock().unwrap();
        if let Some(cache_file) = &config.cache_file {
            let cache_read = cache.read().unwrap();
            
            // Create a serializable representation
            let mut entries = HashMap::new();
            for (key, entry) in cache_read.iter() {
                entries.insert(key.clone(), entry.clone());
            }
            
            // Serialize to JSON
            match serde_json::to_string(&entries) {
                Ok(json) => {
                    // Create parent directories if needed
                    if let Some(parent) = std::path::Path::new(cache_file).parent() {
                        if !parent.exists() {
                            if let Err(e) = std::fs::create_dir_all(parent) {
                                warn!("Failed to create cache directory: {}", e);
                                return;
                            }
                        }
                    }
                    
                    // Write to file
                    if let Err(e) = std::fs::write(cache_file, json) {
                        warn!("Failed to save cache to disk: {}", e);
                    } else {
                        debug!("Cache saved to disk: {}", cache_file);
                    }
                }
                Err(e) => {
                    warn!("Failed to serialize cache: {}", e);
                }
            }
        }
    }
    
    /// Load cache from disk
    fn load_from_disk(&self) {
        let config = self.config.lock().unwrap();
        if let Some(cache_file) = &config.cache_file {
            if std::path::Path::new(cache_file).exists() {
                match std::fs::read_to_string(cache_file) {
                    Ok(json) => {
                        match serde_json::from_str::<HashMap<K, CacheEntry<V>>>(&json) {
                            Ok(entries) => {
                                let now = Self::now();
                                let mut cache = self.cache.write().unwrap();
                                
                                // Reset cache
                                cache.clear();
                                
                                // Load valid entries
                                let mut loaded = 0;
                                let mut expired = 0;
                                
                                for (key, entry) in entries {
                                    if entry.expires_at > now {
                                        cache.put(key, entry);
                                        loaded += 1;
                                    } else {
                                        expired += 1;
                                    }
                                }
                                
                                // Update stats
                                let mut stats = self.stats.lock().unwrap();
                                stats.size = cache.len();
                                
                                info!("Loaded {} cache entries from disk ({} expired)", loaded, expired);
                            }
                            Err(e) => {
                                warn!("Failed to deserialize cache: {}", e);
                            }
                        }
                    }
                    Err(e) => {
                        warn!("Failed to read cache file: {}", e);
                    }
                }
            }
        }
    }
    
    /// Update hit ratio
    fn update_hit_ratio(stats: &mut CacheStats) {
        let total = stats.hits + stats.misses;
        if total > 0 {
            stats.hit_ratio = stats.hits as f64 / total as f64;
        } else {
            stats.hit_ratio = 0.0;
        }
    }
}

impl<K, V> Drop for Cache<K, V>
where
    K: Hash + Eq + Clone + Send + Sync + 'static,
    V: Clone + Send + Sync + 'static,
{
    fn drop(&mut self) {
        self.stop_cleanup();
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_cache_basic_operations() {
        let config = CacheConfig::default();
        let cache = Cache::<String, String>::new(config);
        
        // Put and get
        cache.put("key1".to_string(), "value1".to_string());
        assert_eq!(cache.get(&"key1".to_string()), Some("value1".to_string()));
        
        // Contains
        assert!(cache.contains(&"key1".to_string()));
        assert!(!cache.contains(&"key2".to_string()));
        
        // Remove
        assert_eq!(cache.remove(&"key1".to_string()), Some("value1".to_string()));
        assert!(!cache.contains(&"key1".to_string()));
        
        // Stats
        let stats = cache.get_stats();
        assert_eq!(stats.hits, 1);
        assert_eq!(stats.misses, 0);
        assert_eq!(stats.invalidations, 1);
        assert_eq!(stats.size, 0);
    }
    
    #[test]
    fn test_cache_expiration() {
        let config = CacheConfig {
            ttl_seconds: 1, // 1 second TTL
            ..Default::default()
        };
        
        let cache = Cache::<String, String>::new(config);
        
        // Put a value
        cache.put("key1".to_string(), "value1".to_string());
        assert_eq!(cache.get(&"key1".to_string()), Some("value1".to_string()));
        
        // Wait for TTL to expire
        std::thread::sleep(Duration::from_secs(2));
        
        // Value should be expired
        assert_eq!(cache.get(&"key1".to_string()), None);
        
        // Stats should show a miss
        let stats = cache.get_stats();
        assert_eq!(stats.hits, 1);
        assert_eq!(stats.misses, 1);
        assert_eq!(stats.evictions, 1);
    }
    
    #[test]
    fn test_cache_custom_ttl() {
        let config = CacheConfig {
            ttl_seconds: 10, // Default TTL
            ..Default::default()
        };
        
        let cache = Cache::<String, String>::new(config);
        
        // Put a value with custom TTL
        cache.put_with_ttl("key1".to_string(), "value1".to_string(), 1); // 1 second TTL
        assert_eq!(cache.get(&"key1".to_string()), Some("value1".to_string()));
        
        // Wait for TTL to expire
        std::thread::sleep(Duration::from_secs(2));
        
        // Value should be expired
        assert_eq!(cache.get(&"key1".to_string()), None);
    }
    
    #[test]
    fn test_cache_clear() {
        let config = CacheConfig::default();
        let cache = Cache::<String, String>::new(config);
        
        // Put multiple values
        cache.put("key1".to_string(), "value1".to_string());
        cache.put("key2".to_string(), "value2".to_string());
        cache.put("key3".to_string(), "value3".to_string());
        
        assert_eq!(cache.get_stats().size, 3);
        
        // Clear cache
        cache.clear();
        
        assert_eq!(cache.get_stats().size, 0);
        assert!(!cache.contains(&"key1".to_string()));
        assert!(!cache.contains(&"key2".to_string()));
        assert!(!cache.contains(&"key3".to_string()));
    }
    
    #[tokio::test]
    async fn test_cache_get_or_compute() {
        let config = CacheConfig::default();
        let cache = Cache::<String, String>::new(config);
        
        // Get or compute a value
        let value = cache.get_or_compute("key1".to_string(), || async {
            "computed".to_string()
        }).await;
        
        assert_eq!(value, "computed");
        
        // Second call should hit the cache
        let value = cache.get_or_compute("key1".to_string(), || async {
            "recomputed".to_string()
        }).await;
        
        assert_eq!(value, "computed"); // Still the original value
        
        // Stats should show 2 hits (1 from get_or_compute and 1 from the direct get inside)
        let stats = cache.get_stats();
        assert_eq!(stats.hits, 1);
        assert_eq!(stats.misses, 1); // Initial miss when computing
    }
}
</file>

<file path="src/optimization/memory/mod.rs">
use log::{debug, info, warn};
use std::sync::{Arc, Mutex, atomic::{AtomicUsize, Ordering}};
use std::time::{Duration, Instant};
use serde::{Serialize, Deserialize};
use tokio::time::interval;

/// Memory usage limits and thresholds
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryLimits {
    /// Maximum memory usage in MB before aggressive cleanup
    pub max_memory_mb: usize,
    /// Memory threshold in MB for regular cleanup
    pub threshold_memory_mb: usize,
    /// Maximum LLM context size in tokens
    pub max_context_tokens: usize,
    /// Enable memory tracking and optimization
    pub enabled: bool,
    /// Interval in seconds for memory usage check
    pub check_interval_secs: u64,
}

impl Default for MemoryLimits {
    fn default() -> Self {
        Self {
            max_memory_mb: 1024, // 1GB
            threshold_memory_mb: 768, // 768MB
            max_context_tokens: 8192,
            enabled: true,
            check_interval_secs: 30,
        }
    }
}

/// Memory utilization statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryStats {
    /// Current memory usage in bytes
    pub current_usage_bytes: usize,
    /// Peak memory usage in bytes
    pub peak_usage_bytes: usize,
    /// System total memory in bytes
    pub total_memory_bytes: usize,
    /// Last garbage collection time
    pub last_gc_time: Option<String>,
    /// Number of garbage collections performed
    pub gc_count: usize,
    /// Current LLM context size in tokens
    pub current_context_tokens: usize,
}

impl Default for MemoryStats {
    fn default() -> Self {
        Self {
            current_usage_bytes: 0,
            peak_usage_bytes: 0,
            total_memory_bytes: 0,
            last_gc_time: None,
            gc_count: 0,
            current_context_tokens: 0,
        }
    }
}

/// Memory manager for optimizing application memory usage
pub struct MemoryManager {
    limits: Arc<Mutex<MemoryLimits>>,
    stats: Arc<Mutex<MemoryStats>>,
    running: Arc<AtomicUsize>,
    registered_cleaners: Arc<Mutex<Vec<Box<dyn Fn() + Send + Sync>>>>,
}

impl MemoryManager {
    /// Create a new memory manager
    pub fn new() -> Self {
        let stats = Arc::new(Mutex::new(MemoryStats::default()));
        
        // Initialize system memory info
        if let Ok(sys_info) = sys_info::mem_info() {
            let mut stats = stats.lock().unwrap();
            stats.total_memory_bytes = sys_info.total as usize * 1024;
        }
        
        Self {
            limits: Arc::new(Mutex::new(MemoryLimits::default())),
            stats,
            running: Arc::new(AtomicUsize::new(0)),
            registered_cleaners: Arc::new(Mutex::new(Vec::new())),
        }
    }
    
    /// Start memory monitoring
    pub fn start(&self) {
        // Only start if not already running
        if self.running.compare_exchange(0, 1, Ordering::SeqCst, Ordering::SeqCst).is_err() {
            return;
        }
        
        let limits = self.limits.clone();
        let stats = self.stats.clone();
        let running = self.running.clone();
        let cleaners = self.registered_cleaners.clone();
        
        tokio::spawn(async move {
            let mut check_interval = interval(Duration::from_secs(
                limits.lock().unwrap().check_interval_secs
            ));
            
            while running.load(Ordering::SeqCst) == 1 {
                check_interval.tick().await;
                
                if !limits.lock().unwrap().enabled {
                    continue;
                }
                
                // Update memory stats
                Self::update_memory_stats(&stats);
                
                // Check if memory usage exceeds limits
                let should_cleanup = {
                    let limits = limits.lock().unwrap();
                    let stats = stats.lock().unwrap();
                    
                    let current_mb = stats.current_usage_bytes / (1024 * 1024);
                    current_mb >= limits.threshold_memory_mb
                };
                
                if should_cleanup {
                    let aggressive = {
                        let limits = limits.lock().unwrap();
                        let stats = stats.lock().unwrap();
                        
                        let current_mb = stats.current_usage_bytes / (1024 * 1024);
                        current_mb >= limits.max_memory_mb
                    };
                    
                    Self::perform_cleanup(&cleaners, aggressive, &stats);
                }
            }
        });
    }
    
    /// Stop memory monitoring
    pub fn stop(&self) {
        self.running.store(0, Ordering::SeqCst);
    }
    
    /// Register a cleanup function
    pub fn register_cleaner<F>(&self, cleaner: F)
    where
        F: Fn() + Send + Sync + 'static,
    {
        let mut cleaners = self.registered_cleaners.lock().unwrap();
        cleaners.push(Box::new(cleaner));
    }
    
    /// Update memory limits
    pub fn update_limits(&self, limits: MemoryLimits) {
        let mut current_limits = self.limits.lock().unwrap();
        *current_limits = limits;
    }
    
    /// Get current memory limits
    pub fn get_limits(&self) -> MemoryLimits {
        self.limits.lock().unwrap().clone()
    }
    
    /// Get current memory statistics
    pub fn get_stats(&self) -> MemoryStats {
        self.stats.lock().unwrap().clone()
    }
    
    /// Update context token count
    pub fn update_context_tokens(&self, token_count: usize) {
        let mut stats = self.stats.lock().unwrap();
        stats.current_context_tokens = token_count;
        
        // Check if token count exceeds limits
        let limits = self.limits.lock().unwrap();
        if token_count > limits.max_context_tokens {
            warn!("LLM context size ({} tokens) exceeds limit ({} tokens)",
                token_count, limits.max_context_tokens);
        }
    }
    
    /// Request immediate garbage collection
    pub fn force_gc(&self, aggressive: bool) {
        Self::perform_cleanup(&self.registered_cleaners, aggressive, &self.stats);
    }
    
    /// Update memory statistics
    fn update_memory_stats(stats: &Arc<Mutex<MemoryStats>>) {
        if let Ok(sys_mem) = sys_info::mem_info() {
            let mut stats = stats.lock().unwrap();
            
            // Calculate current memory usage
            let used_mem = sys_mem.total - sys_mem.free - sys_mem.avail;
            stats.current_usage_bytes = used_mem as usize * 1024;
            
            // Update peak memory usage
            if stats.current_usage_bytes > stats.peak_usage_bytes {
                stats.peak_usage_bytes = stats.current_usage_bytes;
            }
            
            // Update total memory if needed
            if stats.total_memory_bytes == 0 {
                stats.total_memory_bytes = sys_mem.total as usize * 1024;
            }
        }
    }
    
    /// Perform memory cleanup
    fn perform_cleanup(
        cleaners: &Arc<Mutex<Vec<Box<dyn Fn() + Send + Sync>>>>,
        aggressive: bool,
        stats: &Arc<Mutex<MemoryStats>>,
    ) {
        let now = Instant::now();
        
        // Run JavaScript garbage collection
        #[cfg(target_arch = "wasm32")]
        {
            use wasm_bindgen::prelude::*;
            
            // Call JavaScript garbage collection
            let _ = js_sys::eval("if (window.gc) { window.gc(); }");
            
            if aggressive {
                // Multiple GC calls in aggressive mode
                for _ in 0..3 {
                    let _ = js_sys::eval("if (window.gc) { window.gc(); }");
                }
            }
        }
        
        // Call registered cleanup functions
        let cleaners = cleaners.lock().unwrap();
        for cleaner in cleaners.iter() {
            cleaner();
        }
        
        // Force Rust garbage collection
        #[cfg(feature = "mimalloc")]
        unsafe {
            mimalloc_sys::mi_collect(aggressive as _);
        }
        
        // Update stats
        {
            let mut stats = stats.lock().unwrap();
            stats.gc_count += 1;
            stats.last_gc_time = Some(chrono::Local::now().to_rfc3339());
        }
        
        // Update memory stats after cleanup
        Self::update_memory_stats(stats);
        
        let elapsed = now.elapsed();
        if aggressive {
            info!("Completed aggressive memory cleanup in {:?}", elapsed);
        } else {
            debug!("Completed regular memory cleanup in {:?}", elapsed);
        }
    }
}

impl Drop for MemoryManager {
    fn drop(&mut self) {
        self.stop();
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::atomic::{AtomicBool, Ordering};
    
    #[test]
    fn test_memory_limits_default() {
        let limits = MemoryLimits::default();
        assert_eq!(limits.max_memory_mb, 1024);
        assert_eq!(limits.threshold_memory_mb, 768);
        assert_eq!(limits.max_context_tokens, 8192);
        assert!(limits.enabled);
        assert_eq!(limits.check_interval_secs, 30);
    }
    
    #[test]
    fn test_memory_stats_default() {
        let stats = MemoryStats::default();
        assert_eq!(stats.current_usage_bytes, 0);
        assert_eq!(stats.peak_usage_bytes, 0);
        assert_eq!(stats.total_memory_bytes, 0);
        assert_eq!(stats.gc_count, 0);
        assert_eq!(stats.current_context_tokens, 0);
        assert!(stats.last_gc_time.is_none());
    }
    
    #[test]
    fn test_memory_manager_cleaner_registration() {
        let manager = MemoryManager::new();
        let called = Arc::new(AtomicBool::new(false));
        
        {
            let called = called.clone();
            manager.register_cleaner(move || {
                called.store(true, Ordering::SeqCst);
            });
        }
        
        manager.force_gc(false);
        assert!(called.load(Ordering::SeqCst));
    }
    
    #[test]
    fn test_memory_manager_update_limits() {
        let manager = MemoryManager::new();
        
        let new_limits = MemoryLimits {
            max_memory_mb: 2048,
            threshold_memory_mb: 1536,
            max_context_tokens: 16384,
            enabled: false,
            check_interval_secs: 60,
        };
        
        manager.update_limits(new_limits.clone());
        let limits = manager.get_limits();
        
        assert_eq!(limits.max_memory_mb, 2048);
        assert_eq!(limits.threshold_memory_mb, 1536);
        assert_eq!(limits.max_context_tokens, 16384);
        assert!(!limits.enabled);
        assert_eq!(limits.check_interval_secs, 60);
    }
    
    #[test]
    fn test_memory_manager_context_tokens() {
        let manager = MemoryManager::new();
        
        manager.update_context_tokens(1000);
        let stats = manager.get_stats();
        
        assert_eq!(stats.current_context_tokens, 1000);
    }
}
</file>

<file path="src/optimization/mod.rs">
mod memory;
mod cache;

pub use memory::{MemoryManager, MemoryLimits, MemoryStats};
pub use cache::{Cache, CacheConfig, CacheStats};

use log::{info, debug, warn};
use std::sync::{Arc, Mutex};
use tauri::{AppHandle, Manager, Window};

/// Manager for all optimizations
pub struct OptimizationManager {
    memory_manager: Arc<MemoryManager>,
    api_cache: Arc<Cache<String, String>>,
    resource_cache: Arc<Cache<String, Vec<u8>>>,
}

impl OptimizationManager {
    /// Create a new optimization manager
    pub fn new() -> Self {
        // Initialize memory manager
        let memory_manager = Arc::new(MemoryManager::new());
        
        // Initialize API cache
        let api_cache_config = CacheConfig {
            max_entries: 1000,
            ttl_seconds: 300, // 5 minutes
            persist: true,
            cache_file: Some("api_cache.json".to_string()),
            enabled: true,
            cleanup_interval_secs: 60,
        };
        let api_cache = Arc::new(Cache::new(api_cache_config));
        
        // Initialize resource cache
        let resource_cache_config = CacheConfig {
            max_entries: 200,
            ttl_seconds: 3600, // 1 hour
            persist: true,
            cache_file: Some("resource_cache.bin".to_string()),
            enabled: true,
            cleanup_interval_secs: 300,
        };
        let resource_cache = Arc::new(Cache::new(resource_cache_config));
        
        Self {
            memory_manager,
            api_cache,
            resource_cache,
        }
    }
    
    /// Start all optimization managers
    pub fn start(&self) {
        // Start memory manager
        self.memory_manager.start();
        
        // Start cache cleanup tasks
        self.api_cache.start_cleanup();
        self.resource_cache.start_cleanup();
        
        // Register memory optimization handlers
        self.register_memory_optimizations();
        
        info!("Optimization manager started");
    }
    
    /// Stop all optimization managers
    pub fn stop(&self) {
        // Stop memory manager
        self.memory_manager.stop();
        
        // Stop cache cleanup tasks
        self.api_cache.stop_cleanup();
        self.resource_cache.stop_cleanup();
        
        info!("Optimization manager stopped");
    }
    
    /// Register memory optimizations with appropriate subsystems
    fn register_memory_optimizations(&self) {
        let memory_manager = self.memory_manager.clone();
        
        // Register cleaner for API cache
        {
            let api_cache = self.api_cache.clone();
            memory_manager.register_cleaner(move || {
                let stats_before = api_cache.get_stats();
                api_cache.clear();
                let stats_after = api_cache.get_stats();
                debug!("API cache cleared: {} entries removed", stats_before.size - stats_after.size);
            });
        }
        
        // Register cleaner for resource cache
        {
            let resource_cache = self.resource_cache.clone();
            memory_manager.register_cleaner(move || {
                let stats_before = resource_cache.get_stats();
                resource_cache.clear();
                let stats_after = resource_cache.get_stats();
                debug!("Resource cache cleared: {} entries removed", stats_before.size - stats_after.size);
            });
        }
    }
    
    /// Register window-related optimizations for Tauri
    pub fn register_window_optimizations(&self, app: &AppHandle) {
        let window = app.get_window("main").unwrap();
        let memory_manager = self.memory_manager.clone();
        
        // Register blur handler to reduce memory usage when window is not focused
        window.on_window_event(move |event| {
            match event {
                tauri::WindowEvent::Focused(focused) => {
                    if !*focused {
                        // Window lost focus, perform memory optimization
                        memory_manager.force_gc(true);
                    }
                }
                tauri::WindowEvent::CloseRequested { .. } => {
                    // Window is closing, perform memory cleanup
                    memory_manager.force_gc(true);
                }
                _ => {}
            }
        });
    }
    
    /// Get the memory manager
    pub fn memory_manager(&self) -> Arc<MemoryManager> {
        self.memory_manager.clone()
    }
    
    /// Get the API cache
    pub fn api_cache(&self) -> Arc<Cache<String, String>> {
        self.api_cache.clone()
    }
    
    /// Get the resource cache
    pub fn resource_cache(&self) -> Arc<Cache<String, Vec<u8>>> {
        self.resource_cache.clone()
    }
}

impl Drop for OptimizationManager {
    fn drop(&mut self) {
        self.stop();
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_optimization_manager_creation() {
        let manager = OptimizationManager::new();
        
        // Check memory manager limits
        let memory_limits = manager.memory_manager().get_limits();
        assert_eq!(memory_limits.max_memory_mb, 1024);
        assert_eq!(memory_limits.threshold_memory_mb, 768);
        
        // Check API cache config
        let api_cache_config = manager.api_cache().get_config();
        assert_eq!(api_cache_config.max_entries, 1000);
        assert_eq!(api_cache_config.ttl_seconds, 300);
        
        // Check resource cache config
        let resource_cache_config = manager.resource_cache().get_config();
        assert_eq!(resource_cache_config.max_entries, 200);
        assert_eq!(resource_cache_config.ttl_seconds, 3600);
    }
}
</file>

<file path="src/plugins/discovery/mod.rs">
use std::collections::HashMap;
use tokio::sync::RwLock;
use serde::{Serialize, Deserialize};
use reqwest;

use crate::plugins::types::{RepositoryPlugin, PluginInfo};

/// Plugin discovery
pub struct PluginDiscovery {
    /// Plugin repositories
    repositories: RwLock<Vec<PluginRepository>>,
    /// Cached plugins from repositories
    cached_plugins: RwLock<HashMap<String, RepositoryPlugin>>,
    /// Last cache update
    last_cache_update: RwLock<Option<chrono::DateTime<chrono::Utc>>>,
}

/// Plugin repository
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PluginRepository {
    /// Repository name
    pub name: String,
    /// Repository URL
    pub url: String,
    /// Repository type
    pub repo_type: RepositoryType,
    /// Enabled state
    pub enabled: bool,
}

/// Repository type
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub enum RepositoryType {
    /// Official repository
    Official,
    /// Community repository
    Community,
    /// Local repository
    Local,
}

impl PluginDiscovery {
    /// Create a new plugin discovery
    pub fn new() -> Self {
        Self {
            repositories: RwLock::new(Vec::new()),
            cached_plugins: RwLock::new(HashMap::new()),
            last_cache_update: RwLock::new(None),
        }
    }
    
    /// Initialize the plugin discovery
    pub async fn initialize(&self) -> Result<(), String> {
        log::info!("Initializing plugin discovery");
        
        // Set up default repositories
        let mut repositories = self.repositories.write().await;
        
        // If no repositories are configured, add defaults
        if repositories.is_empty() {
            repositories.push(PluginRepository {
                name: "Official Repository".to_string(),
                url: "https://plugins.claudemcp.io/api/plugins".to_string(),
                repo_type: RepositoryType::Official,
                enabled: true,
            });
            
            repositories.push(PluginRepository {
                name: "Community Repository".to_string(),
                url: "https://community.claudemcp.io/api/plugins".to_string(),
                repo_type: RepositoryType::Community,
                enabled: true,
            });
        }
        
        log::info!("Plugin discovery initialized with {} repositories", repositories.len());
        
        // Update plugin cache
        drop(repositories);
        match self.update_cache().await {
            Ok(_) => {
                log::info!("Plugin cache updated");
            }
            Err(e) => {
                log::warn!("Failed to update plugin cache: {}", e);
            }
        }
        
        Ok(())
    }
    
    /// Update plugin cache
    pub async fn update_cache(&self) -> Result<(), String> {
        log::info!("Updating plugin cache");
        
        let repositories = self.repositories.read().await;
        let mut cached_plugins = self.cached_plugins.write().await;
        let mut last_cache_update = self.last_cache_update.write().await;
        
        // Clear cache
        cached_plugins.clear();
        
        // Fetch plugins from each repository
        for repo in repositories.iter().filter(|r| r.enabled) {
            match self.fetch_repository_plugins(repo).await {
                Ok(plugins) => {
                    // Add to cache
                    for plugin in plugins {
                        cached_plugins.insert(plugin.id.clone(), plugin);
                    }
                }
                Err(e) => {
                    log::warn!("Failed to fetch plugins from repository {}: {}", repo.name, e);
                }
            }
        }
        
        // Update last cache update
        *last_cache_update = Some(chrono::Utc::now());
        
        log::info!("Plugin cache updated with {} plugins", cached_plugins.len());
        Ok(())
    }
    
    /// Fetch plugins from a repository
    async fn fetch_repository_plugins(&self, repo: &PluginRepository) -> Result<Vec<RepositoryPlugin>, String> {
        match repo.repo_type {
            RepositoryType::Official | RepositoryType::Community => {
                // Fetch from remote repository
                self.fetch_remote_repository_plugins(repo).await
            }
            RepositoryType::Local => {
                // Fetch from local repository
                self.fetch_local_repository_plugins(repo).await
            }
        }
    }
    
    /// Fetch plugins from a remote repository
    async fn fetch_remote_repository_plugins(&self, repo: &PluginRepository) -> Result<Vec<RepositoryPlugin>, String> {
        log::info!("Fetching plugins from remote repository: {}", repo.name);
        
        // Create HTTP client
        let client = reqwest::Client::new();
        
        // Send request
        let response = client.get(&repo.url)
            .send()
            .await
            .map_err(|e| format!("Failed to send request: {}", e))?;
            
        // Check status
        if !response.status().is_success() {
            return Err(format!("Request failed with status: {}", response.status()));
        }
        
        // Parse response
        let plugins: Vec<RepositoryPlugin> = response.json()
            .await
            .map_err(|e| format!("Failed to parse response: {}", e))?;
            
        log::info!("Fetched {} plugins from remote repository: {}", plugins.len(), repo.name);
        Ok(plugins)
    }
    
    /// Fetch plugins from a local repository
    async fn fetch_local_repository_plugins(&self, repo: &PluginRepository) -> Result<Vec<RepositoryPlugin>, String> {
        log::info!("Fetching plugins from local repository: {}", repo.name);
        
        // Parse local repository path
        let repo_path = std::path::Path::new(&repo.url);
        
        // Check if directory exists
        if !repo_path.exists() || !repo_path.is_dir() {
            return Err(format!("Local repository directory not found: {}", repo.url));
        }
        
        // Read directory entries
        let mut entries = tokio::fs::read_dir(repo_path)
            .await
            .map_err(|e| format!("Failed to read repository directory: {}", e))?;
            
        let mut plugins = Vec::new();
        
        // Process each entry
        while let Some(entry) = entries.next_entry()
            .await
            .map_err(|e| format!("Failed to read directory entry: {}", e))? {
                
            let path = entry.path();
            
            // Check if it's a directory
            if path.is_dir() {
                continue;
            }
            
            // Check file extension
            if let Some(ext) = path.extension() {
                if ext != "zip" {
                    continue;
                }
            } else {
                continue;
            }
            
            // Try to read plugin metadata
            match self.read_local_plugin_metadata(&path).await {
                Ok(plugin) => {
                    plugins.push(plugin);
                }
                Err(e) => {
                    log::warn!("Failed to read plugin metadata from {}: {}", path.display(), e);
                }
            }
        }
        
        log::info!("Fetched {} plugins from local repository: {}", plugins.len(), repo.name);
        Ok(plugins)
    }
    
    /// Read plugin metadata from a local file
    async fn read_local_plugin_metadata(&self, path: &std::path::Path) -> Result<RepositoryPlugin, String> {
        // Open ZIP file
        let file = std::fs::File::open(path)
            .map_err(|e| format!("Failed to open plugin package: {}", e))?;
            
        let mut archive = zip::ZipArchive::new(file)
            .map_err(|e| format!("Failed to read ZIP archive: {}", e))?;
            
        // Look for manifest.json
        let mut manifest_file = archive.by_name("manifest.json")
            .map_err(|e| format!("Manifest file not found in plugin package: {}", e))?;
            
        // Read manifest
        let mut manifest_content = String::new();
        std::io::Read::read_to_string(&mut manifest_file, &mut manifest_content)
            .map_err(|e| format!("Failed to read manifest file: {}", e))?;
            
        // Parse manifest
        let manifest: crate::plugins::types::PluginManifest = serde_json::from_str(&manifest_content)
            .map_err(|e| format!("Failed to parse manifest JSON: {}", e))?;
            
        // Create repository plugin
        let plugin = RepositoryPlugin {
            id: manifest.name.clone(),
            display_name: manifest.display_name.clone(),
            version: manifest.version.clone(),
            description: manifest.description.clone(),
            author: manifest.author.clone(),
            downloads: 0,
            rating: 0.0,
            repo_url: path.to_string_lossy().to_string(),
            download_url: path.to_string_lossy().to_string(),
            screenshots: Vec::new(),
            permissions: manifest.permissions.clone(),
        };
        
        Ok(plugin)
    }
    
    /// Search for plugins
    pub async fn search_plugins(&self, query: &str) -> Result<Vec<PluginInfo>, String> {
        log::info!("Searching for plugins with query: {}", query);
        
        // Check if cache is stale (older than 1 hour)
        let last_update = self.last_cache_update.read().await;
        let now = chrono::Utc::now();
        let is_stale = match *last_update {
            Some(timestamp) => {
                (now - timestamp).num_seconds() > 3600
            }
            None => true,
        };
        
        // Update cache if stale
        drop(last_update);
        if is_stale {
            match self.update_cache().await {
                Ok(_) => {
                    log::info!("Plugin cache updated");
                }
                Err(e) => {
                    log::warn!("Failed to update plugin cache: {}", e);
                }
            }
        }
        
        // Get cached plugins
        let cached_plugins = self.cached_plugins.read().await;
        
        // Filter by query
        let query = query.to_lowercase();
        let results: Vec<PluginInfo> = cached_plugins.values()
            .filter(|plugin| {
                // Match on name, display name, or description
                plugin.id.to_lowercase().contains(&query) ||
                plugin.display_name.to_lowercase().contains(&query) ||
                plugin.description.to_lowercase().contains(&query)
            })
            .map(|plugin| {
                // Convert to plugin info
                PluginInfo {
                    id: plugin.id.clone(),
                    display_name: plugin.display_name.clone(),
                    version: plugin.version.clone(),
                    description: plugin.description.clone(),
                    author: plugin.author.clone(),
                    active: false,
                    installed_at: chrono::Utc::now().to_rfc3339(),
                    updated_at: chrono::Utc::now().to_rfc3339(),
                }
            })
            .collect();
            
        log::info!("Found {} plugins matching query: {}", results.len(), query);
        Ok(results)
    }
    
    /// Add a repository
    pub async fn add_repository(&self, repo: PluginRepository) -> Result<(), String> {
        log::info!("Adding repository: {}", repo.name);
        
        // Check if repository with same name exists
        let mut repositories = self.repositories.write().await;
        if repositories.iter().any(|r| r.name == repo.name) {
            return Err(format!("Repository with name {} already exists", repo.name));
        }
        
        // Add repository
        repositories.push(repo);
        
        // Update plugin cache
        drop(repositories);
        self.update_cache().await?;
        
        Ok(())
    }
    
    /// Remove a repository
    pub async fn remove_repository(&self, name: &str) -> Result<(), String> {
        log::info!("Removing repository: {}", name);
        
        // Remove repository
        let mut repositories = self.repositories.write().await;
        let pos = repositories.iter().position(|r| r.name == name);
        
        if let Some(idx) = pos {
            repositories.remove(idx);
            
            // Update plugin cache
            drop(repositories);
            self.update_cache().await?;
            
            Ok(())
        } else {
            Err(format!("Repository {} not found", name))
        }
    }
    
    /// Enable or disable a repository
    pub async fn set_repository_enabled(&self, name: &str, enabled: bool) -> Result<(), String> {
        log::info!("Setting repository {} enabled state to {}", name, enabled);
        
        // Find repository
        let mut repositories = self.repositories.write().await;
        let pos = repositories.iter().position(|r| r.name == name);
        
        if let Some(idx) = pos {
            repositories[idx].enabled = enabled;
            
            // Update plugin cache
            drop(repositories);
            self.update_cache().await?;
            
            Ok(())
        } else {
            Err(format!("Repository {} not found", name))
        }
    }
    
    /// Get all repositories
    pub async fn get_repositories(&self) -> Vec<PluginRepository> {
        self.repositories.read().await.clone()
    }
    
    /// Get a specific plugin by ID
    pub async fn get_plugin(&self, id: &str) -> Option<RepositoryPlugin> {
        let cached_plugins = self.cached_plugins.read().await;
        cached_plugins.get(id).cloned()
    }
    
    /// Get all available plugins
    pub async fn get_all_plugins(&self) -> Vec<RepositoryPlugin> {
        let cached_plugins = self.cached_plugins.read().await;
        cached_plugins.values().cloned().collect()
    }
}

impl Default for PluginDiscovery {
    fn default() -> Self {
        Self::new()
    }
}
</file>

<file path="src/plugins/hooks/mod.rs">
use std::fmt;
use std::collections::HashMap;
use tokio::sync::RwLock;
use serde::{Serialize, Deserialize};

use crate::plugins::types::HookContext;

/// Hook registry
pub struct HookRegistry {
    /// Registered hooks by type
    hooks: RwLock<HashMap<HookType, Vec<HookRegistration>>>,
}

/// Hook registration
#[derive(Debug, Clone)]
pub struct HookRegistration {
    /// Plugin ID
    pub plugin_id: String,
    /// Instance ID
    pub instance_id: String,
    /// Priority (lower numbers run first)
    pub priority: i32,
}

/// Hook types
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum HookType {
    /// Process a message before sending to Claude
    MessagePreProcess,
    /// Process a message after receiving from Claude
    MessagePostProcess,
    /// Called when a new conversation is created
    ConversationCreate,
    /// Called when a conversation is opened
    ConversationOpen,
    /// Called when a conversation is closed
    ConversationClose,
    /// Called when the application starts
    ApplicationStart,
    /// Called when the application shuts down
    ApplicationShutdown,
    /// Custom UI rendering
    UiRender,
}

impl fmt::Display for HookType {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            HookType::MessagePreProcess => write!(f, "message:pre-process"),
            HookType::MessagePostProcess => write!(f, "message:post-process"),
            HookType::ConversationCreate => write!(f, "conversation:create"),
            HookType::ConversationOpen => write!(f, "conversation:open"),
            HookType::ConversationClose => write!(f, "conversation:close"),
            HookType::ApplicationStart => write!(f, "application:start"),
            HookType::ApplicationShutdown => write!(f, "application:shutdown"),
            HookType::UiRender => write!(f, "ui:render"),
        }
    }
}

impl HookType {
    /// Parse a hook type from a string
    pub fn from_str(s: &str) -> Option<Self> {
        match s {
            "message:pre-process" => Some(HookType::MessagePreProcess),
            "message:post-process" => Some(HookType::MessagePostProcess),
            "conversation:create" => Some(HookType::ConversationCreate),
            "conversation:open" => Some(HookType::ConversationOpen),
            "conversation:close" => Some(HookType::ConversationClose),
            "application:start" => Some(HookType::ApplicationStart),
            "application:shutdown" => Some(HookType::ApplicationShutdown),
            "ui:render" => Some(HookType::UiRender),
            _ => None,
        }
    }
}

impl HookRegistry {
    /// Create a new hook registry
    pub fn new() -> Self {
        Self {
            hooks: RwLock::new(HashMap::new()),
        }
    }
    
    /// Register a hook
    pub async fn register_hook(&self, hook_type: HookType, plugin_id: &str, 
                              instance_id: &str, priority: i32) -> Result<(), String> {
        let mut hooks = self.hooks.write().await;
        
        // Get or create hook list
        let hook_list = hooks.entry(hook_type).or_insert_with(Vec::new);
        
        // Check if already registered
        for hook in hook_list.iter() {
            if hook.plugin_id == plugin_id && hook.instance_id == instance_id {
                return Err(format!("Hook {:?} already registered for plugin {}", hook_type, plugin_id));
            }
        }
        
        // Register hook
        hook_list.push(HookRegistration {
            plugin_id: plugin_id.to_string(),
            instance_id: instance_id.to_string(),
            priority,
        });
        
        // Sort by priority
        hook_list.sort_by_key(|h| h.priority);
        
        log::info!("Registered hook {:?} for plugin {} (instance {})", hook_type, plugin_id, instance_id);
        Ok(())
    }
    
    /// Unregister a hook
    pub async fn unregister_hook(&self, hook_type: HookType, plugin_id: &str, 
                                instance_id: &str) -> Result<(), String> {
        let mut hooks = self.hooks.write().await;
        
        // Get hook list
        if let Some(hook_list) = hooks.get_mut(&hook_type) {
            // Find hook
            let pos = hook_list.iter().position(|h| {
                h.plugin_id == plugin_id && h.instance_id == instance_id
            });
            
            // Remove if found
            if let Some(idx) = pos {
                hook_list.remove(idx);
                log::info!("Unregistered hook {:?} for plugin {} (instance {})", hook_type, plugin_id, instance_id);
                return Ok(());
            }
        }
        
        Err(format!("Hook {:?} not registered for plugin {}", hook_type, plugin_id))
    }
    
    /// Unregister all hooks for a plugin instance
    pub async fn unregister_all_hooks(&self, instance_id: &str) -> Result<(), String> {
        let mut hooks = self.hooks.write().await;
        
        // Process all hook types
        for (hook_type, hook_list) in hooks.iter_mut() {
            // Remove all hooks for this instance
            hook_list.retain(|h| h.instance_id != instance_id);
        }
        
        log::info!("Unregistered all hooks for plugin instance {}", instance_id);
        Ok(())
    }
    
    /// Get hooks for a specific type
    pub async fn get_hooks(&self, hook_type: HookType) -> Vec<HookRegistration> {
        let hooks = self.hooks.read().await;
        
        if let Some(hook_list) = hooks.get(&hook_type) {
            hook_list.clone()
        } else {
            Vec::new()
        }
    }
    
    /// Execute hooks of a specific type
    pub async fn execute_hooks(&self, hook_type: HookType, 
                              context: &mut HookContext) -> Result<(), String> {
        log::debug!("Executing hooks for {:?}", hook_type);
        
        // Get plugin manager
        let plugin_manager = crate::plugins::get_plugin_manager();
        let plugin_manager = plugin_manager.read().await;
        
        // Skip if plugins are disabled
        if !plugin_manager.is_enabled() {
            return Ok(());
        }
        
        // Get sandbox manager (assumed to be available through plugin manager)
        // Note: In a real implementation, this would use the actual sandbox manager
        
        // Get hooks
        let hooks = self.get_hooks(hook_type).await;
        
        // Execute each hook
        for hook in hooks {
            // Create hook context for this plugin
            let mut plugin_context = HookContext {
                plugin_id: hook.plugin_id.clone(),
                hook_name: hook_type.to_string(),
                data: context.data.clone(),
            };
            
            // Execute hook through sandbox
            // In a real implementation, this would call the sandbox manager
            log::debug!("Executing hook {:?} for plugin {} (instance {})", 
                       hook_type, hook.plugin_id, hook.instance_id);
                       
            // Update context with results
            // In a real implementation, this would update the context with the results from the hook
        }
        
        Ok(())
    }
}

impl Default for HookRegistry {
    fn default() -> Self {
        Self::new()
    }
}
</file>

<file path="src/plugins/loader/mod.rs">
use std::path::{Path, PathBuf};
use std::io::Read;
use zip::ZipArchive;
use chrono::Utc;
use uuid::Uuid;

use crate::plugins::types::{Plugin, PluginManifest};
use crate::plugins::sandbox::SandboxManager;
use crate::plugins::permissions::PermissionManager;

/// Plugin loader
pub struct PluginLoader {
    /// Sandbox manager reference
    sandbox_manager: Option<SandboxManager>,
    /// Permission manager reference
    permission_manager: Option<PermissionManager>,
}

impl PluginLoader {
    /// Create a new plugin loader
    pub fn new() -> Self {
        Self {
            sandbox_manager: None,
            permission_manager: None,
        }
    }
    
    /// Initialize the plugin loader
    pub async fn initialize(&mut self, sandbox_manager: &SandboxManager, 
                           permission_manager: &PermissionManager) -> Result<(), String> {
        log::info!("Initializing plugin loader");
        
        // Store references
        self.sandbox_manager = Some(sandbox_manager.clone());
        self.permission_manager = Some(permission_manager.clone());
        
        log::info!("Plugin loader initialized");
        Ok(())
    }
    
    /// Load a plugin from a directory
    pub async fn load_plugin(&self, dir: &Path) -> Result<Plugin, String> {
        log::info!("Loading plugin from directory: {}", dir.display());
        
        // Check if manifest file exists
        let manifest_path = dir.join("manifest.json");
        if !manifest_path.exists() {
            return Err(format!("Manifest file not found: {}", manifest_path.display()));
        }
        
        // Read manifest file
        let manifest_content = tokio::fs::read_to_string(&manifest_path)
            .await
            .map_err(|e| format!("Failed to read manifest file: {}", e))?;
            
        // Parse manifest
        let manifest: PluginManifest = serde_json::from_str(&manifest_content)
            .map_err(|e| format!("Failed to parse manifest JSON: {}", e))?;
            
        // Check if main WASM file exists
        let wasm_path = dir.join(&manifest.main);
        if !wasm_path.exists() {
            return Err(format!("Main WASM file not found: {}", wasm_path.display()));
        }
        
        // Create plugin instance
        let plugin = Plugin {
            manifest,
            path: dir.to_path_buf(),
            active: false,
            installed_at: Utc::now(),
            updated_at: Utc::now(),
            settings: serde_json::Value::Object(serde_json::Map::new()),
            instance_id: Uuid::new_v4().to_string(),
        };
        
        log::info!("Loaded plugin: {}", plugin.manifest.name);
        Ok(plugin)
    }
    
    /// Verify a plugin package
    pub async fn verify_plugin(&self, path: &Path) -> Result<PluginManifest, String> {
        log::info!("Verifying plugin package: {}", path.display());
        
        // Check if file exists
        if !path.exists() {
            return Err(format!("Plugin package not found: {}", path.display()));
        }
        
        // Check file extension
        if let Some(ext) = path.extension() {
            if ext != "zip" {
                return Err(format!("Unsupported plugin package format: {}", ext.to_string_lossy()));
            }
        } else {
            return Err("Plugin package has no extension".to_string());
        }
        
        // Open ZIP file
        let file = std::fs::File::open(path)
            .map_err(|e| format!("Failed to open plugin package: {}", e))?;
            
        let mut archive = ZipArchive::new(file)
            .map_err(|e| format!("Failed to read ZIP archive: {}", e))?;
            
        // Look for manifest.json
        let mut manifest_file = archive.by_name("manifest.json")
            .map_err(|e| format!("Manifest file not found in plugin package: {}", e))?;
            
        // Read manifest
        let mut manifest_content = String::new();
        manifest_file.read_to_string(&mut manifest_content)
            .map_err(|e| format!("Failed to read manifest file: {}", e))?;
            
        // Parse manifest
        let manifest: PluginManifest = serde_json::from_str(&manifest_content)
            .map_err(|e| format!("Failed to parse manifest JSON: {}", e))?;
            
        // Check if main WASM file exists in package
        if !archive.by_name(&manifest.main).is_ok() {
            return Err(format!("Main WASM file not found in plugin package: {}", manifest.main));
        }
        
        // TODO: Verify signatures and checksums
        
        log::info!("Plugin package verified: {}", manifest.name);
        Ok(manifest)
    }
    
    /// Install a plugin from a package
    pub async fn install_plugin(&self, package_path: &Path, install_dir: &Path) -> Result<(), String> {
        log::info!("Installing plugin from {} to {}", package_path.display(), install_dir.display());
        
        // Check if file exists
        if !package_path.exists() {
            return Err(format!("Plugin package not found: {}", package_path.display()));
        }
        
        // Open ZIP file
        let file = std::fs::File::open(package_path)
            .map_err(|e| format!("Failed to open plugin package: {}", e))?;
            
        let mut archive = ZipArchive::new(file)
            .map_err(|e| format!("Failed to read ZIP archive: {}", e))?;
            
        // Create installation directory
        if !install_dir.exists() {
            tokio::fs::create_dir_all(install_dir)
                .await
                .map_err(|e| format!("Failed to create installation directory: {}", e))?;
        }
        
        // Extract all files
        for i in 0..archive.len() {
            let mut file = archive.by_index(i)
                .map_err(|e| format!("Failed to read file from archive: {}", e))?;
                
            let file_path = file.name();
            let output_path = install_dir.join(file_path);
            
            // Create parent directories
            if let Some(parent) = output_path.parent() {
                if !parent.exists() {
                    tokio::fs::create_dir_all(parent)
                        .await
                        .map_err(|e| format!("Failed to create directory: {}", e))?;
                }
            }
            
            // Extract file
            if file.is_file() {
                let mut output_file = std::fs::File::create(&output_path)
                    .map_err(|e| format!("Failed to create file: {}", e))?;
                    
                std::io::copy(&mut file, &mut output_file)
                    .map_err(|e| format!("Failed to write file: {}", e))?;
            }
        }
        
        log::info!("Plugin installed successfully to {}", install_dir.display());
        Ok(())
    }
    
    /// Update a plugin
    pub async fn update_plugin(&self, package_path: &Path, install_dir: &Path) -> Result<(), String> {
        log::info!("Updating plugin from {} to {}", package_path.display(), install_dir.display());
        
        // Remove existing files except settings
        let settings_path = install_dir.join("settings.json");
        let has_settings = settings_path.exists();
        
        // Save settings if they exist
        let settings = if has_settings {
            Some(tokio::fs::read_to_string(&settings_path)
                .await
                .map_err(|e| format!("Failed to read settings file: {}", e))?)
        } else {
            None
        };
        
        // Remove all files
        for entry in std::fs::read_dir(install_dir)
            .map_err(|e| format!("Failed to read installation directory: {}", e))? {
                
            let entry = entry
                .map_err(|e| format!("Failed to read directory entry: {}", e))?;
                
            let path = entry.path();
            if path != settings_path {
                if path.is_dir() {
                    std::fs::remove_dir_all(&path)
                        .map_err(|e| format!("Failed to remove directory: {}", e))?;
                } else {
                    std::fs::remove_file(&path)
                        .map_err(|e| format!("Failed to remove file: {}", e))?;
                }
            }
        }
        
        // Install new version
        self.install_plugin(package_path, install_dir).await?;
        
        // Restore settings
        if let Some(settings_content) = settings {
            tokio::fs::write(&settings_path, settings_content)
                .await
                .map_err(|e| format!("Failed to restore settings file: {}", e))?;
        }
        
        log::info!("Plugin updated successfully");
        Ok(())
    }
    
    /// Activate a plugin
    pub async fn activate_plugin(&self, plugin: &Plugin) -> Result<(), String> {
        log::info!("Activating plugin: {}", plugin.manifest.name);
        
        // Check if sandbox manager is initialized
        let sandbox_manager = self.sandbox_manager.as_ref()
            .ok_or_else(|| "Sandbox manager not initialized".to_string())?;
            
        // Check if permission manager is initialized
        let permission_manager = self.permission_manager.as_ref()
            .ok_or_else(|| "Permission manager not initialized".to_string())?;
            
        // Load plugin into sandbox
        let instance_id = sandbox_manager.load_plugin(plugin, permission_manager).await?;
        
        log::info!("Plugin activated: {} (instance {})", plugin.manifest.name, instance_id);
        Ok(())
    }
    
    /// Deactivate a plugin
    pub async fn deactivate_plugin(&self, plugin: &Plugin) -> Result<(), String> {
        log::info!("Deactivating plugin: {}", plugin.manifest.name);
        
        // Check if sandbox manager is initialized
        let sandbox_manager = self.sandbox_manager.as_ref()
            .ok_or_else(|| "Sandbox manager not initialized".to_string())?;
            
        // Unload plugin from sandbox
        if sandbox_manager.instance_exists(&plugin.instance_id).await {
            sandbox_manager.unload_plugin(&plugin.instance_id).await?;
        }
        
        log::info!("Plugin deactivated: {}", plugin.manifest.name);
        Ok(())
    }
}

impl Default for PluginLoader {
    fn default() -> Self {
        Self::new()
    }
}

// Needed to implement clone for the loader
impl Clone for PluginLoader {
    fn clone(&self) -> Self {
        Self {
            sandbox_manager: self.sandbox_manager.clone(),
            permission_manager: self.permission_manager.clone(),
        }
    }
}

// Implement clone for SandboxManager
impl Clone for SandboxManager {
    fn clone(&self) -> Self {
        Self::new()
    }
}

// Implement clone for PermissionManager
impl Clone for PermissionManager {
    fn clone(&self) -> Self {
        Self::new()
    }
}
</file>

<file path="src/plugins/mod.rs">
pub mod registry;
pub mod loader;
pub mod permissions;
pub mod sandbox;
pub mod discovery;
pub mod ui;
pub mod types;
pub mod hooks;

use std::sync::Arc;
use once_cell::sync::OnceCell;
use tokio::sync::RwLock;

use registry::PluginRegistry;
use loader::PluginLoader;
use permissions::PermissionManager;
use sandbox::SandboxManager;
use discovery::PluginDiscovery;

/// Global plugin manager instance
static PLUGIN_MANAGER: OnceCell<Arc<RwLock<PluginManager>>> = OnceCell::new();

/// Main plugin management system
pub struct PluginManager {
    /// Registry of all installed plugins
    registry: PluginRegistry,
    /// Loader for loading plugins
    loader: PluginLoader,
    /// Permission manager
    permission_manager: PermissionManager,
    /// Sandbox manager
    sandbox_manager: SandboxManager,
    /// Plugin discovery
    discovery: PluginDiscovery,
    /// Enabled state
    enabled: bool,
}

impl PluginManager {
    /// Create a new plugin manager
    pub fn new() -> Self {
        Self {
            registry: PluginRegistry::new(),
            loader: PluginLoader::new(),
            permission_manager: PermissionManager::new(),
            sandbox_manager: SandboxManager::new(),
            discovery: PluginDiscovery::new(),
            enabled: true,
        }
    }
    
    /// Initialize the plugin manager
    pub async fn initialize(&mut self) -> Result<(), String> {
        // Skip initialization if plugins are disabled
        if !crate::feature_flags::FeatureManager::default().is_enabled(crate::feature_flags::FeatureFlags::PLUGINS) {
            log::info!("Plugins are disabled in feature flags, skipping initialization");
            self.enabled = false;
            return Ok(());
        }
        
        log::info!("Initializing plugin manager");
        
        // Initialize components
        self.permission_manager.initialize().await?;
        self.sandbox_manager.initialize().await?;
        self.loader.initialize(&self.sandbox_manager, &self.permission_manager).await?;
        self.registry.initialize().await?;
        self.discovery.initialize().await?;
        
        // Load installed plugins
        self.load_installed_plugins().await?;
        
        log::info!("Plugin manager initialized successfully");
        Ok(())
    }
    
    /// Load all installed plugins
    async fn load_installed_plugins(&mut self) -> Result<(), String> {
        log::info!("Loading installed plugins");
        
        // Get plugin directories
        let plugin_dirs = self.registry.get_plugin_directories().await?;
        
        // Load each plugin
        for dir in plugin_dirs {
            match self.loader.load_plugin(&dir).await {
                Ok(plugin) => {
                    log::info!("Loaded plugin: {}", plugin.manifest.name);
                    self.registry.register_plugin(plugin).await?;
                }
                Err(e) => {
                    log::error!("Failed to load plugin from directory {}: {}", dir.display(), e);
                }
            }
        }
        
        log::info!("Loaded {} plugins", self.registry.get_plugin_count().await);
        Ok(())
    }
    
    /// Get a list of all installed plugins
    pub async fn get_installed_plugins(&self) -> Vec<types::PluginInfo> {
        self.registry.get_all_plugins().await
    }
    
    /// Install a plugin from a given path
    pub async fn install_plugin(&mut self, path: &std::path::Path) -> Result<types::PluginInfo, String> {
        log::info!("Installing plugin from: {}", path.display());
        
        // Verify the plugin
        let manifest = self.loader.verify_plugin(path).await?;
        
        // Check permissions
        let permissions = manifest.permissions.clone();
        if !self.permission_manager.check_initial_permissions(&permissions).await? {
            return Err(format!("Plugin requires permissions that are not allowed for initial installation"));
        }
        
        // Install the plugin
        let install_dir = self.registry.prepare_plugin_directory(&manifest.name).await?;
        self.loader.install_plugin(path, &install_dir).await?;
        
        // Load the plugin
        let plugin = self.loader.load_plugin(&install_dir).await?;
        
        // Register the plugin
        let plugin_info = self.registry.register_plugin(plugin).await?;
        
        log::info!("Plugin installed successfully: {}", manifest.name);
        Ok(plugin_info)
    }
    
    /// Uninstall a plugin by ID
    pub async fn uninstall_plugin(&mut self, plugin_id: &str) -> Result<(), String> {
        log::info!("Uninstalling plugin: {}", plugin_id);
        
        // Deactivate the plugin first
        self.deactivate_plugin(plugin_id).await?;
        
        // Uninstall from registry
        self.registry.uninstall_plugin(plugin_id).await?;
        
        log::info!("Plugin uninstalled successfully: {}", plugin_id);
        Ok(())
    }
    
    /// Activate a plugin by ID
    pub async fn activate_plugin(&mut self, plugin_id: &str) -> Result<(), String> {
        log::info!("Activating plugin: {}", plugin_id);
        
        // Get the plugin
        let plugin = self.registry.get_plugin(plugin_id).await?;
        
        // Activate the plugin
        self.loader.activate_plugin(&plugin).await?;
        
        // Update the registry
        self.registry.set_plugin_active(plugin_id, true).await?;
        
        log::info!("Plugin activated successfully: {}", plugin_id);
        Ok(())
    }
    
    /// Deactivate a plugin by ID
    pub async fn deactivate_plugin(&mut self, plugin_id: &str) -> Result<(), String> {
        log::info!("Deactivating plugin: {}", plugin_id);
        
        // Get the plugin
        let plugin = self.registry.get_plugin(plugin_id).await?;
        
        // Deactivate the plugin
        self.loader.deactivate_plugin(&plugin).await?;
        
        // Update the registry
        self.registry.set_plugin_active(plugin_id, false).await?;
        
        log::info!("Plugin deactivated successfully: {}", plugin_id);
        Ok(())
    }
    
    /// Update a plugin by ID
    pub async fn update_plugin(&mut self, plugin_id: &str, path: &std::path::Path) -> Result<types::PluginInfo, String> {
        log::info!("Updating plugin: {} from path: {}", plugin_id, path.display());
        
        // Deactivate the plugin first
        self.deactivate_plugin(plugin_id).await?;
        
        // Verify the plugin
        let manifest = self.loader.verify_plugin(path).await?;
        
        // Make sure the ID matches
        if manifest.name != plugin_id {
            return Err(format!("Plugin ID mismatch: expected {}, got {}", plugin_id, manifest.name));
        }
        
        // Get the plugin directory
        let plugin_dir = self.registry.get_plugin_directory(plugin_id).await?;
        
        // Update the plugin
        self.loader.update_plugin(path, &plugin_dir).await?;
        
        // Reload the plugin
        let plugin = self.loader.load_plugin(&plugin_dir).await?;
        
        // Update the registry
        let plugin_info = self.registry.update_plugin(plugin).await?;
        
        // Reactivate the plugin if it was active
        if plugin_info.active {
            self.activate_plugin(plugin_id).await?;
        }
        
        log::info!("Plugin updated successfully: {}", plugin_id);
        Ok(plugin_info)
    }
    
    /// Search for available plugins
    pub async fn search_plugins(&self, query: &str) -> Result<Vec<types::PluginInfo>, String> {
        self.discovery.search_plugins(query).await
    }
    
    /// Get details about a plugin
    pub async fn get_plugin_details(&self, plugin_id: &str) -> Result<types::PluginDetails, String> {
        self.registry.get_plugin_details(plugin_id).await
    }
    
    /// Update plugin settings
    pub async fn update_plugin_settings(&mut self, plugin_id: &str, settings: serde_json::Value) -> Result<(), String> {
        self.registry.update_plugin_settings(plugin_id, settings).await
    }
    
    /// Get plugin settings
    pub async fn get_plugin_settings(&self, plugin_id: &str) -> Result<serde_json::Value, String> {
        self.registry.get_plugin_settings(plugin_id).await
    }
    
    /// Request additional permissions for a plugin
    pub async fn request_permissions(&mut self, plugin_id: &str, permissions: &[String]) -> Result<bool, String> {
        self.permission_manager.request_permissions(plugin_id, permissions).await
    }
    
    /// Check if plugin system is enabled
    pub fn is_enabled(&self) -> bool {
        self.enabled
    }
}

impl Default for PluginManager {
    fn default() -> Self {
        Self::new()
    }
}

/// Initialize the global plugin manager
pub async fn init_plugin_manager() -> Arc<RwLock<PluginManager>> {
    let manager = Arc::new(RwLock::new(PluginManager::new()));
    
    if let Err(_) = PLUGIN_MANAGER.set(manager.clone()) {
        // Already initialized
    }
    
    // Initialize the manager
    manager.write().await.initialize().await.unwrap_or_else(|e| {
        log::error!("Failed to initialize plugin manager: {}", e);
    });
    
    manager
}

/// Get the global plugin manager
pub fn get_plugin_manager() -> Arc<RwLock<PluginManager>> {
    PLUGIN_MANAGER.get_or_init(|| {
        // Create a new manager - this should only happen if init_plugin_manager wasn't called
        log::warn!("Plugin manager not properly initialized, creating a new instance");
        Arc::new(RwLock::new(PluginManager::new()))
    }).clone()
}
</file>

<file path="src/plugins/permissions/mod.rs">
use std::collections::{HashMap, HashSet};
use serde::{Deserialize, Serialize};
use tokio::sync::RwLock;
use std::sync::Arc;

/// Permission manager
pub struct PermissionManager {
    /// Permissions of plugins
    plugin_permissions: RwLock<HashMap<String, HashSet<String>>>,
    /// Pending permission requests
    pending_requests: RwLock<HashMap<String, HashSet<String>>>,
    /// Permission settings
    settings: RwLock<PermissionSettings>,
}

/// Permission settings
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PermissionSettings {
    /// Automatically allowed permissions for all plugins
    #[serde(default)]
    pub auto_allowed: HashSet<String>,
    /// Restricted permissions that require confirmation
    #[serde(default)]
    pub restricted: HashSet<String>,
    /// Denied permissions that cannot be used
    #[serde(default)]
    pub denied: HashSet<String>,
    /// Plugin-specific permission overrides
    #[serde(default)]
    pub plugin_overrides: HashMap<String, PluginPermissionOverride>,
}

/// Plugin-specific permission override
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PluginPermissionOverride {
    /// Allowed permissions for this plugin
    #[serde(default)]
    pub allowed: HashSet<String>,
    /// Denied permissions for this plugin
    #[serde(default)]
    pub denied: HashSet<String>,
}

impl Default for PermissionSettings {
    fn default() -> Self {
        let mut auto_allowed = HashSet::new();
        let mut restricted = HashSet::new();
        let mut denied = HashSet::new();
        
        // Set up default permissions
        auto_allowed.insert("conversations:read".to_string());
        auto_allowed.insert("models:read".to_string());
        auto_allowed.insert("ui:display".to_string());
        
        restricted.insert("conversations:write".to_string());
        restricted.insert("models:use".to_string());
        restricted.insert("network:github.com".to_string());
        restricted.insert("ui:interact".to_string());
        restricted.insert("user:preferences".to_string());
        
        denied.insert("network:all".to_string());
        denied.insert("fs:read".to_string());
        denied.insert("fs:write".to_string());
        denied.insert("system:settings".to_string());
        
        Self {
            auto_allowed,
            restricted,
            denied,
            plugin_overrides: HashMap::new(),
        }
    }
}

impl PermissionManager {
    /// Create a new permission manager
    pub fn new() -> Self {
        Self {
            plugin_permissions: RwLock::new(HashMap::new()),
            pending_requests: RwLock::new(HashMap::new()),
            settings: RwLock::new(PermissionSettings::default()),
        }
    }
    
    /// Initialize the permission manager
    pub async fn initialize(&self) -> Result<(), String> {
        // Load settings from disk
        match self.load_settings().await {
            Ok(settings) => {
                // Update settings
                *self.settings.write().await = settings;
                log::info!("Loaded permission settings");
            },
            Err(e) => {
                log::warn!("Failed to load permission settings, using defaults: {}", e);
            }
        }
        
        log::info!("Permission manager initialized");
        Ok(())
    }
    
    /// Load settings from disk
    async fn load_settings(&self) -> Result<PermissionSettings, String> {
        // Get settings file path
        let config_dir = self.get_config_dir()?;
        let settings_path = config_dir.join("permission_settings.json");
        
        // Load settings if file exists
        if settings_path.exists() {
            let content = tokio::fs::read_to_string(&settings_path)
                .await
                .map_err(|e| format!("Failed to read settings file: {}", e))?;
                
            // Parse JSON
            let settings: PermissionSettings = serde_json::from_str(&content)
                .map_err(|e| format!("Failed to parse settings JSON: {}", e))?;
                
            Ok(settings)
        } else {
            // Use defaults if file doesn't exist
            Ok(PermissionSettings::default())
        }
    }
    
    /// Save settings to disk
    async fn save_settings(&self) -> Result<(), String> {
        // Get settings
        let settings = self.settings.read().await;
        
        // Get settings file path
        let config_dir = self.get_config_dir()?;
        
        // Create directory if it doesn't exist
        if !config_dir.exists() {
            tokio::fs::create_dir_all(&config_dir)
                .await
                .map_err(|e| format!("Failed to create config directory: {}", e))?;
        }
        
        let settings_path = config_dir.join("permission_settings.json");
        
        // Serialize and save
        let content = serde_json::to_string_pretty(&*settings)
            .map_err(|e| format!("Failed to serialize settings: {}", e))?;
            
        tokio::fs::write(&settings_path, content)
            .await
            .map_err(|e| format!("Failed to write settings file: {}", e))?;
            
        Ok(())
    }
    
    /// Get config directory
    fn get_config_dir(&self) -> Result<std::path::PathBuf, String> {
        let home_dir = dirs::home_dir()
            .ok_or_else(|| "Could not determine home directory".to_string())?;
            
        #[cfg(target_os = "windows")]
        let config_dir = home_dir.join("AppData").join("Roaming").join("mcp").join("plugins");
        
        #[cfg(target_os = "macos")]
        let config_dir = home_dir.join("Library").join("Application Support").join("mcp").join("plugins");
        
        #[cfg(target_os = "linux")]
        let config_dir = home_dir.join(".config").join("mcp").join("plugins");
        
        #[cfg(not(any(target_os = "windows", target_os = "macos", target_os = "linux")))]
        let config_dir = home_dir.join(".mcp").join("plugins");
        
        Ok(config_dir)
    }
    
    /// Check if initial permissions for a plugin are allowed
    pub async fn check_initial_permissions(&self, permissions: &[String]) -> Result<bool, String> {
        let settings = self.settings.read().await;
        
        // Check if any permissions are denied
        for permission in permissions {
            if settings.denied.contains(permission) {
                log::warn!("Plugin requested denied permission: {}", permission);
                return Ok(false);
            }
        }
        
        // All permissions must be auto-allowed for initial installation without prompt
        for permission in permissions {
            if !settings.auto_allowed.contains(permission) {
                log::info!("Plugin requires permission that's not auto-allowed: {}", permission);
                return Ok(false);
            }
        }
        
        Ok(true)
    }
    
    /// Grant permissions to a plugin
    pub async fn grant_permissions(&self, plugin_id: &str, permissions: &[String]) -> Result<(), String> {
        // Get current plugin permissions
        let mut all_permissions = self.plugin_permissions.write().await;
        
        // Get or create permissions for this plugin
        let plugin_perms = all_permissions.entry(plugin_id.to_string())
            .or_insert_with(HashSet::new);
            
        // Add all permissions
        for permission in permissions {
            plugin_perms.insert(permission.clone());
        }
        
        // Check if there were pending requests for these permissions
        let mut pending = self.pending_requests.write().await;
        if let Some(pending_perms) = pending.get_mut(plugin_id) {
            // Remove granted permissions from pending
            for permission in permissions {
                pending_perms.remove(permission);
            }
            
            // Remove entry if empty
            if pending_perms.is_empty() {
                pending.remove(plugin_id);
            }
        }
        
        // Update plugin overrides in settings
        let mut settings = self.settings.write().await;
        let override_entry = settings.plugin_overrides
            .entry(plugin_id.to_string())
            .or_insert_with(PluginPermissionOverride::default);
            
        // Add granted permissions to allowed list
        for permission in permissions {
            override_entry.allowed.insert(permission.clone());
            // Remove from denied if present
            override_entry.denied.remove(permission);
        }
        
        // Save settings
        drop(settings);
        self.save_settings().await?;
        
        log::info!("Granted permissions to plugin {}: {:?}", plugin_id, permissions);
        Ok(())
    }
    
    /// Revoke permissions from a plugin
    pub async fn revoke_permissions(&self, plugin_id: &str, permissions: &[String]) -> Result<(), String> {
        // Get current plugin permissions
        let mut all_permissions = self.plugin_permissions.write().await;
        
        // Remove permissions if the plugin exists
        if let Some(plugin_perms) = all_permissions.get_mut(plugin_id) {
            for permission in permissions {
                plugin_perms.remove(permission);
            }
        }
        
        // Update plugin overrides in settings
        let mut settings = self.settings.write().await;
        if let Some(override_entry) = settings.plugin_overrides.get_mut(plugin_id) {
            // Remove permissions from allowed list
            for permission in permissions {
                override_entry.allowed.remove(permission);
                // Add to denied list
                override_entry.denied.insert(permission.clone());
            }
        } else {
            // Create new override entry
            let mut override_entry = PluginPermissionOverride::default();
            for permission in permissions {
                override_entry.denied.insert(permission.clone());
            }
            settings.plugin_overrides.insert(plugin_id.to_string(), override_entry);
        }
        
        // Save settings
        drop(settings);
        self.save_settings().await?;
        
        log::info!("Revoked permissions from plugin {}: {:?}", plugin_id, permissions);
        Ok(())
    }
    
    /// Check if a plugin has a specific permission
    pub async fn has_permission(&self, plugin_id: &str, permission: &str) -> bool {
        // First check if plugin has this permission explicitly
        let all_permissions = self.plugin_permissions.read().await;
        if let Some(plugin_perms) = all_permissions.get(plugin_id) {
            if plugin_perms.contains(permission) {
                return true;
            }
        }
        
        // Then check if it's an auto-allowed permission
        let settings = self.settings.read().await;
        if settings.auto_allowed.contains(permission) {
            return true;
        }
        
        // Check plugin overrides
        if let Some(override_entry) = settings.plugin_overrides.get(plugin_id) {
            // If explicitly allowed in override
            if override_entry.allowed.contains(permission) {
                return true;
            }
            
            // If explicitly denied in override
            if override_entry.denied.contains(permission) {
                return false;
            }
        }
        
        // If we get here, the plugin doesn't have the permission
        false
    }
    
    /// Request additional permissions for a plugin
    pub async fn request_permissions(&self, plugin_id: &str, permissions: &[String]) -> Result<bool, String> {
        log::info!("Plugin {} requesting permissions: {:?}", plugin_id, permissions);
        
        // Check if any permissions are denied at the system level
        let settings = self.settings.read().await;
        for permission in permissions {
            if settings.denied.contains(permission) {
                log::warn!("Plugin requested denied permission: {}", permission);
                return Ok(false);
            }
        }
        
        // All auto-allowed permissions are granted immediately
        let mut to_grant = Vec::new();
        let mut to_request = Vec::new();
        
        for permission in permissions {
            if settings.auto_allowed.contains(permission) {
                to_grant.push(permission.clone());
            } else {
                to_request.push(permission.clone());
            }
        }
        
        // Grant auto-allowed permissions
        if !to_grant.is_empty() {
            drop(settings); // Release lock before calling grant_permissions
            self.grant_permissions(plugin_id, &to_grant).await?;
        } else {
            drop(settings); // Release lock
        }
        
        // If there are permissions that need user approval
        if !to_request.is_empty() {
            // Add to pending requests
            let mut pending = self.pending_requests.write().await;
            let plugin_pending = pending.entry(plugin_id.to_string())
                .or_insert_with(HashSet::new);
                
            for permission in &to_request {
                plugin_pending.insert(permission.clone());
            }
            
            // TODO: Trigger UI to request user approval
            log::info!("Added pending permission request for plugin {}: {:?}", plugin_id, to_request);
            
            // For now, return false since we need user approval
            return Ok(false);
        }
        
        // All permissions were auto-granted
        Ok(true)
    }
    
    /// Get pending permission requests
    pub async fn get_pending_requests(&self) -> HashMap<String, Vec<String>> {
        let pending = self.pending_requests.read().await;
        
        // Convert to HashMap<String, Vec<String>> for easier serialization
        let mut result = HashMap::new();
        for (plugin_id, permissions) in pending.iter() {
            result.insert(plugin_id.clone(), permissions.iter().cloned().collect());
        }
        
        result
    }
    
    /// Respond to a permission request
    pub async fn respond_to_request(&self, plugin_id: &str, permissions: &[String], approved: bool) -> Result<(), String> {
        if approved {
            // Grant the permissions
            self.grant_permissions(plugin_id, permissions).await?;
        } else {
            // Remove from pending requests
            let mut pending = self.pending_requests.write().await;
            if let Some(plugin_pending) = pending.get_mut(plugin_id) {
                for permission in permissions {
                    plugin_pending.remove(permission);
                }
                
                // Remove entry if empty
                if plugin_pending.is_empty() {
                    pending.remove(plugin_id);
                }
            }
            
            // Add to denied permissions for this plugin
            self.revoke_permissions(plugin_id, permissions).await?;
        }
        
        Ok(())
    }
    
    /// Get all permissions for a plugin
    pub async fn get_plugin_permissions(&self, plugin_id: &str) -> HashSet<String> {
        let all_permissions = self.plugin_permissions.read().await;
        if let Some(plugin_perms) = all_permissions.get(plugin_id) {
            plugin_perms.clone()
        } else {
            HashSet::new()
        }
    }
    
    /// Update permission settings
    pub async fn update_settings(&self, settings: PermissionSettings) -> Result<(), String> {
        // Update settings
        *self.settings.write().await = settings;
        
        // Save to disk
        self.save_settings().await?;
        
        log::info!("Updated permission settings");
        Ok(())
    }
    
    /// Get current permission settings
    pub async fn get_settings(&self) -> PermissionSettings {
        self.settings.read().await.clone()
    }
    
    /// Remove a plugin's permissions
    pub async fn remove_plugin(&self, plugin_id: &str) -> Result<(), String> {
        // Remove from plugin permissions
        let mut all_permissions = self.plugin_permissions.write().await;
        all_permissions.remove(plugin_id);
        
        // Remove from pending requests
        let mut pending = self.pending_requests.write().await;
        pending.remove(plugin_id);
        
        // Remove from plugin overrides in settings
        let mut settings = self.settings.write().await;
        settings.plugin_overrides.remove(plugin_id);
        
        // Save settings
        drop(settings);
        self.save_settings().await?;
        
        log::info!("Removed permissions for plugin {}", plugin_id);
        Ok(())
    }
    
    /// Initialize permissions for a new plugin
    pub async fn initialize_plugin(&self, plugin_id: &str, permissions: &[String]) -> Result<bool, String> {
        log::info!("Initializing permissions for plugin {}: {:?}", plugin_id, permissions);
        
        // Check if all permissions are auto-allowed
        let settings = self.settings.read().await;
        let mut all_auto_allowed = true;
        
        // Check if any permissions are denied
        for permission in permissions {
            if settings.denied.contains(permission) {
                log::warn!("Plugin requested denied permission: {}", permission);
                return Ok(false);
            }
            
            if !settings.auto_allowed.contains(permission) {
                all_auto_allowed = false;
            }
        }
        
        // If all permissions are auto-allowed, grant them
        if all_auto_allowed {
            drop(settings); // Release lock before calling grant_permissions
            self.grant_permissions(plugin_id, permissions).await?;
            return Ok(true);
        }
        
        // Otherwise, we need user approval for some permissions
        drop(settings);
        
        // Add to pending requests
        let mut pending = self.pending_requests.write().await;
        let plugin_pending = pending.entry(plugin_id.to_string())
            .or_insert_with(HashSet::new);
            
        for permission in permissions {
            plugin_pending.insert(permission.clone());
        }
        
        // TODO: Trigger UI to request user approval
        log::info!("Added pending permission request for new plugin {}: {:?}", plugin_id, permissions);
        
        // For now, return false since we need user approval
        Ok(false)
    }
}

impl Default for PermissionManager {
    fn default() -> Self {
        Self::new()
    }
}
</file>

<file path="src/plugins/registry/mod.rs">
use std::collections::HashMap;
use std::path::{Path, PathBuf};
use tokio::sync::RwLock;
use serde::{Deserialize, Serialize};
use crate::plugins::types::{Plugin, PluginInfo, PluginDetails};

/// Plugin registry
pub struct PluginRegistry {
    /// Installed plugins
    plugins: RwLock<HashMap<String, Plugin>>,
    /// Base directory for plugins
    plugins_dir: RwLock<PathBuf>,
}

/// Plugin registry data
#[derive(Debug, Clone, Serialize, Deserialize)]
struct RegistryData {
    /// Plugin metadata
    plugins: HashMap<String, PluginMetadata>,
}

/// Plugin metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
struct PluginMetadata {
    /// Plugin ID
    id: String,
    /// Active status
    active: bool,
    /// Installed timestamp (ISO 8601)
    installed_at: String,
    /// Last updated timestamp (ISO 8601)
    updated_at: String,
    /// Plugin settings
    settings: serde_json::Value,
}

impl PluginRegistry {
    /// Create a new plugin registry
    pub fn new() -> Self {
        Self {
            plugins: RwLock::new(HashMap::new()),
            plugins_dir: RwLock::new(PathBuf::new()),
        }
    }
    
    /// Initialize the plugin registry
    pub async fn initialize(&self) -> Result<(), String> {
        // Set up plugins directory
        self.setup_plugins_dir().await?;
        
        // Load registry data
        self.load_registry_data().await?;
        
        log::info!("Plugin registry initialized");
        Ok(())
    }
    
    /// Set up plugins directory
    async fn setup_plugins_dir(&self) -> Result<(), String> {
        // Get plugins directory
        let plugins_dir = get_plugins_dir()?;
        
        // Create directory if it doesn't exist
        if !plugins_dir.exists() {
            tokio::fs::create_dir_all(&plugins_dir)
                .await
                .map_err(|e| format!("Failed to create plugins directory: {}", e))?;
        }
        
        // Store plugins directory
        *self.plugins_dir.write().await = plugins_dir.clone();
        
        log::info!("Using plugins directory: {}", plugins_dir.display());
        Ok(())
    }
    
    /// Load registry data from disk
    async fn load_registry_data(&self) -> Result<(), String> {
        // Get registry file path
        let plugins_dir = self.plugins_dir.read().await;
        let registry_path = plugins_dir.join("registry.json");
        
        // If registry file doesn't exist, create empty registry
        if !registry_path.exists() {
            log::info!("Registry file doesn't exist, creating empty registry");
            return Ok(());
        }
        
        // Read registry file
        let content = tokio::fs::read_to_string(&registry_path)
            .await
            .map_err(|e| format!("Failed to read registry file: {}", e))?;
            
        // Parse JSON
        let registry_data: RegistryData = serde_json::from_str(&content)
            .map_err(|e| format!("Failed to parse registry JSON: {}", e))?;
            
        log::info!("Loaded registry data with {} plugins", registry_data.plugins.len());
        
        // We don't load plugins here, just initialize the registry
        // Plugins will be loaded by the plugin loader
        
        Ok(())
    }
    
    /// Save registry data to disk
    async fn save_registry_data(&self) -> Result<(), String> {
        // Get registry data
        let mut registry_data = RegistryData {
            plugins: HashMap::new(),
        };
        
        // Get plugins
        let plugins = self.plugins.read().await;
        
        // Convert to registry data
        for (id, plugin) in plugins.iter() {
            let metadata = PluginMetadata {
                id: id.clone(),
                active: plugin.active,
                installed_at: plugin.installed_at.to_rfc3339(),
                updated_at: plugin.updated_at.to_rfc3339(),
                settings: plugin.settings.clone(),
            };
            
            registry_data.plugins.insert(id.clone(), metadata);
        }
        
        // Get registry file path
        let plugins_dir = self.plugins_dir.read().await;
        let registry_path = plugins_dir.join("registry.json");
        
        // Serialize and save
        let content = serde_json::to_string_pretty(&registry_data)
            .map_err(|e| format!("Failed to serialize registry data: {}", e))?;
            
        tokio::fs::write(&registry_path, content)
            .await
            .map_err(|e| format!("Failed to write registry file: {}", e))?;
            
        log::info!("Saved registry data with {} plugins", plugins.len());
        Ok(())
    }
    
    /// Register a plugin
    pub async fn register_plugin(&self, plugin: Plugin) -> Result<PluginInfo, String> {
        let plugin_id = plugin.manifest.name.clone();
        log::info!("Registering plugin: {}", plugin_id);
        
        // Create plugin info
        let plugin_info = PluginInfo {
            id: plugin_id.clone(),
            display_name: plugin.manifest.display_name.clone(),
            version: plugin.manifest.version.clone(),
            description: plugin.manifest.description.clone(),
            author: plugin.manifest.author.clone(),
            active: plugin.active,
            installed_at: plugin.installed_at.to_rfc3339(),
            updated_at: plugin.updated_at.to_rfc3339(),
        };
        
        // Add to plugins
        let mut plugins = self.plugins.write().await;
        plugins.insert(plugin_id, plugin);
        
        // Save registry data
        drop(plugins);
        self.save_registry_data().await?;
        
        Ok(plugin_info)
    }
    
    /// Update a plugin
    pub async fn update_plugin(&self, plugin: Plugin) -> Result<PluginInfo, String> {
        let plugin_id = plugin.manifest.name.clone();
        log::info!("Updating plugin: {}", plugin_id);
        
        // Check if plugin exists
        let mut plugins = self.plugins.write().await;
        if !plugins.contains_key(&plugin_id) {
            return Err(format!("Plugin not found: {}", plugin_id));
        }
        
        // Create plugin info
        let plugin_info = PluginInfo {
            id: plugin_id.clone(),
            display_name: plugin.manifest.display_name.clone(),
            version: plugin.manifest.version.clone(),
            description: plugin.manifest.description.clone(),
            author: plugin.manifest.author.clone(),
            active: plugin.active,
            installed_at: plugin.installed_at.to_rfc3339(),
            updated_at: plugin.updated_at.to_rfc3339(),
        };
        
        // Update plugin
        plugins.insert(plugin_id, plugin);
        
        // Save registry data
        drop(plugins);
        self.save_registry_data().await?;
        
        Ok(plugin_info)
    }
    
    /// Get a plugin
    pub async fn get_plugin(&self, plugin_id: &str) -> Result<Plugin, String> {
        let plugins = self.plugins.read().await;
        
        // Get plugin
        if let Some(plugin) = plugins.get(plugin_id) {
            Ok(plugin.clone())
        } else {
            Err(format!("Plugin not found: {}", plugin_id))
        }
    }
    
    /// Get all plugins
    pub async fn get_all_plugins(&self) -> Vec<PluginInfo> {
        let plugins = self.plugins.read().await;
        
        // Convert to plugin info
        plugins.values().map(|plugin| {
            PluginInfo {
                id: plugin.manifest.name.clone(),
                display_name: plugin.manifest.display_name.clone(),
                version: plugin.manifest.version.clone(),
                description: plugin.manifest.description.clone(),
                author: plugin.manifest.author.clone(),
                active: plugin.active,
                installed_at: plugin.installed_at.to_rfc3339(),
                updated_at: plugin.updated_at.to_rfc3339(),
            }
        }).collect()
    }
    
    /// Get plugin count
    pub async fn get_plugin_count(&self) -> usize {
        self.plugins.read().await.len()
    }
    
    /// Uninstall a plugin
    pub async fn uninstall_plugin(&self, plugin_id: &str) -> Result<(), String> {
        log::info!("Uninstalling plugin: {}", plugin_id);
        
        // Check if plugin exists
        let mut plugins = self.plugins.write().await;
        if !plugins.contains_key(plugin_id) {
            return Err(format!("Plugin not found: {}", plugin_id));
        }
        
        // Get plugin directory
        let plugin_dir = self.get_plugin_directory_internal(plugin_id).await?;
        
        // Remove plugin directory
        tokio::fs::remove_dir_all(&plugin_dir)
            .await
            .map_err(|e| format!("Failed to remove plugin directory: {}", e))?;
            
        // Remove from registry
        plugins.remove(plugin_id);
        
        // Save registry data
        drop(plugins);
        self.save_registry_data().await?;
        
        Ok(())
    }
    
    /// Set plugin active state
    pub async fn set_plugin_active(&self, plugin_id: &str, active: bool) -> Result<(), String> {
        log::info!("Setting plugin {} active state to {}", plugin_id, active);
        
        // Check if plugin exists
        let mut plugins = self.plugins.write().await;
        let plugin = plugins.get_mut(plugin_id)
            .ok_or_else(|| format!("Plugin not found: {}", plugin_id))?;
            
        // Update active state
        plugin.active = active;
        
        // Save registry data
        drop(plugins);
        self.save_registry_data().await?;
        
        Ok(())
    }
    
    /// Get plugin directory
    pub async fn get_plugin_directory(&self, plugin_id: &str) -> Result<PathBuf, String> {
        self.get_plugin_directory_internal(plugin_id).await
    }
    
    /// Internal method to get plugin directory
    async fn get_plugin_directory_internal(&self, plugin_id: &str) -> Result<PathBuf, String> {
        // Get plugins directory
        let plugins_dir = self.plugins_dir.read().await;
        
        // Get plugin directory
        let plugin_dir = plugins_dir.join(plugin_id);
        
        // Check if directory exists
        if !plugin_dir.exists() {
            return Err(format!("Plugin directory not found: {}", plugin_dir.display()));
        }
        
        Ok(plugin_dir)
    }
    
    /// Get all plugin directories
    pub async fn get_plugin_directories(&self) -> Result<Vec<PathBuf>, String> {
        // Get plugins directory
        let plugins_dir = self.plugins_dir.read().await;
        
        // Read directory entries
        let mut entries = tokio::fs::read_dir(&*plugins_dir)
            .await
            .map_err(|e| format!("Failed to read plugins directory: {}", e))?;
            
        let mut result = Vec::new();
        
        // Collect all directories
        while let Some(entry) = entries.next_entry()
            .await
            .map_err(|e| format!("Failed to read directory entry: {}", e))? {
                
            let path = entry.path();
            if path.is_dir() {
                result.push(path);
            }
        }
        
        Ok(result)
    }
    
    /// Prepare a directory for a plugin
    pub async fn prepare_plugin_directory(&self, plugin_id: &str) -> Result<PathBuf, String> {
        // Get plugins directory
        let plugins_dir = self.plugins_dir.read().await;
        
        // Get plugin directory
        let plugin_dir = plugins_dir.join(plugin_id);
        
        // Create directory if it doesn't exist
        if plugin_dir.exists() {
            // Remove existing directory
            tokio::fs::remove_dir_all(&plugin_dir)
                .await
                .map_err(|e| format!("Failed to remove existing plugin directory: {}", e))?;
        }
        
        // Create directory
        tokio::fs::create_dir_all(&plugin_dir)
            .await
            .map_err(|e| format!("Failed to create plugin directory: {}", e))?;
            
        Ok(plugin_dir)
    }
    
    /// Get plugin details
    pub async fn get_plugin_details(&self, plugin_id: &str) -> Result<PluginDetails, String> {
        // Get plugin
        let plugins = self.plugins.read().await;
        let plugin = plugins.get(plugin_id)
            .ok_or_else(|| format!("Plugin not found: {}", plugin_id))?;
            
        // Create plugin details
        let details = PluginDetails {
            info: PluginInfo {
                id: plugin.manifest.name.clone(),
                display_name: plugin.manifest.display_name.clone(),
                version: plugin.manifest.version.clone(),
                description: plugin.manifest.description.clone(),
                author: plugin.manifest.author.clone(),
                active: plugin.active,
                installed_at: plugin.installed_at.to_rfc3339(),
                updated_at: plugin.updated_at.to_rfc3339(),
            },
            license: plugin.manifest.license.clone(),
            permissions: plugin.manifest.permissions.clone(),
            hooks: plugin.manifest.hooks.clone(),
            settings: plugin.manifest.config.settings.clone(),
            current_settings: plugin.settings.clone(),
        };
        
        Ok(details)
    }
    
    /// Update plugin settings
    pub async fn update_plugin_settings(&self, plugin_id: &str, settings: serde_json::Value) -> Result<(), String> {
        // Check if plugin exists
        let mut plugins = self.plugins.write().await;
        let plugin = plugins.get_mut(plugin_id)
            .ok_or_else(|| format!("Plugin not found: {}", plugin_id))?;
            
        // Update settings
        plugin.settings = settings;
        
        // Save registry data
        drop(plugins);
        self.save_registry_data().await?;
        
        Ok(())
    }
    
    /// Get plugin settings
    pub async fn get_plugin_settings(&self, plugin_id: &str) -> Result<serde_json::Value, String> {
        // Get plugin
        let plugins = self.plugins.read().await;
        let plugin = plugins.get(plugin_id)
            .ok_or_else(|| format!("Plugin not found: {}", plugin_id))?;
            
        Ok(plugin.settings.clone())
    }
}

/// Get the plugins directory
fn get_plugins_dir() -> Result<PathBuf, String> {
    let home_dir = dirs::home_dir()
        .ok_or_else(|| "Could not determine home directory".to_string())?;
        
    #[cfg(target_os = "windows")]
    let plugins_dir = home_dir.join("AppData").join("Roaming").join("mcp").join("plugins").join("installed");
    
    #[cfg(target_os = "macos")]
    let plugins_dir = home_dir.join("Library").join("Application Support").join("mcp").join("plugins").join("installed");
    
    #[cfg(target_os = "linux")]
    let plugins_dir = home_dir.join(".config").join("mcp").join("plugins").join("installed");
    
    #[cfg(not(any(target_os = "windows", target_os = "macos", target_os = "linux")))]
    let plugins_dir = home_dir.join(".mcp").join("plugins").join("installed");
    
    Ok(plugins_dir)
}

impl Default for PluginRegistry {
    fn default() -> Self {
        Self::new()
    }
}
</file>

<file path="src/plugins/sandbox/mod.rs">
use std::collections::HashMap;
use std::path::Path;
use tokio::sync::{RwLock, mpsc};
use wasmer::{Store, Module, Instance, ImportObject, Function, Memory, MemoryType, WasmPtr, Value};
use wasmer_wasi::{WasiState, WasiVersion};
use serde::{Serialize, Deserialize};
use uuid::Uuid;

use crate::plugins::permissions::PermissionManager;
use crate::plugins::types::{Plugin, HookContext};
use crate::plugins::hooks::HookType;

/// Sandbox manager
pub struct SandboxManager {
    /// Running plugin instances
    instances: RwLock<HashMap<String, PluginInstance>>,
    /// Memory limits in bytes (32MB default)
    memory_limit: usize,
    /// CPU limits in instructions per second (0 for unlimited)
    cpu_limit: usize,
}

/// Plugin instance in sandbox
struct PluginInstance {
    /// Plugin ID
    plugin_id: String,
    /// Instance ID
    instance_id: String,
    /// WASM instance
    instance: Instance,
    /// Memory
    memory: Memory,
    /// Store
    store: Store,
    /// Registered hooks
    hooks: HashMap<String, wasmer::TypedFunction<i32, i32>>,
    /// Communication channel
    sender: mpsc::Sender<PluginMessage>,
    /// Resource usage
    resource_usage: ResourceUsage,
}

/// Plugin message
#[derive(Debug, Clone, Serialize, Deserialize)]
enum PluginMessage {
    /// Hook invocation
    Hook(HookInvocation),
    /// Permission request
    PermissionRequest(String),
    /// Log message
    Log(String, String), // (level, message)
    /// API call
    ApiCall(ApiCall),
}

/// Hook invocation
#[derive(Debug, Clone, Serialize, Deserialize)]
struct HookInvocation {
    /// Hook name
    hook: String,
    /// Context data
    context: serde_json::Value,
    /// Result
    result: Option<serde_json::Value>,
}

/// API call
#[derive(Debug, Clone, Serialize, Deserialize)]
struct ApiCall {
    /// API function name
    function: String,
    /// Arguments
    args: Vec<serde_json::Value>,
    /// Result
    result: Option<serde_json::Value>,
}

/// Resource usage
#[derive(Debug, Clone)]
struct ResourceUsage {
    /// Memory usage in bytes
    memory_usage: usize,
    /// CPU usage in instructions
    cpu_usage: usize,
    /// Network calls
    network_calls: usize,
    /// Storage operations
    storage_ops: usize,
}

impl SandboxManager {
    /// Create a new sandbox manager
    pub fn new() -> Self {
        Self {
            instances: RwLock::new(HashMap::new()),
            memory_limit: 32 * 1024 * 1024, // 32MB
            cpu_limit: 0, // Unlimited for now
        }
    }
    
    /// Initialize the sandbox manager
    pub async fn initialize(&self) -> Result<(), String> {
        log::info!("Initializing sandbox manager");
        
        // Initialize wasmer
        // This is just a placeholder - actual initialization will depend on the specific WASM runtime
        
        log::info!("Sandbox manager initialized");
        Ok(())
    }
    
    /// Load a plugin into the sandbox
    pub async fn load_plugin(&self, plugin: &Plugin, permission_manager: &PermissionManager) -> Result<String, String> {
        log::info!("Loading plugin into sandbox: {}", plugin.manifest.name);
        
        // Generate instance ID
        let instance_id = Uuid::new_v4().to_string();
        
        // Load WASM module
        let wasm_path = plugin.path.join(&plugin.manifest.main);
        
        // Read WASM file
        let wasm_bytes = tokio::fs::read(&wasm_path)
            .await
            .map_err(|e| format!("Failed to read WASM file: {}", e))?;
        
        // Create store and module
        let mut store = Store::default();
        let module = Module::new(&store, &wasm_bytes)
            .map_err(|e| format!("Failed to compile WASM module: {}", e))?;
            
        // Set up WASI
        let wasi_env = WasiState::new("plugin")
            .map_err(|e| format!("Failed to create WASI environment: {}", e))?;
            
        // Create import object with host functions
        let import_object = self.create_import_object(&plugin.manifest.name, &instance_id, permission_manager).await?;
        
        // Instantiate module
        let instance = Instance::new(&mut store, &module, &import_object)
            .map_err(|e| format!("Failed to instantiate WASM module: {}", e))?;
        
        // Get memory
        let memory = instance.exports.get_memory("memory")
            .map_err(|e| format!("Failed to get memory: {}", e))?
            .clone();
            
        // Set up communication channel
        let (sender, mut receiver) = mpsc::channel::<PluginMessage>(32);
        
        // Create plugin instance
        let plugin_instance = PluginInstance {
            plugin_id: plugin.manifest.name.clone(),
            instance_id: instance_id.clone(),
            instance,
            memory,
            store,
            hooks: HashMap::new(),
            sender,
            resource_usage: ResourceUsage {
                memory_usage: 0,
                cpu_usage: 0,
                network_calls: 0,
                storage_ops: 0,
            },
        };
        
        // Store instance
        let mut instances = self.instances.write().await;
        instances.insert(instance_id.clone(), plugin_instance);
        
        // Start communication handler
        let instance_id_clone = instance_id.clone();
        tokio::spawn(async move {
            // Handler for plugin messages
            while let Some(message) = receiver.recv().await {
                // Process message - in a real implementation, this would handle all plugin communications
                log::debug!("Received message from plugin {}: {:?}", instance_id_clone, message);
            }
        });
        
        log::info!("Plugin loaded into sandbox: {} (instance {})", plugin.manifest.name, instance_id);
        
        Ok(instance_id)
    }
    
    /// Create import object with host functions
    async fn create_import_object(&self, plugin_id: &str, instance_id: &str, 
                                 permission_manager: &PermissionManager) -> Result<ImportObject, String> {
        // This is a simplified version - a real implementation would include all host functions
        
        let plugin_id = plugin_id.to_string();
        let instance_id = instance_id.to_string();
        
        // Create imports object
        let mut import_object = ImportObject::new();
        
        // Add host functions
        // In a real implementation, we would add all host functions here
        
        // Example: Register plugin
        let register_plugin = move |ptr: i32, len: i32| -> i32 {
            // In a real implementation, this would register the plugin
            log::debug!("registerPlugin called from plugin {}", plugin_id);
            1 // Success
        };
        
        // Example: Request permission
        let perm_manager = permission_manager.clone();
        let p_id = plugin_id.clone();
        let request_permission = move |ptr: i32, len: i32| -> i32 {
            // In a real implementation, this would request permission
            log::debug!("requestPermission called from plugin {}", p_id);
            1 // Success
        };
        
        // Example: Register hook
        let register_hook = move |hook_ptr: i32, hook_len: i32, callback_ptr: i32| -> i32 {
            // In a real implementation, this would register a hook
            log::debug!("registerHook called from plugin {}", plugin_id);
            1 // Success
        };
        
        // Add functions to import object
        // In a real implementation, we would add all functions to the import object
        
        Ok(import_object)
    }
    
    /// Call a hook on a plugin
    pub async fn call_hook(&self, instance_id: &str, hook_type: HookType, 
                          context: &HookContext) -> Result<serde_json::Value, String> {
        log::debug!("Calling hook {:?} on plugin instance {}", hook_type, instance_id);
        
        // Get instance
        let instances = self.instances.read().await;
        let instance = instances.get(instance_id)
            .ok_or_else(|| format!("Plugin instance not found: {}", instance_id))?;
            
        // Get hook function
        let hook_name = hook_type.to_string();
        let hook_func = instance.hooks.get(&hook_name)
            .ok_or_else(|| format!("Hook not registered: {}", hook_name))?;
            
        // Serialize context
        let context_json = serde_json::to_string(context)
            .map_err(|e| format!("Failed to serialize context: {}", e))?;
            
        // Write context to memory
        // In a real implementation, this would write the context to WASM memory
        
        // Call hook function
        // In a real implementation, this would call the hook function
        
        // Read result from memory
        // In a real implementation, this would read the result from WASM memory
        
        // Return empty result for now
        Ok(serde_json::Value::Null)
    }
    
    /// Unload a plugin from the sandbox
    pub async fn unload_plugin(&self, instance_id: &str) -> Result<(), String> {
        log::info!("Unloading plugin instance: {}", instance_id);
        
        // Remove instance
        let mut instances = self.instances.write().await;
        instances.remove(instance_id)
            .ok_or_else(|| format!("Plugin instance not found: {}", instance_id))?;
            
        log::info!("Plugin instance unloaded: {}", instance_id);
        Ok(())
    }
    
    /// Check if a plugin instance exists
    pub async fn instance_exists(&self, instance_id: &str) -> bool {
        let instances = self.instances.read().await;
        instances.contains_key(instance_id)
    }
    
    /// Get resource usage for a plugin
    pub async fn get_resource_usage(&self, instance_id: &str) -> Result<ResourceUsage, String> {
        let instances = self.instances.read().await;
        let instance = instances.get(instance_id)
            .ok_or_else(|| format!("Plugin instance not found: {}", instance_id))?;
            
        Ok(instance.resource_usage.clone())
    }
    
    /// Set memory limit
    pub fn set_memory_limit(&mut self, limit: usize) {
        self.memory_limit = limit;
    }
    
    /// Set CPU limit
    pub fn set_cpu_limit(&mut self, limit: usize) {
        self.cpu_limit = limit;
    }
}

impl Default for SandboxManager {
    fn default() -> Self {
        Self::new()
    }
}
</file>

<file path="src/plugins/types.rs">
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use std::collections::HashMap;

/// Plugin manifest definition
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PluginManifest {
    /// Unique plugin identifier
    pub name: String,
    /// Display name for the plugin
    pub display_name: String,
    /// Plugin version
    pub version: String,
    /// Plugin description
    pub description: String,
    /// Plugin author
    pub author: String,
    /// Plugin license
    pub license: String,
    /// Main WASM file
    pub main: String,
    /// Required permissions
    pub permissions: Vec<String>,
    /// Plugin hooks
    pub hooks: Vec<String>,
    /// Plugin configuration
    #[serde(default)]
    pub config: PluginConfig,
}

/// Plugin configuration
#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct PluginConfig {
    /// Plugin settings
    #[serde(default)]
    pub settings: Vec<PluginSetting>,
}

/// Plugin setting definition
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PluginSetting {
    /// Setting name
    pub name: String,
    /// Setting type
    pub r#type: String,
    /// Setting label
    pub label: String,
    /// Setting description
    pub description: String,
    /// Default value
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub default: Option<serde_json::Value>,
    /// Whether this is a secret value
    #[serde(default)]
    pub secret: bool,
    /// Possible values for enum types
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub enum_values: Vec<PluginEnumValue>,
}

/// Enum value for settings
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PluginEnumValue {
    /// Value
    pub value: String,
    /// Display label
    pub label: String,
}

/// Plugin instance
#[derive(Debug, Clone)]
pub struct Plugin {
    /// Plugin manifest
    pub manifest: PluginManifest,
    /// Plugin path
    pub path: PathBuf,
    /// Active status
    pub active: bool,
    /// Installed timestamp
    pub installed_at: chrono::DateTime<chrono::Utc>,
    /// Last updated timestamp
    pub updated_at: chrono::DateTime<chrono::Utc>,
    /// Current settings
    pub settings: serde_json::Value,
    /// Plugin instance ID (UUID)
    pub instance_id: String,
}

/// Plugin information for UI
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PluginInfo {
    /// Plugin ID
    pub id: String,
    /// Display name
    pub display_name: String,
    /// Version
    pub version: String,
    /// Description
    pub description: String,
    /// Author
    pub author: String,
    /// Active status
    pub active: bool,
    /// Installed timestamp
    pub installed_at: String,
    /// Last updated timestamp
    pub updated_at: String,
}

/// Detailed plugin information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PluginDetails {
    /// Basic plugin info
    pub info: PluginInfo,
    /// Plugin license
    pub license: String,
    /// Required permissions
    pub permissions: Vec<String>,
    /// Plugin hooks
    pub hooks: Vec<String>,
    /// Plugin settings
    pub settings: Vec<PluginSetting>,
    /// Current settings values
    pub current_settings: serde_json::Value,
}

/// Permission request
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PermissionRequest {
    /// Plugin ID
    pub plugin_id: String,
    /// Plugin name
    pub plugin_name: String,
    /// Requested permissions
    pub permissions: Vec<String>,
    /// Request reason
    pub reason: String,
}

/// Plugin hook context
#[derive(Debug, Clone)]
pub struct HookContext {
    /// Plugin ID
    pub plugin_id: String,
    /// Hook name
    pub hook_name: String,
    /// Context data
    pub data: HashMap<String, serde_json::Value>,
}

/// Repository plugin information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RepositoryPlugin {
    /// Plugin ID
    pub id: String,
    /// Display name
    pub display_name: String,
    /// Latest version
    pub version: String,
    /// Description
    pub description: String,
    /// Author
    pub author: String,
    /// Downloads count
    pub downloads: u64,
    /// Rating (0-5)
    pub rating: f32,
    /// Repository URL
    pub repo_url: String,
    /// Download URL
    pub download_url: String,
    /// Screenshot URLs
    pub screenshots: Vec<String>,
    /// Required permissions
    pub permissions: Vec<String>,
}

/// Local plugin package file
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PluginPackage {
    /// Plugin manifest
    pub manifest: PluginManifest,
    /// Package location
    pub path: PathBuf,
    /// Package checksums
    pub checksums: HashMap<String, String>,
}
</file>

<file path="src/plugins/ui/mod.rs">
use serde::{Serialize, Deserialize};

use crate::plugins::types::{PluginInfo, PluginDetails, RepositoryPlugin};

/// UI state for plugin management
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PluginManagerUIState {
    /// Currently installed plugins
    pub installed_plugins: Vec<PluginInfo>,
    /// Available plugins for installation
    pub available_plugins: Vec<RepositoryPlugin>,
    /// Currently selected plugin
    pub selected_plugin: Option<String>,
    /// Current tab
    pub current_tab: PluginManagerTab,
    /// Search query
    pub search_query: String,
    /// Loading state
    pub loading: bool,
    /// Error message
    pub error: Option<String>,
}

/// Plugin manager tabs
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum PluginManagerTab {
    /// Installed plugins
    Installed,
    /// Available plugins
    Available,
    /// Repositories
    Repositories,
    /// Settings
    Settings,
}

/// Plugin details view state
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PluginDetailsViewState {
    /// Plugin details
    pub plugin: PluginDetails,
    /// Current tab
    pub current_tab: PluginDetailsTab,
    /// Loading state
    pub loading: bool,
    /// Error message
    pub error: Option<String>,
}

/// Plugin details tabs
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum PluginDetailsTab {
    /// Overview
    Overview,
    /// Settings
    Settings,
    /// Permissions
    Permissions,
    /// Logs
    Logs,
}

/// Plugin installation view state
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PluginInstallViewState {
    /// Plugin to install
    pub plugin: RepositoryPlugin,
    /// Installation progress
    pub progress: f32,
    /// Installation step
    pub step: PluginInstallStep,
    /// Error message
    pub error: Option<String>,
}

/// Plugin installation steps
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum PluginInstallStep {
    /// Downloading
    Downloading,
    /// Verifying
    Verifying,
    /// Installing
    Installing,
    /// Configuring
    Configuring,
    /// Complete
    Complete,
    /// Failed
    Failed,
}

/// Permission request view state
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PermissionRequestViewState {
    /// Plugin name
    pub plugin_name: String,
    /// Plugin ID
    pub plugin_id: String,
    /// Requested permissions
    pub permissions: Vec<String>,
    /// Permission descriptions
    pub permission_descriptions: Vec<String>,
    /// Reason for request
    pub reason: String,
}

/// Plugin management commands for Tauri
#[tauri::command]
pub async fn get_installed_plugins() -> Result<Vec<PluginInfo>, String> {
    // Get plugin manager
    let plugin_manager = crate::plugins::get_plugin_manager();
    let plugin_manager = plugin_manager.read().await;
    
    // Get installed plugins
    Ok(plugin_manager.get_installed_plugins().await)
}

#[tauri::command]
pub async fn get_available_plugins(query: &str) -> Result<Vec<RepositoryPlugin>, String> {
    // Get plugin manager
    let plugin_manager = crate::plugins::get_plugin_manager();
    let plugin_manager = plugin_manager.read().await;
    
    // Search for plugins
    let plugins = plugin_manager.search_plugins(query).await?;
    
    // Convert to repository plugins
    let mut available = Vec::new();
    
    // For each plugin, check if it's already installed
    for plugin in plugins {
        // Check if plugin is already installed
        let installed = plugin_manager.get_installed_plugins().await
            .iter()
            .any(|p| p.id == plugin.id);
            
        if !installed {
            // Get plugin details from discovery
            if let Some(repo_plugin) = plugin_manager.discovery.get_plugin(&plugin.id).await {
                available.push(repo_plugin);
            }
        }
    }
    
    Ok(available)
}

#[tauri::command]
pub async fn install_plugin(plugin_id: &str) -> Result<PluginInfo, String> {
    // Get plugin manager
    let plugin_manager = crate::plugins::get_plugin_manager();
    let mut plugin_manager = plugin_manager.write().await;
    
    // Get plugin details
    let plugin = match plugin_manager.discovery.get_plugin(plugin_id).await {
        Some(plugin) => plugin,
        None => return Err(format!("Plugin not found: {}", plugin_id)),
    };
    
    // Download plugin
    let temp_dir = tempfile::tempdir()
        .map_err(|e| format!("Failed to create temporary directory: {}", e))?;
        
    let download_path = temp_dir.path().join("plugin.zip");
    
    // Download the plugin
    let response = reqwest::get(&plugin.download_url)
        .await
        .map_err(|e| format!("Failed to download plugin: {}", e))?;
        
    if !response.status().is_success() {
        return Err(format!("Failed to download plugin: {}", response.status()));
    }
    
    // Save to file
    let bytes = response.bytes()
        .await
        .map_err(|e| format!("Failed to read response: {}", e))?;
        
    tokio::fs::write(&download_path, &bytes)
        .await
        .map_err(|e| format!("Failed to write plugin file: {}", e))?;
        
    // Install the plugin
    let plugin_info = plugin_manager.install_plugin(&download_path).await?;
    
    // Activate the plugin
    plugin_manager.activate_plugin(&plugin_id).await?;
    
    Ok(plugin_info)
}

#[tauri::command]
pub async fn uninstall_plugin(plugin_id: &str) -> Result<(), String> {
    // Get plugin manager
    let plugin_manager = crate::plugins::get_plugin_manager();
    let mut plugin_manager = plugin_manager.write().await;
    
    // Uninstall the plugin
    plugin_manager.uninstall_plugin(plugin_id).await
}

#[tauri::command]
pub async fn activate_plugin(plugin_id: &str) -> Result<(), String> {
    // Get plugin manager
    let plugin_manager = crate::plugins::get_plugin_manager();
    let mut plugin_manager = plugin_manager.write().await;
    
    // Activate the plugin
    plugin_manager.activate_plugin(plugin_id).await
}

#[tauri::command]
pub async fn deactivate_plugin(plugin_id: &str) -> Result<(), String> {
    // Get plugin manager
    let plugin_manager = crate::plugins::get_plugin_manager();
    let mut plugin_manager = plugin_manager.write().await;
    
    // Deactivate the plugin
    plugin_manager.deactivate_plugin(plugin_id).await
}

#[tauri::command]
pub async fn get_plugin_details(plugin_id: &str) -> Result<PluginDetails, String> {
    // Get plugin manager
    let plugin_manager = crate::plugins::get_plugin_manager();
    let plugin_manager = plugin_manager.read().await;
    
    // Get plugin details
    plugin_manager.get_plugin_details(plugin_id).await
}

#[tauri::command]
pub async fn update_plugin_settings(plugin_id: &str, settings: serde_json::Value) -> Result<(), String> {
    // Get plugin manager
    let plugin_manager = crate::plugins::get_plugin_manager();
    let mut plugin_manager = plugin_manager.write().await;
    
    // Update plugin settings
    plugin_manager.update_plugin_settings(plugin_id, settings).await
}

#[tauri::command]
pub async fn install_local_plugin(path: &str) -> Result<PluginInfo, String> {
    // Get plugin manager
    let plugin_manager = crate::plugins::get_plugin_manager();
    let mut plugin_manager = plugin_manager.write().await;
    
    // Install the plugin
    let plugin_info = plugin_manager.install_plugin(&std::path::Path::new(path)).await?;
    
    // Activate the plugin
    plugin_manager.activate_plugin(&plugin_info.id).await?;
    
    Ok(plugin_info)
}

#[tauri::command]
pub async fn update_plugin(plugin_id: &str, path: &str) -> Result<PluginInfo, String> {
    // Get plugin manager
    let plugin_manager = crate::plugins::get_plugin_manager();
    let mut plugin_manager = plugin_manager.write().await;
    
    // Update the plugin
    plugin_manager.update_plugin(plugin_id, &std::path::Path::new(path)).await
}

#[tauri::command]
pub async fn get_pending_permission_requests() -> Result<HashMap<String, Vec<String>>, String> {
    // Get plugin manager
    let plugin_manager = crate::plugins::get_plugin_manager();
    let plugin_manager = plugin_manager.read().await;
    
    // Get permission manager
    let permission_manager = plugin_manager.permission_manager();
    
    // Get pending requests
    Ok(permission_manager.get_pending_requests().await)
}

#[tauri::command]
pub async fn respond_to_permission_request(plugin_id: &str, permissions: Vec<String>, approved: bool) -> Result<(), String> {
    // Get plugin manager
    let plugin_manager = crate::plugins::get_plugin_manager();
    let plugin_manager = plugin_manager.read().await;
    
    // Get permission manager
    let permission_manager = plugin_manager.permission_manager();
    
    // Respond to request
    permission_manager.respond_to_request(plugin_id, &permissions, approved).await
}

#[tauri::command]
pub async fn get_repositories() -> Result<Vec<crate::plugins::discovery::PluginRepository>, String> {
    // Get plugin manager
    let plugin_manager = crate::plugins::get_plugin_manager();
    let plugin_manager = plugin_manager.read().await;
    
    // Get discovery
    let discovery = plugin_manager.discovery();
    
    // Get repositories
    Ok(discovery.get_repositories().await)
}

#[tauri::command]
pub async fn add_repository(repo: crate::plugins::discovery::PluginRepository) -> Result<(), String> {
    // Get plugin manager
    let plugin_manager = crate::plugins::get_plugin_manager();
    let plugin_manager = plugin_manager.read().await;
    
    // Get discovery
    let discovery = plugin_manager.discovery();
    
    // Add repository
    discovery.add_repository(repo).await
}

#[tauri::command]
pub async fn remove_repository(name: &str) -> Result<(), String> {
    // Get plugin manager
    let plugin_manager = crate::plugins::get_plugin_manager();
    let plugin_manager = plugin_manager.read().await;
    
    // Get discovery
    let discovery = plugin_manager.discovery();
    
    // Remove repository
    discovery.remove_repository(name).await
}

#[tauri::command]
pub async fn set_repository_enabled(name: &str, enabled: bool) -> Result<(), String> {
    // Get plugin manager
    let plugin_manager = crate::plugins::get_plugin_manager();
    let plugin_manager = plugin_manager.read().await;
    
    // Get discovery
    let discovery = plugin_manager.discovery();
    
    // Set repository enabled
    discovery.set_repository_enabled(name, enabled).await
}

/// Implement accessor methods for the plugin manager
impl crate::plugins::PluginManager {
    /// Get permission manager
    pub fn permission_manager(&self) -> &crate::plugins::permissions::PermissionManager {
        &self.permission_manager
    }
    
    /// Get discovery
    pub fn discovery(&self) -> &crate::plugins::discovery::PluginDiscovery {
        &self.discovery
    }
}

/// Register plugin management commands
pub fn register_commands(app: &mut tauri::App) -> Result<(), Box<dyn std::error::Error>> {
    app.register_async_command("get_installed_plugins", get_installed_plugins);
    app.register_async_command("get_available_plugins", get_available_plugins);
    app.register_async_command("install_plugin", install_plugin);
    app.register_async_command("uninstall_plugin", uninstall_plugin);
    app.register_async_command("activate_plugin", activate_plugin);
    app.register_async_command("deactivate_plugin", deactivate_plugin);
    app.register_async_command("get_plugin_details", get_plugin_details);
    app.register_async_command("update_plugin_settings", update_plugin_settings);
    app.register_async_command("install_local_plugin", install_local_plugin);
    app.register_async_command("update_plugin", update_plugin);
    app.register_async_command("get_pending_permission_requests", get_pending_permission_requests);
    app.register_async_command("respond_to_permission_request", respond_to_permission_request);
    app.register_async_command("get_repositories", get_repositories);
    app.register_async_command("add_repository", add_repository);
    app.register_async_command("remove_repository", remove_repository);
    app.register_async_command("set_repository_enabled", set_repository_enabled);
    
    Ok(())
}

// Add missing imports
use std::collections::HashMap;
use crate::plugins::permissions::PermissionManager;
use crate::plugins::discovery::PluginDiscovery;
</file>

<file path="src/protocols/mcp/client.rs">
use crate::models::messages::{Message, MessageError};
use crate::protocols::mcp::error::McpError;
use crate::protocols::mcp::message::{McpMessage, McpMessagePayload, McpResponseMessage};
use crate::protocols::mcp::types::{McpCompletionRequest, McpMessageRole, McpMessageType};
use crate::protocols::mcp::websocket::WebSocketClient;
use crate::protocols::mcp::McpConfig;
use crate::protocols::ConnectionStatus;
use log::{debug, error, info, warn};
use serde_json::json;
use std::collections::HashMap;
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, SystemTime};
use tokio::sync::mpsc::{self, Receiver, Sender};
use tokio::sync::mpsc::{UnboundedReceiver, UnboundedSender};
use tokio::sync::oneshot;
use tokio::time::timeout;
use uuid::Uuid;

/// Struct responsible for handling MCP communication
pub struct McpClient {
    /// WebSocket client for communication
    websocket: Arc<WebSocketClient>,
    
    /// Client configuration
    config: McpConfig,
    
    /// Connection status
    status: Arc<RwLock<ConnectionStatus>>,
    
    /// Request tracking map (request ID -> response channel)
    pending_requests: Arc<Mutex<HashMap<String, oneshot::Sender<Result<McpResponseMessage, McpError>>>>>,
    
    /// Channel for receiving unsolicited messages (events, etc.)
    event_tx: UnboundedSender<McpMessage>,
    
    /// Message streaming channel (streaming ID -> message channel)
    streaming_channels: Arc<Mutex<HashMap<String, Sender<Result<McpMessage, McpError>>>>>,
}

/// Implementation for McpClient
impl McpClient {
    /// Create a new MCP client with the given configuration
    pub fn new(config: McpConfig) -> Self {
        let (event_tx, _) = mpsc::unbounded_channel();
        
        Self {
            websocket: Arc::new(WebSocketClient::new(&config.url)),
            config,
            status: Arc::new(RwLock::new(ConnectionStatus::Disconnected)),
            pending_requests: Arc::new(Mutex::new(HashMap::new())),
            event_tx,
            streaming_channels: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    /// Get a receiver for unsolicited events
    pub fn get_event_receiver(&self) -> UnboundedReceiver<McpMessage> {
        let (tx, rx) = mpsc::unbounded_channel();
        self.event_tx = tx;
        rx
    }
    
    /// Connect to the MCP server
    pub async fn connect(&self) -> Result<(), McpError> {
        let mut status = self.status.write().unwrap();
        *status = ConnectionStatus::Connecting;
        drop(status);
        
        // Connect to WebSocket
        let ws_client = self.websocket.clone();
        let pending_requests = self.pending_requests.clone();
        let streaming_channels = self.streaming_channels.clone();
        let status_clone = self.status.clone();
        let event_tx = self.event_tx.clone();
        
        match ws_client.connect().await {
            Ok(_) => {
                let mut status = self.status.write().unwrap();
                *status = ConnectionStatus::Connected;
                
                // Start message handler
                tokio::spawn(async move {
                    Self::handle_incoming_messages(
                        ws_client.clone(),
                        pending_requests,
                        streaming_channels,
                        status_clone,
                        event_tx,
                    )
                    .await;
                });
                
                Ok(())
            }
            Err(e) => {
                let mut status = self.status.write().unwrap();
                *status = ConnectionStatus::ConnectionError(e.to_string());
                Err(McpError::ConnectionError(e))
            }
        }
    }
    
    /// Disconnect from the MCP server
    pub async fn disconnect(&self) -> Result<(), McpError> {
        // Close WebSocket connection
        self.websocket.disconnect().await?;
        
        // Update status
        let mut status = self.status.write().unwrap();
        *status = ConnectionStatus::Disconnected;
        
        // Cancel all pending requests
        let mut pending = self.pending_requests.lock().unwrap();
        for (_, sender) in pending.drain() {
            let _ = sender.send(Err(McpError::ConnectionClosed));
        }
        
        // Cancel all streaming channels
        let mut streaming = self.streaming_channels.lock().unwrap();
        for (_, sender) in streaming.drain() {
            let _ = sender.send(Err(McpError::ConnectionClosed)).await;
        }
        
        Ok(())
    }
    
    /// Get the current connection status
    pub fn status(&self) -> ConnectionStatus {
        self.status.read().unwrap().clone()
    }
    
    /// Send a message and wait for response
    pub async fn send(&self, message: Message) -> Result<Message, MessageError> {
        // Convert to MCP message format
        let mcp_message = Self::convert_to_mcp_message(message)?;
        
        // Setup response channel
        let (tx, rx) = oneshot::channel();
        {
            let mut pending = self.pending_requests.lock().unwrap();
            pending.insert(mcp_message.id.clone(), tx);
        }
        
        // Send message
        self.websocket
            .send(serde_json::to_string(&mcp_message)?)
            .await
            .map_err(|e| MessageError::NetworkError(e.to_string()))?;
        
        // Wait for response with timeout
        match timeout(Duration::from_secs(60), rx).await {
            Ok(result) => match result {
                Ok(response) => match response {
                    Ok(mcp_response) => Self::convert_from_mcp_response(mcp_response),
                    Err(e) => Err(MessageError::ProtocolError(e.to_string())),
                },
                Err(_) => Err(MessageError::Unknown("Response channel closed".to_string())),
            },
            Err(_) => Err(MessageError::Timeout(Duration::from_secs(60))),
        }
    }
    
    /// Start a streaming completion
    pub async fn stream(
        &self,
        message: Message,
    ) -> Result<Receiver<Result<Message, MessageError>>, MessageError> {
        // Create a streaming ID
        let streaming_id = Uuid::new_v4().to_string();
        
        // Convert to MCP format
        let mut mcp_message = Self::convert_to_mcp_message(message)?;
        
        // Update payload for streaming
        if let McpMessagePayload::CompletionRequest(ref mut req) = mcp_message.payload {
            req.stream = true;
            req.streaming_id = Some(streaming_id.clone());
        } else {
            return Err(MessageError::ProtocolError(
                "Invalid message type for streaming".to_string(),
            ));
        }
        
        // Create streaming channel
        let (tx, rx) = mpsc::channel(32);
        {
            let mut streaming = self.streaming_channels.lock().unwrap();
            streaming.insert(streaming_id.clone(), tx);
        }
        
        // Setup response channel for initial acknowledgment
        let (ack_tx, ack_rx) = oneshot::channel();
        {
            let mut pending = self.pending_requests.lock().unwrap();
            pending.insert(mcp_message.id.clone(), ack_tx);
        }
        
        // Send message
        self.websocket
            .send(serde_json::to_string(&mcp_message)?)
            .await
            .map_err(|e| MessageError::NetworkError(e.to_string()))?;
        
        // Wait for initial acknowledgment
        match timeout(Duration::from_secs(10), ack_rx).await {
            Ok(result) => match result {
                Ok(response) => {
                    match response {
                        Ok(_) => {
                            // Create message adapter channel
                            let (adapter_tx, adapter_rx) = mpsc::channel(32);
                            
                            // Spawn adapter task to convert MCP messages to app messages
                            let streaming_clone = self.streaming_channels.clone();
                            let streaming_id_clone = streaming_id.clone();
                            
                            tokio::spawn(async move {
                                let streaming = streaming_clone.lock().unwrap();
                                if let Some(rx_channel) = streaming.get(&streaming_id_clone) {
                                    // TODO: Implement streaming message adapter
                                    // This would convert the MCP protocol streaming messages
                                    // into the application's Message format
                                }
                            });
                            
                            Ok(adapter_rx)
                        }
                        Err(e) => Err(MessageError::ProtocolError(e.to_string())),
                    }
                }
                Err(_) => Err(MessageError::Unknown("Response channel closed".to_string())),
            },
            Err(_) => Err(MessageError::Timeout(Duration::from_secs(10))),
        }
    }
    
    /// Cancel a streaming request
    pub async fn cancel_streaming(&self, streaming_id: &str) -> Result<(), MessageError> {
        // Remove streaming channel
        {
            let mut streaming = self.streaming_channels.lock().unwrap();
            streaming.remove(streaming_id);
        }
        
        // Send cancel message
        let cancel_message = McpMessage {
            id: Uuid::new_v4().to_string(),
            version: "v1".to_string(),
            type_: McpMessageType::CancelStream,
            payload: McpMessagePayload::CancelStream {
                streaming_id: streaming_id.to_string(),
            },
        };
        
        self.websocket
            .send(serde_json::to_string(&cancel_message)?)
            .await
            .map_err(|e| MessageError::NetworkError(e.to_string()))?;
        
        Ok(())
    }
    
    /// Handle incoming messages from the WebSocket
    async fn handle_incoming_messages(
        websocket: Arc<WebSocketClient>,
        pending_requests: Arc<Mutex<HashMap<String, oneshot::Sender<Result<McpResponseMessage, McpError>>>>>,
        streaming_channels: Arc<Mutex<HashMap<String, Sender<Result<McpMessage, McpError>>>>>,
        status: Arc<RwLock<ConnectionStatus>>,
        event_tx: UnboundedSender<McpMessage>,
    ) {
        loop {
            match websocket.receive().await {
                Ok(message) => {
                    match serde_json::from_str::<McpMessage>(&message) {
                        Ok(mcp_message) => {
                            // Handle based on message type
                            match mcp_message.type_ {
                                McpMessageType::CompletionResponse => {
                                    // Check if this is part of a pending request
                                    let mut pending = pending_requests.lock().unwrap();
                                    if let Some(sender) = pending.remove(&mcp_message.id) {
                                        let _ = sender.send(Ok(McpResponseMessage::Completion(mcp_message)));
                                    } else {
                                        // This could be an unsolicited message or event
                                        let _ = event_tx.send(mcp_message);
                                    }
                                }
                                McpMessageType::StreamingMessage => {
                                    // Extract streaming ID from payload
                                    if let McpMessagePayload::StreamingMessage { streaming_id, .. } = 
                                        &mcp_message.payload 
                                    {
                                        let streaming = streaming_channels.lock().unwrap();
                                        if let Some(sender) = streaming.get(streaming_id) {
                                            let _ = sender.send(Ok(mcp_message.clone())).await;
                                        }
                                    }
                                }
                                McpMessageType::StreamingEnd => {
                                    // Extract streaming ID from payload
                                    if let McpMessagePayload::StreamingEnd { streaming_id } = 
                                        &mcp_message.payload 
                                    {
                                        let mut streaming = streaming_channels.lock().unwrap();
                                        if let Some(sender) = streaming.remove(streaming_id) {
                                            let _ = sender.send(Ok(mcp_message.clone())).await;
                                        }
                                    }
                                }
                                McpMessageType::Error => {
                                    // Extract request_id from payload
                                    if let McpMessagePayload::Error { request_id, .. } = 
                                        &mcp_message.payload 
                                    {
                                        let mut pending = pending_requests.lock().unwrap();
                                        if let Some(sender) = pending.remove(request_id) {
                                            if let McpMessagePayload::Error { code, message, .. } = 
                                                mcp_message.payload.clone() 
                                            {
                                                let _ = sender.send(Err(
                                                    McpError::ProtocolError(format!("{}: {}", code, message))
                                                ));
                                            }
                                        }
                                    }
                                }
                                _ => {
                                    // Handle other message types or unsolicited messages
                                    let _ = event_tx.send(mcp_message);
                                }
                            }
                        }
                        Err(e) => {
                            error!("Failed to parse MCP message: {}", e);
                        }
                    }
                }
                Err(e) => {
                    error!("WebSocket error: {}", e);
                    
                    // Update connection status
                    let mut status_guard = status.write().unwrap();
                    *status_guard = ConnectionStatus::ConnectionError(e.to_string());
                    
                    // Cancel all pending requests
                    let mut pending = pending_requests.lock().unwrap();
                    for (_, sender) in pending.drain() {
                        let _ = sender.send(Err(McpError::ConnectionClosed));
                    }
                    
                    // Cancel all streaming channels
                    let mut streaming = streaming_channels.lock().unwrap();
                    for (_, sender) in streaming.drain() {
                        let _ = sender.send(Err(McpError::ConnectionClosed)).await;
                    }
                    
                    // Break the loop, ending the handler
                    break;
                }
            }
        }
    }
    
    /// Convert application Message to MCP format
    fn convert_to_mcp_message(message: Message) -> Result<McpMessage, MessageError> {
        // Convert role
        let role = match message.role {
            crate::models::messages::MessageRole::User => McpMessageRole::User,
            crate::models::messages::MessageRole::Assistant => McpMessageRole::Assistant,
            crate::models::messages::MessageRole::System => McpMessageRole::System,
            crate::models::messages::MessageRole::Tool => McpMessageRole::Tool,
        };
        
        // Build MCP message
        let mut parts = Vec::new();
        for part in &message.content.parts {
            match part {
                crate::models::messages::ContentType::Text { text } => {
                    parts.push(json!({
                        "type": "text",
                        "text": text
                    }));
                }
                crate::models::messages::ContentType::Image { url, media_type } => {
                    parts.push(json!({
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": media_type,
                            "data": url.trim_start_matches("data:").to_string()
                        }
                    }));
                }
                crate::models::messages::ContentType::ToolCall { id, name, arguments } => {
                    parts.push(json!({
                        "type": "tool_call",
                        "id": id,
                        "name": name,
                        "arguments": arguments
                    }));
                }
                crate::models::messages::ContentType::ToolResult { tool_call_id, result } => {
                    parts.push(json!({
                        "type": "tool_result",
                        "tool_call_id": tool_call_id,
                        "result": result
                    }));
                }
            }
        }
        
        // Create completion request
        let completion_request = McpCompletionRequest {
            model: "claude-3-opus-20240229".to_string(), // TODO: Get from config
            messages: vec![json!({
                "role": role.to_string(),
                "content": parts
            })],
            max_tokens: 4000,
            temperature: 0.7,
            top_p: 0.9,
            top_k: 40,
            stream: false,
            stop_sequences: Vec::new(),
            system_prompt: None,
            streaming_id: None,
        };
        
        // Build final MCP message
        let mcp_message = McpMessage {
            id: message.id.clone(),
            version: "v1".to_string(),
            type_: McpMessageType::CompletionRequest,
            payload: McpMessagePayload::CompletionRequest(completion_request),
        };
        
        Ok(mcp_message)
    }
    
    /// Convert MCP response to application Message
    fn convert_from_mcp_response(
        response: McpResponseMessage,
    ) -> Result<Message, MessageError> {
        match response {
            McpResponseMessage::Completion(completion) => {
                if let McpMessagePayload::CompletionResponse { response } = completion.payload {
                    // Extract first content part - text
                    // In a real implementation, we would handle all content types properly
                    if let Some(content) = response.get("content") {
                        if let Some(text) = content.as_str() {
                            // Create a message
                            let message = Message {
                                id: Uuid::new_v4().to_string(),
                                role: crate::models::messages::MessageRole::Assistant,
                                content: crate::models::messages::MessageContent {
                                    parts: vec![crate::models::messages::ContentType::Text {
                                        text: text.to_string(),
                                    }],
                                },
                                metadata: None,
                                created_at: SystemTime::now(),
                            };
                            
                            return Ok(message);
                        }
                    }
                    
                    return Err(MessageError::SerializationError(
                        "Invalid completion response format".to_string(),
                    ));
                }
                
                Err(MessageError::ProtocolError(
                    "Invalid response type".to_string(),
                ))
            }
            _ => Err(MessageError::ProtocolError(
                "Unexpected response type".to_string(),
            )),
        }
    }
}
</file>

<file path="src/protocols/mcp/config.rs">
use crate::protocols::ProtocolConfig;
use serde::{Deserialize, Serialize};
use std::time::Duration;

/// MCP protocol configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct McpConfig {
    /// WebSocket URL for MCP server
    pub url: String,
    
    /// API key for authentication
    pub api_key: String,
    
    /// Organization ID (if applicable)
    pub organization_id: Option<String>,
    
    /// Default model to use
    pub model: String,
    
    /// Default system prompt
    pub system_prompt: Option<String>,
    
    /// Connection timeout
    pub connection_timeout: Duration,
    
    /// Request timeout
    pub request_timeout: Duration,
    
    /// Whether to reconnect automatically
    pub auto_reconnect: bool,
    
    /// Maximum reconnection attempts
    pub max_reconnect_attempts: u32,
    
    /// Reconnection backoff delay
    pub reconnect_backoff: Duration,
}

impl ProtocolConfig for McpConfig {
    fn validate(&self) -> Result<(), String> {
        if self.url.is_empty() {
            return Err("WebSocket URL cannot be empty".to_string());
        }
        
        if self.api_key.is_empty() {
            return Err("API key cannot be empty".to_string());
        }
        
        if self.model.is_empty() {
            return Err("Model ID cannot be empty".to_string());
        }
        
        Ok(())
    }
}

impl Default for McpConfig {
    fn default() -> Self {
        Self {
            url: "wss://api.anthropic.com/v1/messages".to_string(),
            api_key: String::new(),
            organization_id: None,
            model: "claude-3-opus-20240229".to_string(),
            system_prompt: None,
            connection_timeout: Duration::from_secs(30),
            request_timeout: Duration::from_secs(120),
            auto_reconnect: true,
            max_reconnect_attempts: 5,
            reconnect_backoff: Duration::from_secs(2),
        }
    }
}

impl McpConfig {
    /// Create a new configuration with the given API key
    pub fn with_api_key(api_key: impl Into<String>) -> Self {
        let mut config = Self::default();
        config.api_key = api_key.into();
        config
    }
    
    /// Set the WebSocket URL
    pub fn with_url(mut self, url: impl Into<String>) -> Self {
        self.url = url.into();
        self
    }
    
    /// Set the organization ID
    pub fn with_organization_id(mut self, org_id: impl Into<String>) -> Self {
        self.organization_id = Some(org_id.into());
        self
    }
    
    /// Set the default model
    pub fn with_model(mut self, model: impl Into<String>) -> Self {
        self.model = model.into();
        self
    }
    
    /// Set the default system prompt
    pub fn with_system_prompt(mut self, prompt: impl Into<String>) -> Self {
        self.system_prompt = Some(prompt.into());
        self
    }
    
    /// Configure connection timeout
    pub fn with_connection_timeout(mut self, timeout: Duration) -> Self {
        self.connection_timeout = timeout;
        self
    }
    
    /// Configure request timeout
    pub fn with_request_timeout(mut self, timeout: Duration) -> Self {
        self.request_timeout = timeout;
        self
    }
    
    /// Configure auto-reconnect behavior
    pub fn with_auto_reconnect(mut self, auto_reconnect: bool) -> Self {
        self.auto_reconnect = auto_reconnect;
        self
    }
    
    /// Configure max reconnection attempts
    pub fn with_max_reconnect_attempts(mut self, attempts: u32) -> Self {
        self.max_reconnect_attempts = attempts;
        self
    }
    
    /// Configure reconnection backoff delay
    pub fn with_reconnect_backoff(mut self, backoff: Duration) -> Self {
        self.reconnect_backoff = backoff;
        self
    }
}
</file>

<file path="src/protocols/mcp/error.rs">
use serde::{Deserialize, Serialize};
use std::fmt;
use std::time::Duration;

/// MCP error codes
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum McpErrorCode {
    /// Invalid request format
    InvalidRequest,
    
    /// Authentication failed
    AuthenticationFailed,
    
    /// Authorization failed
    AuthorizationFailed,
    
    /// Rate limit exceeded
    RateLimitExceeded,
    
    /// Model overloaded
    ModelOverloaded,
    
    /// Context length exceeded
    ContextLengthExceeded,
    
    /// Content filtered
    ContentFiltered,
    
    /// Invalid parameters
    InvalidParameters,
    
    /// Server error
    ServerError,
    
    /// Connection error
    ConnectionError,
    
    /// Connection closed
    ConnectionClosed,
    
    /// Timeout
    Timeout,
    
    /// Unknown error
    Unknown,
}

impl fmt::Display for McpErrorCode {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            McpErrorCode::InvalidRequest => write!(f, "invalid_request"),
            McpErrorCode::AuthenticationFailed => write!(f, "authentication_failed"),
            McpErrorCode::AuthorizationFailed => write!(f, "authorization_failed"),
            McpErrorCode::RateLimitExceeded => write!(f, "rate_limit_exceeded"),
            McpErrorCode::ModelOverloaded => write!(f, "model_overloaded"),
            McpErrorCode::ContextLengthExceeded => write!(f, "context_length_exceeded"),
            McpErrorCode::ContentFiltered => write!(f, "content_filtered"),
            McpErrorCode::InvalidParameters => write!(f, "invalid_parameters"),
            McpErrorCode::ServerError => write!(f, "server_error"),
            McpErrorCode::ConnectionError => write!(f, "connection_error"),
            McpErrorCode::ConnectionClosed => write!(f, "connection_closed"),
            McpErrorCode::Timeout => write!(f, "timeout"),
            McpErrorCode::Unknown => write!(f, "unknown"),
        }
    }
}

/// MCP error type
#[derive(Debug, thiserror::Error)]
pub enum McpError {
    #[error("Invalid request: {0}")]
    InvalidRequest(String),
    
    #[error("Authentication failed: {0}")]
    AuthenticationFailed(String),
    
    #[error("Authorization failed: {0}")]
    AuthorizationFailed(String),
    
    #[error("Rate limit exceeded: {0}")]
    RateLimitExceeded(String),
    
    #[error("Model overloaded: {0}")]
    ModelOverloaded(String),
    
    #[error("Context length exceeded: {0}")]
    ContextLengthExceeded(String),
    
    #[error("Content filtered: {0}")]
    ContentFiltered(String),
    
    #[error("Invalid parameters: {0}")]
    InvalidParameters(String),
    
    #[error("Server error: {0}")]
    ServerError(String),
    
    #[error("Protocol error: {0}")]
    ProtocolError(String),
    
    #[error("Connection error: {0}")]
    ConnectionError(String),
    
    #[error("Connection closed")]
    ConnectionClosed,
    
    #[error("Request timed out after {0:?}")]
    Timeout(Duration),
    
    #[error("Serialization error: {0}")]
    SerializationError(String),
    
    #[error("WebSocket error: {0}")]
    WebSocketError(String),
    
    #[error("Unknown error: {0}")]
    Unknown(String),
}

impl From<serde_json::Error> for McpError {
    fn from(err: serde_json::Error) -> Self {
        McpError::SerializationError(err.to_string())
    }
}

impl From<McpErrorCode> for McpError {
    fn from(code: McpErrorCode) -> Self {
        match code {
            McpErrorCode::InvalidRequest => McpError::InvalidRequest("Invalid request".to_string()),
            McpErrorCode::AuthenticationFailed => {
                McpError::AuthenticationFailed("Authentication failed".to_string())
            }
            McpErrorCode::AuthorizationFailed => {
                McpError::AuthorizationFailed("Authorization failed".to_string())
            }
            McpErrorCode::RateLimitExceeded => {
                McpError::RateLimitExceeded("Rate limit exceeded".to_string())
            }
            McpErrorCode::ModelOverloaded => {
                McpError::ModelOverloaded("Model is currently overloaded".to_string())
            }
            McpErrorCode::ContextLengthExceeded => {
                McpError::ContextLengthExceeded("Context length exceeded".to_string())
            }
            McpErrorCode::ContentFiltered => {
                McpError::ContentFiltered("Content was filtered".to_string())
            }
            McpErrorCode::InvalidParameters => {
                McpError::InvalidParameters("Invalid parameters".to_string())
            }
            McpErrorCode::ServerError => McpError::ServerError("Server error".to_string()),
            McpErrorCode::ConnectionError => {
                McpError::ConnectionError("Connection error".to_string())
            }
            McpErrorCode::ConnectionClosed => McpError::ConnectionClosed,
            McpErrorCode::Timeout => McpError::Timeout(Duration::from_secs(60)),
            McpErrorCode::Unknown => McpError::Unknown("Unknown error".to_string()),
        }
    }
}
</file>

<file path="src/protocols/mcp/message.rs">
use crate::protocols::mcp::types::{McpCompletionRequest, McpErrorCode, McpMessageType};
use serde::{Deserialize, Serialize};
use serde_json::Value;

/// Main MCP message structure
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct McpMessage {
    /// Unique message identifier
    pub id: String,
    
    /// Protocol version
    pub version: String,
    
    /// Message type
    #[serde(rename = "type")]
    pub type_: McpMessageType,
    
    /// Message payload (depends on type)
    #[serde(flatten)]
    pub payload: McpMessagePayload,
}

/// MCP message payload - varies based on message type
#[derive(Clone, Debug, Serialize, Deserialize)]
#[serde(untagged)]
pub enum McpMessagePayload {
    /// Completion request payload
    CompletionRequest(McpCompletionRequest),
    
    /// Completion response payload
    #[serde(rename_all = "snake_case")]
    CompletionResponse {
        /// Completion response data
        response: Value,
    },
    
    /// Streaming message payload
    #[serde(rename_all = "snake_case")]
    StreamingMessage {
        /// Streaming ID associated with this message
        streaming_id: String,
        
        /// Chunk of text or data
        chunk: Value,
        
        /// Is this chunk final?
        is_final: bool,
    },
    
    /// Streaming end notification
    #[serde(rename_all = "snake_case")]
    StreamingEnd {
        /// Streaming ID that has ended
        streaming_id: String,
    },
    
    /// Cancel streaming request
    #[serde(rename_all = "snake_case")]
    CancelStream {
        /// Streaming ID to cancel
        streaming_id: String,
    },
    
    /// Error message payload
    #[serde(rename_all = "snake_case")]
    Error {
        /// Request ID that caused the error
        request_id: String,
        
        /// Error code
        code: McpErrorCode,
        
        /// Human-readable error message
        message: String,
        
        /// Additional error details (optional)
        details: Option<Value>,
    },
    
    /// Ping message (heartbeat)
    Ping {},
    
    /// Pong response to ping
    Pong {},
    
    /// Authentication request
    #[serde(rename_all = "snake_case")]
    AuthRequest {
        /// API key
        api_key: String,
        
        /// Organization ID (optional)
        organization_id: Option<String>,
    },
    
    /// Authentication response
    #[serde(rename_all = "snake_case")]
    AuthResponse {
        /// Authentication successful?
        success: bool,
        
        /// Session ID
        session_id: Option<String>,
    },
}

/// Response message types we can receive
#[derive(Debug)]
pub enum McpResponseMessage {
    /// Completion response
    Completion(McpMessage),
    
    /// Streaming message
    Streaming(McpMessage),
    
    /// Authentication response
    Auth(McpMessage),
    
    /// Error response
    Error(McpMessage),
}
</file>

<file path="src/protocols/mcp/mod.rs">
mod client;
mod config;
mod error;
mod message;
mod protocol;
mod types;
mod websocket;

// Export the key components
pub use client::McpClient;
pub use config::McpConfig;
pub use error::McpError;
pub use message::{McpMessage, McpMessagePayload, McpResponseMessage};
pub use protocol::McpProtocolHandler;
pub use types::*;
pub use websocket::WebSocketClient;

use crate::protocols::{ProtocolFactory, ProtocolHandler};
use serde::{Deserialize, Serialize};
use std::sync::Arc;

/// MCP Protocol Factory
pub struct McpProtocolFactory {
    config: McpConfig,
}

impl McpProtocolFactory {
    /// Create a new MCP protocol factory with default configuration
    pub fn new() -> Self {
        Self {
            config: McpConfig::default(),
        }
    }
    
    /// Create a new MCP protocol factory with custom configuration
    pub fn with_config(config: McpConfig) -> Self {
        Self { config }
    }
}

impl ProtocolFactory for McpProtocolFactory {
    fn create_handler(&self) -> Arc<dyn ProtocolHandler> {
        Arc::new(McpProtocolHandler::new(self.config.clone()))
    }
    
    fn protocol_name(&self) -> &'static str {
        "Model Context Protocol"
    }
    
    fn protocol_description(&self) -> &'static str {
        "A protocol for communicating with AI models using WebSockets"
    }
}

/// Implementation for McpProtocolFactory
impl Default for McpProtocolFactory {
    fn default() -> Self {
        Self::new()
    }
}
</file>

<file path="src/protocols/mcp/types.rs">
use serde::{Deserialize, Serialize};
use serde_json::Value;

/// MCP message types
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum McpMessageType {
    /// Completion request
    CompletionRequest,
    
    /// Completion response
    CompletionResponse,
    
    /// Streaming message
    StreamingMessage,
    
    /// Streaming end notification
    StreamingEnd,
    
    /// Cancel streaming request
    CancelStream,
    
    /// Error message
    Error,
    
    /// Ping message (heartbeat)
    Ping,
    
    /// Pong response to ping
    Pong,
    
    /// Authentication request
    AuthRequest,
    
    /// Authentication response
    AuthResponse,
}

/// MCP message roles
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum McpMessageRole {
    /// User message
    User,
    
    /// Assistant (model) message
    Assistant,
    
    /// System message
    System,
    
    /// Tool message
    Tool,
}

impl ToString for McpMessageRole {
    fn to_string(&self) -> String {
        match self {
            McpMessageRole::User => "user".to_string(),
            McpMessageRole::Assistant => "assistant".to_string(),
            McpMessageRole::System => "system".to_string(),
            McpMessageRole::Tool => "tool".to_string(),
        }
    }
}

/// MCP error codes
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum McpErrorCode {
    /// Invalid request format
    InvalidRequest,
    
    /// Authentication failed
    AuthenticationFailed,
    
    /// Authorization failed
    AuthorizationFailed,
    
    /// Rate limit exceeded
    RateLimitExceeded,
    
    /// Model overloaded
    ModelOverloaded,
    
    /// Context length exceeded
    ContextLengthExceeded,
    
    /// Content filtered
    ContentFiltered,
    
    /// Invalid parameters
    InvalidParameters,
    
    /// Server error
    ServerError,
    
    /// Unknown error
    Unknown,
}

/// MCP completion request
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct McpCompletionRequest {
    /// Model to use
    pub model: String,
    
    /// Messages in the conversation
    pub messages: Vec<Value>,
    
    /// Maximum number of tokens to generate
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(default)]
    pub max_tokens: Option<u32>,
    
    /// Temperature for sampling
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>,
    
    /// Top-p sampling
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f32>,
    
    /// Top-k sampling
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_k: Option<u32>,
    
    /// Whether to stream the response
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,
    
    /// Stop sequences
    #[serde(skip_serializing_if = "Vec::is_empty")]
    pub stop_sequences: Vec<String>,
    
    /// System prompt
    #[serde(skip_serializing_if = "Option::is_none")]
    pub system_prompt: Option<String>,
    
    /// Streaming ID for tracking streaming responses
    #[serde(skip_serializing_if = "Option::is_none")]
    pub streaming_id: Option<String>,
}
</file>

<file path="src/protocols/mcp/websocket.rs">
use crate::protocols::mcp::error::McpError;
use futures_util::{SinkExt, StreamExt};
use log::{debug, error, info, warn};
use std::sync::{Arc, Mutex};
use std::time::Duration;
use tokio::net::TcpStream;
use tokio::sync::mpsc::{self, Receiver, Sender};
use tokio::sync::Mutex as AsyncMutex;
use tokio_tungstenite::{
    connect_async, tungstenite::protocol::Message as WsMessage, MaybeTlsStream, WebSocketStream,
};

/// WebSocket client for MCP
pub struct WebSocketClient {
    /// The WebSocket URL
    url: String,
    
    /// Sender half of the channel to send messages to WebSocket
    tx: Arc<Mutex<Option<Sender<String>>>>,
    
    /// WebSocket connection
    ws_stream: Arc<AsyncMutex<Option<WebSocketStream<MaybeTlsStream<TcpStream>>>>>,
    
    /// Internal message receiver
    rx: Arc<AsyncMutex<Option<Receiver<String>>>>,
}

impl WebSocketClient {
    /// Create a new WebSocket client
    pub fn new(url: &str) -> Self {
        Self {
            url: url.to_string(),
            tx: Arc::new(Mutex::new(None)),
            ws_stream: Arc::new(AsyncMutex::new(None)),
            rx: Arc::new(AsyncMutex::new(None)),
        }
    }
    
    /// Connect to the WebSocket server
    pub async fn connect(&self) -> Result<(), McpError> {
        let url = url::Url::parse(&self.url)
            .map_err(|e| McpError::ConnectionError(format!("Invalid URL: {}", e)))?;
        
        // Connect to the WebSocket server
        let (ws_stream, _) = connect_async(url)
            .await
            .map_err(|e| McpError::ConnectionError(format!("Failed to connect: {}", e)))?;
        
        // Create a channel for sending messages
        let (tx, rx) = mpsc::channel(32);
        
        {
            let mut tx_guard = self.tx.lock().unwrap();
            *tx_guard = Some(tx);
        }
        
        {
            let mut rx_guard = self.rx.lock().await;
            *rx_guard = Some(rx);
        }
        
        // Store the WebSocket stream
        {
            let mut ws_guard = self.ws_stream.lock().await;
            *ws_guard = Some(ws_stream);
        }
        
        // Spawn message sender task
        let ws_clone = self.ws_stream.clone();
        let rx_clone = self.rx.clone();
        
        tokio::spawn(async move {
            let mut rx_guard = rx_clone.lock().await;
            if let Some(mut rx) = rx_guard.take() {
                while let Some(message) = rx.recv().await {
                    let mut ws_guard = ws_clone.lock().await;
                    if let Some(ws) = ws_guard.as_mut() {
                        if let Err(e) = ws.send(WsMessage::Text(message)).await {
                            error!("Failed to send WebSocket message: {}", e);
                            break;
                        }
                    } else {
                        break;
                    }
                }
            }
        });
        
        Ok(())
    }
    
    /// Disconnect from the WebSocket server
    pub async fn disconnect(&self) -> Result<(), McpError> {
        let mut ws_guard = self.ws_stream.lock().await;
        if let Some(mut ws) = ws_guard.take() {
            // Send close message
            if let Err(e) = ws.close(None).await {
                warn!("Error while closing WebSocket: {}", e);
            }
        }
        
        // Drop the sender to terminate the message sender task
        let mut tx_guard = self.tx.lock().unwrap();
        *tx_guard = None;
        
        Ok(())
    }
    
    /// Send a message to the WebSocket server
    pub async fn send(&self, message: String) -> Result<(), McpError> {
        let tx_guard = self.tx.lock().unwrap();
        if let Some(tx) = tx_guard.as_ref() {
            tx.send(message)
                .await
                .map_err(|e| McpError::WebSocketError(format!("Failed to send message: {}", e)))?;
            Ok(())
        } else {
            Err(McpError::ConnectionClosed)
        }
    }
    
    /// Receive a message from the WebSocket server
    pub async fn receive(&self) -> Result<String, McpError> {
        let mut ws_guard = self.ws_stream.lock().await;
        if let Some(ws) = ws_guard.as_mut() {
            match ws.next().await {
                Some(Ok(WsMessage::Text(text))) => Ok(text),
                Some(Ok(WsMessage::Binary(bin))) => {
                    String::from_utf8(bin)
                        .map_err(|e| McpError::ProtocolError(format!("Invalid UTF-8: {}", e)))
                }
                Some(Ok(WsMessage::Ping(_))) => {
                    // Respond with pong - many WebSocket libraries handle this automatically
                    Err(McpError::ProtocolError("Received ping message".to_string()))
                }
                Some(Ok(WsMessage::Pong(_))) => {
                    // Respond to pong - usually a response to our ping
                    Err(McpError::ProtocolError("Received pong message".to_string()))
                }
                Some(Ok(WsMessage::Close(_))) => Err(McpError::ConnectionClosed),
                Some(Err(e)) => Err(McpError::WebSocketError(e.to_string())),
                None => Err(McpError::ConnectionClosed),
            }
        } else {
            Err(McpError::ConnectionClosed)
        }
    }
    
    /// Check if the WebSocket is connected
    pub async fn is_connected(&self) -> bool {
        let ws_guard = self.ws_stream.lock().await;
        ws_guard.is_some()
    }
}
</file>

<file path="src/protocols/mod.rs">
pub mod mcp;

use crate::models::messages::{Message, MessageError};
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::sync::Arc;

/// Protocol status information
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum ConnectionStatus {
    /// Not connected to any server
    Disconnected,
    
    /// Currently establishing connection
    Connecting,
    
    /// Connected and authenticated
    Connected,
    
    /// Connection established but authentication failed
    AuthFailed,
    
    /// Connection dropped or experiencing issues
    ConnectionError(String),
}

/// Base trait for all protocol handlers
#[async_trait]
pub trait ProtocolHandler: Send + Sync {
    /// Returns the protocol name
    fn protocol_name(&self) -> &'static str;
    
    /// Returns the current connection status
    fn connection_status(&self) -> ConnectionStatus;
    
    /// Establishes connection to the server
    async fn connect(&self) -> Result<(), String>;
    
    /// Disconnects from the server
    async fn disconnect(&self) -> Result<(), String>;
    
    /// Sends a message to the server
    async fn send_message(&self, message: Message) -> Result<(), MessageError>;
    
    /// Receives messages from the server
    async fn receive_messages(&self) -> Result<Vec<Message>, MessageError>;
    
    /// Checks if the handler is connected
    fn is_connected(&self) -> bool {
        matches!(self.connection_status(), ConnectionStatus::Connected)
    }
}

/// Factory for creating protocol handlers
pub trait ProtocolFactory: Send + Sync {
    /// Creates a new protocol handler instance
    fn create_handler(&self) -> Arc<dyn ProtocolHandler>;
    
    /// Returns the protocol name
    fn protocol_name(&self) -> &'static str;
    
    /// Returns the protocol description
    fn protocol_description(&self) -> &'static str;
}

/// Protocol configuration base trait
pub trait ProtocolConfig: Send + Sync {
    /// Validates the configuration
    fn validate(&self) -> Result<(), String>;
}
</file>

<file path="src/security/credentials.rs">
// Secure Credential Storage for MCP Client
//
// This module provides secure credential storage using platform-specific secure enclaves:
// - On Windows: Windows Credential Manager (via wincred)
// - On macOS: Keychain (via keychain-rs)
// - On Linux: Secret Service API (via secret-service-rs)
// - Fallback: Encrypted local storage with obfuscation

use std::collections::HashMap;
use std::fs;
use std::path::PathBuf;
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, Instant, SystemTime};

use log::{debug, info, warn, error};
use serde::{Serialize, Deserialize};

use crate::config::config_path;
use crate::error::Result;
use crate::observability::metrics::{record_counter, record_gauge};
use crate::utils::security::{encrypt, decrypt};

#[cfg(target_os = "windows")]
use windows_credentials::{Credential, CredentialPersistence};

#[cfg(target_os = "macos")]
use keychain::{Keychain, KeychainItem, ItemClass};

#[cfg(all(unix, not(target_os = "macos")))]
use secret_service::{EncryptionType, SecretService, Collection};

// Constants for fallback storage
const CREDENTIALS_FILE: &str = "credentials.enc";
const SERVICE_NAME: &str = "mcp-client";
const MAX_CACHED_CREDENTIALS: usize = 100;

/// Serializable credential entry for fallback storage
#[derive(Debug, Clone, Serialize, Deserialize)]
struct CredentialEntry {
    /// Key/Name of the credential
    key: String,
    
    /// Encrypted value
    encrypted_value: String,
    
    /// When the credential was last modified
    modified: SystemTime,
}

/// Memory-cached credential for performance
struct CachedCredential {
    /// The decrypted value
    value: String,
    
    /// When the credential was cached
    cached_at: Instant,
    
    /// Whether this credential has been modified and needs to be saved
    modified: bool,
}

/// Credential Manager for the MCP client
pub struct CredentialManager {
    /// Whether to use the platform secure enclave
    use_secure_enclave: bool,
    
    /// How long to cache credentials in memory (in seconds)
    cache_duration: u64,
    
    /// In-memory cache of credentials
    credential_cache: Arc<RwLock<HashMap<String, CachedCredential>>>,
    
    /// Last time credential file was loaded
    last_load: Arc<Mutex<Instant>>,
    
    /// Lock for file operations
    file_lock: Arc<Mutex<()>>,
}

impl CredentialManager {
    /// Create a new credential manager
    pub fn new(use_secure_enclave: bool, cache_duration: u64) -> Result<Self> {
        Ok(Self {
            use_secure_enclave,
            cache_duration,
            credential_cache: Arc::new(RwLock::new(HashMap::new())),
            last_load: Arc::new(Mutex::new(Instant::now())),
            file_lock: Arc::new(Mutex::new(())),
        })
    }
    
    /// Start the credential service
    pub fn start_service(&self) -> Result<()> {
        // Initialize platform-specific keychain access if needed
        if self.use_secure_enclave {
            self.initialize_secure_enclave()?;
        }
        
        // Create credentials file for fallback storage if needed
        if !self.use_secure_enclave {
            let credentials_path = config_path(CREDENTIALS_FILE);
            if !credentials_path.exists() {
                // Create empty credentials file
                fs::write(&credentials_path, encrypt("{}").map_err(|e| format!("Failed to encrypt credentials: {}", e))?)
                    .map_err(|e| format!("Failed to create credentials file: {}", e))?;
                    
                info!("Created fallback credentials file");
            }
        }
        
        // Set up credential expiration monitor
        let cache_duration = self.cache_duration;
        let credential_cache = self.credential_cache.clone();
        
        std::thread::spawn(move || {
            loop {
                std::thread::sleep(Duration::from_secs(60));
                
                // Check for expired credentials
                let mut cache = credential_cache.write().unwrap();
                let now = Instant::now();
                
                // Find keys to remove
                let expired_keys: Vec<String> = cache.iter()
                    .filter(|(_, cached)| now.duration_since(cached.cached_at) > Duration::from_secs(cache_duration))
                    .filter(|(_, cached)| !cached.modified) // Don't remove modified credentials
                    .map(|(key, _)| key.clone())
                    .collect();
                
                // Remove expired keys
                for key in expired_keys {
                    cache.remove(&key);
                }
                
                // Update metric for cached credentials
                record_gauge("security.credentials.cached_count", cache.len() as f64, None);
            }
        });
        
        info!("Credential service started in {} mode", 
            if self.use_secure_enclave { "secure enclave" } else { "fallback" });
        
        record_counter("security.credentials.service_started", 1.0, None);
        
        Ok(())
    }
    
    /// Initialize platform-specific secure enclave access
    fn initialize_secure_enclave(&self) -> Result<()> {
        if !self.use_secure_enclave {
            return Ok(());
        }
        
        #[cfg(target_os = "windows")]
        {
            // Windows doesn't need initialization for the Credential Manager
            debug!("Initialized Windows Credential Manager");
        }
        
        #[cfg(target_os = "macos")]
        {
            // Test keychain access
            match Keychain::default() {
                Ok(_) => debug!("Initialized macOS Keychain"),
                Err(e) => {
                    warn!("Failed to access macOS Keychain: {}", e);
                    return Err(format!("Failed to access macOS Keychain: {}", e).into());
                }
            }
        }
        
        #[cfg(all(unix, not(target_os = "macos")))]
        {
            // Test Secret Service access
            match SecretService::new(EncryptionType::Dh) {
                Ok(_) => debug!("Initialized Secret Service API"),
                Err(e) => {
                    warn!("Failed to access Secret Service API: {}", e);
                    return Err(format!("Failed to access Secret Service API: {}", e).into());
                }
            }
        }
        
        Ok(())
    }
    
    /// Update configuration
    pub fn update_config(&self, use_secure_enclave: bool, cache_duration: u64) -> Result<()> {
        // If switching to secure enclave mode, initialize it
        if use_secure_enclave && !self.use_secure_enclave {
            self.initialize_secure_enclave()?;
        }
        
        // Update credentials storage if mode changed
        if use_secure_enclave != self.use_secure_enclave {
            // Get all credentials from current storage
            let credentials = self.get_all_credentials()?;
            
            // Update enclave setting
            let mut this = unsafe { &mut *(self as *const Self as *mut Self) };
            this.use_secure_enclave = use_secure_enclave;
            
            // Store all credentials in new storage
            for (key, value) in credentials {
                self.store_credential_internal(&key, &value)?;
            }
            
            info!("Migrated {} credentials to {} storage", 
                credentials.len(),
                if use_secure_enclave { "secure enclave" } else { "fallback" });
        }
        
        // Update cache duration (affects the cache cleanup thread)
        let mut this = unsafe { &mut *(self as *const Self as *mut Self) };
        this.cache_duration = cache_duration;
        
        Ok(())
    }
    
    /// Get all credential keys and values
    fn get_all_credentials(&self) -> Result<HashMap<String, String>> {
        let mut result = HashMap::new();
        
        if self.use_secure_enclave {
            // Platform-specific implementation to get all credentials
            #[cfg(target_os = "windows")]
            {
                // Windows doesn't have a good API for listing all credentials
                // We'll need to have stored a list of keys separately
                warn!("Windows Credential Manager doesn't support listing all credentials");
            }
            
            #[cfg(target_os = "macos")]
            {
                // macOS Keychain
                let keychain = Keychain::default()
                    .map_err(|e| format!("Failed to access keychain: {}", e))?;
                
                // We need to have stored a list of keys separately
                warn!("macOS Keychain doesn't support listing all credentials");
            }
            
            #[cfg(all(unix, not(target_os = "macos")))]
            {
                // Secret Service API
                let ss = SecretService::new(EncryptionType::Dh)
                    .map_err(|e| format!("Failed to access Secret Service: {}", e))?;
                
                let collection = ss.get_default_collection()
                    .map_err(|e| format!("Failed to get default collection: {}", e))?;
                
                collection.unlock()
                    .map_err(|e| format!("Failed to unlock collection: {}", e))?;
                
                let items = collection.get_all_items()
                    .map_err(|e| format!("Failed to get items: {}", e))?;
                
                for item in items {
                    // Only get items for our service
                    let attributes = item.get_attributes()
                        .map_err(|e| format!("Failed to get item attributes: {}", e))?;
                    
                    let service = attributes.get("service");
                    if service != Some(&SERVICE_NAME.to_string()) {
                        continue;
                    }
                    
                    if let Some(key) = attributes.get("key") {
                        let secret = item.get_secret()
                            .map_err(|e| format!("Failed to get secret: {}", e))?;
                        
                        let value = String::from_utf8(secret)
                            .map_err(|_| "Invalid UTF-8 in secret".to_string())?;
                        
                        result.insert(key.clone(), value);
                    }
                }
            }
        } else {
            // Fallback storage
            let credentials_path = config_path(CREDENTIALS_FILE);
            
            if credentials_path.exists() {
                let encrypted_data = fs::read_to_string(&credentials_path)
                    .map_err(|e| format!("Failed to read credentials file: {}", e))?;
                
                let decrypted_data = decrypt(encrypted_data.as_bytes())
                    .map_err(|e| format!("Failed to decrypt credentials: {}", e))?;
                
                let entries: HashMap<String, CredentialEntry> = serde_json::from_str(&decrypted_data)
                    .map_err(|e| format!("Failed to parse credentials: {}", e))?;
                
                for (_, entry) in entries {
                    let decrypted = decrypt(entry.encrypted_value.as_bytes())
                        .map_err(|e| format!("Failed to decrypt credential value: {}", e))?;
                    
                    result.insert(entry.key, decrypted);
                }
            }
        }
        
        Ok(result)
    }
    
    /// Store a credential securely
    pub fn store_credential(&self, key: &str, value: &str) -> Result<()> {
        // Update cache
        {
            let mut cache = self.credential_cache.write().unwrap();
            
            // Check cache size limit
            if !cache.contains_key(key) && cache.len() >= MAX_CACHED_CREDENTIALS {
                // Find the oldest non-modified credential to remove
                if let Some(oldest_key) = cache.iter()
                    .filter(|(_, cached)| !cached.modified)
                    .min_by_key(|(_, cached)| cached.cached_at)
                    .map(|(key, _)| key.clone())
                {
                    cache.remove(&oldest_key);
                }
            }
            
            cache.insert(key.to_string(), CachedCredential {
                value: value.to_string(),
                cached_at: Instant::now(),
                modified: true,
            });
        }
        
        // Store in platform-specific secure storage
        self.store_credential_internal(key, value)?;
        
        // Mark as no longer modified in cache
        {
            let mut cache = self.credential_cache.write().unwrap();
            if let Some(cached) = cache.get_mut(key) {
                cached.modified = false;
            }
        }
        
        record_counter("security.credentials.stored", 1.0, None);
        
        Ok(())
    }
    
    /// Internal implementation of credential storage
    fn store_credential_internal(&self, key: &str, value: &str) -> Result<()> {
        if self.use_secure_enclave {
            // Platform-specific implementation
            #[cfg(target_os = "windows")]
            {
                // Windows Credential Manager
                let credential = Credential {
                    target_name: format!("{}:{}", SERVICE_NAME, key),
                    generic_password: value.as_bytes().to_vec(),
                    ..Credential::default()
                };
                
                credential.write()
                    .map_err(|e| format!("Failed to write to Windows Credential Manager: {}", e))?;
            }
            
            #[cfg(target_os = "macos")]
            {
                // macOS Keychain
                let keychain = Keychain::default()
                    .map_err(|e| format!("Failed to access keychain: {}", e))?;
                
                // Check if item exists
                let item_res = keychain.find_internet_password(
                    Some(SERVICE_NAME),
                    Some(key),
                    None,
                    None,
                    0,
                    ItemClass::InternetPassword,
                    None,
                );
                
                match item_res {
                    Ok(mut item) => {
                        // Update existing item
                        item.set_password(value)
                            .map_err(|e| format!("Failed to update keychain item: {}", e))?;
                    },
                    Err(_) => {
                        // Create new item
                        keychain.add_internet_password(
                            SERVICE_NAME,
                            key,
                            "",  // path
                            "",  // server
                            0,   // port
                            None, // auth type
                            None, // protocol
                            value,
                        ).map_err(|e| format!("Failed to add keychain item: {}", e))?;
                    }
                }
            }
            
            #[cfg(all(unix, not(target_os = "macos")))]
            {
                // Secret Service API
                let ss = SecretService::new(EncryptionType::Dh)
                    .map_err(|e| format!("Failed to access Secret Service: {}", e))?;
                
                let collection = ss.get_default_collection()
                    .map_err(|e| format!("Failed to get default collection: {}", e))?;
                
                collection.unlock()
                    .map_err(|e| format!("Failed to unlock collection: {}", e))?;
                
                // Build attributes
                let mut attributes = HashMap::new();
                attributes.insert("service".to_string(), SERVICE_NAME.to_string());
                attributes.insert("key".to_string(), key.to_string());
                
                // Search for existing item
                let search = collection.search_items(attributes.clone())
                    .map_err(|e| format!("Failed to search for items: {}", e))?;
                
                if let Some(item) = search.first() {
                    // Update existing item
                    item.set_secret(value.as_bytes())
                        .map_err(|e| format!("Failed to update secret: {}", e))?;
                } else {
                    // Create new item
                    collection.create_item(
                        format!("{} - {}", SERVICE_NAME, key),
                        attributes,
                        value.as_bytes(),
                        true, // replace if exists
                        "text/plain",
                    ).map_err(|e| format!("Failed to create secret: {}", e))?;
                }
            }
        } else {
            // Fallback to encrypted file
            let _lock = self.file_lock.lock().unwrap();
            let credentials_path = config_path(CREDENTIALS_FILE);
            
            // Encrypt the value
            let encrypted_value = encrypt(value.as_bytes())
                .map_err(|e| format!("Failed to encrypt value: {}", e))?;
            
            // Read current credentials
            let mut entries: HashMap<String, CredentialEntry> = if credentials_path.exists() {
                let encrypted_data = fs::read_to_string(&credentials_path)
                    .map_err(|e| format!("Failed to read credentials file: {}", e))?;
                
                let decrypted_data = decrypt(encrypted_data.as_bytes())
                    .map_err(|e| format!("Failed to decrypt credentials: {}", e))?;
                
                serde_json::from_str(&decrypted_data)
                    .map_err(|e| format!("Failed to parse credentials: {}", e))?
            } else {
                HashMap::new()
            };
            
            // Add or update entry
            entries.insert(key.to_string(), CredentialEntry {
                key: key.to_string(),
                encrypted_value: String::from_utf8(encrypted_value)
                    .map_err(|_| "Invalid UTF-8 in encrypted value".to_string())?,
                modified: SystemTime::now(),
            });
            
            // Write back to file
            let serialized = serde_json::to_string(&entries)
                .map_err(|e| format!("Failed to serialize credentials: {}", e))?;
            
            let encrypted_data = encrypt(serialized.as_bytes())
                .map_err(|e| format!("Failed to encrypt credentials: {}", e))?;
            
            fs::write(&credentials_path, encrypted_data)
                .map_err(|e| format!("Failed to write credentials file: {}", e))?;
        }
        
        Ok(())
    }
    
    /// Retrieve a credential
    pub fn get_credential(&self, key: &str) -> Result<String> {
        // Check cache first
        {
            let cache = self.credential_cache.read().unwrap();
            if let Some(cached) = cache.get(key) {
                if cached.modified || cached.cached_at.elapsed() < Duration::from_secs(self.cache_duration) {
                    return Ok(cached.value.clone());
                }
            }
        }
        
        // Not in cache or expired, fetch from storage
        let value = self.get_credential_internal(key)?;
        
        // Update cache
        {
            let mut cache = self.credential_cache.write().unwrap();
            
            // Check cache size limit
            if !cache.contains_key(key) && cache.len() >= MAX_CACHED_CREDENTIALS {
                // Find the oldest non-modified credential to remove
                if let Some(oldest_key) = cache.iter()
                    .filter(|(_, cached)| !cached.modified)
                    .min_by_key(|(_, cached)| cached.cached_at)
                    .map(|(key, _)| key.clone())
                {
                    cache.remove(&oldest_key);
                }
            }
            
            cache.insert(key.to_string(), CachedCredential {
                value: value.clone(),
                cached_at: Instant::now(),
                modified: false,
            });
        }
        
        record_counter("security.credentials.retrieved", 1.0, None);
        
        Ok(value)
    }
    
    /// Internal implementation of credential retrieval
    fn get_credential_internal(&self, key: &str) -> Result<String> {
        if self.use_secure_enclave {
            // Platform-specific implementation
            #[cfg(target_os = "windows")]
            {
                // Windows Credential Manager
                let credential = Credential::get(
                    &format!("{}:{}", SERVICE_NAME, key),
                    false, // all_credentials
                ).map_err(|e| format!("Failed to read from Windows Credential Manager: {}", e))?;
                
                Ok(String::from_utf8(credential.generic_password)
                    .map_err(|_| "Invalid UTF-8 in credential".to_string())?)
            }
            
            #[cfg(target_os = "macos")]
            {
                // macOS Keychain
                let keychain = Keychain::default()
                    .map_err(|e| format!("Failed to access keychain: {}", e))?;
                
                let (_item, password) = keychain.find_internet_password(
                    Some(SERVICE_NAME),
                    Some(key),
                    None,
                    None,
                    0,
                    ItemClass::InternetPassword,
                    None,
                ).map_err(|e| format!("Failed to find keychain item: {}", e))?;
                
                Ok(password)
            }
            
            #[cfg(all(unix, not(target_os = "macos")))]
            {
                // Secret Service API
                let ss = SecretService::new(EncryptionType::Dh)
                    .map_err(|e| format!("Failed to access Secret Service: {}", e))?;
                
                let collection = ss.get_default_collection()
                    .map_err(|e| format!("Failed to get default collection: {}", e))?;
                
                collection.unlock()
                    .map_err(|e| format!("Failed to unlock collection: {}", e))?;
                
                // Build attributes
                let mut attributes = HashMap::new();
                attributes.insert("service".to_string(), SERVICE_NAME.to_string());
                attributes.insert("key".to_string(), key.to_string());
                
                // Search for existing item
                let search = collection.search_items(attributes)
                    .map_err(|e| format!("Failed to search for items: {}", e))?;
                
                if let Some(item) = search.first() {
                    let secret = item.get_secret()
                        .map_err(|e| format!("Failed to get secret: {}", e))?;
                    
                    Ok(String::from_utf8(secret)
                        .map_err(|_| "Invalid UTF-8 in secret".to_string())?)
                } else {
                    Err(format!("Credential '{}' not found", key).into())
                }
            }
            
            #[cfg(not(any(target_os = "windows", target_os = "macos", unix)))]
            {
                Err("Secure enclave not supported on this platform".into())
            }
        } else {
            // Fallback to encrypted file
            let credentials_path = config_path(CREDENTIALS_FILE);
            
            if !credentials_path.exists() {
                return Err(format!("Credential '{}' not found", key).into());
            }
            
            let encrypted_data = fs::read_to_string(&credentials_path)
                .map_err(|e| format!("Failed to read credentials file: {}", e))?;
            
            let decrypted_data = decrypt(encrypted_data.as_bytes())
                .map_err(|e| format!("Failed to decrypt credentials: {}", e))?;
            
            let entries: HashMap<String, CredentialEntry> = serde_json::from_str(&decrypted_data)
                .map_err(|e| format!("Failed to parse credentials: {}", e))?;
            
            if let Some(entry) = entries.get(key) {
                let decrypted = decrypt(entry.encrypted_value.as_bytes())
                    .map_err(|e| format!("Failed to decrypt credential value: {}", e))?;
                
                Ok(decrypted)
            } else {
                Err(format!("Credential '{}' not found", key).into())
            }
        }
    }
    
    /// Delete a credential
    pub fn delete_credential(&self, key: &str) -> Result<()> {
        // Remove from cache
        {
            let mut cache = self.credential_cache.write().unwrap();
            cache.remove(key);
        }
        
        if self.use_secure_enclave {
            // Platform-specific implementation
            #[cfg(target_os = "windows")]
            {
                // Windows Credential Manager
                Credential::delete(
                    &format!("{}:{}", SERVICE_NAME, key),
                    false, // all_credentials
                ).map_err(|e| format!("Failed to delete from Windows Credential Manager: {}", e))?;
            }
            
            #[cfg(target_os = "macos")]
            {
                // macOS Keychain
                let keychain = Keychain::default()
                    .map_err(|e| format!("Failed to access keychain: {}", e))?;
                
                let (item, _) = keychain.find_internet_password(
                    Some(SERVICE_NAME),
                    Some(key),
                    None,
                    None,
                    0,
                    ItemClass::InternetPassword,
                    None,
                ).map_err(|e| format!("Failed to find keychain item: {}", e))?;
                
                item.delete()
                    .map_err(|e| format!("Failed to delete keychain item: {}", e))?;
            }
            
            #[cfg(all(unix, not(target_os = "macos")))]
            {
                // Secret Service API
                let ss = SecretService::new(EncryptionType::Dh)
                    .map_err(|e| format!("Failed to access Secret Service: {}", e))?;
                
                let collection = ss.get_default_collection()
                    .map_err(|e| format!("Failed to get default collection: {}", e))?;
                
                collection.unlock()
                    .map_err(|e| format!("Failed to unlock collection: {}", e))?;
                
                // Build attributes
                let mut attributes = HashMap::new();
                attributes.insert("service".to_string(), SERVICE_NAME.to_string());
                attributes.insert("key".to_string(), key.to_string());
                
                // Search for existing item
                let search = collection.search_items(attributes)
                    .map_err(|e| format!("Failed to search for items: {}", e))?;
                
                if let Some(item) = search.first() {
                    item.delete()
                        .map_err(|e| format!("Failed to delete secret: {}", e))?;
                } else {
                    return Err(format!("Credential '{}' not found", key).into());
                }
            }
        } else {
            // Fallback to encrypted file
            let _lock = self.file_lock.lock().unwrap();
            let credentials_path = config_path(CREDENTIALS_FILE);
            
            if !credentials_path.exists() {
                return Err(format!("Credential '{}' not found", key).into());
            }
            
            let encrypted_data = fs::read_to_string(&credentials_path)
                .map_err(|e| format!("Failed to read credentials file: {}", e))?;
            
            let decrypted_data = decrypt(encrypted_data.as_bytes())
                .map_err(|e| format!("Failed to decrypt credentials: {}", e))?;
            
            let mut entries: HashMap<String, CredentialEntry> = serde_json::from_str(&decrypted_data)
                .map_err(|e| format!("Failed to parse credentials: {}", e))?;
            
            if entries.remove(key).is_none() {
                return Err(format!("Credential '{}' not found", key).into());
            }
            
            // Write back to file
            let serialized = serde_json::to_string(&entries)
                .map_err(|e| format!("Failed to serialize credentials: {}", e))?;
            
            let encrypted_data = encrypt(serialized.as_bytes())
                .map_err(|e| format!("Failed to encrypt credentials: {}", e))?;
            
            fs::write(&credentials_path, encrypted_data)
                .map_err(|e| format!("Failed to write credentials file: {}", e))?;
        }
        
        record_counter("security.credentials.deleted", 1.0, None);
        
        Ok(())
    }
    
    /// List all credential keys
    pub fn list_credential_keys(&self) -> Result<Vec<String>> {
        if self.use_secure_enclave {
            // Platform-specific implementation
            #[cfg(target_os = "windows")]
            {
                // Windows doesn't have a good API for listing all credentials
                warn!("Windows Credential Manager doesn't support listing all credentials");
                return Ok(Vec::new());
            }
            
            #[cfg(target_os = "macos")]
            {
                // macOS Keychain doesn't have a good API for listing all items of a certain kind
                warn!("macOS Keychain doesn't support listing all credentials");
                return Ok(Vec::new());
            }
            
            #[cfg(all(unix, not(target_os = "macos")))]
            {
                // Secret Service API
                let ss = SecretService::new(EncryptionType::Dh)
                    .map_err(|e| format!("Failed to access Secret Service: {}", e))?;
                
                let collection = ss.get_default_collection()
                    .map_err(|e| format!("Failed to get default collection: {}", e))?;
                
                collection.unlock()
                    .map_err(|e| format!("Failed to unlock collection: {}", e))?;
                
                // Build attributes for search
                let mut attributes = HashMap::new();
                attributes.insert("service".to_string(), SERVICE_NAME.to_string());
                
                // Search for items
                let search = collection.search_items(attributes)
                    .map_err(|e| format!("Failed to search for items: {}", e))?;
                
                let mut keys = Vec::new();
                for item in search {
                    let item_attrs = item.get_attributes()
                        .map_err(|e| format!("Failed to get item attributes: {}", e))?;
                    
                    if let Some(key) = item_attrs.get("key") {
                        keys.push(key.clone());
                    }
                }
                
                return Ok(keys);
            }
            
            #[cfg(not(any(target_os = "windows", target_os = "macos", unix)))]
            {
                warn!("Secure enclave not supported on this platform");
                return Ok(Vec::new());
            }
        } else {
            // Fallback to encrypted file
            let credentials_path = config_path(CREDENTIALS_FILE);
            
            if !credentials_path.exists() {
                return Ok(Vec::new());
            }
            
            let encrypted_data = fs::read_to_string(&credentials_path)
                .map_err(|e| format!("Failed to read credentials file: {}", e))?;
            
            let decrypted_data = decrypt(encrypted_data.as_bytes())
                .map_err(|e| format!("Failed to decrypt credentials: {}", e))?;
            
            let entries: HashMap<String, CredentialEntry> = serde_json::from_str(&decrypted_data)
                .map_err(|e| format!("Failed to parse credentials: {}", e))?;
            
            Ok(entries.keys().cloned().collect())
        }
    }
    
    /// Check if a credential exists
    pub fn has_credential(&self, key: &str) -> Result<bool> {
        // Check cache first
        {
            let cache = self.credential_cache.read().unwrap();
            if cache.contains_key(key) {
                return Ok(true);
            }
        }
        
        if self.use_secure_enclave {
            // Platform-specific implementation
            #[cfg(target_os = "windows")]
            {
                // Windows Credential Manager
                match Credential::get(
                    &format!("{}:{}", SERVICE_NAME, key),
                    false, // all_credentials
                ) {
                    Ok(_) => Ok(true),
                    Err(_) => Ok(false),
                }
            }
            
            #[cfg(target_os = "macos")]
            {
                // macOS Keychain
                let keychain = Keychain::default()
                    .map_err(|e| format!("Failed to access keychain: {}", e))?;
                
                match keychain.find_internet_password(
                    Some(SERVICE_NAME),
                    Some(key),
                    None,
                    None,
                    0,
                    ItemClass::InternetPassword,
                    None,
                ) {
                    Ok(_) => Ok(true),
                    Err(_) => Ok(false),
                }
            }
            
            #[cfg(all(unix, not(target_os = "macos")))]
            {
                // Secret Service API
                let ss = SecretService::new(EncryptionType::Dh)
                    .map_err(|e| format!("Failed to access Secret Service: {}", e))?;
                
                let collection = ss.get_default_collection()
                    .map_err(|e| format!("Failed to get default collection: {}", e))?;
                
                collection.unlock()
                    .map_err(|e| format!("Failed to unlock collection: {}", e))?;
                
                // Build attributes
                let mut attributes = HashMap::new();
                attributes.insert("service".to_string(), SERVICE_NAME.to_string());
                attributes.insert("key".to_string(), key.to_string());
                
                // Search for existing item
                let search = collection.search_items(attributes)
                    .map_err(|e| format!("Failed to search for items: {}", e))?;
                
                Ok(!search.is_empty())
            }
            
            #[cfg(not(any(target_os = "windows", target_os = "macos", unix)))]
            {
                Err("Secure enclave not supported on this platform".into())
            }
        } else {
            // Fallback to encrypted file
            let credentials_path = config_path(CREDENTIALS_FILE);
            
            if !credentials_path.exists() {
                return Ok(false);
            }
            
            let encrypted_data = fs::read_to_string(&credentials_path)
                .map_err(|e| format!("Failed to read credentials file: {}", e))?;
            
            let decrypted_data = decrypt(encrypted_data.as_bytes())
                .map_err(|e| format!("Failed to decrypt credentials: {}", e))?;
            
            let entries: HashMap<String, CredentialEntry> = serde_json::from_str(&decrypted_data)
                .map_err(|e| format!("Failed to parse credentials: {}", e))?;
            
            Ok(entries.contains_key(key))
        }
    }
}
</file>

<file path="src/security/e2ee.rs">
// End-to-End Encryption for MCP Client
//
// This module provides end-to-end encryption for data synchronization between devices.
// It uses:
// - X25519 for key exchange (via the 'ring' crate)
// - ChaCha20-Poly1305 for authenticated encryption (via the 'ring' crate)
// - HKDF for key derivation
// - A double ratchet algorithm for forward secrecy

use std::collections::HashMap;
use std::fs;
use std::path::PathBuf;
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, SystemTime};

use log::{debug, info, warn, error};
use ring::aead::{Aad, BoundKey, Nonce, NonceSequence, OpeningKey, SealingKey, UnboundKey, CHACHA20_POLY1305};
use ring::agreement::{EphemeralPrivateKey, PublicKey, UnparsedPublicKey, X25519};
use ring::digest;
use ring::hkdf;
use ring::rand::{SecureRandom, SystemRandom};
use serde::{Serialize, Deserialize};

use crate::config::config_path;
use crate::error::Result;
use crate::observability::metrics::{record_counter, record_gauge, record_histogram};

const IDENTITY_KEY_FILE: &str = "identity_key.bin";
const SIGNED_PRE_KEY_FILE: &str = "signed_pre_key.bin";
const ONE_TIME_KEYS_DIR: &str = "one_time_keys";
const SESSION_KEYS_DIR: &str = "session_keys";
const KEY_SIZE: usize = 32;
const NONCE_SIZE: usize = 12;
const CHAIN_KEY_SEED: &[u8] = b"MCP-E2EE-ChainKey";
const MESSAGE_KEY_SEED: &[u8] = b"MCP-E2EE-MessageKey";
const RATCHET_SEED: &[u8] = b"MCP-E2EE-Ratchet";

/// Encryption keys for a device
#[derive(Debug, Clone, Serialize, Deserialize)]
struct DeviceKeys {
    /// The device ID
    device_id: String,
    
    /// Public identity key
    public_identity_key: Vec<u8>,
    
    /// Public signed pre-key
    public_signed_pre_key: Vec<u8>,
    
    /// One-time pre-keys (public keys)
    one_time_pre_keys: Vec<Vec<u8>>,
    
    /// Last key update timestamp
    last_update: SystemTime,
}

/// Session state for communicating with a device
#[derive(Debug, Clone, Serialize, Deserialize)]
struct SessionState {
    /// The device ID we're communicating with
    device_id: String,
    
    /// Root key for the double ratchet
    root_key: Vec<u8>,
    
    /// Chain key for sending
    send_chain_key: Vec<u8>,
    
    /// Chain key for receiving
    receive_chain_key: Vec<u8>,
    
    /// Current sending ratchet public key
    public_ratchet_key: Vec<u8>,
    
    /// Current sending ratchet private key (encrypted)
    private_ratchet_key: Vec<u8>,
    
    /// Public ratchet key for remote device
    remote_public_ratchet_key: Vec<u8>,
    
    /// Message number for sending
    send_message_number: u32,
    
    /// Message number for receiving
    receive_message_number: u32,
    
    /// Previous chain keys for out-of-order messages
    previous_chain_keys: HashMap<Vec<u8>, (Vec<u8>, u32)>,
    
    /// Last message timestamp
    last_message_time: SystemTime,
}

/// Encrypted message header
#[derive(Debug, Clone, Serialize, Deserialize)]
struct MessageHeader {
    /// Sender device ID
    sender_id: String,
    
    /// Public ratchet key
    public_ratchet_key: Vec<u8>,
    
    /// Message number in the chain
    message_number: u32,
    
    /// Previous chain length
    previous_chain_length: u32,
}

/// Encrypted message with all components for decryption
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EncryptedMessage {
    /// Message header
    header: MessageHeader,
    
    /// Initialization vector
    iv: Vec<u8>,
    
    /// Encrypted message content
    ciphertext: Vec<u8>,
}

/// Stateful nonce sequence for encryption
struct StatefulNonceSequence {
    nonce: [u8; NONCE_SIZE],
}

impl StatefulNonceSequence {
    fn new(iv: &[u8]) -> Self {
        let mut nonce = [0u8; NONCE_SIZE];
        nonce.copy_from_slice(iv);
        Self { nonce }
    }
}

impl NonceSequence for StatefulNonceSequence {
    fn advance(&self) -> Result<Nonce, ring::error::Unspecified> {
        Nonce::try_assume_unique_for_key(&self.nonce)
    }
}

/// End-to-End Encryption Manager
pub struct E2EEManager {
    /// Whether E2EE is enabled
    enabled: bool,
    
    /// System random generator
    rng: SystemRandom,
    
    /// Private identity key (for long-term identity)
    private_identity_key: Arc<RwLock<Option<Vec<u8>>>>,
    
    /// Public identity key
    public_identity_key: Arc<RwLock<Option<Vec<u8>>>>,
    
    /// Private signed pre-key
    private_signed_pre_key: Arc<RwLock<Option<Vec<u8>>>>,
    
    /// Public signed pre-key
    public_signed_pre_key: Arc<RwLock<Option<Vec<u8>>>>,
    
    /// One-time pre-keys (private keys)
    one_time_pre_keys: Arc<RwLock<HashMap<Vec<u8>, Vec<u8>>>>,
    
    /// Session states for other devices
    sessions: Arc<RwLock<HashMap<String, SessionState>>>,
    
    /// Cache for device keys
    device_keys_cache: Arc<RwLock<HashMap<String, DeviceKeys>>>,
    
    /// Lock for key operations
    key_lock: Arc<Mutex<()>>,
}

impl E2EEManager {
    /// Create a new E2EE Manager
    pub fn new(enabled: bool) -> Result<Self> {
        let rng = SystemRandom::new();
        
        Ok(Self {
            enabled,
            rng,
            private_identity_key: Arc::new(RwLock::new(None)),
            public_identity_key: Arc::new(RwLock::new(None)),
            private_signed_pre_key: Arc::new(RwLock::new(None)),
            public_signed_pre_key: Arc::new(RwLock::new(None)),
            one_time_pre_keys: Arc::new(RwLock::new(HashMap::new())),
            sessions: Arc::new(RwLock::new(HashMap::new())),
            device_keys_cache: Arc::new(RwLock::new(HashMap::new())),
            key_lock: Arc::new(Mutex::new(())),
        })
    }
    
    /// Start the encryption service
    pub fn start_service(&self) -> Result<()> {
        if !self.enabled {
            info!("E2EE is disabled, skipping key initialization");
            return Ok(());
        }
        
        // Create directories if needed
        let config_dir = config_path("");
        let one_time_keys_dir = config_dir.join(ONE_TIME_KEYS_DIR);
        let session_keys_dir = config_dir.join(SESSION_KEYS_DIR);
        
        fs::create_dir_all(&one_time_keys_dir).map_err(|e| format!("Failed to create one-time keys directory: {}", e))?;
        fs::create_dir_all(&session_keys_dir).map_err(|e| format!("Failed to create session keys directory: {}", e))?;
        
        // Initialize keys
        self.initialize_keys()?;
        
        // Load sessions
        self.load_sessions()?;
        
        info!("E2EE service started with {} sessions", self.sessions.read().unwrap().len());
        
        // Metrics
        record_gauge("security.e2ee.sessions_count", self.sessions.read().unwrap().len() as f64, None);
        record_counter("security.e2ee.service_started", 1.0, None);
        
        Ok(())
    }
    
    /// Enable or disable E2EE
    pub fn set_enabled(&self, enabled: bool) -> Result<()> {
        if self.enabled == enabled {
            return Ok(()); // No change needed
        }
        
        // Update enabled state
        if enabled {
            // Enabling E2EE
            info!("Enabling E2EE");
            
            // Initialize keys if they don't exist
            self.initialize_keys()?;
            
            // Load sessions
            self.load_sessions()?;
            
            record_counter("security.e2ee.enabled", 1.0, None);
        } else {
            // Disabling E2EE
            info!("Disabling E2EE");
            
            // Clear keys from memory
            *self.private_identity_key.write().unwrap() = None;
            *self.public_identity_key.write().unwrap() = None;
            *self.private_signed_pre_key.write().unwrap() = None;
            *self.public_signed_pre_key.write().unwrap() = None;
            self.one_time_pre_keys.write().unwrap().clear();
            self.sessions.write().unwrap().clear();
            
            record_counter("security.e2ee.disabled", 1.0, None);
        }
        
        Ok(())
    }
    
    /// Initialize encryption keys
    fn initialize_keys(&self) -> Result<()> {
        let _lock = self.key_lock.lock().unwrap();
        
        // Identity key
        let identity_key_path = config_path(IDENTITY_KEY_FILE);
        if identity_key_path.exists() {
            // Load existing identity key
            let key_data = fs::read(&identity_key_path)
                .map_err(|e| format!("Failed to read identity key: {}", e))?;
            
            // Key data contains private key followed by public key
            let private_key = key_data[0..KEY_SIZE].to_vec();
            let public_key = key_data[KEY_SIZE..].to_vec();
            
            *self.private_identity_key.write().unwrap() = Some(private_key);
            *self.public_identity_key.write().unwrap() = Some(public_key);
            
            debug!("Loaded existing identity key");
        } else {
            // Generate new identity key
            let key_pair = self.generate_key_pair()?;
            
            // Save key pair
            let mut key_data = key_pair.0.clone();
            key_data.extend_from_slice(&key_pair.1);
            
            fs::write(&identity_key_path, &key_data)
                .map_err(|e| format!("Failed to save identity key: {}", e))?;
            
            *self.private_identity_key.write().unwrap() = Some(key_pair.0);
            *self.public_identity_key.write().unwrap() = Some(key_pair.1);
            
            info!("Generated new identity key");
            record_counter("security.e2ee.identity_key_generated", 1.0, None);
        }
        
        // Signed pre-key
        let signed_pre_key_path = config_path(SIGNED_PRE_KEY_FILE);
        if signed_pre_key_path.exists() {
            // Load existing signed pre-key
            let key_data = fs::read(&signed_pre_key_path)
                .map_err(|e| format!("Failed to read signed pre-key: {}", e))?;
            
            // Key data contains private key followed by public key
            let private_key = key_data[0..KEY_SIZE].to_vec();
            let public_key = key_data[KEY_SIZE..].to_vec();
            
            *self.private_signed_pre_key.write().unwrap() = Some(private_key);
            *self.public_signed_pre_key.write().unwrap() = Some(public_key);
            
            debug!("Loaded existing signed pre-key");
        } else {
            // Generate new signed pre-key
            let key_pair = self.generate_key_pair()?;
            
            // Save key pair
            let mut key_data = key_pair.0.clone();
            key_data.extend_from_slice(&key_pair.1);
            
            fs::write(&signed_pre_key_path, &key_data)
                .map_err(|e| format!("Failed to save signed pre-key: {}", e))?;
            
            *self.private_signed_pre_key.write().unwrap() = Some(key_pair.0);
            *self.public_signed_pre_key.write().unwrap() = Some(key_pair.1);
            
            info!("Generated new signed pre-key");
            record_counter("security.e2ee.signed_pre_key_generated", 1.0, None);
        }
        
        // One-time pre-keys
        let one_time_keys_dir = config_path(ONE_TIME_KEYS_DIR);
        let mut one_time_keys = HashMap::new();
        
        // Load existing one-time pre-keys
        if one_time_keys_dir.exists() {
            for entry in fs::read_dir(&one_time_keys_dir)
                .map_err(|e| format!("Failed to read one-time keys directory: {}", e))? {
                let entry = entry.map_err(|e| format!("Failed to read directory entry: {}", e))?;
                let path = entry.path();
                
                if path.is_file() {
                    // Load key data
                    let key_data = fs::read(&path)
                        .map_err(|e| format!("Failed to read one-time key: {}", e))?;
                    
                    // Key data contains private key followed by public key
                    let private_key = key_data[0..KEY_SIZE].to_vec();
                    let public_key = key_data[KEY_SIZE..].to_vec();
                    
                    one_time_keys.insert(public_key, private_key);
                }
            }
            
            debug!("Loaded {} existing one-time pre-keys", one_time_keys.len());
        }
        
        // Generate new one-time pre-keys if needed (aim for 20)
        let keys_to_generate = 20.max(0.max(20 - one_time_keys.len()));
        if keys_to_generate > 0 {
            for _ in 0..keys_to_generate {
                let key_pair = self.generate_key_pair()?;
                
                // Save key pair
                let mut key_data = key_pair.0.clone();
                key_data.extend_from_slice(&key_pair.1);
                
                let key_file = one_time_keys_dir.join(format!("{}.bin", hex::encode(&key_pair.1[0..8])));
                fs::write(&key_file, &key_data)
                    .map_err(|e| format!("Failed to save one-time key: {}", e))?;
                
                one_time_keys.insert(key_pair.1, key_pair.0);
            }
            
            info!("Generated {} new one-time pre-keys", keys_to_generate);
            record_counter("security.e2ee.one_time_keys_generated", keys_to_generate as f64, None);
        }
        
        *self.one_time_pre_keys.write().unwrap() = one_time_keys;
        
        Ok(())
    }
    
    /// Load existing sessions
    fn load_sessions(&self) -> Result<()> {
        let session_keys_dir = config_path(SESSION_KEYS_DIR);
        let mut sessions = HashMap::new();
        
        // Load existing sessions
        if session_keys_dir.exists() {
            for entry in fs::read_dir(&session_keys_dir)
                .map_err(|e| format!("Failed to read session keys directory: {}", e))? {
                let entry = entry.map_err(|e| format!("Failed to read directory entry: {}", e))?;
                let path = entry.path();
                
                if path.is_file() && path.extension().map_or(false, |ext| ext == "json") {
                    // Load session data
                    let session_data = fs::read_to_string(&path)
                        .map_err(|e| format!("Failed to read session file: {}", e))?;
                    
                    // Parse session
                    match serde_json::from_str::<SessionState>(&session_data) {
                        Ok(session) => {
                            let device_id = session.device_id.clone();
                            sessions.insert(device_id, session);
                        },
                        Err(e) => {
                            warn!("Failed to parse session file {}: {}", path.display(), e);
                        }
                    }
                }
            }
            
            debug!("Loaded {} existing sessions", sessions.len());
        }
        
        *self.sessions.write().unwrap() = sessions;
        
        Ok(())
    }
    
    /// Generate a new X25519 key pair
    fn generate_key_pair(&self) -> Result<(Vec<u8>, Vec<u8>)> {
        let private_key = EphemeralPrivateKey::generate(&X25519, &self.rng)
            .map_err(|_| "Failed to generate private key".to_string())?;
        
        let public_key = private_key.compute_public_key()
            .map_err(|_| "Failed to compute public key".to_string())?;
        
        let private_key_bytes = self.extract_private_key_bytes(private_key)?;
        let public_key_bytes = public_key.as_ref().to_vec();
        
        Ok((private_key_bytes, public_key_bytes))
    }
    
    /// Extract private key bytes from an EphemeralPrivateKey
    fn extract_private_key_bytes(&self, private_key: EphemeralPrivateKey) -> Result<Vec<u8>> {
        // In a real implementation, we would use a proper method to extract private key bytes
        // For now, we'll generate a random key as placeholder since ring doesn't expose private key bytes
        let mut private_key_bytes = vec![0u8; KEY_SIZE];
        self.rng.fill(&mut private_key_bytes)
            .map_err(|_| "Failed to generate random bytes".to_string())?;
        
        Ok(private_key_bytes)
    }
    
    /// Get device keys for this device
    pub fn get_device_keys(&self, device_id: &str) -> Result<DeviceKeys> {
        if !self.enabled {
            return Err("E2EE is disabled".into());
        }
        
        let public_identity_key = self.public_identity_key.read().unwrap()
            .clone()
            .ok_or_else(|| "Identity key not initialized".to_string())?;
            
        let public_signed_pre_key = self.public_signed_pre_key.read().unwrap()
            .clone()
            .ok_or_else(|| "Signed pre-key not initialized".to_string())?;
            
        // Get one-time pre-keys (public keys only)
        let one_time_pre_keys: Vec<Vec<u8>> = self.one_time_pre_keys.read().unwrap()
            .keys()
            .take(10) // Only expose up to 10 keys at a time
            .cloned()
            .collect();
            
        Ok(DeviceKeys {
            device_id: device_id.to_string(),
            public_identity_key,
            public_signed_pre_key,
            one_time_pre_keys,
            last_update: SystemTime::now(),
        })
    }
    
    /// Initialize a session with another device
    pub fn initialize_session(&self, device_id: &str, their_keys: DeviceKeys) -> Result<()> {
        if !self.enabled {
            return Err("E2EE is disabled".into());
        }
        
        let _lock = self.key_lock.lock().unwrap();
        
        // Update device keys cache
        self.device_keys_cache.write().unwrap().insert(device_id.to_string(), their_keys.clone());
        
        // Check if we already have a session with this device
        if self.sessions.read().unwrap().contains_key(device_id) {
            debug!("Session already exists for device {}", device_id);
            return Ok(());
        }
        
        // Get our identity key
        let our_identity_key = self.private_identity_key.read().unwrap()
            .clone()
            .ok_or_else(|| "Identity key not initialized".to_string())?;
            
        // Select one of their one-time pre-keys if available
        let mut their_one_time_key = None;
        if !their_keys.one_time_pre_keys.is_empty() {
            their_one_time_key = Some(their_keys.one_time_pre_keys[0].clone());
        }
        
        // Create initial shared secret using X3DH
        // 1. DH1 = DH(our_identity_key, their_signed_prekey)
        // 2. DH2 = DH(our_ephemeral_key, their_identity_key)
        // 3. DH3 = DH(our_ephemeral_key, their_signed_prekey)
        // 4. DH4 = DH(our_ephemeral_key, their_one_time_prekey) [optional]
        // SK = KDF(DH1 || DH2 || DH3 || DH4)
        
        // Generate ephemeral key
        let (our_ephemeral_private_key, our_ephemeral_public_key) = self.generate_key_pair()?;
        
        // Compute DH1
        let dh1 = self.compute_dh(
            &our_identity_key,
            &their_keys.public_signed_pre_key,
        )?;
        
        // Compute DH2
        let dh2 = self.compute_dh(
            &our_ephemeral_private_key,
            &their_keys.public_identity_key,
        )?;
        
        // Compute DH3
        let dh3 = self.compute_dh(
            &our_ephemeral_private_key,
            &their_keys.public_signed_pre_key,
        )?;
        
        // Compute DH4 if available
        let mut shared_secret = Vec::new();
        shared_secret.extend_from_slice(&dh1);
        shared_secret.extend_from_slice(&dh2);
        shared_secret.extend_from_slice(&dh3);
        
        if let Some(their_otk) = their_one_time_key.as_ref() {
            let dh4 = self.compute_dh(
                &our_ephemeral_private_key,
                their_otk,
            )?;
            shared_secret.extend_from_slice(&dh4);
        }
        
        // Derive initial root key and chain keys
        let mut root_key = vec![0u8; KEY_SIZE];
        let hkdf = hkdf::Prk::new_less_safe(
            hkdf::HKDF_SHA256,
            &shared_secret,
        );
        
        hkdf.expand(&[b"root_key"], &mut root_key)
            .map_err(|_| "Failed to derive root key".to_string())?;
            
        // Generate initial ratchet key
        let (private_ratchet_key, public_ratchet_key) = self.generate_key_pair()?;
        
        // Create initial sending and receiving chain keys
        let send_chain_key = self.derive_key_from_constant(&root_key, b"sending_chain")?;
        let receive_chain_key = self.derive_key_from_constant(&root_key, b"receiving_chain")?;
        
        // Create session state
        let session = SessionState {
            device_id: device_id.to_string(),
            root_key,
            send_chain_key,
            receive_chain_key,
            public_ratchet_key: public_ratchet_key.clone(),
            private_ratchet_key,
            remote_public_ratchet_key: their_keys.public_signed_pre_key.clone(),
            send_message_number: 0,
            receive_message_number: 0,
            previous_chain_keys: HashMap::new(),
            last_message_time: SystemTime::now(),
        };
        
        // Save session
        self.save_session(&session)?;
        
        info!("Initialized new session with device {}", device_id);
        record_counter("security.e2ee.session_initialized", 1.0, None);
        
        Ok(())
    }
    
    /// Compute Diffie-Hellman shared secret
    fn compute_dh(&self, private_key: &[u8], public_key: &[u8]) -> Result<Vec<u8>> {
        // In a real implementation, we would use ring's agreement API
        // For now, creating a placeholder that simulates a DH operation
        let mut shared_secret = vec![0u8; KEY_SIZE];
        
        // Use HKDF to derive a simulated shared secret from the keys
        let prk = hkdf::Prk::new_less_safe(
            hkdf::HKDF_SHA256,
            &[private_key, public_key].concat(),
        );
        
        prk.expand(&[b"dh_secret"], &mut shared_secret)
            .map_err(|_| "Failed to derive DH shared secret".to_string())?;
            
        Ok(shared_secret)
    }
    
    /// Derive a key from a root key and a constant
    fn derive_key_from_constant(&self, key: &[u8], constant: &[u8]) -> Result<Vec<u8>> {
        let mut derived_key = vec![0u8; KEY_SIZE];
        
        let prk = hkdf::Prk::new_less_safe(
            hkdf::HKDF_SHA256,
            key,
        );
        
        prk.expand(constant, &mut derived_key)
            .map_err(|_| "Failed to derive key".to_string())?;
            
        Ok(derived_key)
    }
    
    /// Save a session to disk
    fn save_session(&self, session: &SessionState) -> Result<()> {
        let session_keys_dir = config_path(SESSION_KEYS_DIR);
        let session_file = session_keys_dir.join(format!("{}.json", session.device_id));
        
        // Serialize session
        let session_data = serde_json::to_string(session)
            .map_err(|e| format!("Failed to serialize session: {}", e))?;
            
        // Save session
        fs::write(&session_file, session_data)
            .map_err(|e| format!("Failed to save session: {}", e))?;
            
        // Update in-memory sessions
        self.sessions.write().unwrap().insert(session.device_id.clone(), session.clone());
        
        Ok(())
    }
    
    /// Ratchet forward the sending chain
    fn ratchet_send_chain(&self, session: &mut SessionState) -> Result<Vec<u8>> {
        // Derive next chain key
        let next_chain_key = self.derive_key_from_constant(&session.send_chain_key, CHAIN_KEY_SEED)?;
        
        // Derive message key from current chain key
        let message_key = self.derive_key_from_constant(&session.send_chain_key, MESSAGE_KEY_SEED)?;
        
        // Update chain key
        session.send_chain_key = next_chain_key;
        session.send_message_number += 1;
        
        Ok(message_key)
    }
    
    /// Ratchet forward the receiving chain
    fn ratchet_receive_chain(&self, session: &mut SessionState, message_number: u32) -> Result<Vec<u8>> {
        // Check if we already have a message key for this message
        for (ratchet_key, (chain_start, chain_length)) in &session.previous_chain_keys {
            if ratchet_key == &session.remote_public_ratchet_key && message_number >= *chain_start && message_number < *chain_start + *chain_length {
                // We have a saved chain that contains this message key
                let mut chain_key = session.receive_chain_key.clone();
                let skipped = message_number - *chain_start;
                
                // Ratchet forward to the right message key
                for _ in 0..skipped {
                    chain_key = self.derive_key_from_constant(&chain_key, CHAIN_KEY_SEED)?;
                }
                
                // Derive message key
                let message_key = self.derive_key_from_constant(&chain_key, MESSAGE_KEY_SEED)?;
                
                // Update session to skip already used keys
                session.receive_chain_key = self.derive_key_from_constant(&chain_key, CHAIN_KEY_SEED)?;
                session.receive_message_number = message_number + 1;
                
                return Ok(message_key);
            }
        }
        
        // Normal case - ratchet forward the current chain
        if message_number < session.receive_message_number {
            return Err(format!("Message {} is too old (current: {})", message_number, session.receive_message_number).into());
        }
        
        let mut chain_key = session.receive_chain_key.clone();
        let skipped = message_number - session.receive_message_number;
        
        // Save skipped message keys if needed
        if skipped > 0 {
            let mut saved_keys = Vec::new();
            let mut current_key = chain_key.clone();
            
            for _ in 0..skipped {
                let message_key = self.derive_key_from_constant(&current_key, MESSAGE_KEY_SEED)?;
                saved_keys.push(message_key);
                current_key = self.derive_key_from_constant(&current_key, CHAIN_KEY_SEED)?;
            }
            
            // Store skipped keys
            session.previous_chain_keys.insert(
                session.remote_public_ratchet_key.clone(),
                (session.receive_message_number, skipped),
            );
            
            chain_key = current_key;
        }
        
        // Derive message key
        let message_key = self.derive_key_from_constant(&chain_key, MESSAGE_KEY_SEED)?;
        
        // Update chain key
        session.receive_chain_key = self.derive_key_from_constant(&chain_key, CHAIN_KEY_SEED)?;
        session.receive_message_number = message_number + 1;
        
        Ok(message_key)
    }
    
    /// Double ratchet step (for receiving a message with a new ratchet key)
    fn perform_dh_ratchet(&self, session: &mut SessionState, their_ratchet_key: &[u8]) -> Result<()> {
        // Save current receive chain
        session.previous_chain_keys.insert(
            session.remote_public_ratchet_key.clone(),
            (session.receive_message_number, 0),
        );
        
        // Update remote ratchet key
        session.remote_public_ratchet_key = their_ratchet_key.to_vec();
        
        // Calculate new shared secret
        let dh_secret = self.compute_dh(
            &session.private_ratchet_key,
            their_ratchet_key,
        )?;
        
        // Derive new root key and receive chain key
        let mut root_key_info = Vec::new();
        root_key_info.extend_from_slice(&session.root_key);
        root_key_info.extend_from_slice(RATCHET_SEED);
        root_key_info.extend_from_slice(&dh_secret);
        
        let (root_key, receive_chain_key) = self.kdf_ratchet(
            &session.root_key, 
            &dh_secret,
        )?;
        
        // Generate new ratchet key pair
        let (private_ratchet_key, public_ratchet_key) = self.generate_key_pair()?;
        
        // Calculate another shared secret with new keys
        let new_dh_secret = self.compute_dh(
            &private_ratchet_key,
            their_ratchet_key,
        )?;
        
        // Derive new root key and sending chain key
        let (new_root_key, send_chain_key) = self.kdf_ratchet(
            &root_key, 
            &new_dh_secret,
        )?;
        
        // Update session
        session.root_key = new_root_key;
        session.send_chain_key = send_chain_key;
        session.receive_chain_key = receive_chain_key;
        session.private_ratchet_key = private_ratchet_key;
        session.public_ratchet_key = public_ratchet_key;
        session.send_message_number = 0;
        session.receive_message_number = 0;
        
        Ok(())
    }
    
    /// KDF ratchet step (generates a new root key and chain key)
    fn kdf_ratchet(&self, root_key: &[u8], dh_output: &[u8]) -> Result<(Vec<u8>, Vec<u8>)> {
        let mut secret = Vec::new();
        secret.extend_from_slice(root_key);
        secret.extend_from_slice(dh_output);
        
        let prk = hkdf::Prk::new_less_safe(
            hkdf::HKDF_SHA256,
            &secret,
        );
        
        let mut new_root_key = vec![0u8; KEY_SIZE];
        let mut chain_key = vec![0u8; KEY_SIZE];
        
        prk.expand(&[b"root_key"], &mut new_root_key)
            .map_err(|_| "Failed to derive root key".to_string())?;
            
        prk.expand(&[b"chain_key"], &mut chain_key)
            .map_err(|_| "Failed to derive chain key".to_string())?;
            
        Ok((new_root_key, chain_key))
    }
    
    /// Encrypt data using the E2EE system
    pub fn encrypt_data(&self, plaintext: &[u8]) -> Result<Vec<u8>> {
        if !self.enabled {
            // If E2EE is disabled, just return the plaintext
            return Ok(plaintext.to_vec());
        }
        
        // For now, use a simpler encryption scheme
        // In a real app, this would encrypt using the active session with target device
        
        // Generate random IV
        let mut iv = [0u8; NONCE_SIZE];
        self.rng.fill(&mut iv)
            .map_err(|_| "Failed to generate IV".to_string())?;
        
        // Get key (using identity key for simplicity)
        let key = self.private_identity_key.read().unwrap()
            .clone()
            .ok_or_else(|| "Identity key not initialized".to_string())?;
        
        // Create encryption key
        let unbound_key = UnboundKey::new(&CHACHA20_POLY1305, &key[0..KEY_SIZE])
            .map_err(|_| "Failed to create encryption key".to_string())?;
        let nonce_sequence = StatefulNonceSequence::new(&iv);
        let mut sealing_key = SealingKey::new(unbound_key, nonce_sequence);
        
        // Encrypt the data
        let mut ciphertext = plaintext.to_vec();
        sealing_key
            .seal_in_place_append_tag(Aad::empty(), &mut ciphertext)
            .map_err(|_| "Encryption failed".to_string())?;
        
        // Format result as a JSON message (for compatibility)
        let message = EncryptedMessage {
            header: MessageHeader {
                sender_id: "self".to_string(),
                public_ratchet_key: vec![],
                message_number: 0,
                previous_chain_length: 0,
            },
            iv: iv.to_vec(),
            ciphertext,
        };
        
        // Serialize message
        let result = serde_json::to_vec(&message)
            .map_err(|e| format!("Failed to serialize message: {}", e))?;
        
        record_counter("security.e2ee.messages_encrypted", 1.0, None);
        record_histogram("security.e2ee.encrypted_message_size", result.len() as f64, None);
        
        Ok(result)
    }
    
    /// Decrypt data using the E2EE system
    pub fn decrypt_data(&self, ciphertext: &[u8]) -> Result<Vec<u8>> {
        if !self.enabled {
            // If E2EE is disabled, just return the ciphertext
            return Ok(ciphertext.to_vec());
        }
        
        // Try to parse as an encrypted message
        match serde_json::from_slice::<EncryptedMessage>(ciphertext) {
            Ok(message) => {
                // For now, use a simpler decryption scheme
                // In a real app, this would use the appropriate session
                
                // Get key (using identity key for simplicity)
                let key = self.private_identity_key.read().unwrap()
                    .clone()
                    .ok_or_else(|| "Identity key not initialized".to_string())?;
                
                // Create decryption key
                let unbound_key = UnboundKey::new(&CHACHA20_POLY1305, &key[0..KEY_SIZE])
                    .map_err(|_| "Failed to create decryption key".to_string())?;
                let nonce_sequence = StatefulNonceSequence::new(&message.iv);
                let mut opening_key = OpeningKey::new(unbound_key, nonce_sequence);
                
                // Decrypt the data
                let mut plaintext = message.ciphertext.clone();
                let decrypted = opening_key
                    .open_in_place(Aad::empty(), &mut plaintext)
                    .map_err(|_| "Decryption failed".to_string())?;
                
                record_counter("security.e2ee.messages_decrypted", 1.0, None);
                record_histogram("security.e2ee.decrypted_message_size", decrypted.len() as f64, None);
                
                Ok(decrypted.to_vec())
            },
            Err(_) => {
                // Not an encrypted message or parsing failed
                // Just return the original data
                warn!("Received data is not a valid encrypted message");
                Ok(ciphertext.to_vec())
            }
        }
    }
    
    /// Rotate keys (generate new signed pre-key and one-time keys)
    pub fn rotate_keys(&self) -> Result<()> {
        if !self.enabled {
            return Err("E2EE is disabled".into());
        }
        
        let _lock = self.key_lock.lock().unwrap();
        
        // Generate new signed pre-key
        let signed_pre_key_path = config_path(SIGNED_PRE_KEY_FILE);
        let key_pair = self.generate_key_pair()?;
        
        // Save key pair
        let mut key_data = key_pair.0.clone();
        key_data.extend_from_slice(&key_pair.1);
        
        fs::write(&signed_pre_key_path, &key_data)
            .map_err(|e| format!("Failed to save signed pre-key: {}", e))?;
        
        *self.private_signed_pre_key.write().unwrap() = Some(key_pair.0);
        *self.public_signed_pre_key.write().unwrap() = Some(key_pair.1);
        
        // Generate new one-time pre-keys
        let one_time_keys_dir = config_path(ONE_TIME_KEYS_DIR);
        let mut one_time_keys = HashMap::new();
        
        // Generate 20 new keys
        for _ in 0..20 {
            let key_pair = self.generate_key_pair()?;
            
            // Save key pair
            let mut key_data = key_pair.0.clone();
            key_data.extend_from_slice(&key_pair.1);
            
            let key_file = one_time_keys_dir.join(format!("{}.bin", hex::encode(&key_pair.1[0..8])));
            fs::write(&key_file, &key_data)
                .map_err(|e| format!("Failed to save one-time key: {}", e))?;
            
            one_time_keys.insert(key_pair.1, key_pair.0);
        }
        
        *self.one_time_pre_keys.write().unwrap() = one_time_keys;
        
        info!("Rotated E2EE keys");
        record_counter("security.e2ee.keys_rotated", 1.0, None);
        
        Ok(())
    }
}
</file>

<file path="src/security/mod.rs">
// Security and Privacy Module for MCP Client
//
// This module provides comprehensive security and privacy features:
// - End-to-end encryption for data synchronization
// - Secure credential storage using platform-specific secure enclaves
// - Data flow tracking and visualization
// - Granular permission management

pub mod e2ee;
pub mod credentials;
pub mod data_flow;
pub mod permissions;

use std::sync::{Arc, RwLock};
use log::{debug, info, warn, error};
use serde::{Serialize, Deserialize};

use crate::error::Result;
use crate::observability::metrics::record_counter;
use crate::observability::telemetry::track_feature_usage;

/// Security configuration options
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SecurityConfig {
    /// Whether end-to-end encryption is enabled for sync
    pub e2ee_enabled: bool,
    
    /// Whether to use platform secure enclave for credential storage
    pub use_secure_enclave: bool,
    
    /// Whether to collect and display data flow information
    pub data_flow_tracking_enabled: bool,
    
    /// Default permission level for new features and integrations
    pub default_permission_level: PermissionLevel,
    
    /// Whether to prompt for permission changes
    pub interactive_permissions: bool,
    
    /// Whether to anonymize telemetry data
    pub anonymize_telemetry: bool,
    
    /// Whether to encrypt local storage
    pub encrypt_local_storage: bool,
    
    /// How long to cache credentials in memory (in seconds)
    pub credential_cache_duration: u64,
    
    /// Whether to clear clipboard after sensitive operations
    pub clipboard_security_enabled: bool,
}

impl Default for SecurityConfig {
    fn default() -> Self {
        Self {
            e2ee_enabled: true,
            use_secure_enclave: true,
            data_flow_tracking_enabled: true,
            default_permission_level: PermissionLevel::AskEveryTime,
            interactive_permissions: true,
            anonymize_telemetry: true,
            encrypt_local_storage: true,
            credential_cache_duration: 600, // 10 minutes
            clipboard_security_enabled: true,
        }
    }
}

/// Permission levels for features and capabilities
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum PermissionLevel {
    /// Always allow this operation without asking
    AlwaysAllow,
    
    /// Ask for confirmation the first time only
    AskFirstTime,
    
    /// Ask for confirmation every time
    AskEveryTime,
    
    /// Never allow this operation
    NeverAllow,
}

/// Data classification levels for privacy controls
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum DataClassification {
    /// Public data that can be shared without restriction
    Public,
    
    /// Personal data that should be handled with care
    Personal,
    
    /// Sensitive data that requires strong privacy controls
    Sensitive,
    
    /// Highly confidential data that should never leave the device
    Confidential,
}

/// Security manager for the MCP client
pub struct SecurityManager {
    /// Configuration
    config: Arc<RwLock<SecurityConfig>>,
    
    /// E2EE Manager for encryption
    e2ee_manager: Arc<RwLock<e2ee::E2EEManager>>,
    
    /// Credential storage manager
    credential_manager: Arc<RwLock<credentials::CredentialManager>>,
    
    /// Data flow tracking manager
    data_flow_manager: Arc<RwLock<data_flow::DataFlowManager>>,
    
    /// Permission manager
    permission_manager: Arc<RwLock<permissions::PermissionManager>>,
}

impl SecurityManager {
    /// Create a new security manager
    pub fn new(config: SecurityConfig) -> Result<Self> {
        // Initialize E2EE Manager
        let e2ee_manager = e2ee::E2EEManager::new(config.e2ee_enabled)?;
        
        // Initialize Credential Manager
        let credential_manager = credentials::CredentialManager::new(
            config.use_secure_enclave,
            config.credential_cache_duration,
        )?;
        
        // Initialize Data Flow Manager
        let data_flow_manager = data_flow::DataFlowManager::new(
            config.data_flow_tracking_enabled,
        )?;
        
        // Initialize Permission Manager
        let permission_manager = permissions::PermissionManager::new(
            config.default_permission_level,
            config.interactive_permissions,
        )?;
        
        Ok(Self {
            config: Arc::new(RwLock::new(config)),
            e2ee_manager: Arc::new(RwLock::new(e2ee_manager)),
            credential_manager: Arc::new(RwLock::new(credential_manager)),
            data_flow_manager: Arc::new(RwLock::new(data_flow_manager)),
            permission_manager: Arc::new(RwLock::new(permission_manager)),
        })
    }
    
    /// Start security services
    pub fn start_services(&self) -> Result<()> {
        // Start E2EE service
        self.e2ee_manager.read().unwrap().start_service()?;
        
        // Start credential management service
        self.credential_manager.read().unwrap().start_service()?;
        
        // Start data flow tracking service
        self.data_flow_manager.read().unwrap().start_service()?;
        
        // Start permission management service
        self.permission_manager.read().unwrap().start_service()?;
        
        info!("Security services started");
        record_counter("security.services.started", 1.0, None);
        
        Ok(())
    }
    
    /// Get the E2EE manager
    pub fn get_e2ee_manager(&self) -> Arc<RwLock<e2ee::E2EEManager>> {
        self.e2ee_manager.clone()
    }
    
    /// Get the credential manager
    pub fn get_credential_manager(&self) -> Arc<RwLock<credentials::CredentialManager>> {
        self.credential_manager.clone()
    }
    
    /// Get the data flow manager
    pub fn get_data_flow_manager(&self) -> Arc<RwLock<data_flow::DataFlowManager>> {
        self.data_flow_manager.clone()
    }
    
    /// Get the permission manager
    pub fn get_permission_manager(&self) -> Arc<RwLock<permissions::PermissionManager>> {
        self.permission_manager.clone()
    }
    
    /// Check if a permission is granted
    pub fn check_permission(&self, permission: &str) -> Result<bool> {
        self.permission_manager.read().unwrap().check_permission(permission)
    }
    
    /// Request permission for an operation
    pub fn request_permission(&self, permission: &str, reason: &str) -> Result<bool> {
        let granted = self.permission_manager.write().unwrap().request_permission(permission, reason)?;
        
        // Track permission request
        let mut properties = std::collections::HashMap::new();
        properties.insert("permission".to_string(), permission.to_string());
        properties.insert("granted".to_string(), granted.to_string());
        track_feature_usage("security.permission_request", Some(properties));
        
        Ok(granted)
    }
    
    /// Store a credential securely
    pub fn store_credential(&self, key: &str, value: &str) -> Result<()> {
        self.credential_manager.write().unwrap().store_credential(key, value)?;
        
        // Track data flow
        if self.config.read().unwrap().data_flow_tracking_enabled {
            self.data_flow_manager.write().unwrap().track_data_flow(
                "credential_storage",
                key,
                DataClassification::Sensitive,
                "secure_enclave",
            )?;
        }
        
        // Track credential storage
        record_counter("security.credential.stored", 1.0, None);
        
        Ok(())
    }
    
    /// Retrieve a credential
    pub fn get_credential(&self, key: &str) -> Result<String> {
        let credential = self.credential_manager.write().unwrap().get_credential(key)?;
        
        // Track data flow
        if self.config.read().unwrap().data_flow_tracking_enabled {
            self.data_flow_manager.write().unwrap().track_data_flow(
                "credential_retrieval",
                key,
                DataClassification::Sensitive,
                "application",
            )?;
        }
        
        // Track credential retrieval
        record_counter("security.credential.retrieved", 1.0, None);
        
        Ok(credential)
    }
    
    /// Encrypt data for synchronization
    pub fn encrypt_for_sync(&self, data: &[u8]) -> Result<Vec<u8>> {
        let encrypted = self.e2ee_manager.read().unwrap().encrypt_data(data)?;
        
        // Track encryption operation
        record_counter("security.e2ee.encrypt", 1.0, None);
        
        Ok(encrypted)
    }
    
    /// Decrypt data from synchronization
    pub fn decrypt_from_sync(&self, data: &[u8]) -> Result<Vec<u8>> {
        let decrypted = self.e2ee_manager.read().unwrap().decrypt_data(data)?;
        
        // Track decryption operation
        record_counter("security.e2ee.decrypt", 1.0, None);
        
        Ok(decrypted)
    }
    
    /// Update security configuration
    pub fn update_config(&self, new_config: SecurityConfig) -> Result<()> {
        // Update managers with new config values
        self.e2ee_manager.write().unwrap().set_enabled(new_config.e2ee_enabled)?;
        self.credential_manager.write().unwrap().update_config(
            new_config.use_secure_enclave,
            new_config.credential_cache_duration,
        )?;
        self.data_flow_manager.write().unwrap().set_enabled(
            new_config.data_flow_tracking_enabled,
        )?;
        self.permission_manager.write().unwrap().update_config(
            new_config.default_permission_level,
            new_config.interactive_permissions,
        )?;
        
        // Update main config
        *self.config.write().unwrap() = new_config;
        
        info!("Security configuration updated");
        
        Ok(())
    }
    
    /// Get the current security configuration
    pub fn get_config(&self) -> SecurityConfig {
        self.config.read().unwrap().clone()
    }
}

// Global security manager
lazy_static::lazy_static! {
    pub static ref SECURITY_MANAGER: Arc<RwLock<Option<SecurityManager>>> = Arc::new(RwLock::new(None));
}

/// Initialize the security manager
pub fn init_security_manager(config: Option<SecurityConfig>) -> Result<()> {
    let config = config.unwrap_or_default();
    
    let manager = SecurityManager::new(config)?;
    manager.start_services()?;
    
    let mut global_manager = SECURITY_MANAGER.write().unwrap();
    *global_manager = Some(manager);
    
    info!("Security manager initialized");
    
    Ok(())
}

/// Get reference to the security manager
pub fn get_security_manager() -> Result<Arc<SecurityManager>> {
    match SECURITY_MANAGER.read().unwrap().as_ref() {
        Some(manager) => Ok(Arc::new(manager.clone())),
        None => Err(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            "Security manager not initialized",
        ).into()),
    }
}

// Helper functions for common security operations

/// Check if a permission is granted
pub fn check_permission(permission: &str) -> Result<bool> {
    match SECURITY_MANAGER.read().unwrap().as_ref() {
        Some(manager) => manager.check_permission(permission),
        None => Err(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            "Security manager not initialized",
        ).into()),
    }
}

/// Request permission for an operation
pub fn request_permission(permission: &str, reason: &str) -> Result<bool> {
    match SECURITY_MANAGER.read().unwrap().as_ref() {
        Some(manager) => manager.request_permission(permission, reason),
        None => Err(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            "Security manager not initialized",
        ).into()),
    }
}

/// Store a credential securely
pub fn store_credential(key: &str, value: &str) -> Result<()> {
    match SECURITY_MANAGER.read().unwrap().as_ref() {
        Some(manager) => manager.store_credential(key, value),
        None => Err(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            "Security manager not initialized",
        ).into()),
    }
}

/// Retrieve a credential
pub fn get_credential(key: &str) -> Result<String> {
    match SECURITY_MANAGER.read().unwrap().as_ref() {
        Some(manager) => manager.get_credential(key),
        None => Err(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            "Security manager not initialized",
        ).into()),
    }
}

/// Encrypt data for synchronization
pub fn encrypt_for_sync(data: &[u8]) -> Result<Vec<u8>> {
    match SECURITY_MANAGER.read().unwrap().as_ref() {
        Some(manager) => manager.encrypt_for_sync(data),
        None => Err(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            "Security manager not initialized",
        ).into()),
    }
}

/// Decrypt data from synchronization
pub fn decrypt_from_sync(data: &[u8]) -> Result<Vec<u8>> {
    match SECURITY_MANAGER.read().unwrap().as_ref() {
        Some(manager) => manager.decrypt_from_sync(data),
        None => Err(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            "Security manager not initialized",
        ).into()),
    }
}
</file>

<file path="src/security/permissions.rs">
// Granular Permission Management System
//
// This module provides a comprehensive permission management system that allows users
// to control access to features, data, and system resources at a granular level.

use std::collections::HashMap;
use std::fs;
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, SystemTime};

use log::{debug, info, warn, error};
use serde::{Serialize, Deserialize};

use crate::config::config_path;
use crate::error::Result;
use crate::observability::metrics::{record_counter, record_gauge};
use crate::security::PermissionLevel;

const PERMISSIONS_FILE: &str = "permissions.json";

/// A permission setting
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Permission {
    /// Unique identifier for the permission
    pub id: String,
    
    /// Human-readable name
    pub name: String,
    
    /// Description of what the permission grants access to
    pub description: String,
    
    /// Access level
    pub level: PermissionLevel,
    
    /// Category of the permission
    pub category: String,
    
    /// When the permission was last modified
    pub last_modified: SystemTime,
    
    /// How many times this permission has been used
    pub usage_count: usize,
    
    /// Whether this permission is required for core functionality
    pub required: bool,
}

/// A permission request event
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PermissionRequest {
    /// Unique identifier for the request
    pub id: String,
    
    /// Permission being requested
    pub permission_id: String,
    
    /// Reason for the request
    pub reason: String,
    
    /// When the request was made
    pub timestamp: SystemTime,
    
    /// Whether the request was granted
    pub granted: bool,
    
    /// Application context for the request
    pub context: HashMap<String, String>,
}

/// User interaction type for permission requests
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum PermissionInteractionType {
    /// No interaction, use default or saved setting
    None,
    
    /// Display a non-modal notification
    Notification,
    
    /// Show a modal dialog requiring user action
    Modal,
}

/// Callback for permission requests
pub type PermissionCallback = Box<dyn Fn(&str, &str) -> bool + Send + Sync>;

/// Permission Manager
pub struct PermissionManager {
    /// Default permission level for new permissions
    default_level: PermissionLevel,
    
    /// Whether to prompt for permissions interactively
    interactive: bool,
    
    /// Known permissions
    permissions: Arc<RwLock<HashMap<String, Permission>>>,
    
    /// Permission request history
    request_history: Arc<RwLock<Vec<PermissionRequest>>>,
    
    /// Callback for requesting permissions from the user
    permission_callback: Arc<RwLock<Option<PermissionCallback>>>,
    
    /// Lock for file operations
    file_lock: Arc<Mutex<()>>,
}

impl PermissionManager {
    /// Create a new Permission Manager
    pub fn new(
        default_level: PermissionLevel,
        interactive: bool,
    ) -> Result<Self> {
        Ok(Self {
            default_level,
            interactive,
            permissions: Arc::new(RwLock::new(HashMap::new())),
            request_history: Arc::new(RwLock::new(Vec::new())),
            permission_callback: Arc::new(RwLock::new(None)),
            file_lock: Arc::new(Mutex::new(())),
        })
    }
    
    /// Start the permission management service
    pub fn start_service(&self) -> Result<()> {
        // Load permissions from disk
        self.load_permissions()?;
        
        // Initialize default permissions if needed
        self.initialize_default_permissions()?;
        
        info!("Permission management service started with {} permissions", 
            self.permissions.read().unwrap().len());
        
        record_gauge("security.permissions.count", self.permissions.read().unwrap().len() as f64, None);
        record_counter("security.permissions.service_started", 1.0, None);
        
        Ok(())
    }
    
    /// Initialize default permissions
    fn initialize_default_permissions(&self) -> Result<()> {
        let mut permissions = self.permissions.write().unwrap();
        let now = SystemTime::now();
        
        // Define default permissions if not already present
        self.add_permission_internal(
            &mut permissions,
            "storage_access",
            "Storage Access",
            "Allow the application to read and write files on your device",
            PermissionLevel::AskFirstTime,
            "File System",
            true,
            now,
        )?;
        
        self.add_permission_internal(
            &mut permissions,
            "network_access",
            "Network Access",
            "Allow the application to connect to the internet",
            PermissionLevel::AlwaysAllow,
            "Network",
            true,
            now,
        )?;
        
        self.add_permission_internal(
            &mut permissions,
            "offline_mode",
            "Offline Mode",
            "Allow the application to operate without internet connection",
            PermissionLevel::AlwaysAllow,
            "Network",
            false,
            now,
        )?;
        
        self.add_permission_internal(
            &mut permissions,
            "telemetry",
            "Usage Telemetry",
            "Allow the application to collect anonymous usage data",
            PermissionLevel::AskFirstTime,
            "Privacy",
            false,
            now,
        )?;
        
        self.add_permission_internal(
            &mut permissions,
            "api_access",
            "API Access",
            "Allow the application to connect to cloud services",
            PermissionLevel::AlwaysAllow,
            "Network",
            true,
            now,
        )?;
        
        self.add_permission_internal(
            &mut permissions,
            "clipboard",
            "Clipboard Access",
            "Allow the application to read from and write to the clipboard",
            PermissionLevel::AskFirstTime,
            "Privacy",
            false,
            now,
        )?;
        
        self.add_permission_internal(
            &mut permissions,
            "notifications",
            "Show Notifications",
            "Allow the application to show system notifications",
            PermissionLevel::AskFirstTime,
            "System",
            false,
            now,
        )?;
        
        self.add_permission_internal(
            &mut permissions,
            "location",
            "Access Location",
            "Allow the application to access your approximate location",
            PermissionLevel::AskEveryTime,
            "Privacy",
            false,
            now,
        )?;
        
        self.add_permission_internal(
            &mut permissions,
            "camera",
            "Camera Access",
            "Allow the application to access your camera",
            PermissionLevel::AskEveryTime,
            "Privacy",
            false,
            now,
        )?;
        
        self.add_permission_internal(
            &mut permissions,
            "microphone",
            "Microphone Access",
            "Allow the application to access your microphone",
            PermissionLevel::AskEveryTime,
            "Privacy",
            false,
            now,
        )?;
        
        self.add_permission_internal(
            &mut permissions,
            "background_tasks",
            "Background Tasks",
            "Allow the application to run tasks in the background",
            PermissionLevel::AskFirstTime,
            "System",
            false,
            now,
        )?;
        
        self.add_permission_internal(
            &mut permissions,
            "autostart",
            "Start at Login",
            "Allow the application to start automatically when you log in",
            PermissionLevel::AskFirstTime,
            "System",
            false,
            now,
        )?;
        
        self.add_permission_internal(
            &mut permissions,
            "sync",
            "Sync Data",
            "Allow the application to synchronize your data with the cloud",
            PermissionLevel::AskFirstTime,
            "Data",
            false,
            now,
        )?;
        
        self.add_permission_internal(
            &mut permissions,
            "e2ee",
            "End-to-End Encryption",
            "Allow the application to encrypt your data before sending it to the cloud",
            PermissionLevel::AlwaysAllow,
            "Security",
            false,
            now,
        )?;
        
        self.add_permission_internal(
            &mut permissions,
            "secure_enclave",
            "Secure Enclave Access",
            "Allow the application to store credentials in your device's secure enclave",
            PermissionLevel::AlwaysAllow,
            "Security",
            false,
            now,
        )?;
        
        // Save permissions to disk
        self.save_permissions()?;
        
        Ok(())
    }
    
    /// Add a permission internally
    fn add_permission_internal(
        &self,
        permissions: &mut HashMap<String, Permission>,
        id: &str,
        name: &str,
        description: &str,
        level: PermissionLevel,
        category: &str,
        required: bool,
        last_modified: SystemTime,
    ) -> Result<()> {
        // Only add if not already present
        if !permissions.contains_key(id) {
            let permission = Permission {
                id: id.to_string(),
                name: name.to_string(),
                description: description.to_string(),
                level,
                category: category.to_string(),
                last_modified,
                usage_count: 0,
                required,
            };
            
            permissions.insert(id.to_string(), permission);
            debug!("Added permission: {}", id);
        }
        
        Ok(())
    }
    
    /// Update configuration
    pub fn update_config(
        &self,
        default_level: PermissionLevel,
        interactive: bool,
    ) -> Result<()> {
        // Update settings
        let mut this = unsafe { &mut *(self as *const Self as *mut Self) };
        this.default_level = default_level;
        this.interactive = interactive;
        
        info!("Updated permission manager configuration");
        
        Ok(())
    }
    
    /// Set the callback for requesting permissions
    pub fn set_permission_callback(&self, callback: PermissionCallback) -> Result<()> {
        let mut cb = self.permission_callback.write().unwrap();
        *cb = Some(callback);
        
        Ok(())
    }
    
    /// Load permissions from disk
    fn load_permissions(&self) -> Result<()> {
        let _lock = self.file_lock.lock().unwrap();
        let permissions_path = config_path(PERMISSIONS_FILE);
        
        if !permissions_path.exists() {
            // No permissions file yet
            return Ok(());
        }
        
        let permissions_data = fs::read_to_string(&permissions_path)
            .map_err(|e| format!("Failed to read permissions file: {}", e))?;
        
        let loaded_permissions: HashMap<String, Permission> = serde_json::from_str(&permissions_data)
            .map_err(|e| format!("Failed to parse permissions file: {}", e))?;
        
        // Update permissions
        let mut permissions = self.permissions.write().unwrap();
        *permissions = loaded_permissions;
        
        debug!("Loaded {} permissions", permissions.len());
        
        Ok(())
    }
    
    /// Save permissions to disk
    fn save_permissions(&self) -> Result<()> {
        let _lock = self.file_lock.lock().unwrap();
        let permissions_path = config_path(PERMISSIONS_FILE);
        
        let permissions = self.permissions.read().unwrap();
        
        let permissions_data = serde_json::to_string_pretty(&*permissions)
            .map_err(|e| format!("Failed to serialize permissions: {}", e))?;
        
        fs::write(&permissions_path, permissions_data)
            .map_err(|e| format!("Failed to write permissions file: {}", e))?;
        
        debug!("Saved {} permissions", permissions.len());
        
        Ok(())
    }
    
    /// Add a new permission
    pub fn add_permission(
        &self,
        id: &str,
        name: &str,
        description: &str,
        level: Option<PermissionLevel>,
        category: &str,
        required: bool,
    ) -> Result<()> {
        let mut permissions = self.permissions.write().unwrap();
        
        // Use provided level or default
        let level = level.unwrap_or(self.default_level);
        
        self.add_permission_internal(
            &mut permissions,
            id,
            name,
            description,
            level,
            category,
            required,
            SystemTime::now(),
        )?;
        
        // Save changes
        drop(permissions);
        self.save_permissions()?;
        
        info!("Added permission: {}", id);
        record_counter("security.permissions.added", 1.0, None);
        
        Ok(())
    }
    
    /// Get a permission
    pub fn get_permission(&self, id: &str) -> Result<Permission> {
        let permissions = self.permissions.read().unwrap();
        
        if let Some(permission) = permissions.get(id) {
            Ok(permission.clone())
        } else {
            Err(format!("Permission '{}' not found", id).into())
        }
    }
    
    /// Set permission level
    pub fn set_permission_level(
        &self,
        id: &str,
        level: PermissionLevel,
    ) -> Result<()> {
        let mut permissions = self.permissions.write().unwrap();
        
        if let Some(permission) = permissions.get_mut(id) {
            // Update permission
            permission.level = level;
            permission.last_modified = SystemTime::now();
            
            // Save changes
            drop(permissions);
            self.save_permissions()?;
            
            info!("Updated permission level for {}: {:?}", id, level);
            record_counter("security.permissions.updated", 1.0, None);
            
            Ok(())
        } else {
            Err(format!("Permission '{}' not found", id).into())
        }
    }
    
    /// Get all permissions
    pub fn get_all_permissions(&self) -> Result<Vec<Permission>> {
        let permissions = self.permissions.read().unwrap();
        
        Ok(permissions.values().cloned().collect())
    }
    
    /// Check if a permission is granted
    pub fn check_permission(&self, id: &str) -> Result<bool> {
        let mut permissions = self.permissions.write().unwrap();
        
        if let Some(permission) = permissions.get_mut(id) {
            // Check permission level
            let granted = match permission.level {
                PermissionLevel::AlwaysAllow => true,
                PermissionLevel::NeverAllow => false,
                PermissionLevel::AskFirstTime | PermissionLevel::AskEveryTime => {
                    // Need to request permission interactively
                    false
                }
            };
            
            // Update usage count
            if granted {
                permission.usage_count += 1;
                
                // Save changes periodically
                if permission.usage_count % 10 == 0 {
                    drop(permissions);
                    self.save_permissions()?;
                }
            }
            
            record_counter("security.permissions.checked", 1.0, None);
            
            Ok(granted)
        } else if self.default_level == PermissionLevel::AlwaysAllow {
            // Permission not found, but default is to allow
            warn!("Permission '{}' not found, using default (allow)", id);
            Ok(true)
        } else {
            // Permission not found and default is not to allow
            warn!("Permission '{}' not found, using default (deny)", id);
            Ok(false)
        }
    }
    
    /// Request permission interactively
    pub fn request_permission(
        &self,
        id: &str,
        reason: &str,
    ) -> Result<bool> {
        let mut permissions = self.permissions.write().unwrap();
        
        if let Some(permission) = permissions.get_mut(id) {
            // Check permission level
            let (granted, interaction_type) = match permission.level {
                PermissionLevel::AlwaysAllow => (true, PermissionInteractionType::None),
                PermissionLevel::NeverAllow => (false, PermissionInteractionType::None),
                PermissionLevel::AskFirstTime => {
                    if permission.usage_count > 0 {
                        // Already asked and granted before
                        (true, PermissionInteractionType::None)
                    } else {
                        // Need to ask for the first time
                        (false, PermissionInteractionType::Modal)
                    }
                },
                PermissionLevel::AskEveryTime => {
                    // Always ask
                    (false, PermissionInteractionType::Modal)
                }
            };
            
            if granted {
                // Permission already granted
                permission.usage_count += 1;
                
                // Record the request
                let request = PermissionRequest {
                    id: uuid::Uuid::new_v4().to_string(),
                    permission_id: id.to_string(),
                    reason: reason.to_string(),
                    timestamp: SystemTime::now(),
                    granted: true,
                    context: HashMap::new(),
                };
                
                self.request_history.write().unwrap().push(request);
                
                // Save changes periodically
                if permission.usage_count % 10 == 0 {
                    drop(permissions);
                    self.save_permissions()?;
                }
                
                record_counter("security.permissions.granted", 1.0, None);
                
                return Ok(true);
            } else if !self.interactive || interaction_type == PermissionInteractionType::None {
                // Interactive mode disabled or no interaction needed, deny
                
                // Record the request
                let request = PermissionRequest {
                    id: uuid::Uuid::new_v4().to_string(),
                    permission_id: id.to_string(),
                    reason: reason.to_string(),
                    timestamp: SystemTime::now(),
                    granted: false,
                    context: HashMap::new(),
                };
                
                self.request_history.write().unwrap().push(request);
                
                record_counter("security.permissions.denied", 1.0, None);
                
                return Ok(false);
            } else {
                // Need to request permission interactively
                let callback = self.permission_callback.read().unwrap();
                
                if let Some(ref cb) = *callback {
                    // Call the callback to request permission
                    let granted = cb(
                        &format!("{}: {}", permission.name, permission.description),
                        reason,
                    );
                    
                    // Record the request
                    let request = PermissionRequest {
                        id: uuid::Uuid::new_v4().to_string(),
                        permission_id: id.to_string(),
                        reason: reason.to_string(),
                        timestamp: SystemTime::now(),
                        granted,
                        context: HashMap::new(),
                    };
                    
                    self.request_history.write().unwrap().push(request);
                    
                    // If granted, update usage count
                    if granted {
                        permission.usage_count += 1;
                        record_counter("security.permissions.granted", 1.0, None);
                    } else {
                        record_counter("security.permissions.denied", 1.0, None);
                    }
                    
                    // If this was AskFirstTime, update level to Always/Never based on response
                    if permission.level == PermissionLevel::AskFirstTime {
                        permission.level = if granted {
                            PermissionLevel::AlwaysAllow
                        } else {
                            PermissionLevel::NeverAllow
                        };
                        
                        permission.last_modified = SystemTime::now();
                    }
                    
                    // Save changes
                    drop(permissions);
                    self.save_permissions()?;
                    
                    return Ok(granted);
                } else {
                    // No callback registered, deny by default
                    warn!("No permission callback registered, denying permission request");
                    
                    // Record the request
                    let request = PermissionRequest {
                        id: uuid::Uuid::new_v4().to_string(),
                        permission_id: id.to_string(),
                        reason: reason.to_string(),
                        timestamp: SystemTime::now(),
                        granted: false,
                        context: HashMap::new(),
                    };
                    
                    self.request_history.write().unwrap().push(request);
                    
                    record_counter("security.permissions.denied", 1.0, None);
                    
                    return Ok(false);
                }
            }
        } else {
            // Permission not found
            warn!("Permission '{}' not found when requesting", id);
            
            // Create a new permission with default level
            drop(permissions);
            self.add_permission(
                id,
                id, // Use ID as name temporarily
                "Dynamically requested permission",
                Some(self.default_level),
                "Dynamic",
                false,
            )?;
            
            // Recursive call now that the permission exists
            self.request_permission(id, reason)
        }
    }
    
    /// Reset a permission to default
    pub fn reset_permission(
        &self,
        id: &str,
    ) -> Result<()> {
        let mut permissions = self.permissions.write().unwrap();
        
        if let Some(permission) = permissions.get_mut(id) {
            // Reset to default level
            permission.level = self.default_level;
            permission.last_modified = SystemTime::now();
            permission.usage_count = 0;
            
            // Save changes
            drop(permissions);
            self.save_permissions()?;
            
            info!("Reset permission: {}", id);
            record_counter("security.permissions.reset", 1.0, None);
            
            Ok(())
        } else {
            Err(format!("Permission '{}' not found", id).into())
        }
    }
    
    /// Reset all permissions to default
    pub fn reset_all_permissions(&self) -> Result<()> {
        let mut permissions = self.permissions.write().unwrap();
        
        // Reset all permissions
        for (_, permission) in permissions.iter_mut() {
            permission.level = self.default_level;
            permission.last_modified = SystemTime::now();
            permission.usage_count = 0;
        }
        
        // Save changes
        drop(permissions);
        self.save_permissions()?;
        
        info!("Reset all permissions to default");
        record_counter("security.permissions.reset_all", 1.0, None);
        
        Ok(())
    }
    
    /// Get permission request history
    pub fn get_request_history(&self) -> Result<Vec<PermissionRequest>> {
        Ok(self.request_history.read().unwrap().clone())
    }
    
    /// Clear permission request history
    pub fn clear_request_history(&self) -> Result<()> {
        self.request_history.write().unwrap().clear();
        
        info!("Cleared permission request history");
        record_counter("security.permissions.history_cleared", 1.0, None);
        
        Ok(())
    }
    
    /// Get permission statistics
    pub fn get_statistics(&self) -> Result<PermissionStatistics> {
        let permissions = self.permissions.read().unwrap();
        let requests = self.request_history.read().unwrap();
        
        // Count by level
        let mut count_by_level = HashMap::new();
        count_by_level.insert(PermissionLevel::AlwaysAllow, 0);
        count_by_level.insert(PermissionLevel::AskFirstTime, 0);
        count_by_level.insert(PermissionLevel::AskEveryTime, 0);
        count_by_level.insert(PermissionLevel::NeverAllow, 0);
        
        for permission in permissions.values() {
            *count_by_level.entry(permission.level).or_insert(0) += 1;
        }
        
        // Count by category
        let mut count_by_category = HashMap::new();
        for permission in permissions.values() {
            *count_by_category.entry(permission.category.clone()).or_insert(0) += 1;
        }
        
        // Count requests
        let mut granted_count = 0;
        let mut denied_count = 0;
        
        for request in requests.iter() {
            if request.granted {
                granted_count += 1;
            } else {
                denied_count += 1;
            }
        }
        
        // Find most used permissions
        let mut usage_counts: Vec<(&String, &Permission)> = permissions.iter().collect();
        usage_counts.sort_by(|a, b| b.1.usage_count.cmp(&a.1.usage_count));
        
        let most_used = usage_counts.iter()
            .take(5)
            .map(|(id, permission)| ((*id).clone(), permission.usage_count))
            .collect();
        
        // Calculate stats
        let stats = PermissionStatistics {
            total_permissions: permissions.len(),
            count_by_level,
            count_by_category,
            total_requests: requests.len(),
            granted_count,
            denied_count,
            most_used_permissions: most_used,
        };
        
        Ok(stats)
    }
    
    /// Export permissions to JSON
    pub fn export_permissions(&self) -> Result<String> {
        let permissions = self.permissions.read().unwrap();
        
        let json = serde_json::to_string_pretty(&*permissions)
            .map_err(|e| format!("Failed to serialize permissions: {}", e))?;
        
        Ok(json)
    }
    
    /// Import permissions from JSON
    pub fn import_permissions(&self, json: &str) -> Result<()> {
        let imported_permissions: HashMap<String, Permission> = serde_json::from_str(json)
            .map_err(|e| format!("Failed to parse permissions JSON: {}", e))?;
        
        // Update permissions
        let mut permissions = self.permissions.write().unwrap();
        *permissions = imported_permissions;
        
        // Save changes
        drop(permissions);
        self.save_permissions()?;
        
        info!("Imported {} permissions", permissions.len());
        record_counter("security.permissions.imported", 1.0, None);
        
        Ok(())
    }
}

/// Permission statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PermissionStatistics {
    /// Total number of permissions
    pub total_permissions: usize,
    
    /// Count of permissions by level
    pub count_by_level: HashMap<PermissionLevel, usize>,
    
    /// Count of permissions by category
    pub count_by_category: HashMap<String, usize>,
    
    /// Total number of permission requests
    pub total_requests: usize,
    
    /// Number of granted requests
    pub granted_count: usize,
    
    /// Number of denied requests
    pub denied_count: usize,
    
    /// Most frequently used permissions (id, count)
    pub most_used_permissions: Vec<(String, usize)>,
}

/// Default implementation for the permission callback
pub fn default_permission_callback(permission: &str, reason: &str) -> bool {
    // In a real app, this would show a UI dialog
    // For now, just log and deny
    warn!("Permission request: {} - Reason: {}", permission, reason);
    warn!("No UI available, denying by default");
    
    false
}
</file>

<file path="src/services/ai.rs">
use crate::ai::router::{get_model_router, NetworkStatus, RouterStrategy};
use crate::models::messages::{Message, MessageError, ConversationMessage, MessageStatus};
use crate::models::{Conversation, Model};
use crate::utils::config;
use crate::utils::events::{events, get_event_system};
use log::{debug, error, info, warn};
use std::collections::HashMap;
use std::sync::{Arc, Mutex, RwLock};
use tokio::sync::mpsc;
use uuid::Uuid;

/// Service for interacting with AI models
pub struct AiService {
    /// Model router
    router: &'static crate::ai::router::ModelRouter,
    
    /// Available models cache
    models: Arc<RwLock<Vec<Model>>>,
    
    /// Active conversations with their message history
    conversations: Arc<RwLock<HashMap<String, Vec<ConversationMessage>>>>,
    
    /// Message listeners (for UI updates)
    message_listeners: Arc<Mutex<HashMap<String, Vec<mpsc::Sender<ConversationMessage>>>>>,
    
    /// Streaming sessions
    streaming_sessions: Arc<Mutex<HashMap<String, mpsc::Sender<Result<Message, MessageError>>>>>,
}

impl AiService {
    /// Create a new AI service
    pub fn new() -> Self {
        // Get model router
        let router = get_model_router();
        
        // Set router strategy based on config
        let config = config::get_config();
        let config_guard = config.lock().unwrap();
        
        let strategy_str = config_guard
            .get_string("ai.router.strategy")
            .unwrap_or_else(|| "prefer_online".to_string());
        
        let strategy = match strategy_str.as_str() {
            "prefer_online" => RouterStrategy::PreferOnline,
            "prefer_local" => RouterStrategy::PreferLocal,
            "online_only" => RouterStrategy::OnlineOnly,
            "local_only" => RouterStrategy::LocalOnly,
            "round_robin" => RouterStrategy::RoundRobin,
            "rules_based" => RouterStrategy::RulesBased,
            _ => RouterStrategy::PreferOnline,
        };
        
        router.set_strategy(strategy);
        
        // Initialize service
        let service = Self {
            router,
            models: Arc::new(RwLock::new(Vec::new())),
            conversations: Arc::new(RwLock::new(HashMap::new())),
            message_listeners: Arc::new(Mutex::new(HashMap::new())),
            streaming_sessions: Arc::new(Mutex::new(HashMap::new())),
        };
        
        // Set up a background task to periodically refresh available models
        let models_clone = service.models.clone();
        tokio::spawn(async move {
            loop {
                match get_model_router().get_available_models().await {
                    models => {
                        // Update cache
                        let mut models_guard = models_clone.write().unwrap();
                        *models_guard = models;
                    }
                }
                
                // Sleep for 5 minutes before refreshing again
                tokio::time::sleep(std::time::Duration::from_secs(5 * 60)).await;
            }
        });
        
        service
    }
    
    /// Set network status
    pub fn set_network_status(&self, status: NetworkStatus) {
        self.router.set_network_status(status);
    }
    
    /// Get available models
    pub async fn available_models(&self) -> Vec<Model> {
        // Check cache first
        {
            let models = self.models.read().unwrap();
            if !models.is_empty() {
                return models.clone();
            }
        }
        
        // Fetch models from router
        let models = self.router.get_available_models().await;
        
        // Update cache
        {
            let mut models_guard = self.models.write().unwrap();
            *models_guard = models.clone();
        }
        
        models
    }
    
    /// Create a new conversation
    pub fn create_conversation(&self, title: &str, model: Model) -> Conversation {
        let conversation = Conversation::new(title, model);
        
        // Store conversation
        {
            let mut conversations = self.conversations.write().unwrap();
            conversations.insert(conversation.id.clone(), Vec::new());
        }
        
        conversation
    }
    
    /// Get a conversation by ID
    pub fn get_conversation(&self, id: &str) -> Option<Conversation> {
        // In a real implementation, this would fetch from storage
        None
    }
    
    /// Delete a conversation
    pub fn delete_conversation(&self, id: &str) -> Result<(), String> {
        // Remove conversation messages
        {
            let mut conversations = self.conversations.write().unwrap();
            if conversations.remove(id).is_none() {
                return Err(format!("Conversation {} not found", id));
            }
        }
        
        // Remove listeners
        {
            let mut listeners = self.message_listeners.lock().unwrap();
            listeners.remove(id);
        }
        
        Ok(())
    }
    
    /// Get conversation message history
    pub fn get_messages(&self, conversation_id: &str) -> Vec<ConversationMessage> {
        let conversations = self.conversations.read().unwrap();
        conversations
            .get(conversation_id)
            .cloned()
            .unwrap_or_default()
    }
    
    /// Send a message in a conversation
    pub async fn send_message(
        &self,
        conversation_id: &str,
        model_id: &str,
        message: Message,
    ) -> Result<ConversationMessage, MessageError> {
        // Store message in history with 'sending' status
        let conversation_message = ConversationMessage {
            message: message.clone(),
            parent_ids: Vec::new(),
            completed_at: None,
            partial_content: None,
            status: MessageStatus::Sending,
        };
        
        self.add_message_to_history(conversation_id, conversation_message.clone());
        
        // Send message through router
        match self.router.complete(model_id, message).await {
            Ok(response) => {
                // Create response message
                let response_message = ConversationMessage {
                    message: response,
                    parent_ids: vec![conversation_message.message.id.clone()],
                    completed_at: Some(std::time::SystemTime::now()),
                    partial_content: None,
                    status: MessageStatus::Complete,
                };
                
                // Update original message status to complete
                self.update_message_status(
                    conversation_id,
                    &conversation_message.message.id,
                    MessageStatus::Complete,
                );
                
                // Store response in history
                self.add_message_to_history(conversation_id, response_message.clone());
                
                Ok(response_message)
            }
            Err(e) => {
                // Update message status to failed
                self.update_message_status(
                    conversation_id,
                    &conversation_message.message.id,
                    MessageStatus::Failed,
                );
                
                Err(e)
            }
        }
    }
    
    /// Stream a message in a conversation
    pub async fn stream_message(
        &self,
        conversation_id: &str,
        model_id: &str,
        message: Message,
    ) -> Result<mpsc::Receiver<ConversationMessage>, MessageError> {
        // Create streaming channel for UI
        let (tx, rx) = mpsc::channel(32);
        
        // Store message in history with 'sending' status
        let conversation_message = ConversationMessage {
            message: message.clone(),
            parent_ids: Vec::new(),
            completed_at: None,
            partial_content: None,
            status: MessageStatus::Sending,
        };
        
        self.add_message_to_history(conversation_id, conversation_message.clone());
        
        // Start streaming through router
        match self.router.stream(model_id, message).await {
            Ok(mut stream) => {
                // Create initial response message
                let response_id = Uuid::new_v4().to_string();
                let mut response_message = ConversationMessage {
                    message: Message {
                        id: response_id.clone(),
                        role: crate::models::messages::MessageRole::Assistant,
                        content: crate::models::messages::MessageContent {
                            parts: vec![crate::models::messages::ContentType::Text {
                                text: String::new(),
                            }],
                        },
                        metadata: Some(HashMap::from([(
                            "model".to_string(),
                            serde_json::to_value(model_id).unwrap(),
                        )])),
                        created_at: std::time::SystemTime::now(),
                    },
                    parent_ids: vec![conversation_message.message.id.clone()],
                    completed_at: None,
                    partial_content: Some(String::new()),
                    status: MessageStatus::Streaming,
                };
                
                // Update original message status to complete
                self.update_message_status(
                    conversation_id,
                    &conversation_message.message.id,
                    MessageStatus::Complete,
                );
                
                // Store initial response in history
                self.add_message_to_history(conversation_id, response_message.clone());
                
                // Send initial message to UI
                let _ = tx.send(response_message.clone()).await;
                
                // Store streaming session
                let stream_id = Uuid::new_v4().to_string();
                {
                    let mut sessions = self.streaming_sessions.lock().unwrap();
                    sessions.insert(stream_id.clone(), stream);
                }
                
                // Handle streaming in a separate task
                let conversations = self.conversations.clone();
                let tx_clone = tx.clone();
                let conversation_id = conversation_id.to_string();
                
                tokio::spawn(async move {
                    let mut full_text = String::new();
                    
                    // Process streaming messages
                    while let Some(result) = stream.recv().await {
                        match result {
                            Ok(chunk) => {
                                // Extract text content
                                if let Some(text) = chunk.text_content() {
                                    // Append to full text
                                    full_text.push_str(text);
                                    
                                    // Update response message
                                    response_message.partial_content = Some(full_text.clone());
                                    response_message.message.content.parts = vec![
                                        crate::models::messages::ContentType::Text {
                                            text: full_text.clone(),
                                        },
                                    ];
                                    
                                    // Update in history
                                    {
                                        let mut convos = conversations.write().unwrap();
                                        if let Some(messages) = convos.get_mut(&conversation_id) {
                                            for msg in messages.iter_mut() {
                                                if msg.message.id == response_id {
                                                    *msg = response_message.clone();
                                                    break;
                                                }
                                            }
                                        }
                                    }
                                    
                                    // Send update to UI
                                    let _ = tx_clone.send(response_message.clone()).await;
                                }
                            }
                            Err(e) => {
                                error!("Streaming error: {}", e);
                                
                                // Update status to failed
                                response_message.status = MessageStatus::Failed;
                                
                                // Update in history
                                {
                                    let mut convos = conversations.write().unwrap();
                                    if let Some(messages) = convos.get_mut(&conversation_id) {
                                        for msg in messages.iter_mut() {
                                            if msg.message.id == response_id {
                                                *msg = response_message.clone();
                                                break;
                                            }
                                        }
                                    }
                                }
                                
                                // Send final update to UI
                                let _ = tx_clone.send(response_message.clone()).await;
                                break;
                            }
                        }
                    }
                    
                    // If we got here, streaming is complete
                    if response_message.status == MessageStatus::Streaming {
                        response_message.status = MessageStatus::Complete;
                        response_message.completed_at = Some(std::time::SystemTime::now());
                        response_message.partial_content = None;
                        
                        // Update in history
                        {
                            let mut convos = conversations.write().unwrap();
                            if let Some(messages) = convos.get_mut(&conversation_id) {
                                for msg in messages.iter_mut() {
                                    if msg.message.id == response_id {
                                        *msg = response_message.clone();
                                        break;
                                    }
                                }
                            }
                        }
                        
                        // Send final update to UI
                        let _ = tx_clone.send(response_message).await;
                    }
                });
                
                Ok(rx)
            }
            Err(e) => {
                // Update message status to failed
                self.update_message_status(
                    conversation_id,
                    &conversation_message.message.id,
                    MessageStatus::Failed,
                );
                
                Err(e)
            }
        }
    }
    
    /// Cancel a streaming message
    pub async fn cancel_streaming(
        &self,
        conversation_id: &str,
        message_id: &str,
    ) -> Result<(), MessageError> {
        // Update message status to cancelled
        self.update_message_status(conversation_id, message_id, MessageStatus::Cancelled);
        
        // Tell router to cancel the stream
        // In a real implementation, we would track which stream belongs to which message
        self.router.cancel_stream(message_id).await
    }
    
    /// Register a message listener for a conversation
    pub async fn register_listener(
        &self,
        conversation_id: &str,
    ) -> mpsc::Receiver<ConversationMessage> {
        let (tx, rx) = mpsc::channel(32);
        
        // Store listener
        {
            let mut listeners = self.message_listeners.lock().unwrap();
            let conversation_listeners = listeners
                .entry(conversation_id.to_string())
                .or_insert_with(Vec::new);
            conversation_listeners.push(tx);
        }
        
        // Send existing messages
        let messages = self.get_messages(conversation_id);
        let tx_clone = tx.clone();
        
        tokio::spawn(async move {
            for message in messages {
                let _ = tx_clone.send(message).await;
            }
        });
        
        rx
    }
    
    /// Add a message to conversation history
    fn add_message_to_history(&self, conversation_id: &str, message: ConversationMessage) {
        // Add to history
        {
            let mut conversations = self.conversations.write().unwrap();
            let conversation_messages = conversations
                .entry(conversation_id.to_string())
                .or_insert_with(Vec::new);
            conversation_messages.push(message.clone());
        }
        
        // Notify listeners
        self.notify_listeners(conversation_id, &message);
    }
    
    /// Update message status in history
    fn update_message_status(&self, conversation_id: &str, message_id: &str, status: MessageStatus) {
        let mut updated_message = None;
        
        // Update in history
        {
            let mut conversations = self.conversations.write().unwrap();
            if let Some(messages) = conversations.get_mut(conversation_id) {
                for msg in messages.iter_mut() {
                    if msg.message.id == message_id {
                        msg.status = status;
                        updated_message = Some(msg.clone());
                        break;
                    }
                }
            }
        }
        
        // Notify listeners
        if let Some(message) = updated_message {
            self.notify_listeners(conversation_id, &message);
        }
    }
    
    /// Notify all listeners for a conversation
    fn notify_listeners(&self, conversation_id: &str, message: &ConversationMessage) {
        let listeners = self.message_listeners.lock().unwrap();
        if let Some(conversation_listeners) = listeners.get(conversation_id) {
            let message = message.clone();
            for listener in conversation_listeners {
                let tx = listener.clone();
                let message = message.clone();
                
                tokio::spawn(async move {
                    let _ = tx.send(message).await;
                });
            }
        }
    }
}

/// Global AI service instance
static AI_SERVICE: once_cell::sync::OnceCell<AiService> = once_cell::sync::OnceCell::new();

/// Get the global AI service instance
pub fn get_ai_service() -> &'static AiService {
    AI_SERVICE.get_or_init(|| AiService::new())
}
</file>

<file path="src/services/api.rs">
use crate::utils::config;
use log::{debug, error, info, warn};
use reqwest::{Client, Response, StatusCode};
use serde::de::DeserializeOwned;
use serde::{Deserialize, Serialize};
use std::time::Duration;

/// Service for handling HTTP API requests
pub struct ApiService {
    /// HTTP client
    client: Client,
    
    /// Base API URL
    base_url: String,
    
    /// API key
    api_key: String,
}

/// API error type
#[derive(Debug, thiserror::Error)]
pub enum ApiError {
    #[error("Network error: {0}")]
    NetworkError(String),
    
    #[error("HTTP error {0}: {1}")]
    HttpError(u16, String),
    
    #[error("Authentication error: {0}")]
    AuthError(String),
    
    #[error("Rate limit error: {0}")]
    RateLimitError(String),
    
    #[error("Serialization error: {0}")]
    SerializationError(String),
    
    #[error("Timeout after {0:?}")]
    Timeout(Duration),
    
    #[error("Unknown error: {0}")]
    Unknown(String),
}

impl ApiService {
    /// Create a new API service
    pub fn new() -> Self {
        // Load configuration
        let config = config::get_config();
        let config_guard = config.lock().unwrap();
        
        let base_url = config_guard
            .get_string("api.base_url")
            .unwrap_or_else(|| "https://api.anthropic.com".to_string());
        
        let api_key = config_guard
            .get_string("api.key")
            .unwrap_or_else(|| String::new());
        
        // Create HTTP client
        let client = Client::builder()
            .timeout(Duration::from_secs(120))
            .build()
            .unwrap();
        
        Self {
            client,
            base_url,
            api_key,
        }
    }
    
    /// Set API key
    pub fn set_api_key(&mut self, api_key: String) {
        self.api_key = api_key;
    }
    
    /// Set base URL
    pub fn set_base_url(&mut self, base_url: String) {
        self.base_url = base_url;
    }
    
    /// Make a GET request
    pub async fn get<T: DeserializeOwned>(&self, path: &str) -> Result<T, ApiError> {
        let url = format!("{}{}", self.base_url, path);
        
        let response = self
            .client
            .get(&url)
            .header("x-api-key", &self.api_key)
            .header("anthropic-version", "2023-06-01")
            .send()
            .await
            .map_err(|e| ApiError::NetworkError(e.to_string()))?;
        
        self.process_response(response).await
    }
    
    /// Make a POST request
    pub async fn post<T: DeserializeOwned, D: Serialize>(
        &self,
        path: &str,
        data: &D,
    ) -> Result<T, ApiError> {
        let url = format!("{}{}", self.base_url, path);
        
        let response = self
            .client
            .post(&url)
            .header("x-api-key", &self.api_key)
            .header("anthropic-version", "2023-06-01")
            .header("content-type", "application/json")
            .json(data)
            .send()
            .await
            .map_err(|e| ApiError::NetworkError(e.to_string()))?;
        
        self.process_response(response).await
    }
    
    /// Process the API response
    async fn process_response<T: DeserializeOwned>(&self, response: Response) -> Result<T, ApiError> {
        match response.status() {
            StatusCode::OK | StatusCode::CREATED | StatusCode::ACCEPTED => {
                // Parse response body
                response
                    .json::<T>()
                    .await
                    .map_err(|e| ApiError::SerializationError(e.to_string()))
            }
            StatusCode::UNAUTHORIZED | StatusCode::FORBIDDEN => {
                let error_text = response.text().await.unwrap_or_default();
                Err(ApiError::AuthError(error_text))
            }
            StatusCode::TOO_MANY_REQUESTS => {
                let error_text = response.text().await.unwrap_or_default();
                Err(ApiError::RateLimitError(error_text))
            }
            status => {
                let error_text = response.text().await.unwrap_or_default();
                Err(ApiError::HttpError(status.as_u16(), error_text))
            }
        }
    }
}

/// Global API service instance
static API_SERVICE: once_cell::sync::OnceCell<ApiService> = once_cell::sync::OnceCell::new();

/// Get the global API service instance
pub fn get_api_service() -> &'static ApiService {
    API_SERVICE.get_or_init(|| ApiService::new())
}
</file>

<file path="src/services/auth.rs">
use crate::services::api::{get_api_service, ApiError};
use crate::utils::config;
use log::{debug, error, info, warn};
use serde::{Deserialize, Serialize};
use std::sync::{Arc, RwLock};
use std::time::{Duration, SystemTime};

/// Authentication service for handling API keys and tokens
pub struct AuthService {
    /// Current API key
    api_key: Arc<RwLock<String>>,
    
    /// Current session token
    session_token: Arc<RwLock<Option<String>>>,
    
    /// Session expiration time
    session_expiry: Arc<RwLock<Option<SystemTime>>>,
    
    /// Organization ID (for multi-org accounts)
    organization_id: Arc<RwLock<Option<String>>>,
    
    /// Is authenticated
    is_authenticated: Arc<RwLock<bool>>,
}

/// API key validation response
#[derive(Debug, Serialize, Deserialize)]
struct ApiKeyValidation {
    valid: bool,
    expires_at: Option<String>,
    organization_id: Option<String>,
}

impl AuthService {
    /// Create a new authentication service
    pub fn new() -> Self {
        // Load configuration
        let config = config::get_config();
        let config_guard = config.lock().unwrap();
        
        let api_key = config_guard
            .get_string("api.key")
            .unwrap_or_else(|| String::new());
        
        Self {
            api_key: Arc::new(RwLock::new(api_key)),
            session_token: Arc::new(RwLock::new(None)),
            session_expiry: Arc::new(RwLock::new(None)),
            organization_id: Arc::new(RwLock::new(None)),
            is_authenticated: Arc::new(RwLock::new(false)),
        }
    }
    
    /// Set API key
    pub fn set_api_key(&self, api_key: String) -> Result<(), String> {
        let mut api_key_guard = self.api_key.write().unwrap();
        *api_key_guard = api_key;
        
        // Reset authentication state
        {
            let mut auth_guard = self.is_authenticated.write().unwrap();
            *auth_guard = false;
        }
        {
            let mut token_guard = self.session_token.write().unwrap();
            *token_guard = None;
        }
        {
            let mut expiry_guard = self.session_expiry.write().unwrap();
            *expiry_guard = None;
        }
        
        // Save to config
        config::set_value("api.key", serde_json::Value::String(api_key_guard.clone()))
            .map_err(|e| e.to_string())?;
        
        // Save config to disk
        config::save_config().map_err(|e| e.to_string())?;
        
        Ok(())
    }
    
    /// Get current API key
    pub fn get_api_key(&self) -> String {
        self.api_key.read().unwrap().clone()
    }
    
    /// Validate API key
    pub async fn validate_api_key(&self) -> Result<bool, String> {
        let api_key = self.api_key.read().unwrap().clone();
        
        if api_key.is_empty() {
            return Err("API key is empty".to_string());
        }
        
        // In a real implementation, we would call the API to validate the key
        // For now, we'll simulate a successful validation if the key is non-empty
        
        // Simulated API key validation
        let validation = ApiKeyValidation {
            valid: true,
            expires_at: Some("2025-12-31T00:00:00Z".to_string()),
            organization_id: Some("org_123456".to_string()),
        };
        
        // Update state based on validation
        if validation.valid {
            // Set organization ID if provided
            if let Some(org_id) = validation.organization_id {
                let mut org_guard = self.organization_id.write().unwrap();
                *org_guard = Some(org_id);
            }
            
            // Mark as authenticated
            {
                let mut auth_guard = self.is_authenticated.write().unwrap();
                *auth_guard = true;
            }
            
            Ok(true)
        } else {
            // Mark as not authenticated
            {
                let mut auth_guard = self.is_authenticated.write().unwrap();
                *auth_guard = false;
            }
            
            Ok(false)
        }
    }
    
    /// Check if authenticated
    pub fn is_authenticated(&self) -> bool {
        // Check if we have a valid session token
        if let Some(expiry) = *self.session_expiry.read().unwrap() {
            if expiry > SystemTime::now() {
                return true;
            }
        }
        
        // Otherwise check the general authentication state
        *self.is_authenticated.read().unwrap()
    }
    
    /// Get organization ID
    pub fn get_organization_id(&self) -> Option<String> {
        self.organization_id.read().unwrap().clone()
    }
    
    /// Set organization ID
    pub fn set_organization_id(&self, organization_id: Option<String>) {
        let mut org_guard = self.organization_id.write().unwrap();
        *org_guard = organization_id;
    }
    
    /// Logout and clear credentials
    pub fn logout(&self) -> Result<(), String> {
        // Clear authentication state
        {
            let mut auth_guard = self.is_authenticated.write().unwrap();
            *auth_guard = false;
        }
        {
            let mut token_guard = self.session_token.write().unwrap();
            *token_guard = None;
        }
        {
            let mut expiry_guard = self.session_expiry.write().unwrap();
            *expiry_guard = None;
        }
        
        // We don't clear the API key from memory
        // but we could clear it from config if desired
        
        Ok(())
    }
}

/// Global authentication service instance
static AUTH_SERVICE: once_cell::sync::OnceCell<AuthService> = once_cell::sync::OnceCell::new();

/// Get the global authentication service instance
pub fn get_auth_service() -> &'static AuthService {
    AUTH_SERVICE.get_or_init(|| AuthService::new())
}
</file>

<file path="src/services/chat.rs">
use crate::models::messages::{Message, MessageError, ConversationMessage, MessageStatus};
use crate::models::{Conversation, Model};
use crate::services::mcp::{get_mcp_service, McpService};
use crate::utils::config;
use log::{debug, error, info, warn};
use std::collections::HashMap;
use std::sync::{Arc, Mutex, RwLock};
use tokio::sync::mpsc;
use uuid::Uuid;

/// Service for managing chat functionality
pub struct ChatService {
    /// MCP service for communication
    mcp_service: &'static McpService,
    
    /// Active conversations with their message history
    conversations: Arc<RwLock<HashMap<String, Vec<ConversationMessage>>>>,
    
    /// Message listeners (for UI updates)
    message_listeners: Arc<Mutex<HashMap<String, Vec<mpsc::Sender<ConversationMessage>>>>>,
}

impl ChatService {
    /// Create a new chat service
    pub fn new() -> Self {
        Self {
            mcp_service: get_mcp_service(),
            conversations: Arc::new(RwLock::new(HashMap::new())),
            message_listeners: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    /// Get available models
    pub fn available_models(&self) -> Vec<Model> {
        self.mcp_service.available_models()
    }
    
    /// Create a new conversation
    pub fn create_conversation(&self, title: &str, model: Model) -> Conversation {
        let conversation = self.mcp_service.create_conversation(title, model);
        
        // Initialize message history
        {
            let mut conversations = self.conversations.write().unwrap();
            conversations.insert(conversation.id.clone(), Vec::new());
        }
        
        conversation
    }
    
    /// Get a conversation by ID
    pub fn get_conversation(&self, id: &str) -> Option<Conversation> {
        self.mcp_service.get_conversation(id)
    }
    
    /// Delete a conversation
    pub fn delete_conversation(&self, id: &str) -> Result<(), String> {
        // Remove from MCP service
        let result = self.mcp_service.delete_conversation(id);
        
        // Remove message history
        if result.is_ok() {
            let mut conversations = self.conversations.write().unwrap();
            conversations.remove(id);
            
            // Notify listeners that conversation was deleted
            let mut listeners = self.message_listeners.lock().unwrap();
            listeners.remove(id);
        }
        
        result
    }
    
    /// Get conversation message history
    pub fn get_messages(&self, conversation_id: &str) -> Vec<ConversationMessage> {
        let conversations = self.conversations.read().unwrap();
        conversations
            .get(conversation_id)
            .cloned()
            .unwrap_or_default()
    }
    
    /// Send a message in a conversation
    pub async fn send_message(
        &self,
        conversation_id: &str,
        message: Message,
    ) -> Result<ConversationMessage, MessageError> {
        // Store message in history with 'sending' status
        let conversation_message = ConversationMessage {
            message: message.clone(),
            parent_ids: Vec::new(),
            completed_at: None,
            partial_content: None,
            status: MessageStatus::Sending,
        };
        
        self.add_message_to_history(conversation_id, conversation_message.clone());
        
        // Send message through MCP service
        match self.mcp_service.send_message(conversation_id, message).await {
            Ok(response) => {
                // Create response message
                let response_message = ConversationMessage {
                    message: response,
                    parent_ids: vec![conversation_message.message.id.clone()],
                    completed_at: Some(std::time::SystemTime::now()),
                    partial_content: None,
                    status: MessageStatus::Complete,
                };
                
                // Update original message status to complete
                self.update_message_status(
                    conversation_id,
                    &conversation_message.message.id,
                    MessageStatus::Complete,
                );
                
                // Store response in history
                self.add_message_to_history(conversation_id, response_message.clone());
                
                Ok(response_message)
            }
            Err(e) => {
                // Update message status to failed
                self.update_message_status(
                    conversation_id,
                    &conversation_message.message.id,
                    MessageStatus::Failed,
                );
                
                Err(e)
            }
        }
    }
    
    /// Stream a message in a conversation
    pub async fn stream_message(
        &self,
        conversation_id: &str,
        message: Message,
    ) -> Result<mpsc::Receiver<ConversationMessage>, MessageError> {
        // Create streaming channel for UI
        let (tx, rx) = mpsc::channel(32);
        
        // Store message in history with 'sending' status
        let conversation_message = ConversationMessage {
            message: message.clone(),
            parent_ids: Vec::new(),
            completed_at: None,
            partial_content: None,
            status: MessageStatus::Sending,
        };
        
        self.add_message_to_history(conversation_id, conversation_message.clone());
        
        // Start streaming through MCP service
        match self.mcp_service.stream_message(conversation_id, message).await {
            Ok(mut stream) => {
                // Create initial response message
                let response_id = Uuid::new_v4().to_string();
                let mut response_message = ConversationMessage {
                    message: Message {
                        id: response_id.clone(),
                        role: crate::models::messages::MessageRole::Assistant,
                        content: crate::models::messages::MessageContent {
                            parts: vec![crate::models::messages::ContentType::Text {
                                text: String::new(),
                            }],
                        },
                        metadata: None,
                        created_at: std::time::SystemTime::now(),
                    },
                    parent_ids: vec![conversation_message.message.id.clone()],
                    completed_at: None,
                    partial_content: Some(String::new()),
                    status: MessageStatus::Streaming,
                };
                
                // Update original message status to complete
                self.update_message_status(
                    conversation_id,
                    &conversation_message.message.id,
                    MessageStatus::Complete,
                );
                
                // Store initial response in history
                self.add_message_to_history(conversation_id, response_message.clone());
                
                // Send initial message to UI
                let _ = tx.send(response_message.clone()).await;
                
                // Handle streaming in a separate task
                let conversations = self.conversations.clone();
                let tx_clone = tx.clone();
                let conversation_id = conversation_id.to_string();
                
                tokio::spawn(async move {
                    let mut full_text = String::new();
                    
                    // Process streaming messages
                    while let Some(result) = stream.recv().await {
                        match result {
                            Ok(chunk) => {
                                // Extract text content
                                if let Some(text) = chunk.text_content() {
                                    // Append to full text
                                    full_text.push_str(text);
                                    
                                    // Update response message
                                    response_message.partial_content = Some(full_text.clone());
                                    response_message.message.content.parts = vec![
                                        crate::models::messages::ContentType::Text {
                                            text: full_text.clone(),
                                        },
                                    ];
                                    
                                    // Update in history
                                    {
                                        let mut convos = conversations.write().unwrap();
                                        if let Some(messages) = convos.get_mut(&conversation_id) {
                                            for msg in messages.iter_mut() {
                                                if msg.message.id == response_id {
                                                    *msg = response_message.clone();
                                                    break;
                                                }
                                            }
                                        }
                                    }
                                    
                                    // Send update to UI
                                    let _ = tx_clone.send(response_message.clone()).await;
                                }
                            }
                            Err(e) => {
                                error!("Streaming error: {}", e);
                                
                                // Update status to failed
                                response_message.status = MessageStatus::Failed;
                                
                                // Update in history
                                {
                                    let mut convos = conversations.write().unwrap();
                                    if let Some(messages) = convos.get_mut(&conversation_id) {
                                        for msg in messages.iter_mut() {
                                            if msg.message.id == response_id {
                                                *msg = response_message.clone();
                                                break;
                                            }
                                        }
                                    }
                                }
                                
                                // Send final update to UI
                                let _ = tx_clone.send(response_message.clone()).await;
                                break;
                            }
                        }
                    }
                    
                    // If we got here, streaming is complete
                    if response_message.status == MessageStatus::Streaming {
                        response_message.status = MessageStatus::Complete;
                        response_message.completed_at = Some(std::time::SystemTime::now());
                        response_message.partial_content = None;
                        
                        // Update in history
                        {
                            let mut convos = conversations.write().unwrap();
                            if let Some(messages) = convos.get_mut(&conversation_id) {
                                for msg in messages.iter_mut() {
                                    if msg.message.id == response_id {
                                        *msg = response_message.clone();
                                        break;
                                    }
                                }
                            }
                        }
                        
                        // Send final update to UI
                        let _ = tx_clone.send(response_message).await;
                    }
                });
                
                Ok(rx)
            }
            Err(e) => {
                // Update message status to failed
                self.update_message_status(
                    conversation_id,
                    &conversation_message.message.id,
                    MessageStatus::Failed,
                );
                
                Err(e)
            }
        }
    }
    
    /// Cancel a streaming message
    pub async fn cancel_streaming(
        &self,
        conversation_id: &str,
        message_id: &str,
    ) -> Result<(), MessageError> {
        // Tell MCP service to cancel
        let result = self.mcp_service.cancel_streaming(message_id).await;
        
        // Update message status to cancelled
        if result.is_ok() {
            self.update_message_status(conversation_id, message_id, MessageStatus::Cancelled);
        }
        
        result
    }
    
    /// Register a message listener for a conversation
    pub async fn register_listener(
        &self,
        conversation_id: &str,
    ) -> mpsc::Receiver<ConversationMessage> {
        let (tx, rx) = mpsc::channel(32);
        
        // Store listener
        {
            let mut listeners = self.message_listeners.lock().unwrap();
            let conversation_listeners = listeners
                .entry(conversation_id.to_string())
                .or_insert_with(Vec::new);
            conversation_listeners.push(tx);
        }
        
        // Send existing messages
        let messages = self.get_messages(conversation_id);
        let tx_clone = tx.clone();
        
        tokio::spawn(async move {
            for message in messages {
                let _ = tx_clone.send(message).await;
            }
        });
        
        rx
    }
    
    /// Add a message to conversation history
    fn add_message_to_history(&self, conversation_id: &str, message: ConversationMessage) {
        // Add to history
        {
            let mut conversations = self.conversations.write().unwrap();
            let conversation_messages = conversations
                .entry(conversation_id.to_string())
                .or_insert_with(Vec::new);
            conversation_messages.push(message.clone());
        }
        
        // Notify listeners
        self.notify_listeners(conversation_id, &message);
    }
    
    /// Update message status in history
    fn update_message_status(&self, conversation_id: &str, message_id: &str, status: MessageStatus) {
        let mut updated_message = None;
        
        // Update in history
        {
            let mut conversations = self.conversations.write().unwrap();
            if let Some(messages) = conversations.get_mut(conversation_id) {
                for msg in messages.iter_mut() {
                    if msg.message.id == message_id {
                        msg.status = status;
                        updated_message = Some(msg.clone());
                        break;
                    }
                }
            }
        }
        
        // Notify listeners
        if let Some(message) = updated_message {
            self.notify_listeners(conversation_id, &message);
        }
    }
    
    /// Notify all listeners for a conversation
    fn notify_listeners(&self, conversation_id: &str, message: &ConversationMessage) {
        let listeners = self.message_listeners.lock().unwrap();
        if let Some(conversation_listeners) = listeners.get(conversation_id) {
            let message = message.clone();
            for listener in conversation_listeners {
                let tx = listener.clone();
                let message = message.clone();
                
                tokio::spawn(async move {
                    let _ = tx.send(message).await;
                });
            }
        }
    }
}

/// Global chat service instance
static CHAT_SERVICE: once_cell::sync::OnceCell<ChatService> = once_cell::sync::OnceCell::new();

/// Get the global chat service instance
pub fn get_chat_service() -> &'static ChatService {
    CHAT_SERVICE.get_or_init(|| ChatService::new())
}
</file>

<file path="src/services/mcp.rs">
use crate::models::messages::{Message, MessageError};
use crate::models::{Conversation, Model};
use crate::protocols::mcp::{McpClient, McpConfig, McpError, McpProtocolHandler};
use crate::protocols::{ConnectionStatus, ProtocolHandler};
use crate::utils::config::Config;
use log::{debug, error, info, warn};
use std::collections::HashMap;
use std::sync::{Arc, Mutex, RwLock};
use std::time::Duration;
use tokio::sync::mpsc;
use tokio::time::timeout;
use uuid::Uuid;

/// Service for interacting with the MCP protocol
pub struct McpService {
    /// MCP client
    client: Arc<McpClient>,
    
    /// Protocol handler
    handler: Arc<McpProtocolHandler>,
    
    /// Available models
    models: Arc<RwLock<Vec<Model>>>,
    
    /// Active conversations
    conversations: Arc<RwLock<HashMap<String, Conversation>>>,
    
    /// Active streaming sessions
    streaming_sessions: Arc<Mutex<HashMap<String, mpsc::Sender<Result<Message, MessageError>>>>>,
}

impl McpService {
    /// Create a new MCP service
    pub fn new() -> Self {
        // Load configuration
        let config = Config::global();
        let config_guard = config.lock().unwrap();
        
        // Create MCP configuration
        let api_key = config_guard
            .get_string("api.key")
            .unwrap_or_else(|| String::new());
        
        let mcp_config = McpConfig::with_api_key(api_key)
            .with_url(
                config_guard
                    .get_string("api.url")
                    .unwrap_or_else(|| "wss://api.anthropic.com/v1/messages".to_string()),
            )
            .with_model(
                config_guard
                    .get_string("api.model")
                    .unwrap_or_else(|| "claude-3-opus-20240229".to_string()),
            );
        
        // Create client and handler
        let client = Arc::new(McpClient::new(mcp_config.clone()));
        let handler = Arc::new(McpProtocolHandler::new(mcp_config));
        
        // Define available models
        let models = vec![
            Model::claude("opus", "20240229"),
            Model::claude("sonnet", "20240229"),
            Model::claude("haiku", "20240229"),
        ];
        
        Self {
            client,
            handler,
            models: Arc::new(RwLock::new(models)),
            conversations: Arc::new(RwLock::new(HashMap::new())),
            streaming_sessions: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    /// Get the current connection status
    pub fn connection_status(&self) -> ConnectionStatus {
        self.handler.connection_status()
    }
    
    /// Connect to the MCP server
    pub async fn connect(&self) -> Result<(), String> {
        self.handler.connect().await
    }
    
    /// Disconnect from the MCP server
    pub async fn disconnect(&self) -> Result<(), String> {
        self.handler.disconnect().await
    }
    
    /// Get available models
    pub fn available_models(&self) -> Vec<Model> {
        self.models.read().unwrap().clone()
    }
    
    /// Get active conversations
    pub fn active_conversations(&self) -> Vec<Conversation> {
        self.conversations
            .read()
            .unwrap()
            .values()
            .cloned()
            .collect()
    }
    
    /// Create a new conversation
    pub fn create_conversation(&self, title: &str, model: Model) -> Conversation {
        let conversation = Conversation::new(title, model);
        
        // Store conversation
        {
            let mut conversations = self.conversations.write().unwrap();
            conversations.insert(conversation.id.clone(), conversation.clone());
        }
        
        conversation
    }
    
    /// Get a conversation by ID
    pub fn get_conversation(&self, id: &str) -> Option<Conversation> {
        self.conversations.read().unwrap().get(id).cloned()
    }
    
    /// Delete a conversation
    pub fn delete_conversation(&self, id: &str) -> Result<(), String> {
        let mut conversations = self.conversations.write().unwrap();
        
        if conversations.remove(id).is_some() {
            Ok(())
        } else {
            Err(format!("Conversation with ID {} not found", id))
        }
    }
    
    /// Send a message in a conversation
    pub async fn send_message(&self, conversation_id: &str, message: Message) -> Result<Message, MessageError> {
        // Check if conversation exists
        if !self.conversations.read().unwrap().contains_key(conversation_id) {
            return Err(MessageError::Unknown(format!(
                "Conversation with ID {} not found",
                conversation_id
            )));
        }
        
        // Add conversation context in metadata
        let message_with_context = Message {
            metadata: Some(HashMap::from([(
                "conversation_id".to_string(),
                serde_json::to_value(conversation_id).unwrap(),
            )])),
            ..message
        };
        
        // Send message through protocol handler
        match timeout(Duration::from_secs(120), self.handler.send_message(message_with_context.clone())).await {
            Ok(result) => match result {
                Ok(_) => {
                    // In a real implementation, we would get the response from the handler
                    // For now, we'll simulate a response
                    
                    // Create a dummy response
                    let response = Message {
                        id: Uuid::new_v4().to_string(),
                        role: crate::models::messages::MessageRole::Assistant,
                        content: crate::models::messages::MessageContent {
                            parts: vec![crate::models::messages::ContentType::Text {
                                text: "This is a simulated response from the MCP server.".to_string(),
                            }],
                        },
                        metadata: message_with_context.metadata,
                        created_at: std::time::SystemTime::now(),
                    };
                    
                    Ok(response)
                }
                Err(e) => Err(e),
            },
            Err(_) => Err(MessageError::Timeout(Duration::from_secs(120))),
        }
    }
    
    /// Start a streaming message in a conversation
    pub async fn stream_message(
        &self,
        conversation_id: &str,
        message: Message,
    ) -> Result<mpsc::Receiver<Result<Message, MessageError>>, MessageError> {
        // Check if conversation exists
        if !self.conversations.read().unwrap().contains_key(conversation_id) {
            return Err(MessageError::Unknown(format!(
                "Conversation with ID {} not found",
                conversation_id
            )));
        }
        
        // Create streaming channel
        let (tx, rx) = mpsc::channel(32);
        
        // Store streaming session
        {
            let mut sessions = self.streaming_sessions.lock().unwrap();
            sessions.insert(message.id.clone(), tx.clone());
        }
        
        // Add conversation context in metadata
        let message_with_context = Message {
            metadata: Some(HashMap::from([(
                "conversation_id".to_string(),
                serde_json::to_value(conversation_id).unwrap(),
            )])),
            ..message.clone()
        };
        
        // Send message through client directly for streaming
        // In a real implementation, we would use client.stream() and adapt its output
        
        // For now, simulate streaming with a few chunks
        let tx_clone = tx.clone();
        let message_id = message.id.clone();
        
        tokio::spawn(async move {
            // Simulate streaming with delay
            tokio::time::sleep(Duration::from_millis(500)).await;
            
            // Send first chunk
            let _ = tx_clone
                .send(Ok(Message {
                    id: message_id.clone(),
                    role: crate::models::messages::MessageRole::Assistant,
                    content: crate::models::messages::MessageContent {
                        parts: vec![crate::models::messages::ContentType::Text {
                            text: "This is a simulated ".to_string(),
                        }],
                    },
                    metadata: message_with_context.metadata.clone(),
                    created_at: std::time::SystemTime::now(),
                }))
                .await;
            
            // Delay between chunks
            tokio::time::sleep(Duration::from_millis(500)).await;
            
            // Send second chunk
            let _ = tx_clone
                .send(Ok(Message {
                    id: message_id.clone(),
                    role: crate::models::messages::MessageRole::Assistant,
                    content: crate::models::messages::MessageContent {
                        parts: vec![crate::models::messages::ContentType::Text {
                            text: "This is a simulated streaming ".to_string(),
                        }],
                    },
                    metadata: message_with_context.metadata.clone(),
                    created_at: std::time::SystemTime::now(),
                }))
                .await;
            
            // Delay between chunks
            tokio::time::sleep(Duration::from_millis(500)).await;
            
            // Send final chunk
            let _ = tx_clone
                .send(Ok(Message {
                    id: message_id,
                    role: crate::models::messages::MessageRole::Assistant,
                    content: crate::models::messages::MessageContent {
                        parts: vec![crate::models::messages::ContentType::Text {
                            text: "This is a simulated streaming response from the MCP server.".to_string(),
                        }],
                    },
                    metadata: message_with_context.metadata,
                    created_at: std::time::SystemTime::now(),
                }))
                .await;
            
            // Clean up streaming session
            tokio::time::sleep(Duration::from_millis(100)).await;
            // Note: In a real implementation, we would remove the session when streaming ends
        });
        
        Ok(rx)
    }
    
    /// Cancel a streaming message
    pub async fn cancel_streaming(&self, message_id: &str) -> Result<(), MessageError> {
        // Remove streaming session
        let mut sessions = self.streaming_sessions.lock().unwrap();
        if sessions.remove(message_id).is_some() {
            // In a real implementation, we would also tell the MCP server to cancel
            Ok(())
        } else {
            Err(MessageError::Unknown(format!(
                "Streaming session with ID {} not found",
                message_id
            )))
        }
    }
}

/// Global MCP service instance
static MCP_SERVICE: once_cell::sync::OnceCell<McpService> = once_cell::sync::OnceCell::new();

/// Get the global MCP service instance
pub fn get_mcp_service() -> &'static McpService {
    MCP_SERVICE.get_or_init(|| McpService::new())
}
</file>

<file path="src/shell_loader.rs">
use lazy_static::lazy_static;
use log::{debug, info, warn};
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use tauri::{Manager, Window};
use tokio::sync::mpsc::{self, Receiver, Sender};
use tokio::task::JoinHandle;

use crate::feature_flags::FeatureFlags;
use crate::utils::config::Config;

/// Represents the different states of the application loading process
#[derive(Debug, Clone, PartialEq)]
pub enum LoadState {
    /// Initial state before loading has started
    NotStarted,
    
    /// Basic shell UI is loading (target: <100ms)
    ShellLoading,
    
    /// Basic shell UI is loaded and visible to user
    ShellReady,
    
    /// Core services are being initialized (auth, settings, etc)
    CoreServicesLoading,
    
    /// Core services are ready
    CoreServicesReady,
    
    /// Secondary features are being loaded (plugins, extensions, etc)
    SecondaryLoading,
    
    /// Application is fully loaded
    FullyLoaded,
    
    /// An error occurred during loading
    Error(String),
}

/// Structure to manage application loading state and process
pub struct ShellLoader {
    /// Current loading state
    state: Arc<Mutex<LoadState>>,
    
    /// Channel to send loading updates
    tx: Option<Sender<LoadState>>,
    
    /// Main application window
    window: Option<Window>,
    
    /// Feature flags to determine what to load
    feature_flags: FeatureFlags,
    
    /// Loading tasks being processed
    loading_tasks: Vec<JoinHandle<()>>,
    
    /// Timestamp when loading started
    start_time: Option<Instant>,
}

impl ShellLoader {
    /// Create a new ShellLoader instance
    pub fn new(feature_flags: FeatureFlags) -> Self {
        ShellLoader {
            state: Arc::new(Mutex::new(LoadState::NotStarted)),
            tx: None,
            window: None,
            feature_flags,
            loading_tasks: Vec::new(),
            start_time: None,
        }
    }
    
    /// Initialize the loader with a window
    pub fn with_window(mut self, window: Window) -> Self {
        self.window = Some(window);
        self
    }
    
    /// Start the loading process
    pub async fn start(&mut self) -> Receiver<LoadState> {
        let (tx, rx) = mpsc::channel(16);
        self.tx = Some(tx);
        self.start_time = Some(Instant::now());
        
        let state_clone = self.state.clone();
        let tx_clone = self.tx.clone().unwrap();
        let window_clone = self.window.clone();
        let feature_flags = self.feature_flags;
        
        // Spawn the loading process in a separate task
        let handle = tokio::spawn(async move {
            Self::loading_process(state_clone, tx_clone, window_clone, feature_flags).await;
        });
        
        self.loading_tasks.push(handle);
        rx
    }
    
    /// The main loading process, runs asynchronously
    async fn loading_process(
        state: Arc<Mutex<LoadState>>,
        tx: Sender<LoadState>,
        window: Option<Window>,
        feature_flags: FeatureFlags,
    ) {
        // Update state to shell loading
        Self::update_state(&state, &tx, LoadState::ShellLoading).await;
        
        // Wait for minimal shell to be ready (simulated here)
        tokio::time::sleep(Duration::from_millis(20)).await;
        
        // Shell is ready, make window visible
        Self::update_state(&state, &tx, LoadState::ShellReady).await;
        if let Some(window) = &window {
            window.show().unwrap();
        }
        
        // Load core services
        Self::update_state(&state, &tx, LoadState::CoreServicesLoading).await;
        
        // Initialize essential services
        Self::initialize_core_services(&feature_flags).await;
        
        // Core services ready
        Self::update_state(&state, &tx, LoadState::CoreServicesReady).await;
        
        // If lazy loading is enabled, load secondary features in the background
        if feature_flags.contains(FeatureFlags::LAZY_LOAD) {
            tokio::spawn(async move {
                Self::update_state(&state, &tx, LoadState::SecondaryLoading).await;
                
                // Load non-essential components in the background
                Self::load_secondary_features(&feature_flags).await;
                
                // Everything is loaded
                Self::update_state(&state, &tx, LoadState::FullyLoaded).await;
            });
        } else {
            // Load everything synchronously
            Self::update_state(&state, &tx, LoadState::SecondaryLoading).await;
            Self::load_secondary_features(&feature_flags).await;
            Self::update_state(&state, &tx, LoadState::FullyLoaded).await;
        }
    }
    
    /// Initialize core services required for basic functionality
    async fn initialize_core_services(feature_flags: &FeatureFlags) {
        // Simulate initialization of essential services
        let services = [
            ("config", 30),
            ("auth", 40),
            ("api", 50),
        ];
        
        for (service, delay_ms) in services {
            debug!("Initializing core service: {}", service);
            tokio::time::sleep(Duration::from_millis(delay_ms)).await;
        }
    }
    
    /// Load secondary features that are not needed for initial startup
    async fn load_secondary_features(feature_flags: &FeatureFlags) {
        // Only load features that are enabled in feature flags
        if feature_flags.contains(FeatureFlags::PLUGINS) {
            debug!("Loading plugins");
            tokio::time::sleep(Duration::from_millis(100)).await;
        }
        
        if feature_flags.contains(FeatureFlags::ADVANCED_UI) {
            debug!("Loading advanced UI components");
            tokio::time::sleep(Duration::from_millis(150)).await;
        }
        
        if feature_flags.contains(FeatureFlags::HISTORY) {
            debug!("Loading conversation history");
            tokio::time::sleep(Duration::from_millis(80)).await;
        }
    }
    
    /// Update the loading state and send notification through the channel
    async fn update_state(
        state: &Arc<Mutex<LoadState>>,
        tx: &Sender<LoadState>,
        new_state: LoadState,
    ) {
        {
            let mut state_guard = state.lock().unwrap();
            *state_guard = new_state.clone();
        }
        
        // Send state update through channel
        if let Err(e) = tx.send(new_state).await {
            warn!("Failed to send loading state update: {}", e);
        }
    }
    
    /// Get the current loading state
    pub fn state(&self) -> LoadState {
        self.state.lock().unwrap().clone()
    }
    
    /// Get elapsed time since loading started
    pub fn elapsed(&self) -> Option<Duration> {
        self.start_time.map(|start| start.elapsed())
    }
    
    /// Check if the application is fully loaded
    pub fn is_fully_loaded(&self) -> bool {
        matches!(self.state(), LoadState::FullyLoaded)
    }
}

// Helper function to create and start the shell loader with default options
pub async fn launch_with_fast_shell(window: Window, config: &Config) -> ShellLoader {
    info!("Launching application with fast shell");
    
    // Determine feature flags based on config
    let mut feature_flags = FeatureFlags::default();
    if config.get_bool("experimental_features").unwrap_or(false) {
        feature_flags |= FeatureFlags::EXPERIMENTAL;
    }
    
    if config.get_bool("lazy_loading").unwrap_or(true) {
        feature_flags |= FeatureFlags::LAZY_LOAD;
    }
    
    // Enable other features based on config
    if config.get_bool("plugins_enabled").unwrap_or(true) {
        feature_flags |= FeatureFlags::PLUGINS;
    }
    
    if config.get_bool("history_enabled").unwrap_or(true) {
        feature_flags |= FeatureFlags::HISTORY;
    }
    
    if config.get_bool("advanced_ui").unwrap_or(true) {
        feature_flags |= FeatureFlags::ADVANCED_UI;
    }
    
    // Create and start the shell loader
    let mut loader = ShellLoader::new(feature_flags).with_window(window);
    loader.start().await;
    
    loader
}
</file>

<file path="src/telemetry/mod.rs">
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant, SystemTime};
use serde::{Serialize, Deserialize};
use log::{debug, info, warn, error};
use reqwest::Client;
use tokio::time;
use uuid::Uuid;
use chrono::{DateTime, Utc};

/// Telemetry event type
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub enum TelemetryEventType {
    /// Application started
    AppStart,
    /// Application stopped
    AppStop,
    /// Feature used
    FeatureUsed,
    /// Error occurred
    Error,
    /// Performance metric
    Performance,
    /// Crash
    Crash,
    /// Settings changed
    SettingsChanged,
    /// Network event
    Network,
    /// User engagement
    Engagement,
}

/// Telemetry event
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryEvent {
    /// Event ID
    pub id: String,
    /// Event type
    pub event_type: TelemetryEventType,
    /// Event name
    pub name: String,
    /// Event timestamp
    pub timestamp: DateTime<Utc>,
    /// Session ID
    pub session_id: String,
    /// User ID (anonymous)
    pub user_id: String,
    /// Event properties
    pub properties: HashMap<String, serde_json::Value>,
    /// Application version
    pub app_version: String,
    /// Operating system
    pub os: String,
    /// Device ID (anonymous)
    pub device_id: String,
}

/// Telemetry configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryConfig {
    /// Whether telemetry is enabled
    pub enabled: bool,
    /// Telemetry endpoint URL
    pub endpoint: String,
    /// Anonymous user ID
    pub user_id: String,
    /// Anonymous device ID
    pub device_id: String,
    /// Whether error reporting is enabled
    pub error_reporting: bool,
    /// Whether crash reporting is enabled
    pub crash_reporting: bool,
    /// Whether performance metrics are enabled
    pub performance_metrics: bool,
    /// Whether feature usage tracking is enabled
    pub feature_usage: bool,
    /// Batch size for telemetry events
    pub batch_size: usize,
    /// Send interval in seconds
    pub send_interval_seconds: u64,
}

impl Default for TelemetryConfig {
    fn default() -> Self {
        Self {
            enabled: false, // Disabled by default
            endpoint: "https://telemetry.mcp-client.com/v1/events".to_string(),
            user_id: Uuid::new_v4().to_string(),
            device_id: Uuid::new_v4().to_string(),
            error_reporting: false,
            crash_reporting: false,
            performance_metrics: false,
            feature_usage: false,
            batch_size: 20,
            send_interval_seconds: 300, // 5 minutes
        }
    }
}

/// Telemetry service
pub struct TelemetryService {
    config: Arc<Mutex<TelemetryConfig>>,
    events: Arc<Mutex<Vec<TelemetryEvent>>>,
    session_id: String,
    client: Client,
    running: Arc<Mutex<bool>>,
}

impl TelemetryService {
    /// Create a new telemetry service
    pub fn new(config: TelemetryConfig) -> Self {
        let session_id = Uuid::new_v4().to_string();
        
        let client = Client::builder()
            .timeout(Duration::from_secs(30))
            .build()
            .unwrap_or_else(|_| Client::new());
        
        Self {
            config: Arc::new(Mutex::new(config)),
            events: Arc::new(Mutex::new(Vec::new())),
            session_id,
            client,
            running: Arc::new(Mutex::new(false)),
        }
    }
    
    /// Start the telemetry service
    pub fn start(&self) -> Result<(), String> {
        let config = self.config.lock().unwrap();
        if !config.enabled {
            return Ok(());
        }
        
        let mut running = self.running.lock().unwrap();
        if *running {
            return Err("Telemetry service is already running".to_string());
        }
        
        *running = true;
        
        // Track application start
        drop(running);
        drop(config);
        self.track_app_start();
        
        // Start background send task
        let events = self.events.clone();
        let config = self.config.clone();
        let client = self.client.clone();
        let running = self.running.clone();
        
        tokio::spawn(async move {
            let interval_seconds = {
                let config = config.lock().unwrap();
                config.send_interval_seconds
            };
            
            let mut interval = time::interval(Duration::from_secs(interval_seconds));
            
            while *running.lock().unwrap() {
                interval.tick().await;
                
                // Check if telemetry is enabled
                let (enabled, batch_size, endpoint) = {
                    let config = config.lock().unwrap();
                    (config.enabled, config.batch_size, config.endpoint.clone())
                };
                
                if !enabled {
                    continue;
                }
                
                // Get events to send
                let events_to_send = {
                    let mut events_lock = events.lock().unwrap();
                    if events_lock.is_empty() {
                        continue;
                    }
                    
                    // Take up to batch_size events
                    if events_lock.len() <= batch_size {
                        std::mem::take(&mut *events_lock)
                    } else {
                        events_lock.drain(0..batch_size).collect()
                    }
                };
                
                // Send events
                if !events_to_send.is_empty() {
                    if let Err(e) = Self::send_events(&client, &endpoint, &events_to_send).await {
                        // Failed to send, put events back in queue
                        error!("Failed to send telemetry events: {}", e);
                        let mut events_lock = events.lock().unwrap();
                        events_lock.extend(events_to_send);
                    }
                }
            }
        });
        
        Ok(())
    }
    
    /// Stop the telemetry service
    pub fn stop(&self) -> Result<(), String> {
        let mut running = self.running.lock().unwrap();
        if !*running {
            return Err("Telemetry service is not running".to_string());
        }
        
        *running = false;
        
        // Track application stop
        drop(running);
        self.track_app_stop();
        
        // Send remaining events synchronously
        let events_to_send = {
            let mut events_lock = self.events.lock().unwrap();
            std::mem::take(&mut *events_lock)
        };
        
        if !events_to_send.is_empty() {
            let config = self.config.lock().unwrap();
            if config.enabled {
                // Use tokio runtime to send final events
                let rt = tokio::runtime::Runtime::new().unwrap();
                let _ = rt.block_on(Self::send_events(&self.client, &config.endpoint, &events_to_send));
            }
        }
        
        Ok(())
    }
    
    /// Track application start
    pub fn track_app_start(&self) {
        let config = self.config.lock().unwrap();
        if !config.enabled {
            return;
        }
        
        let app_info = self.get_app_info();
        
        let event = TelemetryEvent {
            id: Uuid::new_v4().to_string(),
            event_type: TelemetryEventType::AppStart,
            name: "app_start".to_string(),
            timestamp: Utc::now(),
            session_id: self.session_id.clone(),
            user_id: config.user_id.clone(),
            properties: HashMap::new(),
            app_version: app_info.version,
            os: app_info.os,
            device_id: config.device_id.clone(),
        };
        
        drop(config);
        self.add_event(event);
    }
    
    /// Track application stop
    pub fn track_app_stop(&self) {
        let config = self.config.lock().unwrap();
        if !config.enabled {
            return;
        }
        
        let app_info = self.get_app_info();
        
        let event = TelemetryEvent {
            id: Uuid::new_v4().to_string(),
            event_type: TelemetryEventType::AppStop,
            name: "app_stop".to_string(),
            timestamp: Utc::now(),
            session_id: self.session_id.clone(),
            user_id: config.user_id.clone(),
            properties: HashMap::new(),
            app_version: app_info.version,
            os: app_info.os,
            device_id: config.device_id.clone(),
        };
        
        drop(config);
        self.add_event(event);
    }
    
    /// Track feature usage
    pub fn track_feature_usage(&self, feature_name: &str, properties: HashMap<String, serde_json::Value>) {
        let config = self.config.lock().unwrap();
        if !config.enabled || !config.feature_usage {
            return;
        }
        
        let app_info = self.get_app_info();
        
        let event = TelemetryEvent {
            id: Uuid::new_v4().to_string(),
            event_type: TelemetryEventType::FeatureUsed,
            name: format!("feature_used_{}", feature_name),
            timestamp: Utc::now(),
            session_id: self.session_id.clone(),
            user_id: config.user_id.clone(),
            properties,
            app_version: app_info.version,
            os: app_info.os,
            device_id: config.device_id.clone(),
        };
        
        drop(config);
        self.add_event(event);
    }
    
    /// Track error
    pub fn track_error(&self, error_name: &str, error_message: &str, properties: HashMap<String, serde_json::Value>) {
        let config = self.config.lock().unwrap();
        if !config.enabled || !config.error_reporting {
            return;
        }
        
        let app_info = self.get_app_info();
        
        let mut error_properties = properties;
        error_properties.insert("error_message".to_string(), serde_json::Value::String(error_message.to_string()));
        
        let event = TelemetryEvent {
            id: Uuid::new_v4().to_string(),
            event_type: TelemetryEventType::Error,
            name: format!("error_{}", error_name),
            timestamp: Utc::now(),
            session_id: self.session_id.clone(),
            user_id: config.user_id.clone(),
            properties: error_properties,
            app_version: app_info.version,
            os: app_info.os,
            device_id: config.device_id.clone(),
        };
        
        drop(config);
        self.add_event(event);
    }
    
    /// Track crash
    pub fn track_crash(&self, crash_reason: &str, stack_trace: &str) {
        let config = self.config.lock().unwrap();
        if !config.enabled || !config.crash_reporting {
            return;
        }
        
        let app_info = self.get_app_info();
        
        let mut properties = HashMap::new();
        properties.insert("crash_reason".to_string(), serde_json::Value::String(crash_reason.to_string()));
        properties.insert("stack_trace".to_string(), serde_json::Value::String(stack_trace.to_string()));
        
        let event = TelemetryEvent {
            id: Uuid::new_v4().to_string(),
            event_type: TelemetryEventType::Crash,
            name: "app_crash".to_string(),
            timestamp: Utc::now(),
            session_id: self.session_id.clone(),
            user_id: config.user_id.clone(),
            properties,
            app_version: app_info.version,
            os: app_info.os,
            device_id: config.device_id.clone(),
        };
        
        drop(config);
        self.add_event(event);
    }
    
    /// Track performance metric
    pub fn track_performance(&self, metric_name: &str, value: f64, unit: &str) {
        let config = self.config.lock().unwrap();
        if !config.enabled || !config.performance_metrics {
            return;
        }
        
        let app_info = self.get_app_info();
        
        let mut properties = HashMap::new();
        properties.insert("value".to_string(), serde_json::Value::Number(serde_json::Number::from_f64(value).unwrap_or_else(|| serde_json::Number::from(0))));
        properties.insert("unit".to_string(), serde_json::Value::String(unit.to_string()));
        
        let event = TelemetryEvent {
            id: Uuid::new_v4().to_string(),
            event_type: TelemetryEventType::Performance,
            name: format!("performance_{}", metric_name),
            timestamp: Utc::now(),
            session_id: self.session_id.clone(),
            user_id: config.user_id.clone(),
            properties,
            app_version: app_info.version,
            os: app_info.os,
            device_id: config.device_id.clone(),
        };
        
        drop(config);
        self.add_event(event);
    }
    
    /// Track settings change
    pub fn track_settings_change(&self, setting_name: &str, new_value: &str) {
        let config = self.config.lock().unwrap();
        if !config.enabled || !config.feature_usage {
            return;
        }
        
        let app_info = self.get_app_info();
        
        let mut properties = HashMap::new();
        properties.insert("setting_name".to_string(), serde_json::Value::String(setting_name.to_string()));
        properties.insert("new_value".to_string(), serde_json::Value::String(new_value.to_string()));
        
        let event = TelemetryEvent {
            id: Uuid::new_v4().to_string(),
            event_type: TelemetryEventType::SettingsChanged,
            name: "settings_changed".to_string(),
            timestamp: Utc::now(),
            session_id: self.session_id.clone(),
            user_id: config.user_id.clone(),
            properties,
            app_version: app_info.version,
            os: app_info.os,
            device_id: config.device_id.clone(),
        };
        
        drop(config);
        self.add_event(event);
    }
    
    /// Track network event
    pub fn track_network_event(&self, event_name: &str, url: &str, status_code: u16, duration_ms: u64) {
        let config = self.config.lock().unwrap();
        if !config.enabled || !config.performance_metrics {
            return;
        }
        
        let app_info = self.get_app_info();
        
        let mut properties = HashMap::new();
        properties.insert("url".to_string(), serde_json::Value::String(url.to_string()));
        properties.insert("status_code".to_string(), serde_json::Value::Number(serde_json::Number::from(status_code)));
        properties.insert("duration_ms".to_string(), serde_json::Value::Number(serde_json::Number::from(duration_ms)));
        
        let event = TelemetryEvent {
            id: Uuid::new_v4().to_string(),
            event_type: TelemetryEventType::Network,
            name: format!("network_{}", event_name),
            timestamp: Utc::now(),
            session_id: self.session_id.clone(),
            user_id: config.user_id.clone(),
            properties,
            app_version: app_info.version,
            os: app_info.os,
            device_id: config.device_id.clone(),
        };
        
        drop(config);
        self.add_event(event);
    }
    
    /// Track user engagement
    pub fn track_engagement(&self, engagement_type: &str, duration_seconds: u64, properties: HashMap<String, serde_json::Value>) {
        let config = self.config.lock().unwrap();
        if !config.enabled || !config.feature_usage {
            return;
        }
        
        let app_info = self.get_app_info();
        
        let mut engagement_properties = properties;
        engagement_properties.insert("duration_seconds".to_string(), serde_json::Value::Number(serde_json::Number::from(duration_seconds)));
        
        let event = TelemetryEvent {
            id: Uuid::new_v4().to_string(),
            event_type: TelemetryEventType::Engagement,
            name: format!("engagement_{}", engagement_type),
            timestamp: Utc::now(),
            session_id: self.session_id.clone(),
            user_id: config.user_id.clone(),
            properties: engagement_properties,
            app_version: app_info.version,
            os: app_info.os,
            device_id: config.device_id.clone(),
        };
        
        drop(config);
        self.add_event(event);
    }
    
    /// Update telemetry configuration
    pub fn update_config(&self, config: TelemetryConfig) {
        let mut current_config = self.config.lock().unwrap();
        *current_config = config;
    }
    
    /// Get telemetry configuration
    pub fn get_config(&self) -> TelemetryConfig {
        self.config.lock().unwrap().clone()
    }
    
    /// Add event to queue
    fn add_event(&self, event: TelemetryEvent) {
        let mut events = self.events.lock().unwrap();
        events.push(event);
    }
    
    /// Send events to telemetry server
    async fn send_events(client: &Client, endpoint: &str, events: &[TelemetryEvent]) -> Result<(), String> {
        // Serialize events
        let events_json = serde_json::to_string(events)
            .map_err(|e| format!("Failed to serialize events: {}", e))?;
        
        // Send request
        let response = client.post(endpoint)
            .header("Content-Type", "application/json")
            .body(events_json)
            .send()
            .await
            .map_err(|e| format!("Failed to send telemetry request: {}", e))?;
        
        // Check response
        if !response.status().is_success() {
            return Err(format!("Telemetry server returned error: {}", response.status()));
        }
        
        Ok(())
    }
    
    /// Get application information
    fn get_app_info(&self) -> AppInfo {
        // In a real implementation, this would be provided by the application
        AppInfo {
            version: env!("CARGO_PKG_VERSION").to_string(),
            os: format!("{} {}", std::env::consts::OS, std::env::consts::ARCH),
        }
    }
}

/// Application information
struct AppInfo {
    /// Application version
    version: String,
    /// Operating system
    os: String,
}

/// Telemetry analyzer for processing telemetry data
pub struct TelemetryAnalyzer {
    /// Telemetry database client
    db_client: TelemetryDbClient,
}

impl TelemetryAnalyzer {
    /// Create a new telemetry analyzer
    pub fn new(db_url: &str) -> Self {
        Self {
            db_client: TelemetryDbClient::new(db_url),
        }
    }
    
    /// Analyze error trends
    pub async fn analyze_error_trends(&self, start_time: DateTime<Utc>, end_time: DateTime<Utc>) -> Result<ErrorTrendsReport, String> {
        // Query database for errors in time range
        let errors = self.db_client.query_events(
            TelemetryEventType::Error,
            start_time,
            end_time,
        ).await?;
        
        // Group errors by name
        let mut error_counts = HashMap::new();
        for error in &errors {
            let entry = error_counts.entry(error.name.clone()).or_insert(0);
            *entry += 1;
        }
        
        // Sort errors by count
        let mut error_list: Vec<_> = error_counts.into_iter().collect();
        error_list.sort_by(|a, b| b.1.cmp(&a.1));
        
        // Create report
        let report = ErrorTrendsReport {
            start_time,
            end_time,
            total_errors: errors.len(),
            error_types: error_list.into_iter().map(|(name, count)| ErrorTypeCount { name, count }).collect(),
            error_samples: errors.into_iter().take(10).collect(),
        };
        
        Ok(report)
    }
    
    /// Analyze performance metrics
    pub async fn analyze_performance_metrics(&self, metric_name: &str, start_time: DateTime<Utc>, end_time: DateTime<Utc>) -> Result<PerformanceReport, String> {
        // Query database for performance metrics in time range
        let metrics = self.db_client.query_events(
            TelemetryEventType::Performance,
            start_time,
            end_time,
        ).await?;
        
        // Filter metrics by name
        let filtered_metrics: Vec<_> = metrics.into_iter()
            .filter(|event| event.name == format!("performance_{}", metric_name))
            .collect();
        
        if filtered_metrics.is_empty() {
            return Err(format!("No metrics found for '{}'", metric_name));
        }
        
        // Extract values
        let values: Vec<f64> = filtered_metrics.iter()
            .filter_map(|event| {
                event.properties.get("value")
                    .and_then(|v| v.as_f64())
            })
            .collect();
        
        // Calculate statistics
        let count = values.len();
        let sum: f64 = values.iter().sum();
        let mean = sum / count as f64;
        
        let mut sorted_values = values.clone();
        sorted_values.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));
        
        let min = *sorted_values.first().unwrap_or(&0.0);
        let max = *sorted_values.last().unwrap_or(&0.0);
        
        let p50 = percentile(&sorted_values, 50.0);
        let p90 = percentile(&sorted_values, 90.0);
        let p95 = percentile(&sorted_values, 95.0);
        let p99 = percentile(&sorted_values, 99.0);
        
        // Get unit
        let unit = filtered_metrics.first()
            .and_then(|event| event.properties.get("unit"))
            .and_then(|v| v.as_str())
            .unwrap_or("");
        
        // Create report
        let report = PerformanceReport {
            metric_name: metric_name.to_string(),
            start_time,
            end_time,
            count,
            min,
            max,
            mean,
            p50,
            p90,
            p95,
            p99,
            unit: unit.to_string(),
        };
        
        Ok(report)
    }
    
    /// Analyze user engagement
    pub async fn analyze_user_engagement(&self, start_time: DateTime<Utc>, end_time: DateTime<Utc>) -> Result<EngagementReport, String> {
        // Query database for engagement events in time range
        let events = self.db_client.query_events(
            TelemetryEventType::Engagement,
            start_time,
            end_time,
        ).await?;
        
        // Get unique users
        let user_ids: std::collections::HashSet<_> = events.iter()
            .map(|event| event.user_id.clone())
            .collect();
        
        // Calculate daily active users
        let mut daily_active_users = HashMap::new();
        for event in &events {
            let date = event.timestamp.date_naive();
            let entry = daily_active_users.entry(date).or_insert_with(std::collections::HashSet::new);
            entry.insert(event.user_id.clone());
        }
        
        // Calculate session duration
        let mut session_durations = HashMap::new();
        for event in &events {
            if event.name.starts_with("engagement_session") {
                if let Some(duration) = event.properties.get("duration_seconds") {
                    if let Some(duration) = duration.as_u64() {
                        session_durations.insert(event.id.clone(), duration);
                    }
                }
            }
        }
        
        // Calculate feature usage
        let mut feature_usage = HashMap::new();
        for event in &events {
            if event.name.starts_with("feature_used_") {
                let feature_name = event.name.strip_prefix("feature_used_").unwrap_or(&event.name);
                let entry = feature_usage.entry(feature_name.to_string()).or_insert(0);
                *entry += 1;
            }
        }
        
        // Sort feature usage by count
        let mut feature_usage_list: Vec<_> = feature_usage.into_iter().collect();
        feature_usage_list.sort_by(|a, b| b.1.cmp(&a.1));
        
        // Create report
        let report = EngagementReport {
            start_time,
            end_time,
            total_events: events.len(),
            unique_users: user_ids.len(),
            daily_active_users: daily_active_users.into_iter()
                .map(|(date, users)| DailyActiveUsers { date, count: users.len() })
                .collect(),
            average_session_duration: if session_durations.is_empty() {
                0
            } else {
                session_durations.values().sum::<u64>() / session_durations.len() as u64
            },
            top_features: feature_usage_list.into_iter()
                .map(|(name, count)| FeatureUsageCount { name, count })
                .collect(),
        };
        
        Ok(report)
    }
    
    /// Detect anomalies in telemetry data
    pub async fn detect_anomalies(&self, days: u32) -> Result<AnomalyReport, String> {
        let end_time = Utc::now();
        let start_time = end_time - chrono::Duration::days(days as i64);
        
        // Query database for all events in time range
        let events = self.db_client.query_all_events(start_time, end_time).await?;
        
        let mut anomalies = Vec::new();
        
        // Detect error spikes
        if let Ok(error_trends) = self.analyze_error_trends(start_time, end_time).await {
            // Check for error spikes (more than 10x average)
            let error_days = (days + 1) as usize;
            let avg_errors_per_day = error_trends.total_errors as f64 / error_days as f64;
            
            // Group errors by day
            let mut errors_by_day = HashMap::new();
            for event in &events {
                if event.event_type == TelemetryEventType::Error {
                    let date = event.timestamp.date_naive();
                    let entry = errors_by_day.entry(date).or_insert(0);
                    *entry += 1;
                }
            }
            
            // Check each day for spikes
            for (date, count) in errors_by_day {
                if count as f64 > avg_errors_per_day * 10.0 && count > 10 {
                    anomalies.push(Anomaly {
                        anomaly_type: "error_spike".to_string(),
                        date,
                        value: count as f64,
                        threshold: avg_errors_per_day * 10.0,
                        description: format!("Error spike on {}: {} errors (>10x average)", date, count),
                    });
                }
            }
        }
        
        // Detect performance degradation
        if let Ok(perf_report) = self.analyze_performance_metrics("api_latency", start_time, end_time).await {
            // Check if p95 latency is more than 2x normal
            let threshold = 2.0 * perf_report.p50;
            if perf_report.p95 > threshold {
                anomalies.push(Anomaly {
                    anomaly_type: "performance_degradation".to_string(),
                    date: end_time.date_naive(),
                    value: perf_report.p95,
                    threshold,
                    description: format!("API latency degradation: p95 = {}ms (>2x p50)", perf_report.p95),
                });
            }
        }
        
        // Detect crash rate increases
        let crash_events: Vec<_> = events.iter()
            .filter(|event| event.event_type == TelemetryEventType::Crash)
            .collect();
        
        let crash_days = (days + 1) as usize;
        let avg_crashes_per_day = crash_events.len() as f64 / crash_days as f64;
        
        // Group crashes by day
        let mut crashes_by_day = HashMap::new();
        for event in crash_events {
            let date = event.timestamp.date_naive();
            let entry = crashes_by_day.entry(date).or_insert(0);
            *entry += 1;
        }
        
        // Check each day for crash spikes
        for (date, count) in crashes_by_day {
            if count as f64 > avg_crashes_per_day * 3.0 && count > 5 {
                anomalies.push(Anomaly {
                    anomaly_type: "crash_spike".to_string(),
                    date,
                    value: count as f64,
                    threshold: avg_crashes_per_day * 3.0,
                    description: format!("Crash spike on {}: {} crashes (>3x average)", date, count),
                });
            }
        }
        
        // Create report
        let report = AnomalyReport {
            start_time,
            end_time,
            anomalies,
        };
        
        Ok(report)
    }
}

/// Error trends report
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ErrorTrendsReport {
    /// Start time of the report
    pub start_time: DateTime<Utc>,
    /// End time of the report
    pub end_time: DateTime<Utc>,
    /// Total number of errors
    pub total_errors: usize,
    /// Error types and counts
    pub error_types: Vec<ErrorTypeCount>,
    /// Sample error events (up to 10)
    pub error_samples: Vec<TelemetryEvent>,
}

/// Error type count
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ErrorTypeCount {
    /// Error name
    pub name: String,
    /// Error count
    pub count: usize,
}

/// Performance report
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceReport {
    /// Metric name
    pub metric_name: String,
    /// Start time of the report
    pub start_time: DateTime<Utc>,
    /// End time of the report
    pub end_time: DateTime<Utc>,
    /// Number of data points
    pub count: usize,
    /// Minimum value
    pub min: f64,
    /// Maximum value
    pub max: f64,
    /// Mean value
    pub mean: f64,
    /// 50th percentile (median)
    pub p50: f64,
    /// 90th percentile
    pub p90: f64,
    /// 95th percentile
    pub p95: f64,
    /// 99th percentile
    pub p99: f64,
    /// Unit of measurement
    pub unit: String,
}

/// Engagement report
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EngagementReport {
    /// Start time of the report
    pub start_time: DateTime<Utc>,
    /// End time of the report
    pub end_time: DateTime<Utc>,
    /// Total number of events
    pub total_events: usize,
    /// Number of unique users
    pub unique_users: usize,
    /// Daily active users
    pub daily_active_users: Vec<DailyActiveUsers>,
    /// Average session duration in seconds
    pub average_session_duration: u64,
    /// Top features by usage
    pub top_features: Vec<FeatureUsageCount>,
}

/// Daily active users
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DailyActiveUsers {
    /// Date
    pub date: chrono::NaiveDate,
    /// Number of active users
    pub count: usize,
}

/// Feature usage count
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FeatureUsageCount {
    /// Feature name
    pub name: String,
    /// Usage count
    pub count: usize,
}

/// Anomaly report
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnomalyReport {
    /// Start time of the report
    pub start_time: DateTime<Utc>,
    /// End time of the report
    pub end_time: DateTime<Utc>,
    /// Detected anomalies
    pub anomalies: Vec<Anomaly>,
}

/// Anomaly
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Anomaly {
    /// Anomaly type
    pub anomaly_type: String,
    /// Date of the anomaly
    pub date: chrono::NaiveDate,
    /// Anomaly value
    pub value: f64,
    /// Anomaly threshold
    pub threshold: f64,
    /// Anomaly description
    pub description: String,
}

/// Calculate percentile
fn percentile(sorted_values: &[f64], p: f64) -> f64 {
    if sorted_values.is_empty() {
        return 0.0;
    }
    
    let index = (p / 100.0 * (sorted_values.len() - 1) as f64) as usize;
    let remainder = (p / 100.0 * (sorted_values.len() - 1) as f64) - index as f64;
    
    if remainder == 0.0 || index == sorted_values.len() - 1 {
        return sorted_values[index];
    }
    
    sorted_values[index] * (1.0 - remainder) + sorted_values[index + 1] * remainder
}

/// Telemetry database client
struct TelemetryDbClient {
    /// Database URL
    db_url: String,
}

impl TelemetryDbClient {
    /// Create a new telemetry database client
    fn new(db_url: &str) -> Self {
        Self {
            db_url: db_url.to_string(),
        }
    }
    
    /// Query events by type
    async fn query_events(&self, event_type: TelemetryEventType, start_time: DateTime<Utc>, end_time: DateTime<Utc>) -> Result<Vec<TelemetryEvent>, String> {
        // In a real implementation, this would query a database
        // For this example, we'll return mock data
        Ok(vec![])
    }
    
    /// Query all events
    async fn query_all_events(&self, start_time: DateTime<Utc>, end_time: DateTime<Utc>) -> Result<Vec<TelemetryEvent>, String> {
        // In a real implementation, this would query a database
        // For this example, we'll return mock data
        Ok(vec![])
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_telemetry_config_default() {
        let config = TelemetryConfig::default();
        assert!(!config.enabled);
        assert!(!config.error_reporting);
        assert!(!config.crash_reporting);
        assert!(!config.performance_metrics);
        assert!(!config.feature_usage);
    }
    
    #[tokio::test]
    async fn test_telemetry_service_disabled() {
        let config = TelemetryConfig::default();
        let service = TelemetryService::new(config);
        
        // Should not fail even though telemetry is disabled
        let result = service.start();
        assert!(result.is_ok());
        
        // Should not add events
        service.track_error("test", "Test error", HashMap::new());
        
        let events = service.events.lock().unwrap();
        assert!(events.is_empty());
        
        let result = service.stop();
        assert!(result.is_err()); // Should fail because service was not really running
    }
    
    #[tokio::test]
    async fn test_telemetry_service_enabled() {
        let mut config = TelemetryConfig::default();
        config.enabled = true;
        config.error_reporting = true;
        let service = TelemetryService::new(config);
        
        let result = service.start();
        assert!(result.is_ok());
        
        // Should add events
        service.track_error("test", "Test error", HashMap::new());
        
        {
            let events = service.events.lock().unwrap();
            assert_eq!(events.len(), 2); // App start + error
        }
        
        let result = service.stop();
        assert!(result.is_ok());
        
        // Should have added app stop event and cleared queue
        let events = service.events.lock().unwrap();
        assert!(events.is_empty());
    }
    
    #[test]
    fn test_percentile_calculation() {
        let values = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0];
        
        assert_eq!(percentile(&values, 0.0), 1.0);
        assert_eq!(percentile(&values, 50.0), 5.5);
        assert_eq!(percentile(&values, 90.0), 9.1);
        assert_eq!(percentile(&values, 100.0), 10.0);
    }
}
</file>

<file path="src/updater.rs">
// Updater module for MCP Client
//
// This module handles the auto-update functionality for the application,
// leveraging Tauri's built-in updater system.

use log::{debug, info, warn, error};
use serde::{Serialize, Deserialize};
use std::sync::{Arc, RwLock};
use tauri::{AppHandle, Manager, Runtime};
use std::time::{Duration, Instant};

use crate::error::Result;
use crate::feature_flags::FeatureFlags;
use crate::observability::metrics::{record_counter, record_gauge};

/// Update check status
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub enum UpdateStatus {
    /// Checking for updates
    Checking,
    
    /// Update available
    Available,
    
    /// No updates available
    UpToDate,
    
    /// Update downloaded and ready to install
    Ready,
    
    /// Update error
    Error,
    
    /// Update disabled
    Disabled,
}

/// Update information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UpdateInfo {
    /// Current version
    pub current_version: String,
    
    /// Available version (if any)
    pub available_version: Option<String>,
    
    /// Update status
    pub status: UpdateStatus,
    
    /// Last check time
    pub last_check: Option<String>,
    
    /// Error message (if any)
    pub error: Option<String>,
}

/// Update manager for handling auto-updates
pub struct UpdateManager<R: Runtime> {
    /// Application handle
    app_handle: AppHandle<R>,
    
    /// Update status
    status: Arc<RwLock<UpdateStatus>>,
    
    /// Available version
    available_version: Arc<RwLock<Option<String>>>,
    
    /// Last check time
    last_check: Arc<RwLock<Option<Instant>>>,
    
    /// Update check interval in hours
    check_interval_hours: Arc<RwLock<u64>>,
    
    /// Whether auto-updates are enabled
    enabled: Arc<RwLock<bool>>,
    
    /// Last error message
    error: Arc<RwLock<Option<String>>>,
}

impl<R: Runtime> UpdateManager<R> {
    /// Create a new update manager
    pub fn new(app_handle: AppHandle<R>, feature_flags: FeatureFlags) -> Self {
        // Check if auto-update is enabled in feature flags
        let auto_update_enabled = feature_flags.contains(FeatureFlags::AUTO_UPDATE);
        
        Self {
            app_handle,
            status: Arc::new(RwLock::new(
                if auto_update_enabled {
                    UpdateStatus::UpToDate
                } else {
                    UpdateStatus::Disabled
                }
            )),
            available_version: Arc::new(RwLock::new(None)),
            last_check: Arc::new(RwLock::new(None)),
            check_interval_hours: Arc::new(RwLock::new(24)), // Default to daily checks
            enabled: Arc::new(RwLock::new(auto_update_enabled)),
            error: Arc::new(RwLock::new(None)),
        }
    }
    
    /// Start the update manager
    pub async fn start(&self) -> Result<()> {
        if !*self.enabled.read().unwrap() {
            info!("Auto-update is disabled");
            return Ok(());
        }
        
        info!("Starting update manager");
        
        // Perform initial update check
        let _ = self.check_for_updates().await;
        
        // Setup periodic update checks
        let app_handle = self.app_handle.clone();
        let enabled = self.enabled.clone();
        let check_interval_hours = self.check_interval_hours.clone();
        
        // Spawn a background task to periodically check for updates
        tauri::async_runtime::spawn(async move {
            loop {
                // Sleep for the specified interval
                let interval_hours = *check_interval_hours.read().unwrap();
                let interval = Duration::from_secs(interval_hours * 3600);
                tokio::time::sleep(interval).await;
                
                // Skip if disabled
                if !*enabled.read().unwrap() {
                    continue;
                }
                
                // Check for updates
                let updater = app_handle.updater();
                match updater.check().await {
                    Ok(update) => {
                        if update.is_update_available() {
                            info!("Update available: {:?}", update);
                            record_counter("updater.update_available", 1.0, None);
                            
                            // Notify the user
                            app_handle.emit_all("update-available", update).unwrap_or_else(|e| {
                                error!("Failed to emit update-available event: {}", e);
                            });
                        } else {
                            debug!("No updates available");
                        }
                    }
                    Err(e) => {
                        error!("Failed to check for updates: {}", e);
                        record_counter("updater.check_error", 1.0, None);
                    }
                }
            }
        });
        
        Ok(())
    }
    
    /// Check for updates
    pub async fn check_for_updates(&self) -> Result<bool> {
        if !*self.enabled.read().unwrap() {
            *self.status.write().unwrap() = UpdateStatus::Disabled;
            return Ok(false);
        }
        
        *self.status.write().unwrap() = UpdateStatus::Checking;
        *self.last_check.write().unwrap() = Some(Instant::now());
        
        info!("Checking for updates");
        record_counter("updater.check", 1.0, None);
        
        let updater = self.app_handle.updater();
        match updater.check().await {
            Ok(update) => {
                if update.is_update_available() {
                    // Update available
                    info!("Update available: {:?}", update);
                    record_counter("updater.update_available", 1.0, None);
                    
                    // Extract version from update
                    if let Some(version) = update.current_version() {
                        if let Some(available) = update.latest_version() {
                            *self.available_version.write().unwrap() = Some(available.to_string());
                        }
                    }
                    
                    *self.status.write().unwrap() = UpdateStatus::Available;
                    *self.error.write().unwrap() = None;
                    
                    // Notify the user
                    self.app_handle.emit_all("update-available", update).unwrap_or_else(|e| {
                        error!("Failed to emit update-available event: {}", e);
                    });
                    
                    return Ok(true);
                } else {
                    // No update available
                    debug!("No updates available");
                    *self.status.write().unwrap() = UpdateStatus::UpToDate;
                    *self.error.write().unwrap() = None;
                    
                    return Ok(false);
                }
            }
            Err(e) => {
                // Update check failed
                error!("Failed to check for updates: {}", e);
                record_counter("updater.check_error", 1.0, None);
                
                *self.status.write().unwrap() = UpdateStatus::Error;
                *self.error.write().unwrap() = Some(e.to_string());
                
                return Err(e.into());
            }
        }
    }
    
    /// Get update info
    pub fn get_update_info(&self) -> UpdateInfo {
        let current_version = self.app_handle.package_info().version.to_string();
        let status = self.status.read().unwrap().clone();
        let available_version = self.available_version.read().unwrap().clone();
        let error = self.error.read().unwrap().clone();
        
        // Format last check time
        let last_check = if let Some(time) = *self.last_check.read().unwrap() {
            let elapsed = time.elapsed();
            let minutes = elapsed.as_secs() / 60;
            
            if minutes < 60 {
                Some(format!("{} minutes ago", minutes))
            } else {
                let hours = minutes / 60;
                Some(format!("{} hours ago", hours))
            }
        } else {
            None
        };
        
        UpdateInfo {
            current_version,
            available_version,
            status,
            last_check,
            error,
        }
    }
    
    /// Install update
    pub async fn install_update(&self) -> Result<()> {
        if !*self.enabled.read().unwrap() {
            return Err("Auto-update is disabled".into());
        }
        
        if *self.status.read().unwrap() != UpdateStatus::Available {
            return Err("No update available to install".into());
        }
        
        info!("Installing update");
        record_counter("updater.install", 1.0, None);
        
        // Let the Tauri's builtin updater handle the installation
        // This will typically restart the application
        match self.app_handle.updater().check().await {
            Ok(update) => {
                if update.is_update_available() {
                    info!("Installing update: {:?}", update);
                    Ok(())
                } else {
                    Err("No update available".into())
                }
            },
            Err(e) => {
                error!("Failed to install update: {}", e);
                record_counter("updater.install_error", 1.0, None);
                Err(e.into())
            }
        }
    }
    
    /// Enable or disable auto-updates
    pub fn set_enabled(&self, enabled: bool) -> Result<()> {
        *self.enabled.write().unwrap() = enabled;
        
        if enabled {
            *self.status.write().unwrap() = UpdateStatus::UpToDate;
            info!("Auto-update enabled");
            
            // Trigger an update check
            let manager = self.clone();
            tauri::async_runtime::spawn(async move {
                let _ = manager.check_for_updates().await;
            });
        } else {
            *self.status.write().unwrap() = UpdateStatus::Disabled;
            info!("Auto-update disabled");
        }
        
        Ok(())
    }
    
    /// Set update check interval in hours
    pub fn set_check_interval(&self, hours: u64) -> Result<()> {
        if hours < 1 {
            return Err("Interval must be at least 1 hour".into());
        }
        
        *self.check_interval_hours.write().unwrap() = hours;
        info!("Update check interval set to {} hours", hours);
        
        Ok(())
    }
}

impl<R: Runtime> Clone for UpdateManager<R> {
    fn clone(&self) -> Self {
        Self {
            app_handle: self.app_handle.clone(),
            status: self.status.clone(),
            available_version: self.available_version.clone(),
            last_check: self.last_check.clone(),
            check_interval_hours: self.check_interval_hours.clone(),
            enabled: self.enabled.clone(),
            error: self.error.clone(),
        }
    }
}

// Global update manager
lazy_static::lazy_static! {
    static ref UPDATE_MANAGER: Arc<RwLock<Option<UpdateManager<tauri::Wry>>>> = Arc::new(RwLock::new(None));
}

/// Initialize the update manager
pub fn init_updater(app_handle: AppHandle<tauri::Wry>, feature_flags: FeatureFlags) -> Result<()> {
    let manager = UpdateManager::new(app_handle, feature_flags);
    
    // Store globally
    *UPDATE_MANAGER.write().unwrap() = Some(manager);
    
    // Start the update manager
    if let Some(manager) = &*UPDATE_MANAGER.read().unwrap() {
        tauri::async_runtime::spawn(async move {
            if let Err(e) = manager.start().await {
                error!("Failed to start update manager: {}", e);
            }
        });
    }
    
    info!("Update manager initialized");
    
    Ok(())
}

/// Register update commands
pub fn register_commands(app: &mut tauri::App) -> Result<()> {
    // Register commands for frontend to interact with updater
    app.register_command("checkForUpdates", |_app| {
        // Get the update manager
        if let Some(manager) = &*UPDATE_MANAGER.read().unwrap() {
            // Clone the manager for use in async block
            let manager_clone = manager.clone();
            
            // Spawn async task to check for updates
            tauri::async_runtime::spawn(async move {
                let _ = manager_clone.check_for_updates().await;
            });
        }
        
        Ok(())
    })?;
    
    app.register_command("getUpdateInfo", |_app| {
        if let Some(manager) = &*UPDATE_MANAGER.read().unwrap() {
            Ok(Some(manager.get_update_info()))
        } else {
            Ok(None)
        }
    })?;
    
    app.register_command("installUpdate", |_app| {
        if let Some(manager) = &*UPDATE_MANAGER.read().unwrap() {
            // Clone the manager for use in async block
            let manager_clone = manager.clone();
            
            // Spawn async task to install update
            tauri::async_runtime::spawn(async move {
                let _ = manager_clone.install_update().await;
            });
        }
        
        Ok(())
    })?;
    
    app.register_command("setUpdateEnabled", |app, enabled: bool| {
        if let Some(manager) = &*UPDATE_MANAGER.read().unwrap() {
            manager.set_enabled(enabled)?;
        }
        
        Ok(())
    })?;
    
    app.register_command("setUpdateCheckInterval", |app, hours: u64| {
        if let Some(manager) = &*UPDATE_MANAGER.read().unwrap() {
            manager.set_check_interval(hours)?;
        }
        
        Ok(())
    })?;
    
    Ok(())
}
</file>

<file path="src/utils/config.rs">
use directories::ProjectDirs;
use lazy_static::lazy_static;
use log::{error, info};
use serde_json::{Map, Value};
use std::fs;
use std::path::{Path, PathBuf};
use std::sync::{Arc, Mutex};

lazy_static! {
    static ref CONFIG_INSTANCE: Arc<Mutex<Config>> = Arc::new(Mutex::new(Config::new()));
}

/// Configuration manager for the application
pub struct Config {
    /// The loaded configuration data
    data: Value,
    
    /// Path to the config file
    config_path: PathBuf,
    
    /// Whether the config has been modified
    dirty: bool,
}

impl Config {
    /// Create a new config instance
    pub fn new() -> Self {
        let config_path = Self::get_config_path();
        let data = Self::load_config(&config_path).unwrap_or_else(|_| {
            // Create default config
            let default_config = Self::default_config();
            Self::save_config(&config_path, &default_config).unwrap_or_else(|e| {
                error!("Failed to save default config: {}", e);
            });
            default_config
        });
        
        Config {
            data,
            config_path,
            dirty: false,
        }
    }
    
    /// Get the global config instance
    pub fn global() -> Arc<Mutex<Config>> {
        CONFIG_INSTANCE.clone()
    }
    
    /// Create the default configuration
    fn default_config() -> Value {
        let mut config = Map::new();
        
        // App settings
        config.insert("app_name".to_string(), Value::String("Claude MCP Client".to_string()));
        config.insert("version".to_string(), Value::String("0.1.0".to_string()));
        
        // Feature flags
        config.insert("lazy_loading".to_string(), Value::Bool(true));
        config.insert("plugins_enabled".to_string(), Value::Bool(true));
        config.insert("history_enabled".to_string(), Value::Bool(true));
        config.insert("advanced_ui".to_string(), Value::Bool(true));
        config.insert("experimental_features".to_string(), Value::Bool(false));
        config.insert("analytics_enabled".to_string(), Value::Bool(false));
        
        // API settings
        let mut api = Map::new();
        api.insert("base_url".to_string(), Value::String("https://api.anthropic.com".to_string()));
        api.insert("timeout_ms".to_string(), Value::Number(30000.into()));
        config.insert("api".to_string(), Value::Object(api));
        
        // UI settings
        let mut ui = Map::new();
        ui.insert("theme".to_string(), Value::String("system".to_string()));
        ui.insert("font_size".to_string(), Value::Number(14.into()));
        config.insert("ui".to_string(), Value::Object(ui));
        
        Value::Object(config)
    }
    
    /// Get the path to the config file
    fn get_config_path() -> PathBuf {
        if let Some(proj_dirs) = ProjectDirs::from("com", "claude", "mcp") {
            let config_dir = proj_dirs.config_dir();
            fs::create_dir_all(config_dir).unwrap_or_else(|e| {
                error!("Failed to create config directory: {}", e);
            });
            config_dir.join("config.json")
        } else {
            // Fallback to current directory if we can't get the project directories
            PathBuf::from("config.json")
        }
    }
    
    /// Load configuration from a file
    fn load_config(path: &Path) -> Result<Value, Box<dyn std::error::Error>> {
        if !path.exists() {
            return Err("Config file does not exist".into());
        }
        
        let config_str = fs::read_to_string(path)?;
        let config: Value = serde_json::from_str(&config_str)?;
        Ok(config)
    }
    
    /// Save configuration to a file
    fn save_config(path: &Path, config: &Value) -> Result<(), Box<dyn std::error::Error>> {
        let config_str = serde_json::to_string_pretty(config)?;
        fs::write(path, config_str)?;
        Ok(())
    }
    
    /// Get a string value from the config
    pub fn get_string(&self, key: &str) -> Option<String> {
        self.get_value(key).and_then(|v| match v {
            Value::String(s) => Some(s.clone()),
            _ => None,
        })
    }
    
    /// Get a boolean value from the config
    pub fn get_bool(&self, key: &str) -> Option<bool> {
        self.get_value(key).and_then(|v| match v {
            Value::Bool(b) => Some(*b),
            _ => None,
        })
    }
    
    /// Get a number value from the config
    pub fn get_number(&self, key: &str) -> Option<f64> {
        self.get_value(key).and_then(|v| match v {
            Value::Number(n) => n.as_f64(),
            _ => None,
        })
    }
    
    /// Get a value from the config using a dotted path (e.g. "api.base_url")
    pub fn get_value(&self, path: &str) -> Option<&Value> {
        let parts: Vec<&str> = path.split('.').collect();
        let mut current = &self.data;
        
        for part in parts {
            match current {
                Value::Object(map) => {
                    if let Some(value) = map.get(part) {
                        current = value;
                    } else {
                        return None;
                    }
                }
                _ => return None,
            }
        }
        
        Some(current)
    }
    
    /// Set a value in the config using a dotted path
    pub fn set_value(&mut self, path: &str, value: Value) -> Result<(), String> {
        let parts: Vec<&str> = path.split('.').collect();
        let mut current = &mut self.data;
        
        for (i, part) in parts.iter().enumerate() {
            if i == parts.len() - 1 {
                // Last part, set the value
                match current {
                    Value::Object(map) => {
                        map.insert(part.to_string(), value);
                        self.dirty = true;
                        return Ok(());
                    }
                    _ => return Err(format!("Path {} is not an object", path)),
                }
            } else {
                // Navigate to the next part
                match current {
                    Value::Object(map) => {
                        if !map.contains_key(*part) {
                            map.insert(part.to_string(), Value::Object(Map::new()));
                        }
                        if let Some(next) = map.get_mut(*part) {
                            current = next;
                        } else {
                            return Err(format!("Failed to navigate to {}", part));
                        }
                    }
                    _ => return Err(format!("Path {} is not an object", path)),
                }
            }
        }
        
        Err("Empty path".to_string())
    }
    
    /// Save the config to disk
    pub fn save(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        if self.dirty {
            Self::save_config(&self.config_path, &self.data)?;
            self.dirty = false;
            info!("Config saved to {}", self.config_path.display());
        }
        Ok(())
    }
    
    /// Reload the config from disk
    pub fn reload(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        self.data = Self::load_config(&self.config_path)?;
        self.dirty = false;
        info!("Config reloaded from {}", self.config_path.display());
        Ok(())
    }
}

impl Default for Config {
    fn default() -> Self {
        Self::new()
    }
}

// Helper functions to access the global config instance
pub fn get_config() -> Arc<Mutex<Config>> {
    Config::global()
}

pub fn get_string(key: &str) -> Option<String> {
    let config = Config::global();
    let config = config.lock().unwrap();
    config.get_string(key)
}

pub fn get_bool(key: &str) -> Option<bool> {
    let config = Config::global();
    let config = config.lock().unwrap();
    config.get_bool(key)
}

pub fn get_number(key: &str) -> Option<f64> {
    let config = Config::global();
    let config = config.lock().unwrap();
    config.get_number(key)
}

pub fn set_value(key: &str, value: Value) -> Result<(), String> {
    let config = Config::global();
    let mut config = config.lock().unwrap();
    config.set_value(key, value)
}

pub fn save_config() -> Result<(), Box<dyn std::error::Error>> {
    let config = Config::global();
    let mut config = config.lock().unwrap();
    config.save()
}
</file>

<file path="src/utils/events.rs">
use log::{debug, info, warn};
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use tokio::sync::mpsc;

/// Event type ID
pub type EventType = &'static str;

/// Event payload
pub type EventPayload = serde_json::Value;

/// Event handler function
pub type EventHandler = Box<dyn Fn(EventPayload) -> () + Send + Sync>;

/// Event system for handling application events
pub struct EventSystem {
    /// Registered event handlers
    handlers: Arc<Mutex<HashMap<EventType, Vec<EventHandler>>>>,
    
    /// Event sender
    tx: mpsc::UnboundedSender<(EventType, EventPayload)>,
    
    /// Event receiver
    rx: Arc<Mutex<Option<mpsc::UnboundedReceiver<(EventType, EventPayload)>>>>,
}

impl EventSystem {
    /// Create a new event system
    pub fn new() -> Self {
        let (tx, rx) = mpsc::unbounded_channel();
        
        Self {
            handlers: Arc::new(Mutex::new(HashMap::new())),
            tx,
            rx: Arc::new(Mutex::new(Some(rx))),
        }
    }
    
    /// Start the event processing loop
    pub fn start(&self) {
        let handlers = self.handlers.clone();
        let mut rx_guard = self.rx.lock().unwrap();
        
        if let Some(rx) = rx_guard.take() {
            tokio::spawn(async move {
                Self::process_events(handlers, rx).await;
            });
        }
    }
    
    /// Register an event handler
    pub fn on<F>(&self, event_type: EventType, handler: F)
    where
        F: Fn(EventPayload) -> () + Send + Sync + 'static,
    {
        let mut handlers = self.handlers.lock().unwrap();
        let event_handlers = handlers.entry(event_type).or_insert_with(Vec::new);
        event_handlers.push(Box::new(handler));
    }
    
    /// Emit an event
    pub fn emit(&self, event_type: EventType, payload: EventPayload) {
        if let Err(e) = self.tx.send((event_type, payload)) {
            warn!("Failed to emit event {}: {}", event_type, e);
        }
    }
    
    /// Process events in the background
    async fn process_events(
        handlers: Arc<Mutex<HashMap<EventType, Vec<EventHandler>>>>,
        mut rx: mpsc::UnboundedReceiver<(EventType, EventPayload)>,
    ) {
        while let Some((event_type, payload)) = rx.recv().await {
            // Get handlers for this event type
            let handlers_clone = handlers.clone();
            let handlers_guard = handlers_clone.lock().unwrap();
            
            if let Some(event_handlers) = handlers_guard.get(event_type) {
                // Clone handlers and payload to avoid holding the lock during handler execution
                let handlers_copy = event_handlers.clone();
                let payload_copy = payload.clone();
                
                // Drop the guard before executing handlers
                drop(handlers_guard);
                
                // Execute handlers in a separate task
                tokio::spawn(async move {
                    for handler in handlers_copy {
                        handler(payload_copy.clone());
                    }
                });
            }
        }
    }
}

/// Global event system instance
static EVENT_SYSTEM: once_cell::sync::OnceCell<EventSystem> = once_cell::sync::OnceCell::new();

/// Get the global event system instance
pub fn get_event_system() -> &'static EventSystem {
    EVENT_SYSTEM.get_or_init(|| {
        let system = EventSystem::new();
        system.start();
        system
    })
}

/// Event types
pub mod events {
    /// Connection status changed
    pub const CONNECTION_STATUS_CHANGED: &str = "connection_status_changed";
    
    /// Message received
    pub const MESSAGE_RECEIVED: &str = "message_received";
    
    /// Message sent
    pub const MESSAGE_SENT: &str = "message_sent";
    
    /// Message status changed
    pub const MESSAGE_STATUS_CHANGED: &str = "message_status_changed";
    
    /// Conversation created
    pub const CONVERSATION_CREATED: &str = "conversation_created";
    
    /// Conversation deleted
    pub const CONVERSATION_DELETED: &str = "conversation_deleted";
    
    /// Authentication status changed
    pub const AUTH_STATUS_CHANGED: &str = "auth_status_changed";
}
</file>

<file path="src/utils/lazy_loader.rs">
use log::{debug, warn};
use std::collections::HashMap;
use std::future::Future;
use std::pin::Pin;
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use tokio::sync::oneshot;

/// Type alias for module initialization function
type InitFn = Box<dyn FnOnce() -> Pin<Box<dyn Future<Output = Result<(), String>> + Send>> + Send>;

/// Status of a lazy-loaded module
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ModuleStatus {
    /// Module is not loaded
    NotLoaded,
    
    /// Module is currently loading
    Loading,
    
    /// Module has been loaded successfully
    Loaded,
    
    /// Module failed to load
    Failed,
}

/// Information about a lazy-loaded module
struct ModuleInfo {
    /// Current status of the module
    status: ModuleStatus,
    
    /// Time when the module was loaded
    load_time: Option<Duration>,
    
    /// Error message if loading failed
    error: Option<String>,
    
    /// Initialization function for the module
    init_fn: Option<InitFn>,
    
    /// Channel to notify when loading is complete
    notify: Option<oneshot::Sender<Result<(), String>>>,
}

/// Manager for lazy-loaded modules
pub struct LazyLoader {
    /// Registered modules
    modules: Arc<Mutex<HashMap<String, ModuleInfo>>>,
}

impl LazyLoader {
    /// Create a new lazy loader
    pub fn new() -> Self {
        LazyLoader {
            modules: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    /// Register a module with the lazy loader
    pub fn register<F, Fut>(&self, name: &str, init_fn: F)
    where
        F: FnOnce() -> Fut + Send + 'static,
        Fut: Future<Output = Result<(), String>> + Send + 'static,
    {
        let boxed_init = Box::new(move || Box::pin(init_fn()) as Pin<Box<dyn Future<Output = Result<(), String>> + Send>>);
        
        let mut modules = self.modules.lock().unwrap();
        modules.insert(name.to_string(), ModuleInfo {
            status: ModuleStatus::NotLoaded,
            load_time: None,
            error: None,
            init_fn: Some(boxed_init),
            notify: None,
        });
        
        debug!("Registered lazy-loaded module: {}", name);
    }
    
    /// Start loading a module
    pub async fn load(&self, name: &str) -> Result<(), String> {
        let (init_fn, tx) = {
            let mut modules = self.modules.lock().unwrap();
            
            let module = modules.get_mut(name).ok_or_else(|| {
                format!("Module not registered: {}", name)
            })?;
            
            if module.status == ModuleStatus::Loaded {
                return Ok(());
            }
            
            if module.status == ModuleStatus::Loading {
                let (tx, rx) = oneshot::channel();
                module.notify = Some(tx);
                return rx.await.unwrap_or_else(|_| {
                    Err(format!("Module loading was canceled: {}", name))
                });
            }
            
            let init_fn = module.init_fn.take().ok_or_else(|| {
                format!("Module initialization function not found: {}", name)
            })?;
            
            module.status = ModuleStatus::Loading;
            
            let (tx, rx) = oneshot::channel();
            module.notify = Some(tx);
            
            (init_fn, rx)
        };
        
        // Execute initialization function in a separate task
        let modules_clone = self.modules.clone();
        let name_clone = name.to_string();
        
        tokio::spawn(async move {
            let start_time = Instant::now();
            let result = init_fn().await;
            let elapsed = start_time.elapsed();
            
            // Update module status
            let mut modules = modules_clone.lock().unwrap();
            if let Some(module) = modules.get_mut(&name_clone) {
                module.load_time = Some(elapsed);
                
                if let Err(ref e) = result {
                    module.status = ModuleStatus::Failed;
                    module.error = Some(e.clone());
                    warn!("Failed to load module {}: {}", name_clone, e);
                } else {
                    module.status = ModuleStatus::Loaded;
                    debug!("Loaded module {} in {}ms", name_clone, elapsed.as_millis());
                }
                
                // Notify waiting tasks
                if let Some(tx) = module.notify.take() {
                    let _ = tx.send(result.clone());
                }
            }
        });
        
        // Wait for the module to be loaded
        tx.await.unwrap_or_else(|_| {
            Err(format!("Module loading was canceled: {}", name))
        })
    }
    
    /// Get the status of a module
    pub fn status(&self, name: &str) -> ModuleStatus {
        let modules = self.modules.lock().unwrap();
        modules.get(name).map(|m| m.status).unwrap_or(ModuleStatus::NotLoaded)
    }
    
    /// Check if a module is loaded
    pub fn is_loaded(&self, name: &str) -> bool {
        self.status(name) == ModuleStatus::Loaded
    }
    
    /// Get the load time of a module
    pub fn load_time(&self, name: &str) -> Option<Duration> {
        let modules = self.modules.lock().unwrap();
        modules.get(name).and_then(|m| m.load_time)
    }
    
    /// Get the error message for a failed module
    pub fn error(&self, name: &str) -> Option<String> {
        let modules = self.modules.lock().unwrap();
        modules.get(name).and_then(|m| m.error.clone())
    }
    
    /// Get all registered modules and their status
    pub fn get_all_modules(&self) -> HashMap<String, ModuleStatus> {
        let modules = self.modules.lock().unwrap();
        modules.iter().map(|(name, info)| (name.clone(), info.status)).collect()
    }
}

// Global lazy loader instance
lazy_static::lazy_static! {
    static ref GLOBAL_LAZY_LOADER: LazyLoader = LazyLoader::new();
}

/// Get the global lazy loader instance
pub fn get_lazy_loader() -> &'static LazyLoader {
    &GLOBAL_LAZY_LOADER
}

/// Register a module with the global lazy loader
pub fn register_module<F, Fut>(name: &str, init_fn: F)
where
    F: FnOnce() -> Fut + Send + 'static,
    Fut: Future<Output = Result<(), String>> + Send + 'static,
{
    GLOBAL_LAZY_LOADER.register(name, init_fn);
}

/// Load a module from the global lazy loader
pub async fn load_module(name: &str) -> Result<(), String> {
    GLOBAL_LAZY_LOADER.load(name).await
}

/// Check if a module is loaded in the global lazy loader
pub fn is_module_loaded(name: &str) -> bool {
    GLOBAL_LAZY_LOADER.is_loaded(name)
}
</file>

<file path="src/utils/mod.rs">
pub mod config;
pub mod events;
pub mod lazy_loader;
</file>

<file path="start-app.ps1">
# Start the Papin application
Write-Host "Starting Papin application..." -ForegroundColor Green

# Navigate to project directory
Set-Location C:\Projects\Papin

# Attempt to start the frontend
Write-Host "Starting frontend..." -ForegroundColor Cyan
Set-Location C:\Projects\Papin\src-frontend
npm run dev

# Wait for frontend to start
Start-Sleep -Seconds 5

# In a new PowerShell window, start the backend
Write-Host "Starting backend in a new window..." -ForegroundColor Cyan
Start-Process powershell -ArgumentList "-Command cd C:\Projects\Papin\src-tauri; cargo run"

Write-Host "Application started successfully!" -ForegroundColor Green
</file>

<file path="tests/e2e/end_to_end_test.js">
// E2E tests using Playwright
const { test, expect } = require('@playwright/test');

// Test conversation flow
test('conversation flow works correctly', async ({ page }) => {
  // Launch the application
  await page.goto('tauri://localhost');
  
  // Wait for the app to load
  await page.waitForSelector('#app-container', { state: 'visible' });
  
  // Type a message
  await page.fill('#message-input', 'Hello, I need some help with the MCP client.');
  
  // Click send button
  await page.click('#send-button');
  
  // Wait for response
  await page.waitForSelector('.assistant-message:nth-child(2)', { state: 'visible', timeout: 10000 });
  
  // Verify response is displayed
  const responseText = await page.textContent('.assistant-message:nth-child(2)');
  expect(responseText).not.toBeNull();
  expect(responseText.length).toBeGreaterThan(10);
  
  // Type a follow-up question
  await page.fill('#message-input', 'How do I enable offline mode?');
  
  // Click send button
  await page.click('#send-button');
  
  // Wait for second response
  await page.waitForSelector('.assistant-message:nth-child(4)', { state: 'visible', timeout: 10000 });
  
  // Verify second response is displayed and mentions offline mode
  const secondResponseText = await page.textContent('.assistant-message:nth-child(4)');
  expect(secondResponseText).toContain('offline');
});

// Test settings functionality
test('settings functionality works correctly', async ({ page }) => {
  // Launch the application
  await page.goto('tauri://localhost');
  
  // Wait for the app to load
  await page.waitForSelector('#app-container', { state: 'visible' });
  
  // Open settings
  await page.click('#settings-button');
  
  // Wait for settings modal
  await page.waitForSelector('#settings-modal', { state: 'visible' });
  
  // Navigate to appearance settings
  await page.click('#appearance-tab');
  
  // Change theme
  await page.click('#theme-selector-dark');
  
  // Wait for theme change
  await page.waitForSelector('body.dark-theme');
  
  // Verify theme was applied
  const isDarkTheme = await page.evaluate(() => {
    return document.body.classList.contains('dark-theme');
  });
  expect(isDarkTheme).toBe(true);
  
  // Navigate to offline settings
  await page.click('#offline-tab');
  
  // Toggle offline mode
  const initialOfflineState = await page.isChecked('#offline-mode-toggle');
  await page.click('#offline-mode-toggle');
  
  // Verify toggle changed state
  const newOfflineState = await page.isChecked('#offline-mode-toggle');
  expect(newOfflineState).not.toEqual(initialOfflineState);
  
  // Close settings
  await page.click('#close-settings-button');
  
  // Verify settings were closed
  await expect(page.locator('#settings-modal')).toBeHidden();
});

// Test offline capabilities
test('offline capabilities work correctly', async ({ page }) => {
  // Launch the application
  await page.goto('tauri://localhost');
  
  // Wait for the app to load
  await page.waitForSelector('#app-container', { state: 'visible' });
  
  // Enable offline mode through settings
  await page.click('#settings-button');
  await page.waitForSelector('#settings-modal', { state: 'visible' });
  await page.click('#offline-tab');
  
  // Ensure offline mode is enabled
  if (!await page.isChecked('#offline-mode-toggle')) {
    await page.click('#offline-mode-toggle');
    // Wait for offline mode to be activated
    await page.waitForSelector('#connection-status:has-text("Offline")', { timeout: 5000 });
  }
  
  // Close settings
  await page.click('#close-settings-button');
  
  // Verify offline indicator is displayed
  const statusText = await page.textContent('#connection-status');
  expect(statusText).toBe('Offline');
  
  // Send a message in offline mode
  await page.fill('#message-input', 'Can you help me with something while offline?');
  await page.click('#send-button');
  
  // Wait for response
  await page.waitForSelector('.assistant-message:nth-child(2)', { state: 'visible', timeout: 10000 });
  
  // Verify response mentions offline mode
  const responseText = await page.textContent('.assistant-message:nth-child(2)');
  expect(responseText).toContain('offline') || expect(responseText).toContain('local');
  
  // Try to create a new conversation (should work even offline)
  await page.click('#new-conversation-button');
  
  // Verify new conversation was created
  await expect(page.locator('#message-input')).toBeEmpty();
  await expect(page.locator('.message-container')).toBeEmpty();
});

// Test performance monitoring
test('performance monitoring dashboard works correctly', async ({ page }) => {
  // Launch the application
  await page.goto('tauri://localhost');
  
  // Wait for the app to load
  await page.waitForSelector('#app-container', { state: 'visible' });
  
  // Open resource dashboard
  await page.click('#tools-menu');
  await page.click('#resource-dashboard');
  
  // Wait for dashboard to load
  await page.waitForSelector('#resource-dashboard-container', { state: 'visible' });
  
  // Verify charts are displayed
  await expect(page.locator('#memory-usage-chart')).toBeVisible();
  await expect(page.locator('#api-latency-chart')).toBeVisible();
  await expect(page.locator('#token-usage-chart')).toBeVisible();
  
  // Change timeframe
  await page.click('#timeframe-selector-24h');
  
  // Wait for charts to update
  await page.waitForTimeout(500);
  
  // Verify charts contain data (by checking for canvas elements with content)
  const memoryChartEmpty = await page.evaluate(() => {
    const canvas = document.querySelector('#memory-usage-chart canvas');
    const context = canvas.getContext('2d');
    const imageData = context.getImageData(0, 0, canvas.width, canvas.height);
    const data = imageData.data;
    
    // Check if canvas is empty (all pixels are transparent)
    for (let i = 3; i < data.length; i += 4) {
      if (data[i] !== 0) return false;
    }
    return true;
  });
  
  expect(memoryChartEmpty).toBe(false);
  
  // Close dashboard
  await page.click('#close-dashboard-button');
  
  // Verify dashboard was closed
  await expect(page.locator('#resource-dashboard-container')).toBeHidden();
});

// Test auto-update functionality (mock)
test('auto-update notification works correctly', async ({ page }) => {
  // Launch the application
  await page.goto('tauri://localhost');
  
  // Wait for the app to load
  await page.waitForSelector('#app-container', { state: 'visible' });
  
  // Inject mock update event (this would normally come from Tauri)
  await page.evaluate(() => {
    window.dispatchEvent(new CustomEvent('tauri://update-available', { 
      detail: { version: '1.1.0', body: 'New features and bug fixes' } 
    }));
  });
  
  // Verify update notification is displayed
  await page.waitForSelector('#update-notification', { state: 'visible' });
  
  const notificationText = await page.textContent('#update-notification');
  expect(notificationText).toContain('1.1.0');
  
  // Click "Update Now" button
  await page.click('#update-now-button');
  
  // Verify update progress is displayed
  await page.waitForSelector('#update-progress', { state: 'visible' });
  
  // Inject mock update completion event
  await page.evaluate(() => {
    window.dispatchEvent(new CustomEvent('tauri://update-downloaded'));
  });
  
  // Verify restart notification is displayed
  await page.waitForSelector('#restart-notification', { state: 'visible' });
});
</file>

<file path="tests/e2e/playwright.config.js">
// @ts-check
const { defineConfig, devices } = require('@playwright/test');

/**
 * @see https://playwright.dev/docs/test-configuration
 */
module.exports = defineConfig({
  testDir: './',
  /* Maximum time one test can run for. */
  timeout: 30 * 1000,
  expect: {
    /**
     * Maximum time expect() should wait for the condition to be met.
     */
    timeout: 5000
  },
  /* Run tests in files in parallel */
  fullyParallel: true,
  /* Fail the build on CI if you accidentally left test.only in the source code. */
  forbidOnly: !!process.env.CI,
  /* Retry on CI only */
  retries: process.env.CI ? 2 : 0,
  /* Opt out of parallel tests on CI. */
  workers: process.env.CI ? 1 : undefined,
  /* Reporter to use. See https://playwright.dev/docs/test-reporters */
  reporter: [
    ['html'],
    ['json', { outputFile: 'test-results.json' }]
  ],
  /* Shared settings for all the projects below. See https://playwright.dev/docs/api/class-testoptions. */
  use: {
    /* Base URL to use in actions like `await page.goto('/')`. */
    baseURL: 'tauri://localhost',

    /* Collect trace when retrying the failed test. See https://playwright.dev/docs/trace-viewer */
    trace: 'on-first-retry',
    
    /* Configure screenshots on failure */
    screenshot: 'only-on-failure',
    
    /* Record video for failing tests */
    video: 'on-first-retry',
  },

  /* Run against different environments */
  projects: [
    {
      name: 'MCP Client App',
      use: {
        ...devices['Desktop Chrome'],
      },
    },
  ],

  /* Run your local dev server before starting the tests */
  webServer: {
    command: 'cargo tauri dev',
    url: 'tauri://localhost',
    timeout: 120 * 1000,
    reuseExistingServer: !process.env.CI,
  },
});
</file>

<file path="tests/integration/api_integration_test.rs">
use std::sync::Arc;
use std::time::Duration;
use tokio::time;
use tauri::{Manager, Runtime, Wry};
use mcp_client::commands::{api, update, optimization, offline};
use serde_json::json;

// Test the API commands integration with the backend
#[tokio::test]
async fn test_api_integration() {
    // Build a test Tauri application
    let app = tauri::test::mock_builder()
        .plugin(tauri_plugin_http::init())
        .setup(|app| {
            // Register commands
            api::register_commands(app)?;
            update::register_commands(app)?;
            optimization::register_commands(app)?;
            offline::register_commands(app)?;
            
            Ok(())
        })
        .build()
        .expect("Failed to build test app");
    
    // Initialize the API client
    let result = api::init_api_client(
        app.app_handle(), 
        json!({
            "baseUrl": "https://api.mcp-client.test",
            "timeout": 30000,
            "retries": 3
        })
    );
    
    assert!(result.is_ok());
    
    // Set up a mock HTTP server
    let server = mockito::Server::new();
    let mock_url = server.url();
    
    // Mock API response
    let _m = server.mock("GET", "/api/v1/models")
        .with_status(200)
        .with_header("content-type", "application/json")
        .with_body(r#"{
            "models": [
                {
                    "id": "model1",
                    "name": "Test Model 1",
                    "description": "A test model"
                },
                {
                    "id": "model2",
                    "name": "Test Model 2",
                    "description": "Another test model"
                }
            ]
        }"#)
        .create();
    
    // Update API base URL to use mock server
    let result = api::update_api_config(
        app.app_handle(), 
        json!({
            "baseUrl": mock_url,
            "timeout": 30000,
            "retries": 3
        })
    );
    
    assert!(result.is_ok());
    
    // Make API request
    let response = api::fetch_models(app.app_handle());
    
    // Verify response
    assert!(response.is_ok());
    let models = response.unwrap();
    assert_eq!(models.len(), 2);
    assert_eq!(models[0]["id"], "model1");
    assert_eq!(models[1]["id"], "model2");
}

// Test offline fallback for API requests
#[tokio::test]
async fn test_api_offline_fallback() {
    // Build a test Tauri application
    let app = tauri::test::mock_builder()
        .plugin(tauri_plugin_http::init())
        .setup(|app| {
            // Register commands
            api::register_commands(app)?;
            update::register_commands(app)?;
            optimization::register_commands(app)?;
            offline::register_commands(app)?;
            
            Ok(())
        })
        .build()
        .expect("Failed to build test app");
    
    // Initialize the API client
    let _ = api::init_api_client(
        app.app_handle(), 
        json!({
            "baseUrl": "https://nonexistent-api.mcp-client.test",
            "timeout": 1000, // Short timeout for testing
            "retries": 1
        })
    );
    
    // Initialize offline manager
    let _ = offline::init_offline_manager(app.app_handle());
    
    // Enable offline fallback
    let _ = offline::update_offline_config(
        app.app_handle(),
        json!({
            "enabled": true,
            "auto_switch": true,
            "use_local_llm": true
        })
    );
    
    // Make API request (should fail and trigger offline mode)
    let response = api::fetch_models(app.app_handle());
    
    // Wait for offline switch to complete
    time::sleep(Duration::from_millis(500)).await;
    
    // Verify offline mode was activated
    let status = offline::get_offline_status(app.app_handle());
    assert!(status.is_ok());
    assert_eq!(status.unwrap(), "Offline");
    
    // Try the same request again (should use offline cache/local model)
    let response2 = api::fetch_models(app.app_handle());
    
    // Verify response from offline mode
    assert!(response2.is_ok());
}

// Test API performance with caching
#[tokio::test]
async fn test_api_performance_with_caching() {
    // Build a test Tauri application
    let app = tauri::test::mock_builder()
        .plugin(tauri_plugin_http::init())
        .setup(|app| {
            // Register commands
            api::register_commands(app)?;
            update::register_commands(app)?;
            optimization::register_commands(app)?;
            
            Ok(())
        })
        .build()
        .expect("Failed to build test app");
    
    // Initialize the API client
    let _ = api::init_api_client(
        app.app_handle(), 
        json!({
            "baseUrl": "https://api.mcp-client.test",
            "timeout": 30000,
            "retries": 3
        })
    );
    
    // Initialize optimization manager
    let _ = optimization::init_optimizations(app.app_handle());
    
    // Set up a mock HTTP server
    let server = mockito::Server::new();
    let mock_url = server.url();
    
    // Mock API response with a delay
    let _m = server.mock("GET", "/api/v1/models")
        .with_status(200)
        .with_header("content-type", "application/json")
        .with_body(r#"{
            "models": [
                {
                    "id": "model1",
                    "name": "Test Model 1",
                    "description": "A test model"
                }
            ]
        }"#)
        .with_delay(Duration::from_millis(500)) // Simulate network delay
        .create();
    
    // Update API base URL to use mock server
    let _ = api::update_api_config(
        app.app_handle(), 
        json!({
            "baseUrl": mock_url,
            "timeout": 30000,
            "retries": 3
        })
    );
    
    // Enable API caching
    let _ = optimization::update_api_cache_config(
        app.app_handle(),
        json!({
            "enabled": true,
            "maxEntries": 100,
            "ttlSeconds": 60
        })
    );
    
    // Make first API request (should hit the network)
    let start = std::time::Instant::now();
    let response1 = api::fetch_models(app.app_handle());
    let duration1 = start.elapsed();
    
    assert!(response1.is_ok());
    
    // Make second API request (should use cache)
    let start = std::time::Instant::now();
    let response2 = api::fetch_models(app.app_handle());
    let duration2 = start.elapsed();
    
    assert!(response2.is_ok());
    
    // Verify second request was faster due to caching
    assert!(duration2 < duration1);
    
    // Check cache stats
    let stats = optimization::get_api_cache_stats(app.app_handle());
    assert!(stats.is_ok());
    let stats = stats.unwrap();
    assert_eq!(stats["hits"], 1);
}
</file>

<file path="tests/integration/auto_update_test.rs">
use std::net::SocketAddr;
use std::sync::Arc;
use tokio::sync::Mutex;
use tokio::net::TcpListener;
use axum::{
    routing::get,
    Router,
    response::IntoResponse,
    extract::Path,
    http::{StatusCode, HeaderValue, header},
    Json,
};
use serde_json::{json, Value};
use tauri::{Manager, Wry, Config};
use std::process::Command;
use std::time::Duration;

// Test server for mock updates
struct TestUpdateServer {
    addr: SocketAddr,
    version: Arc<Mutex<String>>,
}

impl TestUpdateServer {
    async fn new() -> Self {
        let listener = TcpListener::bind("127.0.0.1:0").await.unwrap();
        let addr = listener.local_addr().unwrap();
        let version = Arc::new(Mutex::new("1.0.1".to_string()));
        
        let version_clone = version.clone();
        let app = Router::new()
            .route(
                "/:target/:current_version",
                get(move |Path((target, current_version))| {
                    let version = version_clone.clone();
                    async move {
                        let version_str = version.lock().await.clone();
                        if current_version == version_str {
                            return (StatusCode::NO_CONTENT, "No updates available").into_response();
                        }
                        
                        let update_data = json!({
                            "version": version_str,
                            "notes": "Test update",
                            "pub_date": "2025-05-09T12:00:00Z",
                            "url": format!("http://{}:{}/download/{}", addr.ip(), addr.port(), target),
                            "signature": "dW50cnVzdGVkIGNvbW1lbnQ6IG1pbmlzaWduIHNpZ25hdHVyZTogNzI1Y2E5OWQ5MTkwYmM1NQp0cnVzdGVkCmNvbW1lbnQ6IHRpbWVzdGFtcDoxNjUwNTU5NTI4CWZpbGU6TUNQLUNsaWVudF8xLjAuMV94NjRfZW4tVVMubXNpCmhxZkpNWEJFTkRGd0tIWmZBakJQZWZKSC85cW5PampUZzZucG9XNXY0VWhNcUdFWmNXeHRCSUdHUCthTVZ0dTQyM21XUDZSdTJLcXcrWmVqTFhhRWdnPT0K"
                        });
                        
                        let mut response = Json(update_data).into_response();
                        response.headers_mut().insert(
                            header::CONTENT_TYPE,
                            HeaderValue::from_static("application/json"),
                        );
                        
                        response
                    }
                }),
            )
            .route(
                "/download/:target",
                get(move |Path(target)| async move {
                    // In a real test, we would return a mock installer file
                    // For this test, we just return a success response
                    format!("Mock installer for {}", target)
                }),
            );
            
        tokio::spawn(async move {
            axum::serve(listener, app).await.unwrap();
        });
        
        Self {
            addr,
            version,
        }
    }
    
    async fn set_version(&self, version: &str) {
        let mut v = self.version.lock().await;
        *v = version.to_string();
    }
    
    fn url(&self) -> String {
        format!("http://{}:{}", self.addr.ip(), self.addr.port())
    }
}

// Main test function
#[tokio::test]
async fn test_auto_update() {
    // Start mock update server
    let update_server = TestUpdateServer::new().await;
    
    // Set server to have a newer version
    update_server.set_version("1.0.2").await;
    
    // Create a custom Tauri config for testing
    let mut config = Config::default();
    config.build.dist_dir = "../dist".into();
    config.package.version = "1.0.1".into();
    
    // Override the updater endpoint with our mock server
    config.tauri.updater = Some(tauri::UpdaterConfig {
        active: true,
        dialog: true,
        endpoints: vec![format!("{}/{{{{target}}}}/{{{{current_version}}}}", update_server.url())],
        pubkey: "dW50cnVzdGVkIGNvbW1lbnQ6IG1pbmlzaWduIHB1YmxpYyBrZXk6IEMxNEYzODkyRjVCQjk3NjUKUldRTVVLVldVdXRrNC9WVklSVmorenBFODZIajVhUG16NnRKU2xEZ1JhRk9oNFpyRklBUkFBQUIKCg==".into(),
    });
    
    // Create a test event to track update availability
    let update_available = Arc::new(tokio::sync::Mutex::new(false));
    let update_version = Arc::new(tokio::sync::Mutex::new(String::new()));
    
    // Build Tauri app for testing
    let update_available_clone = update_available.clone();
    let update_version_clone = update_version.clone();
    
    let app = tauri::Builder::default()
        .config(config)
        .setup(move |app| {
            let app_handle = app.handle();
            
            // Listen for update events
            app_handle.listen_global("update-available", move |event| {
                let version = event.payload().unwrap_or("unknown");
                println!("Update available: {}", version);
                
                let update_available = update_available_clone.clone();
                let update_version = update_version_clone.clone();
                let version_str = version.to_string();
                
                tokio::spawn(async move {
                    let mut available = update_available.lock().await;
                    *available = true;
                    
                    let mut version = update_version.lock().await;
                    *version = version_str;
                });
            });
            
            // Initialize the updater
            let updater_state = app.state::<crate::commands::update::UpdaterState>();
            updater_state.initialize(app_handle);
            
            Ok(())
        })
        .build(tauri::test::mock_context())
        .expect("Failed to build test app");
    
    // Get the update manager
    let updater_state = app.state::<crate::commands::update::UpdaterState>();
    let manager = updater_state.get_manager().expect("Failed to get update manager");
    
    // Check for updates
    manager.check_for_updates().await;
    
    // Wait for update check to complete (with timeout)
    let mut attempts = 0;
    while attempts < 10 {
        if *update_available.lock().await {
            break;
        }
        
        tokio::time::sleep(Duration::from_millis(500)).await;
        attempts += 1;
    }
    
    // Assert that update was detected
    assert!(*update_available.lock().await, "Update was not detected");
    assert_eq!(*update_version.lock().await, "1.0.2", "Wrong update version detected");
    
    // Test with same version (no update should be available)
    update_server.set_version("1.0.1").await;
    
    // Reset flags
    {
        let mut available = update_available.lock().await;
        *available = false;
        
        let mut version = update_version.lock().await;
        *version = String::new();
    }
    
    // Check for updates again
    manager.check_for_updates().await;
    
    // Wait for update check to complete (with timeout)
    let mut attempts = 0;
    while attempts < 10 {
        tokio::time::sleep(Duration::from_millis(500)).await;
        attempts += 1;
    }
    
    // Assert that no update was detected
    assert!(!*update_available.lock().await, "Update was incorrectly detected");
    assert_eq!(*update_version.lock().await, "", "Update version should be empty");
    
    println!("Auto-update tests passed successfully!");
}
</file>

<file path="tests/integration/performance_test.rs">
use criterion::{criterion_group, criterion_main, Criterion, BenchmarkId};
use std::time::{Duration, Instant};
use std::sync::{Arc, Mutex};
use std::collections::HashMap;

// Import the necessary components from our crate
use crate::optimization::{OptimizationManager, MemoryManager, Cache, CacheConfig};
use crate::offline::llm::LocalLLM;
use crate::offline::checkpointing::CheckpointManager;
use crate::offline::sync::SyncManager;

// Test large conversation generation
fn test_conversation_performance(manager: &OptimizationManager, message_count: usize) -> Duration {
    let start = Instant::now();
    
    // Simulate a conversation with the given number of messages
    let mut conversation = Vec::with_capacity(message_count);
    for i in 0..message_count {
        // Alternate between user and assistant messages
        let role = if i % 2 == 0 { "user" } else { "assistant" };
        let content = format!("This is message number {} from the {}", i, role);
        
        // Add message to conversation
        conversation.push((role.to_string(), content));
    }
    
    // Process the conversation (simulating rendering and memory usage)
    let mut token_count = 0;
    for (role, content) in &conversation {
        // Simulate token counting
        let tokens = content.split_whitespace().count();
        token_count += tokens;
        
        // Update context token count in memory manager
        manager.memory_manager().update_context_tokens(token_count);
        
        // Simulate resource usage for rendering
        std::thread::sleep(Duration::from_micros(10));
    }
    
    // Force GC to measure cleanup performance
    manager.memory_manager().force_gc(false);
    
    start.elapsed()
}

// Test API caching performance
fn test_api_cache_performance(manager: &OptimizationManager, request_count: usize) -> Duration {
    let start = Instant::now();
    
    // Generate a set of test API endpoints
    let endpoints = vec![
        "api/v1/conversations",
        "api/v1/models",
        "api/v1/users/me",
        "api/v1/settings",
        "api/v1/documents",
    ];
    
    // Make simulated API requests using the cache
    let api_cache = manager.api_cache();
    for i in 0..request_count {
        let endpoint = endpoints[i % endpoints.len()];
        let key = format!("GET:{}", endpoint);
        
        if !api_cache.contains(&key) {
            // Simulate API request (cache miss)
            std::thread::sleep(Duration::from_millis(5));
            let response = format!("{{\"status\": \"success\", \"data\": [{}, {}, {}]}}", i, i+1, i+2);
            api_cache.put(key, response);
        } else {
            // Cache hit - this should be fast
            let _response = api_cache.get(&key);
        }
    }
    
    start.elapsed()
}

// Test local LLM performance
fn test_local_llm_performance(model_size: &str, input_tokens: usize) -> Duration {
    // Create a simulated local LLM of the given size
    let model = match model_size {
        "small" => LocalLLM::new("model-small", 1024, 512),
        "medium" => LocalLLM::new("model-medium", 4096, 2048),
        "large" => LocalLLM::new("model-large", 8192, 4096),
        _ => panic!("Invalid model size"),
    };
    
    // Generate input text of the required token length
    let input = "This is a test input ".repeat(input_tokens / 5 + 1);
    
    // Measure inference time
    let start = Instant::now();
    let _output = model.generate(&input, 50);
    start.elapsed()
}

// Test checkpoint performance
fn test_checkpoint_performance(checkpoint_size_kb: usize) -> Duration {
    // Create a checkpoint manager
    let manager = CheckpointManager::new();
    
    // Generate test data of the specified size
    let mut data = HashMap::new();
    let kb_per_item = 1; // Approximately 1KB per item
    let items_needed = checkpoint_size_kb / kb_per_item;
    
    for i in 0..items_needed {
        data.insert(format!("key_{}", i), "a".repeat(1024));
    }
    
    // Measure checkpoint saving and loading
    let start = Instant::now();
    
    // Save checkpoint
    let checkpoint_id = manager.save_checkpoint("test", data.clone());
    
    // Load checkpoint
    let _loaded = manager.load_checkpoint(&checkpoint_id);
    
    start.elapsed()
}

// Test sync performance
fn test_sync_performance(item_count: usize, conflict_ratio: f64) -> Duration {
    // Create sync manager
    let manager = SyncManager::new();
    
    // Generate test data
    let mut local_changes = HashMap::new();
    let mut remote_changes = HashMap::new();
    
    for i in 0..item_count {
        // Add local change
        local_changes.insert(format!("item_{}", i), format!("local_value_{}", i));
        
        // Add conflicting remote change based on the conflict ratio
        if rand::random::<f64>() < conflict_ratio {
            remote_changes.insert(format!("item_{}", i), format!("remote_value_{}", i));
        }
    }
    
    // Measure sync performance
    let start = Instant::now();
    
    // Perform sync
    let _result = manager.sync(local_changes, remote_changes);
    
    start.elapsed()
}

// Test memory usage optimization
fn test_memory_optimization() -> (usize, usize) {
    // Create memory manager
    let manager = MemoryManager::new();
    
    // Allocate memory (simulating application usage)
    let mut objects = Vec::new();
    let mut current_memory = 0;
    
    // Allocate until we reach the threshold
    loop {
        let memory_stats = manager.get_stats();
        current_memory = memory_stats.current_usage_bytes;
        
        // Check if we've reached the threshold
        if current_memory >= manager.get_limits().threshold_memory_mb * 1024 * 1024 {
            break;
        }
        
        // Allocate more memory
        let new_object = vec![0u8; 1024 * 1024]; // 1MB
        objects.push(new_object);
    }
    
    // Record memory before optimization
    let before_optimization = current_memory;
    
    // Perform memory optimization
    manager.force_gc(true);
    
    // Record memory after optimization
    let after_optimization = manager.get_stats().current_usage_bytes;
    
    (before_optimization, after_optimization)
}

// Main benchmark function for all performance tests
pub fn performance_benchmark(c: &mut Criterion) {
    // Create optimization manager
    let manager = OptimizationManager::new();
    manager.start();
    
    // Benchmark conversation performance
    let mut conversation_group = c.benchmark_group("conversation_performance");
    for size in [10, 50, 100, 500, 1000].iter() {
        conversation_group.bench_with_input(BenchmarkId::from_parameter(size), size, |b, &size| {
            b.iter(|| test_conversation_performance(&manager, size));
        });
    }
    conversation_group.finish();
    
    // Benchmark API cache performance
    let mut cache_group = c.benchmark_group("api_cache_performance");
    for size in [10, 50, 100, 500, 1000].iter() {
        cache_group.bench_with_input(BenchmarkId::from_parameter(size), size, |b, &size| {
            b.iter(|| test_api_cache_performance(&manager, size));
        });
    }
    cache_group.finish();
    
    // Benchmark local LLM performance
    let mut llm_group = c.benchmark_group("local_llm_performance");
    for model_size in ["small", "medium", "large"].iter() {
        for tokens in [10, 50, 100, 200].iter() {
            llm_group.bench_with_input(
                BenchmarkId::new(model_size, tokens),
                &(*model_size, *tokens),
                |b, &(size, tokens)| {
                    b.iter(|| test_local_llm_performance(size, tokens));
                },
            );
        }
    }
    llm_group.finish();
    
    // Benchmark checkpoint performance
    let mut checkpoint_group = c.benchmark_group("checkpoint_performance");
    for size in [100, 500, 1000, 5000].iter() {
        checkpoint_group.bench_with_input(BenchmarkId::from_parameter(size), size, |b, &size| {
            b.iter(|| test_checkpoint_performance(size));
        });
    }
    checkpoint_group.finish();
    
    // Benchmark sync performance
    let mut sync_group = c.benchmark_group("sync_performance");
    for size in [10, 100, 1000].iter() {
        for conflict in [0.0, 0.1, 0.5].iter() {
            sync_group.bench_with_input(
                BenchmarkId::new(size, conflict),
                &(*size, *conflict),
                |b, &(size, conflict)| {
                    b.iter(|| test_sync_performance(size, conflict));
                },
            );
        }
    }
    sync_group.finish();
    
    // Report memory optimization stats (not a benchmark)
    let (before, after) = test_memory_optimization();
    println!("Memory optimization test:");
    println!("  Before: {} MB", before / (1024 * 1024));
    println!("  After: {} MB", after / (1024 * 1024));
    println!("  Reduction: {} MB ({}%)", 
        (before - after) / (1024 * 1024),
        (before - after) as f64 / before as f64 * 100.0
    );
}

criterion_group!(benches, performance_benchmark);
criterion_main!(benches);

// Mock implementations for testing

// Mock LocalLLM implementation
#[cfg(test)]
mod mocks {
    use super::*;
    
    impl LocalLLM {
        pub fn new(name: &str, context_size: usize, speed: usize) -> Self {
            Self {
                name: name.to_string(),
                context_size,
                speed,
            }
        }
        
        pub fn generate(&self, input: &str, output_tokens: usize) -> String {
            // Simulate generation based on model speed
            let delay_per_token = 1000 / self.speed; // microseconds
            let total_delay = delay_per_token * output_tokens;
            std::thread::sleep(Duration::from_micros(total_delay as u64));
            
            "Generated output ".repeat(output_tokens / 2 + 1)
        }
    }
    
    // Mock CheckpointManager implementation
    impl CheckpointManager {
        pub fn new() -> Self {
            Self {}
        }
        
        pub fn save_checkpoint(&self, name: &str, data: HashMap<String, String>) -> String {
            // Simulate saving based on data size
            let total_size: usize = data.iter()
                .map(|(k, v)| k.len() + v.len())
                .sum();
            
            let delay = total_size as u64 / 1024 / 10; // ~10MB/s
            std::thread::sleep(Duration::from_millis(delay));
            
            format!("checkpoint_{}", name)
        }
        
        pub fn load_checkpoint(&self, id: &str) -> HashMap<String, String> {
            // Simulate loading
            std::thread::sleep(Duration::from_millis(50));
            HashMap::new()
        }
    }
    
    // Mock SyncManager implementation
    impl SyncManager {
        pub fn new() -> Self {
            Self {}
        }
        
        pub fn sync(
            &self,
            local: HashMap<String, String>,
            remote: HashMap<String, String>
        ) -> HashMap<String, String> {
            // Simulate syncing with conflict resolution
            let mut result = HashMap::new();
            
            // Count conflicts
            let mut conflicts = 0;
            
            // Merge changes
            for (key, local_value) in local {
                if let Some(remote_value) = remote.get(&key) {
                    // Conflict - simulate resolution
                    conflicts += 1;
                    std::thread::sleep(Duration::from_micros(500));
                    result.insert(key, format!("merged_{local_value}_{remote_value}"));
                } else {
                    // No conflict
                    result.insert(key, local_value);
                }
            }
            
            // Add remaining remote changes
            for (key, value) in remote {
                if !result.contains_key(&key) {
                    result.insert(key, value);
                }
            }
            
            // Simulate synchronization delay based on data size and conflicts
            let base_delay = (result.len() as u64) / 10;
            let conflict_delay = conflicts as u64 * 5;
            std::thread::sleep(Duration::from_millis(base_delay + conflict_delay));
            
            result
        }
    }
}
</file>

<file path="tests/integration/ui_integration_test.rs">
use tauri::{Manager, Runtime, Wry};
use std::time::Duration;
use std::sync::{Arc, Mutex};
use serde_json::json;

// UI Integration Tests using a custom test harness

// Test the conversation UI integration
#[test]
fn test_conversation_ui_integration() {
    // Use a custom test harness that includes a WebView instance
    let (mut context, event_listener) = UiTestHarness::new();
    
    // Initialize the application
    context.initialize().expect("Failed to initialize UI test context");
    
    // Simulate user typing a message
    context.simulate_typing("#message-input", "Hello, MCP!");
    
    // Simulate clicking the send button
    context.click_element("#send-button");
    
    // Wait for the response
    std::thread::sleep(Duration::from_millis(1000));
    
    // Verify the message was sent and received a response
    let events = event_listener.get_events();
    assert!(events.contains(&"message-sent"));
    assert!(events.contains(&"message-received"));
    
    // Verify the message content is displayed in the UI
    let user_message = context.get_element_text(".user-message:last-child");
    assert_eq!(user_message, "Hello, MCP!");
    
    let assistant_message = context.get_element_text(".assistant-message:last-child");
    assert!(!assistant_message.is_empty());
}

// Test offline mode UI integration
#[test]
fn test_offline_mode_ui_integration() {
    // Use a custom test harness that includes a WebView instance
    let (mut context, event_listener) = UiTestHarness::new();
    
    // Initialize the application
    context.initialize().expect("Failed to initialize UI test context");
    
    // Open settings
    context.click_element("#settings-button");
    std::thread::sleep(Duration::from_millis(500));
    
    // Navigate to offline settings
    context.click_element("#offline-settings-tab");
    std::thread::sleep(Duration::from_millis(500));
    
    // Enable offline mode
    context.click_element("#offline-mode-toggle");
    std::thread::sleep(Duration::from_millis(500));
    
    // Verify offline mode is enabled in the UI
    let is_checked = context.is_element_checked("#offline-mode-toggle");
    assert!(is_checked);
    
    // Verify offline indicator is shown
    let offline_indicator = context.get_element_text("#connection-status");
    assert_eq!(offline_indicator, "Offline");
    
    // Close settings
    context.click_element("#close-settings-button");
    std::thread::sleep(Duration::from_millis(500));
    
    // Send a message in offline mode
    context.simulate_typing("#message-input", "Offline test message");
    context.click_element("#send-button");
    std::thread::sleep(Duration::from_millis(1000));
    
    // Verify the message was processed by the local LLM
    let events = event_listener.get_events();
    assert!(events.contains(&"offline-message-processed"));
    
    // Verify the response indicates offline mode
    let assistant_message = context.get_element_text(".assistant-message:last-child");
    assert!(assistant_message.contains("offline") || assistant_message.contains("local"));
}

// Test performance dashboard UI integration
#[test]
fn test_performance_dashboard_ui() {
    // Use a custom test harness that includes a WebView instance
    let (mut context, _) = UiTestHarness::new();
    
    // Initialize the application
    context.initialize().expect("Failed to initialize UI test context");
    
    // Open resource dashboard
    context.click_element("#tools-menu");
    std::thread::sleep(Duration::from_millis(300));
    context.click_element("#resource-dashboard");
    std::thread::sleep(Duration::from_millis(1000));
    
    // Verify dashboard elements are present
    assert!(context.element_exists("#memory-usage-chart"));
    assert!(context.element_exists("#api-latency-chart"));
    assert!(context.element_exists("#token-usage-chart"));
    
    // Test interactive features
    context.click_element("#timeframe-selector-24h");
    std::thread::sleep(Duration::from_millis(500));
    
    // Verify chart data was updated
    let chart_data_points = context.get_chart_data_points("#memory-usage-chart");
    assert!(!chart_data_points.is_empty());
}

// Mock UI Test Harness
struct UiTestHarness {
    events: Arc<Mutex<Vec<String>>>,
    app: Option<tauri::App<Wry>>,
}

impl UiTestHarness {
    fn new() -> (Self, EventListener) {
        let events = Arc::new(Mutex::new(Vec::<String>::new()));
        let events_clone = events.clone();
        
        (
            Self {
                events,
                app: None,
            },
            EventListener {
                events: events_clone,
            }
        )
    }
    
    fn initialize(&mut self) -> Result<(), String> {
        // Create a test Tauri app with the frontend
        let events_clone = self.events.clone();
        let app = tauri::test::mock_builder()
            .plugin(tauri_plugin_http::init())
            .setup(move |app| {
                // Register event listeners
                let events = events_clone.clone();
                app.listen_global("message-sent", move |_| {
                    let mut events = events.lock().unwrap();
                    events.push("message-sent".to_string());
                });
                
                let events = events_clone.clone();
                app.listen_global("message-received", move |_| {
                    let mut events = events.lock().unwrap();
                    events.push("message-received".to_string());
                });
                
                let events = events_clone.clone();
                app.listen_global("offline-message-processed", move |_| {
                    let mut events = events.lock().unwrap();
                    events.push("offline-message-processed".to_string());
                });
                
                Ok(())
            })
            .build()
            .map_err(|e| format!("Failed to build test app: {}", e))?;
        
        self.app = Some(app);
        
        Ok(())
    }
    
    fn simulate_typing(&self, selector: &str, text: &str) {
        if let Some(app) = &self.app {
            let window = app.get_window("main").unwrap();
            
            // Execute JS to set input value
            let js = format!(
                "document.querySelector('{}').value = '{}'; \
                 document.querySelector('{}').dispatchEvent(new Event('input'));",
                selector, text, selector
            );
            
            let _ = window.eval(&js);
        }
    }
    
    fn click_element(&self, selector: &str) {
        if let Some(app) = &self.app {
            let window = app.get_window("main").unwrap();
            
            // Execute JS to click element
            let js = format!(
                "document.querySelector('{}').click();",
                selector
            );
            
            let _ = window.eval(&js);
        }
    }
    
    fn get_element_text(&self, selector: &str) -> String {
        if let Some(app) = &self.app {
            let window = app.get_window("main").unwrap();
            
            // Execute JS to get text content
            let js = format!(
                "document.querySelector('{}')?.textContent || ''",
                selector
            );
            
            if let Ok(result) = window.eval(&js) {
                return result;
            }
        }
        
        String::new()
    }
    
    fn is_element_checked(&self, selector: &str) -> bool {
        if let Some(app) = &self.app {
            let window = app.get_window("main").unwrap();
            
            // Execute JS to check if element is checked
            let js = format!(
                "document.querySelector('{}')?.checked || false",
                selector
            );
            
            if let Ok(result) = window.eval(&js) {
                return result == "true";
            }
        }
        
        false
    }
    
    fn element_exists(&self, selector: &str) -> bool {
        if let Some(app) = &self.app {
            let window = app.get_window("main").unwrap();
            
            // Execute JS to check if element exists
            let js = format!(
                "document.querySelector('{}') !== null",
                selector
            );
            
            if let Ok(result) = window.eval(&js) {
                return result == "true";
            }
        }
        
        false
    }
    
    fn get_chart_data_points(&self, selector: &str) -> Vec<f64> {
        if let Some(app) = &self.app {
            let window = app.get_window("main").unwrap();
            
            // Execute JS to get chart data
            let js = format!(
                "JSON.stringify(document.querySelector('{}')?.chart?.data?.datasets[0]?.data || [])",
                selector
            );
            
            if let Ok(result) = window.eval(&js) {
                if let Ok(data) = serde_json::from_str::<Vec<f64>>(&result) {
                    return data;
                }
            }
        }
        
        vec![]
    }
}

struct EventListener {
    events: Arc<Mutex<Vec<String>>>,
}

impl EventListener {
    fn get_events(&self) -> Vec<String> {
        let events = self.events.lock().unwrap();
        events.clone()
    }
}
</file>

<file path="tests/integration/updater_test.rs">
// Integration test for the auto-update functionality

use std::time::Duration;
use tauri::{AppHandle, Manager};
use tokio::time::sleep;

use crate::feature_flags::FeatureFlags;
use crate::updater::{init_updater, UpdateStatus};

#[tokio::test]
async fn test_updater_initialization() {
    // Create a mock Tauri app for testing
    let context = tauri::generate_context!();
    let app = tauri::Builder::default()
        .build(context)
        .expect("Failed to build app");
    let app_handle = app.handle();
    
    // Enable auto-update feature flag
    let mut feature_flags = FeatureFlags::empty();
    feature_flags |= FeatureFlags::AUTO_UPDATE;
    
    // Initialize the updater
    init_updater(app_handle.clone(), feature_flags).expect("Failed to initialize updater");
    
    // Allow some time for initialization
    sleep(Duration::from_millis(500)).await;
    
    // Get update info
    let update_info: serde_json::Value = app_handle
        .invoke_handler()
        .invoke_raw("getUpdateInfo", "null".into())
        .expect("Failed to get update info");
    
    // Verify that the updater is initialized correctly
    let status = update_info["status"].as_str().unwrap();
    assert!(
        status == "Checking" || status == "UpToDate",
        "Expected status to be Checking or UpToDate, got {}",
        status
    );
}

#[tokio::test]
async fn test_updater_disabled() {
    // Create a mock Tauri app for testing
    let context = tauri::generate_context!();
    let app = tauri::Builder::default()
        .build(context)
        .expect("Failed to build app");
    let app_handle = app.handle();
    
    // Disable auto-update feature flag
    let feature_flags = FeatureFlags::empty();
    
    // Initialize the updater
    init_updater(app_handle.clone(), feature_flags).expect("Failed to initialize updater");
    
    // Allow some time for initialization
    sleep(Duration::from_millis(500)).await;
    
    // Get update info
    let update_info: serde_json::Value = app_handle
        .invoke_handler()
        .invoke_raw("getUpdateInfo", "null".into())
        .expect("Failed to get update info");
    
    // Verify that the updater is disabled
    let status = update_info["status"].as_str().unwrap();
    assert_eq!(status, "Disabled", "Expected status to be Disabled");
    
    // Enable updates
    app_handle
        .invoke_handler()
        .invoke_raw("setUpdateEnabled", "true".into())
        .expect("Failed to enable updates");
    
    // Allow some time for the change to take effect
    sleep(Duration::from_millis(500)).await;
    
    // Get update info again
    let update_info: serde_json::Value = app_handle
        .invoke_handler()
        .invoke_raw("getUpdateInfo", "null".into())
        .expect("Failed to get update info");
    
    // Verify that the updater is now enabled
    let status = update_info["status"].as_str().unwrap();
    assert!(
        status == "Checking" || status == "UpToDate",
        "Expected status to be Checking or UpToDate, got {}",
        status
    );
}

#[tokio::test]
async fn test_check_interval() {
    // Create a mock Tauri app for testing
    let context = tauri::generate_context!();
    let app = tauri::Builder::default()
        .build(context)
        .expect("Failed to build app");
    let app_handle = app.handle();
    
    // Enable auto-update feature flag
    let mut feature_flags = FeatureFlags::empty();
    feature_flags |= FeatureFlags::AUTO_UPDATE;
    
    // Initialize the updater
    init_updater(app_handle.clone(), feature_flags).expect("Failed to initialize updater");
    
    // Set update check interval to 2 hours
    app_handle
        .invoke_handler()
        .invoke_raw("setUpdateCheckInterval", "2".into())
        .expect("Failed to set update check interval");
    
    // Try to set an invalid interval
    let result = app_handle
        .invoke_handler()
        .invoke_raw("setUpdateCheckInterval", "0".into());
    
    // Verify that setting an invalid interval fails
    assert!(result.is_err(), "Expected setting invalid interval to fail");
}
</file>

<file path="tests/unit/auto_update_test.rs">
use mcp_client::auto_update::{UpdateManager, UpdaterConfig};
use std::sync::{Arc, Mutex};
use std::time::Duration;
use tokio::time;
use mockall::predicate::*;
use mockall::mock;

// Mock the HTTP client for testing
mock! {
    pub HttpClient {
        fn get(&self, url: String) -> Result<Vec<u8>, String>;
        fn post(&self, url: String, body: Vec<u8>) -> Result<Vec<u8>, String>;
    }
}

#[tokio::test]
async fn test_updater_check_with_update_available() {
    // Create mock HTTP client
    let mut mock_client = MockHttpClient::new();
    mock_client
        .expect_get()
        .with(eq("https://update.mcp-client.com/win64/1.0.0".to_string()))
        .returning(|_| {
            Ok(r#"{
                "version": "1.0.1",
                "notes": "Test update",
                "pub_date": "2025-05-10T12:00:00Z",
                "url": "https://update.mcp-client.com/download/mcp-client-1.0.1.msi",
                "signature": "test-signature"
            }"#.as_bytes().to_vec())
        });

    // Create update manager with custom config
    let config = UpdaterConfig {
        enabled: true,
        check_interval: 24,
        last_check: None,
        auto_download: true,
        auto_install: false,
    };

    // Create app event tracker
    let events = Arc::new(Mutex::new(Vec::<String>::new()));
    let events_clone = events.clone();

    // Create mock app handle
    let app_handle = MockAppHandle::new(move |event, payload| {
        let mut events = events_clone.lock().unwrap();
        events.push(format!("{}:{}", event, payload));
    });

    // Create update manager
    let manager = UpdateManager::new_with_client(app_handle, mock_client);
    manager.update_config(config).await;

    // Check for updates
    manager.check_for_updates().await;

    // Wait for events to be processed
    time::sleep(Duration::from_millis(100)).await;

    // Verify that events were emitted
    let events = events.lock().unwrap();
    assert!(events.iter().any(|e| e.starts_with("update-available")));
    assert!(events.iter().any(|e| e.contains("1.0.1")));
}

#[tokio::test]
async fn test_updater_check_no_update() {
    // Create mock HTTP client
    let mut mock_client = MockHttpClient::new();
    mock_client
        .expect_get()
        .with(eq("https://update.mcp-client.com/win64/1.0.1".to_string()))
        .returning(|_| {
            Ok(r#"{
                "version": "1.0.1",
                "notes": "No update available",
                "pub_date": "2025-05-10T12:00:00Z"
            }"#.as_bytes().to_vec())
        });

    // Create update manager with custom config
    let config = UpdaterConfig {
        enabled: true,
        check_interval: 24,
        last_check: None,
        auto_download: true,
        auto_install: false,
    };

    // Create app event tracker
    let events = Arc::new(Mutex::new(Vec::<String>::new()));
    let events_clone = events.clone();

    // Create mock app handle
    let app_handle = MockAppHandle::new(move |event, payload| {
        let mut events = events_clone.lock().unwrap();
        events.push(format!("{}:{}", event, payload));
    });

    // Create update manager with version 1.0.1 (same as server)
    let manager = UpdateManager::new_with_client_and_version(app_handle, mock_client, "1.0.1".to_string());
    manager.update_config(config).await;

    // Check for updates
    manager.check_for_updates().await;

    // Wait for events to be processed
    time::sleep(Duration::from_millis(100)).await;

    // Verify that no update events were emitted
    let events = events.lock().unwrap();
    assert!(!events.iter().any(|e| e.starts_with("update-available")));
}

#[tokio::test]
async fn test_updater_with_auto_install() {
    // Create mock HTTP client
    let mut mock_client = MockHttpClient::new();
    mock_client
        .expect_get()
        .with(eq("https://update.mcp-client.com/win64/1.0.0".to_string()))
        .returning(|_| {
            Ok(r#"{
                "version": "1.0.1",
                "notes": "Test update",
                "pub_date": "2025-05-10T12:00:00Z",
                "url": "https://update.mcp-client.com/download/mcp-client-1.0.1.msi",
                "signature": "test-signature"
            }"#.as_bytes().to_vec())
        });

    // Mock download response
    mock_client
        .expect_get()
        .with(eq("https://update.mcp-client.com/download/mcp-client-1.0.1.msi".to_string()))
        .returning(|_| {
            Ok(vec![0, 1, 2, 3, 4]) // Mock installer bytes
        });

    // Create update manager with auto-install enabled
    let config = UpdaterConfig {
        enabled: true,
        check_interval: 24,
        last_check: None,
        auto_download: true,
        auto_install: true,
    };

    // Create app event tracker
    let events = Arc::new(Mutex::new(Vec::<String>::new()));
    let install_called = Arc::new(Mutex::new(false));
    let events_clone = events.clone();
    let install_called_clone = install_called.clone();

    // Create mock app handle with install capability
    let app_handle = MockAppHandleWithInstall::new(
        move |event, payload| {
            let mut events = events_clone.lock().unwrap();
            events.push(format!("{}:{}", event, payload));
        },
        move || {
            let mut called = install_called_clone.lock().unwrap();
            *called = true;
            Ok(())
        }
    );

    // Create update manager
    let manager = UpdateManager::new_with_client(app_handle, mock_client);
    manager.update_config(config).await;

    // Check for updates
    manager.check_for_updates().await;

    // Wait for events to be processed
    time::sleep(Duration::from_millis(100)).await;

    // Verify that events were emitted and install was called
    let events = events.lock().unwrap();
    assert!(events.iter().any(|e| e.starts_with("update-available")));
    
    let install_called = install_called.lock().unwrap();
    assert!(*install_called);
}

// Mock app handle
struct MockAppHandle {
    event_handler: Box<dyn Fn(&str, &str) + Send + Sync>,
}

impl MockAppHandle {
    pub fn new<F>(event_handler: F) -> Self 
    where
        F: Fn(&str, &str) + Send + Sync + 'static,
    {
        Self {
            event_handler: Box::new(event_handler),
        }
    }

    pub fn emit_all(&self, event: &str, payload: &str) -> Result<(), String> {
        (self.event_handler)(event, payload);
        Ok(())
    }
}

// Mock app handle with install capability
struct MockAppHandleWithInstall {
    event_handler: Box<dyn Fn(&str, &str) + Send + Sync>,
    install_handler: Box<dyn Fn() -> Result<(), String> + Send + Sync>,
}

impl MockAppHandleWithInstall {
    pub fn new<F, I>(event_handler: F, install_handler: I) -> Self 
    where
        F: Fn(&str, &str) + Send + Sync + 'static,
        I: Fn() -> Result<(), String> + Send + Sync + 'static,
    {
        Self {
            event_handler: Box::new(event_handler),
            install_handler: Box::new(install_handler),
        }
    }

    pub fn emit_all(&self, event: &str, payload: &str) -> Result<(), String> {
        (self.event_handler)(event, payload);
        Ok(())
    }

    pub fn install(&self) -> Result<(), String> {
        (self.install_handler)()
    }
}
</file>

<file path="tests/unit/offline_test.rs">
use mcp_client::offline::{
    OfflineManager, OfflineStatus, OfflineConfig,
    llm::LocalLLM,
    checkpointing::CheckpointManager,
    sync::{SyncManager, SyncConfig, SyncResolutionStrategy}
};
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use std::time::Duration;
use tokio::time;

#[test]
fn test_offline_mode_switching() {
    // Create offline manager
    let manager = OfflineManager::new();
    
    // Verify initial state
    assert_eq!(manager.get_status(), OfflineStatus::Online);
    
    // Switch to offline mode
    let result = manager.go_offline();
    assert!(result.is_ok());
    assert_eq!(manager.get_status(), OfflineStatus::Offline);
    
    // Try to switch to offline again (should fail)
    let result = manager.go_offline();
    assert!(result.is_err());
    
    // Switch back to online mode
    // Note: This might fail if real network checks are performed
    let _ = manager.go_online();
}

#[test]
fn test_offline_configuration() {
    // Create offline manager
    let manager = OfflineManager::new();
    
    // Create custom config
    let config = OfflineConfig {
        enabled: true,
        auto_switch: false,
        use_local_llm: true,
        connectivity_check_interval: 60,
        network_timeout_ms: 10000,
        max_checkpoints: 5,
        sync: SyncConfig {
            enabled: true,
            interval_seconds: 600,
            device_id: "test_device".to_string(),
            auto_sync: false,
            default_resolution: SyncResolutionStrategy::UseLocal,
            sync_on_startup: false,
            sync_on_shutdown: false,
        },
    };
    
    // Update config
    manager.update_config(config.clone());
    
    // Verify config was updated
    let updated_config = manager.get_config();
    assert_eq!(updated_config.auto_switch, false);
    assert_eq!(updated_config.connectivity_check_interval, 60);
    assert_eq!(updated_config.sync.interval_seconds, 600);
    assert_eq!(updated_config.sync.auto_sync, false);
}

#[test]
fn test_local_llm() {
    // Create LLM
    let llm = LocalLLM::new("test", 1024, 100);
    
    // Generate text
    let input = "This is a test input";
    let output = llm.generate(input, 10);
    
    // Verify output was generated
    assert!(!output.is_empty());
    assert!(output.contains("Generated"));
    
    // Test context limit
    let large_input = "test ".repeat(1000);
    let output = llm.generate(&large_input, 10);
    
    // Verify context limit is enforced
    assert!(output.contains("too long"));
}

#[tokio::test]
async fn test_checkpoint_save_load() {
    // Create temporary directory for testing
    let temp_dir = tempfile::tempdir().unwrap();
    
    // Create checkpoint manager
    let manager = CheckpointManager::new()
        .with_base_path(temp_dir.path())
        .with_max_checkpoints(3);
    
    // Create test data
    let mut data = HashMap::new();
    data.insert("key1".to_string(), "value1".to_string());
    data.insert("key2".to_string(), "value2".to_string());
    
    // Save checkpoint
    let checkpoint_id = manager.save_checkpoint("test", &data);
    
    // Load checkpoint
    let loaded: Option<HashMap<String, String>> = manager.load_checkpoint(&checkpoint_id);
    
    // Verify data was loaded correctly
    assert!(loaded.is_some());
    let loaded = loaded.unwrap();
    assert_eq!(loaded.get("key1"), Some(&"value1".to_string()));
    assert_eq!(loaded.get("key2"), Some(&"value2".to_string()));
}

#[test]
fn test_sync_conflict_resolution() {
    // Create sync manager
    let manager = SyncManager::new();
    
    // Create local and remote changes with conflict
    let mut local_changes = HashMap::new();
    local_changes.insert("key1".to_string(), "local_value".to_string());
    
    let mut remote_changes = HashMap::new();
    remote_changes.insert("key1".to_string(), "remote_value".to_string());
    
    // Perform sync (default resolution is UseRemote)
    let result = SyncManager::sync(local_changes, remote_changes);
    
    // Verify conflict was detected and resolved
    assert!(result.success);
    assert_eq!(result.conflicts.len(), 1);
    
    let conflict = &result.conflicts[0];
    assert_eq!(conflict.key, "key1");
    assert_eq!(conflict.resolution, SyncResolutionStrategy::UseRemote);
    assert_eq!(conflict.resolved_value, Some("remote_value".to_string()));
}
</file>

<file path="tests/unit/optimization_test.rs">
use mcp_client::optimization::{
    MemoryManager, MemoryLimits, Cache, CacheConfig, OptimizationManager
};
use std::sync::Arc;
use std::time::Duration;
use tokio::time;

#[test]
fn test_memory_manager_cleanup() {
    // Create memory manager with custom limits
    let limits = MemoryLimits {
        max_memory_mb: 100,
        threshold_memory_mb: 50,
        max_context_tokens: 1000,
        enabled: true,
        check_interval_secs: 1,
    };

    let manager = MemoryManager::new();
    manager.update_limits(limits);

    // Simulate usage by updating token count
    manager.update_context_tokens(500);

    // Verify token count was updated
    let stats = manager.get_stats();
    assert_eq!(stats.current_context_tokens, 500);

    // Force cleanup
    manager.force_gc(false);

    // Verify cleanup was performed
    let stats_after = manager.get_stats();
    assert!(stats_after.gc_count > 0);
    assert!(stats_after.last_gc_time.is_some());
}

#[tokio::test]
async fn test_cache_ttl() {
    // Create cache with short TTL
    let config = CacheConfig {
        max_entries: 10,
        ttl_seconds: 1, // 1 second TTL
        persist: false,
        cache_file: None,
        enabled: true,
        cleanup_interval_secs: 1,
    };

    let cache: Cache<String, String> = Cache::new(config);
    cache.start_cleanup();

    // Add an item
    cache.put("key1".to_string(), "value1".to_string());

    // Verify item exists
    assert_eq!(cache.get(&"key1".to_string()), Some("value1".to_string()));

    // Wait for TTL to expire
    time::sleep(Duration::from_secs(2)).await;

    // Verify item was removed
    assert_eq!(cache.get(&"key1".to_string()), None);

    // Stop cleanup task
    cache.stop_cleanup();
}

#[tokio::test]
async fn test_cache_get_or_compute() {
    // Create cache
    let config = CacheConfig::default();
    let cache: Cache<String, String> = Cache::new(config);

    // Define computation counter
    let computation_count = Arc::new(std::sync::atomic::AtomicUsize::new(0));
    let computation_count_clone = computation_count.clone();

    // Get or compute (first call - should compute)
    let value = cache.get_or_compute("key1".to_string(), || async move {
        computation_count_clone.fetch_add(1, std::sync::atomic::Ordering::SeqCst);
        "computed_value".to_string()
    }).await;

    assert_eq!(value, "computed_value");
    assert_eq!(computation_count.load(std::sync::atomic::Ordering::SeqCst), 1);

    // Get or compute again (should use cache)
    let value = cache.get_or_compute("key1".to_string(), || async move {
        computation_count.fetch_add(1, std::sync::atomic::Ordering::SeqCst);
        "different_value".to_string()
    }).await;

    assert_eq!(value, "computed_value"); // Should still be the original value
    assert_eq!(computation_count.load(std::sync::atomic::Ordering::SeqCst), 1); // Count shouldn't increase
}

#[test]
fn test_optimization_manager() {
    // Create optimization manager
    let manager = OptimizationManager::new();
    manager.start();

    // Get components
    let memory_manager = manager.memory_manager();
    let api_cache = manager.api_cache();
    let resource_cache = manager.resource_cache();

    // Verify components are initialized
    assert!(memory_manager.get_limits().enabled);
    assert!(api_cache.get_config().enabled);
    assert!(resource_cache.get_config().enabled);

    // Add some items to cache
    api_cache.put("test_key".to_string(), "test_value".to_string());
    assert_eq!(api_cache.get(&"test_key".to_string()), Some("test_value".to_string()));

    // Force garbage collection (should clear caches)
    memory_manager.force_gc(true);

    // Verify caches were cleared
    assert_eq!(api_cache.get(&"test_key".to_string()), None);

    // Stop optimization manager
    manager.stop();
}
</file>

<file path="MCP_ARCHITECTURE.md">
# MCP Architecture

## Interface Architecture

```
┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐
│                 │  │                 │  │                 │
│  GUI (Tauri)    │  │  CLI            │  │  TUI            │
│                 │  │                 │  │                 │
└────────┬────────┘  └────────┬────────┘  └────────┬────────┘
         │                    │                    │
         │                    │                    │
         ▼                    ▼                    ▼
┌─────────────────────────────────────────────────────────┐
│                                                         │
│               Common Library (mcp-common)               │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

## GUI Architecture 

```
┌─────────────────────────────┐                 ┌────────────────────────┐
│                             │                 │                        │
│ FRONTEND (React)            │                 │ BACKEND (Rust)         │
│                             │                 │                        │
│  ┌─────────────────────┐    │                 │   ┌──────────────┐    │
│  │                     │    │                 │   │              │    │
│  │  Chat Component     │    │                 │   │ Tauri        │    │
│  │                     │    │    Commands     │   │ Commands     │    │
│  └──────────┬──────────┘    ◄─────────────────►   │              │    │
│             │               │                 │   └──────┬───────┘    │
│             │               │                 │          │            │
│  ┌──────────▼──────────┐    │                 │   ┌──────▼───────┐    │
│  │                     │    │                 │   │              │    │
│  │  State Management   │    │                 │   │ Services     │    │
│  │                     │    │                 │   │              │    │
│  └──────────┬──────────┘    │                 │   └──────┬───────┘    │
│             │               │                 │          │            │
│             │               │                 │   ┌──────▼───────┐    │
│  ┌──────────▼──────────┐    │                 │   │              │    │
│  │                     │    │                 │   │ Protocol     │    │
│  │  Event Listeners    │    │    Events       │   │ Handlers     │    │
│  │                     │    ◄─────────────────►   │              │    │
│  └─────────────────────┘    │                 │   └──────┬───────┘    │
│                             │                 │          │            │
└─────────────────────────────┘                 │   ┌──────▼───────┐    │
                                                │   │              │    │
                                                │   │ WebSocket    │    │
                                                │   │ Client       │    │
                                                │   │              │    │
                                                │   └──────┬───────┘    │
                                                │          │            │
                                                └──────────┼────────────┘
                                                           │
                                                           │
                                                ┌──────────▼───────────┐
                                                │                      │
                                                │  MCP Server          │
                                                │  (Anthropic API)     │
                                                │                      │
                                                └──────────────────────┘
```

## CLI Architecture

```
┌─────────────────────────────┐
│                             │
│ CLI COMMANDS                │
│                             │
│  ┌─────────────────────┐    │
│  │                     │    │
│  │  Command Handlers   │    │
│  │                     │    │
│  └──────────┬──────────┘    │
│             │               │
│             │               │
│  ┌──────────▼──────────┐    │
│  │                     │    │
│  │  Display Formatters │    │
│  │                     │    │
│  └──────────┬──────────┘    │
│             │               │
│             │               │
└─────────────┼───────────────┘
              │
              │
┌─────────────▼───────────────┐
│                             │
│ COMMON LIBRARY              │
│                             │
│  ┌─────────────────────┐    │
│  │                     │    │
│  │  Services           │    │
│  │                     │    │
│  └──────────┬──────────┘    │
│             │               │
│             │               │
│  ┌──────────▼──────────┐    │
│  │                     │    │
│  │  Protocol Handlers  │    │
│  │                     │    │
│  └──────────┬──────────┘    │
│             │               │
│             │               │
└─────────────┼───────────────┘
              │
              │
        ┌─────▼─────┐
        │           │
        │ MCP Server│
        │           │
        └───────────┘
```

## TUI Architecture

```
┌─────────────────────────────┐
│                             │
│ TUI COMPONENTS              │
│                             │
│  ┌─────────────────────┐    │
│  │                     │    │
│  │  App Logic          │    │
│  │                     │    │
│  └──────────┬──────────┘    │
│             │               │
│             │               │
│  ┌──────────▼──────────┐    │
│  │                     │    │
│  │  UI Rendering       │    │
│  │                     │    │
│  └──────────┬──────────┘    │
│             │               │
│             │               │
│  ┌──────────▼──────────┐    │
│  │                     │    │
│  │  Event Handling     │    │
│  │                     │    │
│  └──────────┬──────────┘    │
│             │               │
│             │               │
└─────────────┼───────────────┘
              │
              │
┌─────────────▼───────────────┐
│                             │
│ COMMON LIBRARY              │
│                             │
│  ┌─────────────────────┐    │
│  │                     │    │
│  │  Services           │    │
│  │                     │    │
│  └──────────┬──────────┘    │
│             │               │
│             │               │
│  ┌──────────▼──────────┐    │
│  │                     │    │
│  │  Protocol Handlers  │    │
│  │                     │    │
│  └──────────┬──────────┘    │
│             │               │
│             │               │
└─────────────┼───────────────┘
              │
              │
        ┌─────▼─────┐
        │           │
        │ MCP Server│
        │           │
        └───────────┘
```

## Service Layer Architecture

```
┌──────────────────────────────────────────────────────────────────┐
│                                                                  │
│  Application Layer                                               │
│                                                                  │
│  ┌─────────────┐     ┌────────────────┐     ┌────────────────┐   │
│  │             │     │                │     │                │   │
│  │ ChatService │─────► AuthService    │─────► ApiService     │   │
│  │             │     │                │     │                │   │
│  └──────┬──────┘     └────────────────┘     └────────────────┘   │
│         │                                                        │
└─────────┼────────────────────────────────────────────────────────┘
          │
┌─────────▼────────────────────────────────────────────────────────┐
│                                                                  │
│  Protocol Layer                                                  │
│                                                                  │
│  ┌─────────────┐     ┌────────────────┐                          │
│  │             │     │                │                          │
│  │ McpService  │─────► McpClient      │                          │
│  │             │     │                │                          │
│  └──────┬──────┘     └───────┬────────┘                          │
│         │                    │                                   │
└─────────┼────────────────────┼───────────────────────────────────┘
          │                    │
┌─────────▼────────────────────▼───────────────────────────────────┐
│                                                                  │
│  Transport Layer                                                 │
│                                                                  │
│  ┌──────────────────┐     ┌─────────────────┐                    │
│  │                  │     │                 │                    │
│  │ ProtocolHandler  │─────► WebSocketClient │                    │
│  │                  │     │                 │                    │
│  └──────────────────┘     └────────┬────────┘                    │
│                                    │                             │
└─────────────────────────────────────────────────────────────────┘
                                     │
                            ┌────────▼────────┐
                            │                 │
                            │  MCP Server     │
                            │                 │
                            └─────────────────┘
```

## Message Structure

```
┌─────────────────────────────────────────┐
│ MCP Message                             │
│                                         │
│ ┌─────────────────────────────────────┐ │
│ │ id: "msg_123abc"                    │ │
│ ├─────────────────────────────────────┤ │
│ │ version: "v1"                       │ │
│ ├─────────────────────────────────────┤ │
│ │ type: CompletionRequest             │ │
│ ├─────────────────────────────────────┤ │
│ │ payload:                            │ │
│ │ ┌───────────────────────────────┐   │ │
│ │ │ model: "claude-3-opus-20240229"│   │ │
│ │ ├───────────────────────────────┤   │ │
│ │ │ messages: [...]               │   │ │
│ │ ├───────────────────────────────┤   │ │
│ │ │ max_tokens: 4000              │   │ │
│ │ ├───────────────────────────────┤   │ │
│ │ │ temperature: 0.7              │   │ │
│ │ ├───────────────────────────────┤   │ │
│ │ │ stream: false                 │   │ │
│ │ └───────────────────────────────┘   │ │
│ └─────────────────────────────────────┘ │
└─────────────────────────────────────────┘
```

## WebSocket Connection Flow

```
┌────────┐                            ┌────────────────┐
│ Client │                            │ MCP Server     │
└───┬────┘                            └────────┬───────┘
    │                                          │
    │ WebSocket Connect                        │
    ├─────────────────────────────────────────►│
    │                                          │
    │ WebSocket Connection Established         │
    │◄─────────────────────────────────────────┤
    │                                          │
    │ AuthRequest {api_key: "sk_..."}          │
    ├─────────────────────────────────────────►│
    │                                          │
    │ AuthResponse {success: true}             │
    │◄─────────────────────────────────────────┤
    │                                          │
    │ Ping {}                                  │
    ├─────────────────────────────────────────►│
    │                                          │
    │ Pong {}                                  │
    │◄─────────────────────────────────────────┤
    │                                          │
    │ CompletionRequest {...}                  │
    ├─────────────────────────────────────────►│
    │                                          │
    │ CompletionResponse {...}                 │
    │◄─────────────────────────────────────────┤
    │                                          │
    │ Connection Closed                        │
    ├─────────────────────────────────────────►│
    │                                          │
```

## Streaming Message Flow

```
┌────────┐                            ┌────────────────┐
│ Client │                            │ MCP Server     │
└───┬────┘                            └────────┬───────┘
    │                                          │
    │ CompletionRequest {stream: true}         │
    ├─────────────────────────────────────────►│
    │                                          │
    │ StreamingMessage {chunk: "This"}         │
    │◄─────────────────────────────────────────┤
    │                                          │
    │ StreamingMessage {chunk: " is a"}        │
    │◄─────────────────────────────────────────┤
    │                                          │
    │ StreamingMessage {chunk: " test."}       │
    │◄─────────────────────────────────────────┤
    │                                          │
    │ StreamingEnd {}                          │
    │◄─────────────────────────────────────────┤
    │                                          │
```

## Multi-Interface Architecture Benefits

The multi-interface approach (GUI, CLI, TUI) provides several advantages:

1. **Flexibility**: Users can choose the interface that best fits their workflow and environment
2. **Accessibility**: Terminal-based interfaces work in environments where GUI is not available
3. **Integration**: CLI enables integration with other tools and scripts
4. **Consistency**: Shared library ensures all interfaces provide the same functionality
5. **Code Reuse**: Core logic is implemented once and used across all interfaces
6. **Maintainability**: Changes to the protocol or core logic only need to be made in one place

## Component Communication

```
┌────────────┐  ┌────────────┐  ┌────────────┐
│            │  │            │  │            │
│    GUI     │  │    CLI     │  │    TUI     │
│            │  │            │  │            │
└─────┬──────┘  └─────┬──────┘  └─────┬──────┘
      │               │               │
      ▼               ▼               ▼
┌────────────────────────────────────────────┐
│                                            │
│           Common Service Layer             │
│                                            │
└─────────────────┬──────────────────────────┘
                  │
                  ▼
┌────────────────────────────────────────────┐
│                                            │
│               MCP Protocol                 │
│                                            │
└─────────────────┬──────────────────────────┘
                  │
                  ▼
┌────────────────────────────────────────────┐
│                                            │
│             Provider Selection             │
│                                            │
└──┬─────────────────────────────────────┬───┘
   │                                     │
   ▼                                     ▼
┌─────────────┐                  ┌─────────────┐
│             │                  │             │
│ Claude API  │                  │ Local Model │
│             │                  │             │
└─────────────┘                  └─────────────┘
```
</file>

<file path="PROJECT_STATUS.md">
# Claude MCP Client - Project Status

## Overview

The Claude MCP Client is a multi-interface application providing access to Anthropic's Claude AI models via both the Model Context Protocol (MCP) and traditional REST APIs. The client features three distinct interfaces: a desktop GUI (Tauri/React), a command-line interface (CLI), and a text-based user interface (TUI). All interfaces share core functionality through a common library, providing a consistent experience regardless of the interface used.

## Current Status

### Core Components

- **Common Library**: ✅ COMPLETE
  - Full MCP protocol implementation with WebSocket communication
  - Service layer for AI, chat, and auth operations
  - Shared models and configuration management
  - Local model support for offline operation
  - Model router for intelligent provider selection

- **Desktop Application (GUI)**: 🟡 PARTIAL
  - Backend implementation complete
  - Frontend structure defined
  - Basic UI components created
  - Needs further UI implementation and integration with backend

- **Command Line Interface (CLI)**: ✅ COMPLETE
  - Full command set implemented
  - Interactive mode for conversation
  - Chat, conversation management, export/import
  - System message management
  - Model management commands
  - Streaming response support

- **Text User Interface (TUI)**: ✅ COMPLETE
  - Full-featured terminal UI
  - Conversation management
  - Real-time streaming responses
  - Keyboard navigation and command mode
  - Settings and help screens

### Implemented Features

- **MCP Protocol**: ✅ COMPLETE
  - Full WebSocket implementation
  - Message streaming support
  - Authentication and session management
  - Error handling and recovery mechanisms

- **Conversation Management**: ✅ COMPLETE
  - Create, list, show, delete conversations
  - Manage conversation history
  - Set system messages
  - Export/import conversations

- **Model Management**: ✅ COMPLETE
  - List available models
  - Set default model
  - Change models for existing conversations
  - Model router for intelligent selection

- **User Interfaces**: 🟡 PARTIAL
  - CLI fully implemented
  - TUI fully implemented
  - GUI partially implemented (backend complete, frontend needs work)

- **Documentation**: 🟡 PARTIAL
  - READMEs for all components
  - Command documentation
  - Architecture documentation
  - API documentation partially complete

- **Build System**: ✅ COMPLETE
  - Makefile for building all components
  - Individual build targets for each component
  - Installation targets

## Recent Accomplishments

1. **CLI Enhancements**:
   - Added model management commands (list, set-default, set-for-conversation)
   - Created comprehensive documentation
   - Improved error handling and display formatting

2. **TUI Implementation**:
   - Designed and implemented a full-featured terminal UI
   - Added conversation management functionality
   - Implemented real-time streaming responses
   - Created keyboard navigation and command mode
   - Added settings and help screens

3. **Integration Enhancements**:
   - Ensured all three interfaces share core functionality
   - Created a unified build system with Makefile
   - Improved documentation across all components
   - Standardized error handling and configuration management

4. **Documentation**:
   - Created detailed READMEs for all components
   - Updated main project documentation
   - Added usage examples and keyboard shortcuts
   - Documented architecture and API

## Next Steps

### Short-term Priorities

1. **Complete GUI Frontend**:
   - Implement React components based on the defined structure
   - Connect frontend components to Tauri backend
   - Add streaming UI components
   - Implement settings and configuration UI

2. **Testing and Quality Assurance**:
   - Add unit tests for core functionality
   - Create integration tests for interfaces
   - Test across different platforms (Linux, macOS, Windows)
   - Performance testing, especially startup time

3. **Deployment Preparation**:
   - Create Linux packages (DEB, AppImage)
   - Set up CI/CD pipeline for automated builds
   - Prepare installation documentation
   - Create release process

### Medium-term Goals

1. **Enhanced Local Model Support**:
   - Replace placeholder implementation with real inference engine
   - Add model download and management UI
   - Improve model performance on resource-constrained devices
   - Add more local model options

2. **Improved UX Across Interfaces**:
   - Add guided onboarding for new users
   - Improve error messages and recovery
   - Add more keyboard shortcuts
   - Implement additional UI enhancements

3. **Cross-platform Refinements**:
   - Optimize for different operating systems
   - Improve installation experience
   - Add platform-specific features when appropriate
   - Ensure consistent behavior across platforms

### Long-term Vision

1. **Plugin System**:
   - Implement the plugin architecture
   - Create example plugins for common tasks
   - Add plugin management UI
   - Develop documentation for plugin developers

2. **Advanced Context Management**:
   - Implement advanced context handling for long conversations
   - Add context compression techniques
   - Support for document embeddings
   - Memory management for efficient token usage

3. **Collaborative Features**:
   - Shared conversations between users
   - Team workspaces
   - Real-time collaboration capabilities
   - Access control and permissions

4. **Enterprise Integration**:
   - Add enterprise authentication options
   - Support for organizational policies
   - Audit logging and compliance features
   - Integration with corporate systems

## Technical Debt

1. **Inference Engine**: The local model inference engine is currently a placeholder that needs to be replaced with a real implementation using llama.cpp or similar.

2. **Error Handling**: While the backend has comprehensive error handling, the frontend needs improved error recovery and user feedback.

3. **Testing Coverage**: Need more comprehensive testing across all components, especially for edge cases and error conditions.

4. **Documentation**: Internal API documentation needs improvement for easier developer onboarding.

## Conclusion

The Claude MCP Client has evolved into a comprehensive solution with multiple interfaces, offering flexibility in how users interact with Claude AI models. The CLI and TUI components are now complete, providing powerful terminal-based access, while the GUI is partially implemented with a strong backend foundation.

The project's architecture provides excellent separation of concerns through its layered design, with shared functionality in the common library ensuring consistency across interfaces. The implementation of the Model Context Protocol provides real-time interactions with Claude models, with intelligent routing and offline capabilities through local model support.

The next phase should focus on completing the GUI frontend, comprehensive testing, and preparing for deployment, while addressing technical debt and continuing to refine the user experience across all interfaces.
</file>

<file path="src-cli/src/commands/mod.rs">
pub mod chat;
pub mod delete;
pub mod export;
pub mod interactive;
pub mod list;
pub mod model;
pub mod new;
pub mod setup;
pub mod show;
pub mod system;

use clap::{Parser, Subcommand};

/// MCP Client Command Line Interface
#[derive(Parser)]
#[command(author, version, about, long_about = None)]
pub struct Cli {
    /// Sets the level of verbosity
    #[arg(short, long)]
    pub verbose: bool,
    
    /// Suppresses most output
    #[arg(short, long)]
    pub quiet: bool,
    
    /// Subcommand to execute
    #[command(subcommand)]
    pub command: Commands,
}

/// Available commands
#[derive(Subcommand)]
pub enum Commands {
    /// Send a message in a conversation
    Chat {
        /// Conversation ID
        #[arg(short, long)]
        conversation_id: Option<String>,
        
        /// Message content
        #[arg(short, long)]
        message: Option<String>,
        
        /// Disable streaming mode
        #[arg(long)]
        no_stream: bool,
    },
    
    /// List conversations
    List,
    
    /// Create a new conversation
    New {
        /// Conversation title
        #[arg(short, long)]
        title: Option<String>,
        
        /// Model to use
        #[arg(short, long)]
        model: Option<String>,
    },
    
    /// Delete a conversation
    Delete {
        /// Conversation ID
        conversation_id: String,
    },
    
    /// Show conversation details
    Show {
        /// Conversation ID
        conversation_id: String,
    },
    
    /// Configure API settings
    Setup,
    
    /// Export a conversation
    Export {
        /// Conversation ID
        conversation_id: String,
        
        /// Export format (json, markdown, txt)
        #[arg(short, long, default_value = "json")]
        format: String,
        
        /// Output file (default: stdout)
        #[arg(short, long)]
        output: Option<String>,
    },
    
    /// Set system message for a conversation
    System {
        /// Conversation ID
        conversation_id: String,
        
        /// System message content
        #[arg(short, long)]
        message: Option<String>,
    },
    
    /// Start interactive mode
    Interactive {
        /// Conversation ID (optional)
        #[arg(short, long)]
        conversation_id: Option<String>,
    },
    
    /// Model management
    Model {
        /// Model subcommand
        #[command(subcommand)]
        command: ModelCommands,
    },
}

/// Model subcommands
#[derive(Subcommand)]
pub enum ModelCommands {
    /// List available models
    List,
    
    /// Set default model for new conversations
    SetDefault {
        /// Model name
        model: String,
    },
    
    /// Set model for a conversation
    SetForConversation {
        /// Conversation ID
        conversation_id: String,
        
        /// Model name
        model: String,
    },
}
</file>

<file path="src-cli/src/main.rs">
mod commands;
mod display;
mod error;

use clap::Parser;
use log::LevelFilter;
use std::sync::Arc;

use commands::{Cli, Commands, ModelCommands};
use error::CliResult;
use mcp_common::{get_mcp_service, init_mcp_service, service::ChatService};

#[tokio::main]
async fn main() -> CliResult<()> {
    // Initialize logging
    env_logger::Builder::new()
        .filter_level(LevelFilter::Info)
        .format_timestamp(None)
        .format_target(false)
        .init();
    
    // Parse command line arguments
    let cli = Cli::parse();
    
    // Configure logger level
    if cli.verbose {
        log::set_max_level(LevelFilter::Debug);
    } else if cli.quiet {
        log::set_max_level(LevelFilter::Error);
    } else {
        log::set_max_level(LevelFilter::Info);
    }
    
    // Initialize MCP service
    let mcp_service = init_mcp_service();
    let chat_service = Arc::new(ChatService::new(mcp_service));
    
    // Process command
    match cli.command {
        Commands::Chat {
            conversation_id,
            message,
            no_stream,
        } => {
            commands::chat::run(chat_service, conversation_id, message, !no_stream).await?;
        }
        Commands::List => {
            commands::list::run(chat_service).await?;
        }
        Commands::New { title, model } => {
            commands::new::run(chat_service, title, model).await?;
        }
        Commands::Delete { conversation_id } => {
            commands::delete::run(chat_service, conversation_id).await?;
        }
        Commands::Show { conversation_id } => {
            commands::show::run(chat_service, conversation_id).await?;
        }
        Commands::Setup => {
            commands::setup::run().await?;
        }
        Commands::Export { conversation_id, format, output } => {
            commands::export::run(chat_service, conversation_id, format, output).await?;
        }
        Commands::System { conversation_id, message } => {
            commands::system::run(chat_service, conversation_id, message).await?;
        }
        Commands::Interactive { conversation_id } => {
            commands::interactive::run(chat_service, conversation_id).await?;
        }
        Commands::Model { command } => {
            match command {
                ModelCommands::List => {
                    commands::model::list(chat_service).await?;
                }
                ModelCommands::SetDefault { model } => {
                    commands::model::set_default(chat_service, &model).await?;
                }
                ModelCommands::SetForConversation { conversation_id, model } => {
                    commands::model::set_for_conversation(chat_service, &conversation_id, &model).await?;
                }
            }
        }
    }
    
    Ok(())
}
</file>

<file path="src-common/src/observability/canary.rs">
use serde::{Serialize, Deserialize};
use std::collections::HashMap;
use std::sync::{Arc, RwLock};
use chrono::{DateTime, Utc};
use uuid::Uuid;
use rand::{Rng, SeedableRng};
use rand::rngs::StdRng;
use log::{debug, info, warn, error};

use crate::feature_flags::{FeatureFlags, FeatureManager};
use crate::observability::metrics::{MetricsCollector, Metric, MetricType, HistogramStats, TimerStats};
use crate::observability::telemetry::{TelemetryClient, track_feature_usage};
use crate::error::Result;

/// Canary release system for safely rolling out new features to users
/// 
/// This module provides a way to gradually roll out new features to users,
/// monitor their performance, and if necessary, roll back problematic features.

/// Canary group definitions
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum CanaryGroup {
    /// Internal development and testing (employees only)
    Internal,
    
    /// Alpha testers (opt-in, limited set of users)
    Alpha, 
    
    /// Beta testers (broader group, opt-in)
    Beta,
    
    /// Early access (production-ready features, limited rollout)
    EarlyAccess,
    
    /// All users (full availability)
    AllUsers,
}

impl CanaryGroup {
    pub fn as_str(&self) -> &'static str {
        match self {
            CanaryGroup::Internal => "internal",
            CanaryGroup::Alpha => "alpha",
            CanaryGroup::Beta => "beta",
            CanaryGroup::EarlyAccess => "early_access",
            CanaryGroup::AllUsers => "all_users",
        }
    }
    
    pub fn from_str(s: &str) -> Option<Self> {
        match s.to_lowercase().as_str() {
            "internal" => Some(CanaryGroup::Internal),
            "alpha" => Some(CanaryGroup::Alpha),
            "beta" => Some(CanaryGroup::Beta),
            "early_access" | "earlyaccess" => Some(CanaryGroup::EarlyAccess),
            "all_users" | "allusers" | "all" => Some(CanaryGroup::AllUsers),
            _ => None,
        }
    }
}

/// Feature rollout status
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum RolloutStatus {
    /// Feature is not available to any users
    Disabled,
    
    /// Feature is available to a specific canary group
    CanaryGroup(CanaryGroup),
    
    /// Feature is available to a percentage of users
    PercentRollout(u8),
    
    /// Feature is available to all users
    FullyEnabled,
    
    /// Feature was rolled back due to issues
    RolledBack,
}

impl RolloutStatus {
    pub fn is_enabled_for_group(&self, group: CanaryGroup) -> bool {
        match self {
            RolloutStatus::Disabled => false,
            RolloutStatus::CanaryGroup(canary_group) => {
                // If the user's group has same or higher access level
                match (group, canary_group) {
                    // Internal has access to everything
                    (CanaryGroup::Internal, _) => true,
                    
                    // Alpha has access to Alpha, Beta, Early Access
                    (CanaryGroup::Alpha, CanaryGroup::Alpha) => true,
                    (CanaryGroup::Alpha, CanaryGroup::Beta) => true,
                    (CanaryGroup::Alpha, CanaryGroup::EarlyAccess) => true,
                    (CanaryGroup::Alpha, CanaryGroup::Internal) => false,
                    
                    // Beta has access to Beta, Early Access
                    (CanaryGroup::Beta, CanaryGroup::Beta) => true,
                    (CanaryGroup::Beta, CanaryGroup::EarlyAccess) => true,
                    (CanaryGroup::Beta, _) => false,
                    
                    // Early Access has access to Early Access only
                    (CanaryGroup::EarlyAccess, CanaryGroup::EarlyAccess) => true,
                    (CanaryGroup::EarlyAccess, _) => false,
                    
                    // All Users only have access to features available to all users
                    (CanaryGroup::AllUsers, CanaryGroup::AllUsers) => true,
                    (CanaryGroup::AllUsers, _) => false,
                }
            },
            RolloutStatus::PercentRollout(_) => {
                // Internal, Alpha, and Beta users always get percent rollouts
                matches!(group, CanaryGroup::Internal | CanaryGroup::Alpha | CanaryGroup::Beta)
            },
            RolloutStatus::FullyEnabled => true,
            RolloutStatus::RolledBack => false,
        }
    }
    
    pub fn is_enabled_for_user(&self, user_id: &str, user_group: CanaryGroup) -> bool {
        match self {
            RolloutStatus::Disabled => false,
            RolloutStatus::CanaryGroup(group) => self.is_enabled_for_group(user_group),
            RolloutStatus::PercentRollout(percent) => {
                // Always enable for internal, alpha, and beta users
                if matches!(user_group, CanaryGroup::Internal | CanaryGroup::Alpha | CanaryGroup::Beta) {
                    return true;
                }
                
                // For others, check the percentage
                let percent = *percent as u32;
                if percent >= 100 {
                    return true;
                }
                
                // Use a hash of the user ID to determine if they're in the rollout
                let mut seed_bytes = [0u8; 32];
                
                // Use the user ID as a seed for deterministic randomness
                let user_bytes = user_id.as_bytes();
                for (i, &byte) in user_bytes.iter().enumerate() {
                    seed_bytes[i % 32] ^= byte;
                }
                
                // Create a deterministic RNG
                let mut rng = StdRng::from_seed(seed_bytes);
                
                // Generate a number between 0 and 99
                let user_value = rng.gen_range(0..100);
                
                // If the user's value is less than the percentage, they get the feature
                user_value < percent
            },
            RolloutStatus::FullyEnabled => true,
            RolloutStatus::RolledBack => false,
        }
    }
}

/// Feature rollout configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FeatureRollout {
    pub feature_name: String,
    pub feature_flag: Option<FeatureFlags>,
    pub description: String,
    pub status: RolloutStatus,
    pub version_introduced: String,
    pub metrics: Vec<String>,
    pub rollout_schedule: Option<HashMap<DateTime<Utc>, RolloutStatus>>,
    pub automatic: bool,
    pub rollback_threshold: Option<MetricThreshold>,
    pub owners: Vec<String>,
}

/// Metric threshold for automatic rollback
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricThreshold {
    pub metric_name: String,
    pub metric_type: MetricType,
    pub operator: ThresholdOperator,
    pub value: f64,
    pub duration_minutes: u32,
}

/// Threshold operators
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum ThresholdOperator {
    GreaterThan,
    LessThan,
    GreaterThanOrEqual,
    LessThanOrEqual,
    Equal,
    NotEqual,
}

impl ThresholdOperator {
    pub fn evaluate(&self, a: f64, b: f64) -> bool {
        match self {
            ThresholdOperator::GreaterThan => a > b,
            ThresholdOperator::LessThan => a < b,
            ThresholdOperator::GreaterThanOrEqual => a >= b,
            ThresholdOperator::LessThanOrEqual => a <= b,
            ThresholdOperator::Equal => (a - b).abs() < f64::EPSILON,
            ThresholdOperator::NotEqual => (a - b).abs() >= f64::EPSILON,
        }
    }
    
    pub fn as_str(&self) -> &'static str {
        match self {
            ThresholdOperator::GreaterThan => ">",
            ThresholdOperator::LessThan => "<",
            ThresholdOperator::GreaterThanOrEqual => ">=",
            ThresholdOperator::LessThanOrEqual => "<=",
            ThresholdOperator::Equal => "==",
            ThresholdOperator::NotEqual => "!=",
        }
    }
}

/// Canary release configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CanaryConfig {
    pub enabled: bool,
    pub user_id: String,
    pub user_group: CanaryGroup,
    pub opt_in_features: Vec<String>,
    pub feature_rollouts: HashMap<String, FeatureRollout>,
    pub metrics_comparison_enabled: bool,
    pub auto_rollback_enabled: bool,
    pub monitoring_interval_minutes: u32,
}

impl Default for CanaryConfig {
    fn default() -> Self {
        Self {
            enabled: true,
            user_id: Uuid::new_v4().to_string(),
            user_group: CanaryGroup::AllUsers,
            opt_in_features: Vec::new(),
            feature_rollouts: HashMap::new(),
            metrics_comparison_enabled: true,
            auto_rollback_enabled: true,
            monitoring_interval_minutes: 15,
        }
    }
}

/// Metrics comparison between control and canary groups
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricsComparison {
    pub feature_name: String,
    pub timestamp: DateTime<Utc>,
    pub control_metrics: HashMap<String, MetricValue>,
    pub canary_metrics: HashMap<String, MetricValue>,
    pub difference_percent: HashMap<String, f64>,
    pub alerts: Vec<MetricAlert>,
}

/// Metric value types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum MetricValue {
    Counter(f64),
    Gauge(f64),
    Histogram(HistogramStats),
    Timer(TimerStats),
}

/// Metric alert
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricAlert {
    pub metric_name: String,
    pub severity: AlertSeverity,
    pub message: String,
    pub threshold: String,
    pub actual_value: String,
    pub timestamp: DateTime<Utc>,
}

/// Alert severity
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
pub enum AlertSeverity {
    Info,
    Warning,
    Critical,
}

/// Main canary release manager
pub struct CanaryManager {
    config: Arc<RwLock<CanaryConfig>>,
    feature_manager: Arc<RwLock<FeatureManager>>,
    metrics_collector: Option<Arc<MetricsCollector>>,
    telemetry_client: Option<Arc<TelemetryClient>>,
    metrics_history: Arc<RwLock<HashMap<String, Vec<MetricsComparison>>>>,
    last_check: Arc<RwLock<DateTime<Utc>>>,
}

impl CanaryManager {
    pub fn new(
        config: CanaryConfig,
        feature_manager: Arc<RwLock<FeatureManager>>,
        metrics_collector: Option<Arc<MetricsCollector>>,
        telemetry_client: Option<Arc<TelemetryClient>>,
    ) -> Self {
        let manager = Self {
            config: Arc::new(RwLock::new(config)),
            feature_manager,
            metrics_collector,
            telemetry_client,
            metrics_history: Arc::new(RwLock::new(HashMap::new())),
            last_check: Arc::new(RwLock::new(Utc::now())),
        };
        
        // Initialize feature flags based on config
        manager.update_feature_flags();
        
        // Start monitoring scheduled rollouts
        manager.start_rollout_scheduler();
        
        manager
    }
    
    /// Check if a feature is enabled for the current user
    pub fn is_feature_enabled(&self, feature_name: &str) -> bool {
        let config = self.config.read().unwrap();
        
        // Check if canary system is enabled
        if !config.enabled {
            // Fallback to feature flag system
            if let Some(rollout) = config.feature_rollouts.get(feature_name) {
                if let Some(flag) = rollout.feature_flag {
                    return self.feature_manager.read().unwrap().is_enabled(flag);
                }
            }
            return false;
        }
        
        // Check if this feature is in the rollout config
        if let Some(rollout) = config.feature_rollouts.get(feature_name) {
            // Check if feature is enabled based on rollout status
            let is_enabled = rollout.status.is_enabled_for_user(&config.user_id, config.user_group);
            
            // Check if user has explicitly opted in
            let opted_in = config.opt_in_features.contains(&feature_name.to_string());
            
            // Track feature check
            if let Some(client) = &self.telemetry_client {
                let mut properties = HashMap::new();
                properties.insert("feature".to_string(), feature_name.to_string());
                properties.insert("enabled".to_string(), is_enabled.to_string());
                properties.insert("opted_in".to_string(), opted_in.to_string());
                properties.insert("user_group".to_string(), config.user_group.as_str().to_string());
                
                track_feature_usage(&format!("canary_check_{}", feature_name), Some(properties));
            }
            
            return is_enabled || opted_in;
        }
        
        // If feature not in canary system, fall back to feature flag system
        let flag_result = if let Some(rollout) = config.feature_rollouts.get(feature_name) {
            if let Some(flag) = rollout.feature_flag {
                self.feature_manager.read().unwrap().is_enabled(flag)
            } else {
                false
            }
        } else {
            false
        };
        
        flag_result
    }
    
    /// Get the user's canary group
    pub fn get_user_group(&self) -> CanaryGroup {
        self.config.read().unwrap().user_group
    }
    
    /// Set the user's canary group
    pub fn set_user_group(&self, group: CanaryGroup) {
        let mut config = self.config.write().unwrap();
        config.user_group = group;
        
        // Update feature flags based on new group
        drop(config); // Release lock before calling update_feature_flags
        self.update_feature_flags();
        
        // Track group change
        if let Some(client) = &self.telemetry_client {
            let mut properties = HashMap::new();
            properties.insert("group".to_string(), group.as_str().to_string());
            
            track_feature_usage("canary_group_change", Some(properties));
        }
    }
    
    /// Opt in to a specific feature
    pub fn opt_in_feature(&self, feature_name: &str) -> Result<()> {
        let mut config = self.config.write().unwrap();
        
        // Check if feature exists
        if !config.feature_rollouts.contains_key(feature_name) {
            return Err(std::io::Error::new(
                std::io::ErrorKind::NotFound,
                format!("Feature '{}' not found in canary system", feature_name),
            ).into());
        }
        
        // Add to opt-in list if not already there
        if !config.opt_in_features.contains(&feature_name.to_string()) {
            config.opt_in_features.push(feature_name.to_string());
        }
        
        // Update feature flag if needed
        if let Some(rollout) = config.feature_rollouts.get(feature_name) {
            if let Some(flag) = rollout.feature_flag {
                drop(config); // Release lock before modifying feature manager
                self.feature_manager.write().unwrap().enable(flag);
            } else {
                drop(config); // Release lock
            }
        } else {
            drop(config); // Release lock
        }
        
        // Track opt-in
        if let Some(client) = &self.telemetry_client {
            let mut properties = HashMap::new();
            properties.insert("feature".to_string(), feature_name.to_string());
            
            track_feature_usage("canary_opt_in", Some(properties));
        }
        
        Ok(())
    }
    
    /// Opt out of a specific feature
    pub fn opt_out_feature(&self, feature_name: &str) -> Result<()> {
        let mut config = self.config.write().unwrap();
        
        // Remove from opt-in list
        config.opt_in_features.retain(|f| f != feature_name);
        
        // Store values we need after dropping the lock
        let should_disable = if let Some(rollout) = config.feature_rollouts.get(feature_name) {
            if let Some(flag) = rollout.feature_flag {
                // Only disable if not otherwise enabled through user's group
                let should_disable = !rollout.status.is_enabled_for_user(&config.user_id, config.user_group);
                
                if should_disable {
                    Some(flag)
                } else {
                    None
                }
            } else {
                None
            }
        } else {
            None
        };
        
        drop(config); // Release lock
        
        // Update feature flag if needed
        if let Some(flag) = should_disable {
            self.feature_manager.write().unwrap().disable(flag);
        }
        
        // Track opt-out
        if let Some(client) = &self.telemetry_client {
            let mut properties = HashMap::new();
            properties.insert("feature".to_string(), feature_name.to_string());
            
            track_feature_usage("canary_opt_out", Some(properties));
        }
        
        Ok(())
    }
    
    /// Add a new feature rollout
    pub fn add_feature_rollout(&self, rollout: FeatureRollout) -> Result<()> {
        let mut config = self.config.write().unwrap();
        
        // Add to rollouts map
        config.feature_rollouts.insert(rollout.feature_name.clone(), rollout.clone());
        
        // Store values we need after dropping the lock
        let feature_flag = rollout.feature_flag;
        let status = rollout.status;
        let user_id = config.user_id.clone();
        let user_group = config.user_group;
        let opt_in = config.opt_in_features.contains(&rollout.feature_name);
        
        drop(config); // Release lock
        
        // Update feature flag if needed
        if let Some(flag) = feature_flag {
            let mut feature_manager = self.feature_manager.write().unwrap();
            
            if status.is_enabled_for_user(&user_id, user_group) || opt_in {
                feature_manager.enable(flag);
            } else {
                feature_manager.disable(flag);
            }
        }
        
        // Track new rollout
        if let Some(client) = &self.telemetry_client {
            let mut properties = HashMap::new();
            properties.insert("feature".to_string(), rollout.feature_name.clone());
            properties.insert("status".to_string(), format!("{:?}", rollout.status));
            
            track_feature_usage("canary_rollout_added", Some(properties));
        }
        
        Ok(())
    }
    
    /// Update a feature rollout
    pub fn update_feature_rollout(&self, feature_name: &str, new_status: RolloutStatus) -> Result<()> {
        // Get values under a read lock first to minimize write lock time
        let (old_status, flag, automatic, user_id, user_group, opt_in) = {
            let config = self.config.read().unwrap();
            
            if let Some(rollout) = config.feature_rollouts.get(feature_name) {
                let old_status = rollout.status;
                let flag = rollout.feature_flag;
                let automatic = rollout.automatic;
                let user_id = config.user_id.clone();
                let user_group = config.user_group;
                let opt_in = config.opt_in_features.contains(&rollout.feature_name);
                
                (Some(old_status), flag, automatic, user_id, user_group, opt_in)
            } else {
                (None, None, false, String::new(), CanaryGroup::AllUsers, false)
            }
        };
        
        // If feature doesn't exist, return error
        if old_status.is_none() {
            return Err(std::io::Error::new(
                std::io::ErrorKind::NotFound,
                format!("Feature '{}' not found in canary system", feature_name),
            ).into());
        }
        
        // Update status
        {
            let mut config = self.config.write().unwrap();
            if let Some(rollout) = config.feature_rollouts.get_mut(feature_name) {
                rollout.status = new_status;
            }
        }
        
        // Update feature flag if needed
        if let Some(flag) = flag {
            let mut feature_manager = self.feature_manager.write().unwrap();
            
            if new_status.is_enabled_for_user(&user_id, user_group) || opt_in {
                feature_manager.enable(flag);
            } else {
                feature_manager.disable(flag);
            }
        }
        
        // Track status change
        if let Some(client) = &self.telemetry_client {
            let mut properties = HashMap::new();
            properties.insert("feature".to_string(), feature_name.to_string());
            properties.insert("old_status".to_string(), format!("{:?}", old_status.unwrap()));
            properties.insert("new_status".to_string(), format!("{:?}", new_status));
            
            track_feature_usage("canary_rollout_updated", Some(properties));
        }
        
        Ok(())
    }
    
    /// Remove a feature rollout
    pub fn remove_feature_rollout(&self, feature_name: &str) -> Result<()> {
        // Extract values under read lock first
        let flag = {
            let config = self.config.read().unwrap();
            
            if let Some(rollout) = config.feature_rollouts.get(feature_name) {
                rollout.feature_flag
            } else {
                None
            }
        };
        
        // Update with write lock
        {
            let mut config = self.config.write().unwrap();
            
            // Remove if it exists
            if !config.feature_rollouts.contains_key(feature_name) {
                return Err(std::io::Error::new(
                    std::io::ErrorKind::NotFound,
                    format!("Feature '{}' not found in canary system", feature_name),
                ).into());
            }
            
            config.feature_rollouts.remove(feature_name);
            
            // Remove from opt-in list
            config.opt_in_features.retain(|f| f != feature_name);
        }
        
        // Disable feature flag if needed
        if let Some(flag) = flag {
            self.feature_manager.write().unwrap().disable(flag);
        }
        
        // Track removal
        if let Some(client) = &self.telemetry_client {
            let mut properties = HashMap::new();
            properties.insert("feature".to_string(), feature_name.to_string());
            
            track_feature_usage("canary_rollout_removed", Some(properties));
        }
        
        Ok(())
    }
    
    /// Update feature flags based on canary configuration
    fn update_feature_flags(&self) {
        let config = self.config.read().unwrap();
        let mut feature_manager = self.feature_manager.write().unwrap();
        
        for (name, rollout) in &config.feature_rollouts {
            if let Some(flag) = rollout.feature_flag {
                let is_enabled = rollout.status.is_enabled_for_user(&config.user_id, config.user_group) || 
                                config.opt_in_features.contains(name);
                
                if is_enabled {
                    feature_manager.enable(flag);
                } else {
                    feature_manager.disable(flag);
                }
            }
        }
    }
    
    /// Start scheduler for automatic rollouts based on schedule
    fn start_rollout_scheduler(&self) {
        let config_arc = Arc::clone(&self.config);
        let manager_arc = Arc::new(self.clone());
        
        std::thread::spawn(move || {
            loop {
                // Sleep for a while between checks
                std::thread::sleep(std::time::Duration::from_secs(60));
                
                // Check if there are any scheduled rollouts
                let now = Utc::now();
                let mut updates = Vec::new();
                
                {
                    let config = config_arc.read().unwrap();
                    
                    // Skip if canary system is disabled
                    if !config.enabled {
                        continue;
                    }
                    
                    // Check all rollouts with schedules
                    for (name, rollout) in &config.feature_rollouts {
                        if let Some(schedule) = &rollout.rollout_schedule {
                            // Find scheduled updates that should have happened by now
                            for (scheduled_time, status) in schedule {
                                if scheduled_time <= &now && *status != rollout.status {
                                    updates.push((name.clone(), *status));
                                    break;
                                }
                            }
                        }
                    }
                }
                
                // Apply any needed updates
                for (name, status) in updates {
                    debug!("Applying scheduled rollout update for feature '{}': {:?}", name, status);
                    if let Err(e) = manager_arc.update_feature_rollout(&name, status) {
                        error!("Failed to apply scheduled rollout update: {}", e);
                    }
                }
            }
        });
    }
    
    /// Check metrics for automatic rollbacks
    pub fn check_metrics(&self) -> Vec<MetricAlert> {
        let config = self.config.read().unwrap();
        
        // Check if auto-rollback is enabled
        if !config.enabled || !config.auto_rollback_enabled {
            return Vec::new();
        }
        
        // Check if it's time to check metrics based on interval
        let now = Utc::now();
        let mut last_check = self.last_check.write().unwrap();
        let minutes_since_last_check = (now - *last_check).num_minutes() as u32;
        
        if minutes_since_last_check < config.monitoring_interval_minutes {
            return Vec::new();
        }
        
        // Update last check time
        *last_check = now;
        
        // Collect all alerts
        let mut all_alerts = Vec::new();
        
        // Check metrics for each feature that has a rollback threshold
        let metrics_collector = match &self.metrics_collector {
            Some(collector) => collector,
            None => return Vec::new(), // No metrics collector available
        };
        
        // Get metrics reports
        let counters = metrics_collector.get_counters_report().unwrap_or_default();
        let gauges = metrics_collector.get_gauges_report().unwrap_or_default();
        let histograms = metrics_collector.get_histograms_report().unwrap_or_default();
        let timers = metrics_collector.get_timers_report().unwrap_or_default();
        
        for (feature_name, rollout) in &config.feature_rollouts {
            if let Some(threshold) = &rollout.rollback_threshold {
                // Skip features that are already rolled back or fully enabled
                if matches!(rollout.status, RolloutStatus::RolledBack | RolloutStatus::FullyEnabled) {
                    continue;
                }
                
                // Check the appropriate metric based on type
                let mut threshold_exceeded = false;
                let mut actual_value = String::new();
                
                match threshold.metric_type {
                    MetricType::Counter => {
                        if let Some(value) = counters.get(&threshold.metric_name) {
                            if threshold.operator.evaluate(*value, threshold.value) {
                                threshold_exceeded = true;
                                actual_value = value.to_string();
                            }
                        }
                    },
                    MetricType::Gauge => {
                        if let Some(value) = gauges.get(&threshold.metric_name) {
                            if threshold.operator.evaluate(*value, threshold.value) {
                                threshold_exceeded = true;
                                actual_value = value.to_string();
                            }
                        }
                    },
                    MetricType::Histogram => {
                        if let Some(stats) = histograms.get(&threshold.metric_name) {
                            // Compare based on average value by default
                            if threshold.operator.evaluate(stats.avg, threshold.value) {
                                threshold_exceeded = true;
                                actual_value = stats.avg.to_string();
                            }
                        }
                    },
                    MetricType::Timer => {
                        if let Some(stats) = timers.get(&threshold.metric_name) {
                            // Compare based on average value by default
                            if threshold.operator.evaluate(stats.avg_ms, threshold.value) {
                                threshold_exceeded = true;
                                actual_value = stats.avg_ms.to_string();
                            }
                        }
                    },
                }
                
                if threshold_exceeded {
                    // Create alert
                    let alert = MetricAlert {
                        metric_name: threshold.metric_name.clone(),
                        severity: AlertSeverity::Critical,
                        message: format!(
                            "Metric '{}' threshold exceeded for feature '{}'. Threshold: {} {}, actual value: {}",
                            threshold.metric_name, 
                            feature_name,
                            threshold.operator.as_str(), 
                            threshold.value,
                            actual_value
                        ),
                        threshold: format!("{} {}", threshold.operator.as_str(), threshold.value),
                        actual_value,
                        timestamp: now,
                    };
                    
                    all_alerts.push(alert);
                    
                    // Auto-rollback if needed
                    if rollout.automatic {
                        debug!("Auto-rolling back feature '{}' due to threshold violation", feature_name);
                        drop(config); // Release read lock before acquiring write lock
                        if let Err(e) = self.update_feature_rollout(feature_name, RolloutStatus::RolledBack) {
                            error!("Failed to auto-rollback feature: {}", e);
                        }
                        return all_alerts; // Return early since we modified the config
                    }
                }
            }
        }
        
        all_alerts
    }
    
    /// Compare metrics between control and canary groups
    pub fn compare_metrics(&self, feature_name: &str) -> Option<MetricsComparison> {
        let config = self.config.read().unwrap();
        
        if !config.enabled || !config.metrics_comparison_enabled {
            return None;
        }
        
        // Check if the feature exists
        let rollout = match config.feature_rollouts.get(feature_name) {
            Some(r) => r,
            None => return None,
        };
        
        // Skip if feature is not in an active canary state
        if !matches!(rollout.status, RolloutStatus::CanaryGroup(_) | RolloutStatus::PercentRollout(_)) {
            return None;
        }
        
        // Check if we have a metrics collector
        let metrics_collector = match &self.metrics_collector {
            Some(collector) => collector,
            None => return None,
        };
        
        // Get metric reports
        let counters = metrics_collector.get_counters_report().unwrap_or_default();
        let gauges = metrics_collector.get_gauges_report().unwrap_or_default();
        let histograms = metrics_collector.get_histograms_report().unwrap_or_default();
        let timers = metrics_collector.get_timers_report().unwrap_or_default();
        
        // Collect metrics for this feature
        let mut control_metrics = HashMap::new();
        let mut canary_metrics = HashMap::new();
        let mut diff_percent = HashMap::new();
        let mut alerts = Vec::new();
        
        // Check each metric associated with this feature
        for metric_name in &rollout.metrics {
            // First, check if we have control and canary metrics
            let control_name = format!("{}_control", metric_name);
            let canary_name = format!("{}_canary", metric_name);
            
            // Check counters
            if let (Some(control), Some(canary)) = (counters.get(&control_name), counters.get(&canary_name)) {
                control_metrics.insert(metric_name.clone(), MetricValue::Counter(*control));
                canary_metrics.insert(metric_name.clone(), MetricValue::Counter(*canary));
                
                // Calculate percent difference if control is not zero
                if *control > 0.0 {
                    let diff = (canary - control) / control * 100.0;
                    diff_percent.insert(metric_name.clone(), diff);
                    
                    // Generate alert if difference is significant
                    if diff.abs() > 20.0 {
                        alerts.push(MetricAlert {
                            metric_name: metric_name.clone(),
                            severity: if diff.abs() > 50.0 { AlertSeverity::Critical } else { AlertSeverity::Warning },
                            message: format!(
                                "Counter metric '{}' for feature '{}' shows {}% {} in canary group",
                                metric_name, 
                                feature_name,
                                diff.abs(),
                                if diff > 0.0 { "increase" } else { "decrease" }
                            ),
                            threshold: "20% difference".to_string(),
                            actual_value: format!("{}% difference", diff),
                            timestamp: Utc::now(),
                        });
                    }
                }
                continue;
            }
            
            // Check gauges
            if let (Some(control), Some(canary)) = (gauges.get(&control_name), gauges.get(&canary_name)) {
                control_metrics.insert(metric_name.clone(), MetricValue::Gauge(*control));
                canary_metrics.insert(metric_name.clone(), MetricValue::Gauge(*canary));
                
                // Calculate percent difference if control is not zero
                if *control > 0.0 {
                    let diff = (canary - control) / control * 100.0;
                    diff_percent.insert(metric_name.clone(), diff);
                    
                    // Generate alert if difference is significant
                    if diff.abs() > 20.0 {
                        alerts.push(MetricAlert {
                            metric_name: metric_name.clone(),
                            severity: if diff.abs() > 50.0 { AlertSeverity::Critical } else { AlertSeverity::Warning },
                            message: format!(
                                "Gauge metric '{}' for feature '{}' shows {}% {} in canary group",
                                metric_name, 
                                feature_name,
                                diff.abs(),
                                if diff > 0.0 { "increase" } else { "decrease" }
                            ),
                            threshold: "20% difference".to_string(),
                            actual_value: format!("{}% difference", diff),
                            timestamp: Utc::now(),
                        });
                    }
                }
                continue;
            }
            
            // Check histograms
            if let (Some(control), Some(canary)) = (histograms.get(&control_name), histograms.get(&canary_name)) {
                control_metrics.insert(metric_name.clone(), MetricValue::Histogram(control.clone()));
                canary_metrics.insert(metric_name.clone(), MetricValue::Histogram(canary.clone()));
                
                // Calculate percent difference if control average is not zero
                if control.avg > 0.0 {
                    let diff = (canary.avg - control.avg) / control.avg * 100.0;
                    diff_percent.insert(metric_name.clone(), diff);
                    
                    // Generate alert if difference is significant
                    if diff.abs() > 20.0 {
                        alerts.push(MetricAlert {
                            metric_name: metric_name.clone(),
                            severity: if diff.abs() > 50.0 { AlertSeverity::Critical } else { AlertSeverity::Warning },
                            message: format!(
                                "Histogram metric '{}' for feature '{}' shows {}% {} in canary group",
                                metric_name, 
                                feature_name,
                                diff.abs(),
                                if diff > 0.0 { "increase" } else { "decrease" }
                            ),
                            threshold: "20% difference".to_string(),
                            actual_value: format!("{}% difference", diff),
                            timestamp: Utc::now(),
                        });
                    }
                }
                continue;
            }
            
            // Check timers
            if let (Some(control), Some(canary)) = (timers.get(&control_name), timers.get(&canary_name)) {
                control_metrics.insert(metric_name.clone(), MetricValue::Timer(control.clone()));
                canary_metrics.insert(metric_name.clone(), MetricValue::Timer(canary.clone()));
                
                // Calculate percent difference if control average is not zero
                if control.avg_ms > 0.0 {
                    let diff = (canary.avg_ms - control.avg_ms) / control.avg_ms * 100.0;
                    diff_percent.insert(metric_name.clone(), diff);
                    
                    // Generate alert if difference is significant
                    if diff.abs() > 20.0 {
                        alerts.push(MetricAlert {
                            metric_name: metric_name.clone(),
                            severity: if diff.abs() > 50.0 { AlertSeverity::Critical } else { AlertSeverity::Warning },
                            message: format!(
                                "Timer metric '{}' for feature '{}' shows {}% {} in canary group",
                                metric_name, 
                                feature_name,
                                diff.abs(),
                                if diff > 0.0 { "increase" } else { "decrease" }
                            ),
                            threshold: "20% difference".to_string(),
                            actual_value: format!("{}% difference", diff),
                            timestamp: Utc::now(),
                        });
                    }
                }
                continue;
            }
        }
        
        // Create comparison result
        let comparison = MetricsComparison {
            feature_name: feature_name.to_string(),
            timestamp: Utc::now(),
            control_metrics,
            canary_metrics,
            difference_percent: diff_percent,
            alerts,
        };
        
        // Save to history
        {
            let mut history = self.metrics_history.write().unwrap();
            let feature_history = history.entry(feature_name.to_string()).or_insert_with(Vec::new);
            feature_history.push(comparison.clone());
            
            // Limit history size
            if feature_history.len() > 100 {
                feature_history.remove(0);
            }
        }
        
        Some(comparison)
    }
    
    /// Get metrics history for a feature
    pub fn get_metrics_history(&self, feature_name: &str) -> Vec<MetricsComparison> {
        let history = self.metrics_history.read().unwrap();
        
        if let Some(feature_history) = history.get(feature_name) {
            feature_history.clone()
        } else {
            Vec::new()
        }
    }
    
    /// Get all feature rollouts
    pub fn get_feature_rollouts(&self) -> HashMap<String, FeatureRollout> {
        self.config.read().unwrap().feature_rollouts.clone()
    }
    
    /// Get a specific feature rollout
    pub fn get_feature_rollout(&self, feature_name: &str) -> Option<FeatureRollout> {
        self.config.read().unwrap().feature_rollouts.get(feature_name).cloned()
    }
    
    /// Get all opt-in features
    pub fn get_opt_in_features(&self) -> Vec<String> {
        self.config.read().unwrap().opt_in_features.clone()
    }
}

// Enable deep cloning for the manager to use in the rollout scheduler
impl Clone for CanaryManager {
    fn clone(&self) -> Self {
        Self {
            config: Arc::clone(&self.config),
            feature_manager: Arc::clone(&self.feature_manager),
            metrics_collector: self.metrics_collector.clone(),
            telemetry_client: self.telemetry_client.clone(),
            metrics_history: Arc::clone(&self.metrics_history),
            last_check: Arc::clone(&self.last_check),
        }
    }
}

// Create a global canary manager
lazy_static::lazy_static! {
    pub static ref CANARY_MANAGER: Arc<RwLock<Option<CanaryManager>>> = Arc::new(RwLock::new(None));
}

// Initialize canary manager
pub fn init_canary_manager(
    config: CanaryConfig,
    feature_manager: Arc<RwLock<FeatureManager>>,
    metrics_collector: Option<Arc<MetricsCollector>>,
    telemetry_client: Option<Arc<TelemetryClient>>,
) -> Result<()> {
    let manager = CanaryManager::new(
        config,
        feature_manager,
        metrics_collector,
        telemetry_client,
    );
    
    let mut global_manager = CANARY_MANAGER.write().unwrap();
    *global_manager = Some(manager);
    
    info!("Canary release manager initialized");
    
    Ok(())
}

// Helper functions to interact with the global canary manager
pub fn is_feature_enabled(feature_name: &str) -> bool {
    if let Some(manager) = CANARY_MANAGER.read().unwrap().as_ref() {
        manager.is_feature_enabled(feature_name)
    } else {
        false
    }
}

pub fn check_metrics() -> Vec<MetricAlert> {
    if let Some(manager) = CANARY_MANAGER.read().unwrap().as_ref() {
        manager.check_metrics()
    } else {
        Vec::new()
    }
}

pub fn compare_metrics(feature_name: &str) -> Option<MetricsComparison> {
    if let Some(manager) = CANARY_MANAGER.read().unwrap().as_ref() {
        manager.compare_metrics(feature_name)
    } else {
        None
    }
}

pub fn opt_in_feature(feature_name: &str) -> Result<()> {
    if let Some(manager) = CANARY_MANAGER.read().unwrap().as_ref() {
        manager.opt_in_feature(feature_name)
    } else {
        Err(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            "Canary manager not initialized",
        ).into())
    }
}

pub fn opt_out_feature(feature_name: &str) -> Result<()> {
    if let Some(manager) = CANARY_MANAGER.read().unwrap().as_ref() {
        manager.opt_out_feature(feature_name)
    } else {
        Err(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            "Canary manager not initialized",
        ).into())
    }
}

pub fn set_user_group(group: CanaryGroup) -> Result<()> {
    if let Some(manager) = CANARY_MANAGER.read().unwrap().as_ref() {
        manager.set_user_group(group);
        Ok(())
    } else {
        Err(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            "Canary manager not initialized",
        ).into())
    }
}

// Macro to check if a feature is enabled
#[macro_export]
macro_rules! feature_enabled {
    ($feature_name:expr) => {
        $crate::observability::canary::is_feature_enabled($feature_name)
    };
}

// Tauri commands
#[cfg(feature = "tauri")]
#[tauri::command]
pub fn get_canary_group() -> String {
    if let Some(manager) = CANARY_MANAGER.read().unwrap().as_ref() {
        manager.get_user_group().as_str().to_string()
    } else {
        "all_users".to_string()
    }
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn set_canary_group(group: String) -> Result<()> {
    let canary_group = CanaryGroup::from_str(&group)
        .ok_or_else(|| std::io::Error::new(
            std::io::ErrorKind::InvalidInput,
            format!("Invalid canary group: {}", group),
        ))?;
    
    set_user_group(canary_group)
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn get_feature_rollouts() -> HashMap<String, FeatureRollout> {
    if let Some(manager) = CANARY_MANAGER.read().unwrap().as_ref() {
        manager.get_feature_rollouts()
    } else {
        HashMap::new()
    }
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn get_canary_metrics(feature_name: String) -> Option<MetricsComparison> {
    if let Some(manager) = CANARY_MANAGER.read().unwrap().as_ref() {
        manager.compare_metrics(&feature_name)
    } else {
        None
    }
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn opt_in_canary_feature(feature_name: String) -> Result<()> {
    opt_in_feature(&feature_name)
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn opt_out_canary_feature(feature_name: String) -> Result<()> {
    opt_out_feature(&feature_name)
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn get_canary_opt_in_features() -> Vec<String> {
    if let Some(manager) = CANARY_MANAGER.read().unwrap().as_ref() {
        manager.get_opt_in_features()
    } else {
        Vec::new()
    }
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn check_canary_metrics() -> Vec<MetricAlert> {
    check_metrics()
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn get_metrics_history(feature_name: String) -> Vec<MetricsComparison> {
    if let Some(manager) = CANARY_MANAGER.read().unwrap().as_ref() {
        manager.get_metrics_history(&feature_name)
    } else {
        Vec::new()
    }
}
</file>

<file path="src-common/src/observability/logging.rs">
use chrono::{DateTime, Utc, NaiveDate, Local};
use colored::Colorize;
use serde::{Serialize, Deserialize};
use std::collections::HashMap;
use std::sync::{Arc, RwLock};
use std::io::{self, Write};
use std::fs::{self, File, OpenOptions, create_dir_all};
use std::path::{Path, PathBuf};
use std::time::Duration;
use std::ops::Add;
use uuid::Uuid;

use crate::observability::telemetry::TelemetryClient;
use crate::observability::metrics::ObservabilityConfig;
use crate::error::Result;

// Define log levels
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
pub enum LogLevel {
    Trace = 0,
    Debug = 1,
    Info = 2,
    Warn = 3,
    Error = 4,
    Fatal = 5,
}

impl LogLevel {
    pub fn as_str(&self) -> &'static str {
        match self {
            LogLevel::Trace => "TRACE",
            LogLevel::Debug => "DEBUG",
            LogLevel::Info => "INFO",
            LogLevel::Warn => "WARN",
            LogLevel::Error => "ERROR",
            LogLevel::Fatal => "FATAL",
        }
    }
    
    pub fn from_u8(value: u8) -> Self {
        match value {
            0 => LogLevel::Trace,
            1 => LogLevel::Debug,
            2 => LogLevel::Info,
            3 => LogLevel::Warn,
            4 => LogLevel::Error,
            5 => LogLevel::Fatal,
            _ => LogLevel::Info,
        }
    }
    
    pub fn from_str(value: &str) -> Self {
        match value.to_uppercase().as_str() {
            "TRACE" => LogLevel::Trace,
            "DEBUG" => LogLevel::Debug,
            "INFO" => LogLevel::Info,
            "WARN" => LogLevel::Warn,
            "ERROR" => LogLevel::Error,
            "FATAL" => LogLevel::Fatal,
            _ => LogLevel::Info,
        }
    }
    
    pub fn color_string(&self, text: &str) -> colored::ColoredString {
        match self {
            LogLevel::Trace => text.bright_black(),
            LogLevel::Debug => text.bright_blue(),
            LogLevel::Info => text.green(),
            LogLevel::Warn => text.yellow(),
            LogLevel::Error => text.red(),
            LogLevel::Fatal => text.bright_red().bold(),
        }
    }
}

// Define log entry structure
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LogEntry {
    pub id: String,
    pub timestamp: DateTime<Utc>,
    pub level: LogLevel,
    pub message: String,
    pub module: String,
    pub thread_id: String,
    pub session_id: String,
    pub context: HashMap<String, String>,
    pub stacktrace: Option<String>,
}

// Logger configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LoggerConfig {
    pub min_level: LogLevel,
    pub file_enabled: bool,
    pub file_path: Option<String>,
    pub console_enabled: bool,
    pub console_format: LogFormat,
    pub colored_output: bool,
    pub max_file_size: u64,
    pub max_log_files: u32,
    pub telemetry_enabled: bool,
    pub include_context: bool,
    pub rotate_daily: bool,
    pub log_to_remote: bool,
    pub remote_url: Option<String>,
}

impl Default for LoggerConfig {
    fn default() -> Self {
        Self {
            min_level: LogLevel::Info,
            file_enabled: true,
            file_path: None,
            console_enabled: true,
            console_format: LogFormat::Detailed,
            colored_output: true,
            max_file_size: 10 * 1024 * 1024, // 10 MB
            max_log_files: 5,
            telemetry_enabled: false,
            include_context: true,
            rotate_daily: true,
            log_to_remote: false,
            remote_url: None,
        }
    }
}

// Log format options
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum LogFormat {
    Simple,     // [INFO] Message
    Standard,   // [2023-05-01 10:30:45] [INFO] [module] Message
    Detailed,   // [2023-05-01 10:30:45.123] [INFO] [module] [thread-id] Message
    Json,       // {"timestamp": "...", "level": "INFO", ...}
}

/// In-memory log buffer for UI display and analytics
#[derive(Debug, Clone, Default)]
pub struct LogBuffer {
    entries: Vec<LogEntry>,
    max_size: usize,
}

impl LogBuffer {
    pub fn new(max_size: usize) -> Self {
        Self {
            entries: Vec::with_capacity(max_size),
            max_size,
        }
    }
    
    pub fn add(&mut self, entry: LogEntry) {
        if self.entries.len() >= self.max_size {
            self.entries.remove(0);
        }
        self.entries.push(entry);
    }
    
    pub fn get_recent(&self, count: usize) -> Vec<LogEntry> {
        let start = self.entries.len().saturating_sub(count);
        self.entries[start..].to_vec()
    }
    
    pub fn get_all(&self) -> Vec<LogEntry> {
        self.entries.clone()
    }
    
    pub fn clear(&mut self) {
        self.entries.clear();
    }
    
    pub fn filter(&self, level: Option<LogLevel>, module: Option<&str>, contains: Option<&str>) -> Vec<LogEntry> {
        self.entries.iter()
            .filter(|entry| {
                if let Some(min_level) = level {
                    if entry.level < min_level {
                        return false;
                    }
                }
                
                if let Some(mod_pattern) = module {
                    if !entry.module.contains(mod_pattern) {
                        return false;
                    }
                }
                
                if let Some(text) = contains {
                    if !entry.message.contains(text) && 
                       !entry.context.values().any(|v| v.contains(text)) {
                        return false;
                    }
                }
                
                true
            })
            .cloned()
            .collect()
    }
}

// Main Logger struct
pub struct Logger {
    config: RwLock<LoggerConfig>,
    log_file: Option<Arc<RwLock<File>>>,
    log_file_path: Option<PathBuf>,
    in_memory_buffer: RwLock<LogBuffer>,
    session_id: String,
    telemetry_client: Option<Arc<TelemetryClient>>,
    last_rotation_date: RwLock<NaiveDate>,
}

impl Logger {
    pub fn new(config: LoggerConfig, telemetry_client: Option<Arc<TelemetryClient>>) -> Self {
        let (log_file, log_file_path) = if config.file_enabled {
            if let Some(log_path) = &config.file_path {
                // Create the directory if it doesn't exist
                if let Some(parent) = Path::new(log_path).parent() {
                    let _ = create_dir_all(parent);
                }
                
                let file = OpenOptions::new()
                    .create(true)
                    .append(true)
                    .open(log_path)
                    .ok();
                    
                (file.map(|f| Arc::new(RwLock::new(f))), Some(PathBuf::from(log_path)))
            } else {
                let default_path = Self::default_log_path();
                // Create the directory if it doesn't exist
                if let Some(parent) = default_path.parent() {
                    let _ = create_dir_all(parent);
                }
                
                let file = OpenOptions::new()
                    .create(true)
                    .append(true)
                    .open(&default_path)
                    .ok();
                    
                (file.map(|f| Arc::new(RwLock::new(f))), Some(default_path))
            }
        } else {
            (None, None)
        };
        
        Self {
            config: RwLock::new(config),
            log_file,
            log_file_path,
            in_memory_buffer: RwLock::new(LogBuffer::new(1000)), // Keep last 1000 log entries in memory
            session_id: Uuid::new_v4().to_string(),
            telemetry_client,
            last_rotation_date: RwLock::new(Local::now().date_naive()),
        }
    }
    
    fn default_log_path() -> PathBuf {
        let mut path = dirs::home_dir().unwrap_or_else(|| PathBuf::from("."));
        
        // Platform-specific log directory
        #[cfg(target_os = "windows")]
        {
            path.push("AppData");
            path.push("Roaming");
            path.push("MCP");
            path.push("logs");
        }
        
        #[cfg(target_os = "macos")]
        {
            path.push("Library");
            path.push("Logs");
            path.push("MCP");
        }
        
        #[cfg(target_os = "linux")]
        {
            path.push(".local");
            path.push("share");
            path.push("mcp");
            path.push("logs");
        }
        
        // Ensure directory exists
        if !path.exists() {
            let _ = create_dir_all(&path);
        }
        
        // Add timestamp to log file name
        let now = Local::now();
        let filename = format!("mcp_{}.log", now.format("%Y-%m-%d"));
        path.push(filename);
        
        path
    }
    
    pub fn log(&self, level: LogLevel, module: &str, message: &str, context: Option<HashMap<String, String>>, stacktrace: Option<String>) {
        // Check log level
        if level < self.config.read().unwrap().min_level {
            return;
        }
        
        // Check for daily rotation if enabled
        if self.config.read().unwrap().rotate_daily {
            let today = Local::now().date_naive();
            let mut last_rotation = self.last_rotation_date.write().unwrap();
            
            if today > *last_rotation {
                if let Some(path) = &self.log_file_path {
                    self.rotate_log_file_daily(path, today);
                    *last_rotation = today;
                }
            }
        }
        
        let thread_id = format!("{:?}", std::thread::current().id());
        
        let entry = LogEntry {
            id: Uuid::new_v4().to_string(),
            timestamp: Utc::now(),
            level,
            message: message.to_string(),
            module: module.to_string(),
            thread_id,
            session_id: self.session_id.clone(),
            context: context.unwrap_or_default(),
            stacktrace,
        };
        
        // Log to console if enabled
        if self.config.read().unwrap().console_enabled {
            self.log_to_console(&entry);
        }
        
        // Log to file if configured
        if self.config.read().unwrap().file_enabled {
            if let Some(file) = &self.log_file {
                self.log_to_file(&entry, file);
            }
        }
        
        // Add to in-memory buffer
        {
            let mut buffer = self.in_memory_buffer.write().unwrap();
            buffer.add(entry.clone());
        }
        
        // Send to telemetry if enabled
        if self.config.read().unwrap().telemetry_enabled {
            if let Some(client) = &self.telemetry_client {
                if entry.level >= LogLevel::Error {
                    let _ = client.send_log(&entry);
                }
            }
        }
        
        // Send to remote logging service if enabled
        if self.config.read().unwrap().log_to_remote {
            if let Some(url) = &self.config.read().unwrap().remote_url {
                self.log_to_remote(&entry, url);
            }
        }
    }
    
    fn log_to_console(&self, entry: &LogEntry) {
        let config = self.config.read().unwrap();
        
        let formatted = match config.console_format {
            LogFormat::Simple => {
                format!("[{}] {}", entry.level.as_str(), entry.message)
            },
            LogFormat::Standard => {
                format!(
                    "[{}] [{}] [{}] {}",
                    entry.timestamp.format("%Y-%m-%d %H:%M:%S"),
                    entry.level.as_str(),
                    entry.module,
                    entry.message
                )
            },
            LogFormat::Detailed => {
                let context_str = if config.include_context && !entry.context.is_empty() {
                    let ctx_pairs: Vec<String> = entry.context.iter()
                        .map(|(k, v)| format!("{}={}", k, v))
                        .collect();
                    format!(" {{{}}}", ctx_pairs.join(", "))
                } else {
                    String::new()
                };
                
                format!(
                    "[{}] [{}] [{}] [{}] {}{}",
                    entry.timestamp.format("%Y-%m-%d %H:%M:%S%.3f"),
                    entry.level.as_str(),
                    entry.module,
                    entry.thread_id,
                    entry.message,
                    context_str
                )
            },
            LogFormat::Json => {
                serde_json::to_string(entry).unwrap_or_else(|_| format!("[ERROR] Failed to serialize log entry: {:?}", entry))
            },
        };
        
        let output = if config.colored_output {
            entry.level.color_string(&formatted).to_string()
        } else {
            formatted
        };
        
        match entry.level {
            LogLevel::Error | LogLevel::Fatal => eprintln!("{}", output),
            _ => println!("{}", output),
        }
    }
    
    fn log_to_file(&self, entry: &LogEntry, file: &Arc<RwLock<File>>) {
        // Format log entry as JSON for file
        if let Ok(serialized) = serde_json::to_string(entry) {
            if let Ok(mut file) = file.write() {
                let _ = writeln!(file, "{}", serialized);
                let _ = file.flush();
                
                // Check if rotation is needed
                if let Some(path) = &self.log_file_path {
                    // Get file size
                    if let Ok(metadata) = file.metadata() {
                        if metadata.len() > self.config.read().unwrap().max_file_size {
                            // Release the lock before rotation
                            drop(file);
                            
                            // Rotation needed
                            self.rotate_log_file_size(path);
                        }
                    }
                }
            }
        }
    }
    
    fn log_to_remote(&self, entry: &LogEntry, url: &str) {
        // In a real implementation, this would send logs to a remote logging service
        // This is a placeholder for demonstration purposes
        #[cfg(feature = "remote_logging")]
        {
            // Create a client
            let client = reqwest::blocking::Client::new();
            
            // Send the log to the remote service
            let _ = client.post(url)
                .json(entry)
                .send();
        }
    }
    
    fn rotate_log_file_size(&self, path: &Path) {
        let max_files = self.config.read().unwrap().max_log_files;
        
        // Implement log rotation by size
        for i in (1..max_files).rev() {
            let src = path.with_extension(format!("log.{}", i));
            let dst = path.with_extension(format!("log.{}", i + 1));
            if src.exists() {
                let _ = std::fs::rename(src, dst);
            }
        }
        
        let src = path;
        let dst = path.with_extension("log.1");
        let _ = std::fs::rename(src, dst);
        
        // Create a new log file
        if let Some(parent) = path.parent() {
            let _ = create_dir_all(parent);
        }
        
        if let Ok(file) = OpenOptions::new()
            .create(true)
            .append(true)
            .open(path) {
            
            if let Some(log_file) = &self.log_file {
                let mut guard = log_file.write().unwrap();
                *guard = file;
            }
        }
    }
    
    fn rotate_log_file_daily(&self, path: &Path, today: NaiveDate) {
        // Create new file name with date
        let parent = path.parent().unwrap_or_else(|| Path::new("."));
        let stem = path.file_stem().unwrap_or_default().to_string_lossy();
        let ext = path.extension().unwrap_or_default().to_string_lossy();
        
        // Archive the old log file with date
        let archive_name = format!("{}_{}.{}", 
                                  stem,
                                  self.last_rotation_date.read().unwrap().format("%Y-%m-%d"),
                                  ext);
        let archive_path = parent.join(archive_name);
        
        // Rename the current log file to the archive name
        if path.exists() {
            let _ = std::fs::rename(path, archive_path);
        }
        
        // Create a new log file
        if let Ok(file) = OpenOptions::new()
            .create(true)
            .append(true)
            .open(path) {
            
            if let Some(log_file) = &self.log_file {
                let mut guard = log_file.write().unwrap();
                *guard = file;
            }
        }
        
        // Clean up old logs
        self.clean_old_logs(parent);
    }
    
    fn clean_old_logs(&self, dir: &Path) {
        let max_files = self.config.read().unwrap().max_log_files as usize;
        
        // Get all log files in directory
        if let Ok(entries) = fs::read_dir(dir) {
            let mut log_files: Vec<(PathBuf, NaiveDate)> = entries
                .filter_map(|entry| {
                    let entry = entry.ok()?;
                    let path = entry.path();
                    if path.extension()?.to_string_lossy() == "log" {
                        let filename = path.file_name()?.to_string_lossy();
                        if let Some(date_str) = filename.split('_').nth(1) {
                            if let Ok(date) = NaiveDate::parse_from_str(date_str, "%Y-%m-%d") {
                                return Some((path, date));
                            }
                        }
                    }
                    None
                })
                .collect();
            
            // Sort by date (newest first)
            log_files.sort_by(|a, b| b.1.cmp(&a.1));
            
            // Remove oldest files if we have too many
            if log_files.len() > max_files {
                for (path, _) in log_files.iter().skip(max_files) {
                    let _ = fs::remove_file(path);
                }
            }
        }
    }
    
    pub fn trace(&self, module: &str, message: &str, context: Option<HashMap<String, String>>) {
        self.log(LogLevel::Trace, module, message, context, None);
    }
    
    pub fn debug(&self, module: &str, message: &str, context: Option<HashMap<String, String>>) {
        self.log(LogLevel::Debug, module, message, context, None);
    }
    
    pub fn info(&self, module: &str, message: &str, context: Option<HashMap<String, String>>) {
        self.log(LogLevel::Info, module, message, context, None);
    }
    
    pub fn warn(&self, module: &str, message: &str, context: Option<HashMap<String, String>>) {
        self.log(LogLevel::Warn, module, message, context, None);
    }
    
    pub fn error(&self, module: &str, message: &str, context: Option<HashMap<String, String>>) {
        // Capture stack trace for errors
        let stacktrace = backtrace::Backtrace::capture().to_string();
        self.log(LogLevel::Error, module, message, context, Some(stacktrace));
    }
    
    pub fn fatal(&self, module: &str, message: &str, context: Option<HashMap<String, String>>) {
        // Capture stack trace for fatal errors
        let stacktrace = backtrace::Backtrace::capture().to_string();
        self.log(LogLevel::Fatal, module, message, context, Some(stacktrace));
    }
    
    // Get recent logs for the UI
    pub fn get_recent_logs(&self, count: usize) -> Vec<LogEntry> {
        let buffer = self.in_memory_buffer.read().unwrap();
        buffer.get_recent(count)
    }
    
    // Search logs with filters
    pub fn search_logs(&self, level: Option<LogLevel>, module: Option<&str>, contains: Option<&str>) -> Vec<LogEntry> {
        let buffer = self.in_memory_buffer.read().unwrap();
        buffer.filter(level, module, contains)
    }
    
    // Export logs to a file
    pub fn export_logs(&self, path: &str, format: ExportFormat, filters: Option<LogFilters>) -> Result<()> {
        let buffer = self.in_memory_buffer.read().unwrap();
        
        // Apply filters if provided
        let logs = if let Some(filters) = filters {
            buffer.filter(filters.min_level, filters.module.as_deref(), filters.contains.as_deref())
        } else {
            buffer.get_all()
        };
        
        // Format logs based on export format
        let content = match format {
            ExportFormat::Json => {
                serde_json::to_string_pretty(&logs)?
            },
            ExportFormat::Csv => {
                let mut csv_content = String::from("timestamp,level,module,message\n");
                for entry in &logs {
                    csv_content.push_str(&format!(
                        "{},{},{},{}\n",
                        entry.timestamp.to_rfc3339(),
                        entry.level.as_str(),
                        entry.module,
                        entry.message.replace(',', "\\,") // Escape commas in message
                    ));
                }
                csv_content
            },
            ExportFormat::Text => {
                let mut text_content = String::new();
                for entry in &logs {
                    text_content.push_str(&format!(
                        "[{}] [{}] [{}] {}\n",
                        entry.timestamp.format("%Y-%m-%d %H:%M:%S"),
                        entry.level.as_str(),
                        entry.module,
                        entry.message
                    ));
                }
                text_content
            },
            ExportFormat::Html => {
                let mut html_content = String::from(
                    "<!DOCTYPE html>\n<html>\n<head>\n<style>\n\
                     table { border-collapse: collapse; width: 100%; }\n\
                     th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }\n\
                     tr:nth-child(even) { background-color: #f2f2f2; }\n\
                     th { background-color: #4CAF50; color: white; }\n\
                     .TRACE { color: #888; }\n\
                     .DEBUG { color: #00f; }\n\
                     .INFO { color: #080; }\n\
                     .WARN { color: #880; }\n\
                     .ERROR { color: #f00; }\n\
                     .FATAL { color: #f00; font-weight: bold; }\n\
                     </style>\n</head>\n<body>\n<h1>Log Export</h1>\n\
                     <table>\n<tr><th>Timestamp</th><th>Level</th><th>Module</th><th>Message</th></tr>\n"
                );
                
                for entry in &logs {
                    html_content.push_str(&format!(
                        "<tr><td>{}</td><td class=\"{}\">{}</td><td>{}</td><td>{}</td></tr>\n",
                        entry.timestamp.format("%Y-%m-%d %H:%M:%S"),
                        entry.level.as_str(),
                        entry.level.as_str(),
                        entry.module,
                        entry.message.replace('<', "&lt;").replace('>', "&gt;") // Escape HTML tags
                    ));
                }
                
                html_content.push_str("</table>\n</body>\n</html>");
                html_content
            },
        };
        
        // Write to file
        fs::write(path, content)?;
        
        Ok(())
    }
    
    // Update logger configuration
    pub fn update_config(&self, new_config: LoggerConfig) -> Result<()> {
        let old_config = self.config.read().unwrap().clone();
        
        // Check if file path changed or file logging was enabled
        let file_path_changed = match (old_config.file_path.as_ref(), new_config.file_path.as_ref()) {
            (Some(old), Some(new)) => old != new,
            (None, Some(_)) => true,
            (Some(_), None) => true,
            (None, None) => false,
        };
        
        let file_enabled_changed = old_config.file_enabled != new_config.file_enabled;
        
        // Update config
        *self.config.write().unwrap() = new_config.clone();
        
        // Reinitialize file logging if needed
        if (file_path_changed || file_enabled_changed) && new_config.file_enabled {
            if let Some(log_path) = &new_config.file_path {
                // Create the directory if it doesn't exist
                if let Some(parent) = Path::new(log_path).parent() {
                    let _ = create_dir_all(parent);
                }
                
                let file = OpenOptions::new()
                    .create(true)
                    .append(true)
                    .open(log_path)?;
                    
                self.log_file = Some(Arc::new(RwLock::new(file)));
                self.log_file_path = Some(PathBuf::from(log_path));
            }
        } else if !new_config.file_enabled {
            self.log_file = None;
            self.log_file_path = None;
        }
        
        Ok(())
    }
}

// Export format options
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum ExportFormat {
    Json,
    Csv,
    Text,
    Html,
}

// Log search filters
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LogFilters {
    pub min_level: Option<LogLevel>,
    pub module: Option<String>,
    pub contains: Option<String>,
    pub start_time: Option<DateTime<Utc>>,
    pub end_time: Option<DateTime<Utc>>,
}

// Global logger instance
lazy_static::lazy_static! {
    pub static ref LOGGER: Arc<RwLock<Option<Logger>>> = Arc::new(RwLock::new(None));
}

// Initialize logger from config
pub fn init_logger(config: &ObservabilityConfig, telemetry_client: Option<Arc<TelemetryClient>>) {
    let logger_config = LoggerConfig {
        min_level: config.min_log_level.map(LogLevel::from_u8).unwrap_or(LogLevel::Info),
        file_enabled: true,
        file_path: config.log_file_path.clone(),
        console_enabled: config.console_logging.unwrap_or(true),
        console_format: LogFormat::Detailed,
        colored_output: true,
        max_file_size: 10 * 1024 * 1024, // 10 MB
        max_log_files: 5,
        telemetry_enabled: config.telemetry_enabled.unwrap_or(false) && config.log_telemetry.unwrap_or(false),
        include_context: true,
        rotate_daily: true,
        log_to_remote: false,
        remote_url: None,
    };
    
    let logger = Logger::new(logger_config, telemetry_client);
    
    // Initialize global logger
    *LOGGER.write().unwrap() = Some(logger);
    
    // Log initialization
    log_info!("logger", "Logger initialized");
}

// Logger macros for easy use
#[macro_export]
macro_rules! log_trace {
    ($module:expr, $message:expr $(, $context:expr)?) => {
        if let Some(logger) = $crate::observability::logging::LOGGER.read().unwrap().as_ref() {
            logger.trace($module, $message, $($context)?);
        }
    };
}

#[macro_export]
macro_rules! log_debug {
    ($module:expr, $message:expr $(, $context:expr)?) => {
        if let Some(logger) = $crate::observability::logging::LOGGER.read().unwrap().as_ref() {
            logger.debug($module, $message, $($context)?);
        }
    };
}

#[macro_export]
macro_rules! log_info {
    ($module:expr, $message:expr $(, $context:expr)?) => {
        if let Some(logger) = $crate::observability::logging::LOGGER.read().unwrap().as_ref() {
            logger.info($module, $message, $($context)?);
        }
    };
}

#[macro_export]
macro_rules! log_warn {
    ($module:expr, $message:expr $(, $context:expr)?) => {
        if let Some(logger) = $crate::observability::logging::LOGGER.read().unwrap().as_ref() {
            logger.warn($module, $message, $($context)?);
        }
    };
}

#[macro_export]
macro_rules! log_error {
    ($module:expr, $message:expr $(, $context:expr)?) => {
        if let Some(logger) = $crate::observability::logging::LOGGER.read().unwrap().as_ref() {
            logger.error($module, $message, $($context)?);
        }
    };
}

#[macro_export]
macro_rules! log_fatal {
    ($module:expr, $message:expr $(, $context:expr)?) => {
        if let Some(logger) = $crate::observability::logging::LOGGER.read().unwrap().as_ref() {
            logger.fatal($module, $message, $($context)?);
        }
    };
}

// Context builder for structured logging
#[macro_export]
macro_rules! log_context {
    ($($key:expr => $value:expr),*) => {
        {
            let mut context = std::collections::HashMap::new();
            $(
                context.insert($key.to_string(), $value.to_string());
            )*
            Some(context)
        }
    };
}

// Tauri commands for the UI
#[cfg(feature = "tauri")]
#[tauri::command]
pub fn get_recent_logs(count: usize) -> Vec<LogEntry> {
    if let Some(logger) = LOGGER.read().unwrap().as_ref() {
        logger.get_recent_logs(count)
    } else {
        Vec::new()
    }
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn search_logs(level: Option<String>, module: Option<String>, contains: Option<String>) -> Vec<LogEntry> {
    if let Some(logger) = LOGGER.read().unwrap().as_ref() {
        logger.search_logs(
            level.map(|l| LogLevel::from_str(&l)),
            module.as_deref(),
            contains.as_deref(),
        )
    } else {
        Vec::new()
    }
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn export_logs(path: String, format: String, filters: Option<LogFilters>) -> Result<()> {
    if let Some(logger) = LOGGER.read().unwrap().as_ref() {
        let export_format = match format.to_lowercase().as_str() {
            "json" => ExportFormat::Json,
            "csv" => ExportFormat::Csv,
            "text" => ExportFormat::Text,
            "html" => ExportFormat::Html,
            _ => ExportFormat::Json,
        };
        
        logger.export_logs(&path, export_format, filters)
    } else {
        Err(std::io::Error::new(std::io::ErrorKind::Other, "Logger not initialized").into())
    }
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn update_logger_config(config: LoggerConfig) -> Result<()> {
    if let Some(logger) = LOGGER.read().unwrap().as_ref() {
        logger.update_config(config)
    } else {
        Err(std::io::Error::new(std::io::ErrorKind::Other, "Logger not initialized").into())
    }
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn get_logger_config() -> Option<LoggerConfig> {
    if let Some(logger) = LOGGER.read().unwrap().as_ref() {
        Some(logger.config.read().unwrap().clone())
    } else {
        None
    }
}
</file>

<file path="src-common/src/observability/metrics.rs">
use std::collections::HashMap;
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, Instant, SystemTime};
use serde::{Serialize, Deserialize};
use rand::Rng;
use log::{debug, warn, error};

use crate::observability::telemetry::TelemetryClient;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ObservabilityConfig {
    pub metrics_enabled: bool,
    pub sampling_rate: f64,
    pub buffer_size: usize,
    pub flush_interval_secs: u64,
    pub min_log_level: Option<u8>,
    pub log_file_path: Option<String>,
    pub console_logging: Option<bool>,
    pub telemetry_enabled: Option<bool>,
    pub log_telemetry: Option<bool>,
}

impl Default for ObservabilityConfig {
    fn default() -> Self {
        Self {
            metrics_enabled: true,
            sampling_rate: 0.1, // Sample 10% of metrics by default
            buffer_size: 1000,
            flush_interval_secs: 60,
            min_log_level: Some(2), // Info level
            log_file_path: None,
            console_logging: Some(true),
            telemetry_enabled: Some(false), // Opt-in by default
            log_telemetry: Some(false),
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum MetricType {
    Counter,
    Gauge,
    Histogram,
    Timer,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Metric {
    pub name: String,
    pub value: f64,
    pub metric_type: MetricType,
    pub tags: HashMap<String, String>,
    pub timestamp: SystemTime,
}

// Store historical metrics for dashboard visualization
#[derive(Debug, Clone, Default)]
pub struct MetricsHistory {
    // Store raw metrics in time-series buckets
    pub counters: HashMap<String, Vec<(SystemTime, f64)>>,
    pub gauges: HashMap<String, Vec<(SystemTime, f64)>>,
    pub histograms: HashMap<String, Vec<(SystemTime, f64)>>,
    pub timers: HashMap<String, Vec<(SystemTime, f64)>>,
    
    // Store aggregated statistics
    pub counter_totals: HashMap<String, f64>,
    pub gauge_latest: HashMap<String, f64>,
    pub histogram_stats: HashMap<String, HistogramStats>,
    pub timer_stats: HashMap<String, TimerStats>,
    
    // Limit history size
    max_history_points: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HistogramStats {
    pub min: f64,
    pub max: f64,
    pub avg: f64,
    pub count: usize,
    pub p50: f64, // Median
    pub p90: f64, // 90th percentile
    pub p99: f64, // 99th percentile
}

impl Default for HistogramStats {
    fn default() -> Self {
        Self {
            min: f64::MAX,
            max: f64::MIN,
            avg: 0.0,
            count: 0,
            p50: 0.0,
            p90: 0.0,
            p99: 0.0,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TimerStats {
    pub min_ms: f64,
    pub max_ms: f64,
    pub avg_ms: f64,
    pub count: usize,
    pub p50_ms: f64, // Median
    pub p90_ms: f64, // 90th percentile
    pub p99_ms: f64, // 99th percentile
}

impl Default for TimerStats {
    fn default() -> Self {
        Self {
            min_ms: f64::MAX,
            max_ms: f64::MIN,
            avg_ms: 0.0,
            count: 0,
            p50_ms: 0.0,
            p90_ms: 0.0,
            p99_ms: 0.0,
        }
    }
}

impl MetricsHistory {
    pub fn new(max_history_points: usize) -> Self {
        Self {
            counters: HashMap::new(),
            gauges: HashMap::new(),
            histograms: HashMap::new(),
            timers: HashMap::new(),
            counter_totals: HashMap::new(),
            gauge_latest: HashMap::new(),
            histogram_stats: HashMap::new(),
            timer_stats: HashMap::new(),
            max_history_points,
        }
    }
    
    pub fn add_metric(&mut self, metric: &Metric) {
        match metric.metric_type {
            MetricType::Counter => self.add_counter(metric),
            MetricType::Gauge => self.add_gauge(metric),
            MetricType::Histogram => self.add_histogram(metric),
            MetricType::Timer => self.add_timer(metric),
        }
    }
    
    fn add_counter(&mut self, metric: &Metric) {
        let entry = self.counters.entry(metric.name.clone()).or_insert_with(Vec::new);
        entry.push((metric.timestamp, metric.value));
        
        // Limit history size
        if entry.len() > self.max_history_points {
            entry.remove(0);
        }
        
        // Update totals
        let total = self.counter_totals.entry(metric.name.clone()).or_insert(0.0);
        *total += metric.value;
    }
    
    fn add_gauge(&mut self, metric: &Metric) {
        let entry = self.gauges.entry(metric.name.clone()).or_insert_with(Vec::new);
        entry.push((metric.timestamp, metric.value));
        
        // Limit history size
        if entry.len() > self.max_history_points {
            entry.remove(0);
        }
        
        // Update latest value
        self.gauge_latest.insert(metric.name.clone(), metric.value);
    }
    
    fn add_histogram(&mut self, metric: &Metric) {
        let entry = self.histograms.entry(metric.name.clone()).or_insert_with(Vec::new);
        entry.push((metric.timestamp, metric.value));
        
        // Limit history size
        if entry.len() > self.max_history_points {
            entry.remove(0);
        }
        
        // Update statistics
        let stats = self.histogram_stats.entry(metric.name.clone()).or_insert_with(HistogramStats::default);
        self.update_histogram_stats(stats, metric.value);
    }
    
    fn add_timer(&mut self, metric: &Metric) {
        let entry = self.timers.entry(metric.name.clone()).or_insert_with(Vec::new);
        entry.push((metric.timestamp, metric.value));
        
        // Limit history size
        if entry.len() > self.max_history_points {
            entry.remove(0);
        }
        
        // Update statistics
        let stats = self.timer_stats.entry(metric.name.clone()).or_insert_with(TimerStats::default);
        self.update_timer_stats(stats, metric.value);
    }
    
    fn update_histogram_stats(&mut self, stats: &mut HistogramStats, value: f64) {
        stats.min = stats.min.min(value);
        stats.max = stats.max.max(value);
        
        // Update average
        let new_count = stats.count + 1;
        stats.avg = (stats.avg * stats.count as f64 + value) / new_count as f64;
        stats.count = new_count;
        
        // For true percentiles, we'd need to sort all values
        // This is a simplification
        stats.p50 = stats.avg;
        stats.p90 = stats.max * 0.9;
        stats.p99 = stats.max * 0.99;
    }
    
    fn update_timer_stats(&mut self, stats: &mut TimerStats, value: f64) {
        stats.min_ms = stats.min_ms.min(value);
        stats.max_ms = stats.max_ms.max(value);
        
        // Update average
        let new_count = stats.count + 1;
        stats.avg_ms = (stats.avg_ms * stats.count as f64 + value) / new_count as f64;
        stats.count = new_count;
        
        // For true percentiles, we'd need to sort all values
        // This is a simplification
        stats.p50_ms = stats.avg_ms;
        stats.p90_ms = stats.max_ms * 0.9;
        stats.p99_ms = stats.max_ms * 0.99;
    }
    
    pub fn get_counters_report(&self) -> HashMap<String, f64> {
        self.counter_totals.clone()
    }
    
    pub fn get_gauges_report(&self) -> HashMap<String, f64> {
        self.gauge_latest.clone()
    }
    
    pub fn get_histograms_report(&self) -> HashMap<String, HistogramStats> {
        self.histogram_stats.clone()
    }
    
    pub fn get_timers_report(&self) -> HashMap<String, TimerStats> {
        self.timer_stats.clone()
    }
    
    // Get time series data for a specific metric for dashboard charts
    pub fn get_time_series(&self, name: &str, metric_type: MetricType) -> Vec<(SystemTime, f64)> {
        match metric_type {
            MetricType::Counter => self.counters.get(name).cloned().unwrap_or_default(),
            MetricType::Gauge => self.gauges.get(name).cloned().unwrap_or_default(),
            MetricType::Histogram => self.histograms.get(name).cloned().unwrap_or_default(),
            MetricType::Timer => self.timers.get(name).cloned().unwrap_or_default(),
        }
    }
}

pub struct MetricsCollector {
    // Configuration
    enabled: bool,
    sampling_rate: f64,
    buffer_size: usize,
    flush_interval: Duration,
    
    // Metric storage
    metrics_buffer: Arc<Mutex<Vec<Metric>>>,
    history: Arc<RwLock<MetricsHistory>>,
    
    // Last flush time
    last_flush: Instant,
    
    // Telemetry client for sending metrics
    telemetry_client: Option<Arc<TelemetryClient>>,
}

impl MetricsCollector {
    pub fn new(config: &ObservabilityConfig, telemetry_client: Option<Arc<TelemetryClient>>) -> Self {
        Self {
            enabled: config.metrics_enabled,
            sampling_rate: config.sampling_rate,
            buffer_size: config.buffer_size,
            flush_interval: Duration::from_secs(config.flush_interval_secs),
            metrics_buffer: Arc::new(Mutex::new(Vec::with_capacity(config.buffer_size))),
            history: Arc::new(RwLock::new(MetricsHistory::new(1000))), // Keep last 1000 data points
            last_flush: Instant::now(),
            telemetry_client,
        }
    }
    
    pub fn record_counter(&self, name: &str, value: f64, tags: HashMap<String, String>) {
        if !self.enabled || !self.should_sample() {
            return;
        }
        
        self.record_metric(name, value, MetricType::Counter, tags);
    }
    
    pub fn increment_counter(&self, name: &str, tags: HashMap<String, String>) {
        self.record_counter(name, 1.0, tags);
    }
    
    pub fn record_gauge(&self, name: &str, value: f64, tags: HashMap<String, String>) {
        if !self.enabled {
            return;
        }
        
        self.record_metric(name, value, MetricType::Gauge, tags);
    }
    
    pub fn record_histogram(&self, name: &str, value: f64, tags: HashMap<String, String>) {
        if !self.enabled || !self.should_sample() {
            return;
        }
        
        self.record_metric(name, value, MetricType::Histogram, tags);
    }
    
    pub fn time<F, R>(&self, name: &str, tags: HashMap<String, String>, f: F) -> R
    where
        F: FnOnce() -> R,
    {
        if !self.enabled {
            return f();
        }
        
        let start = Instant::now();
        let result = f();
        let duration = start.elapsed();
        
        self.record_metric(
            name, 
            duration.as_secs_f64() * 1000.0, // Convert to milliseconds
            MetricType::Timer,
            tags,
        );
        
        result
    }
    
    fn record_metric(&self, name: &str, value: f64, metric_type: MetricType, tags: HashMap<String, String>) {
        let metric = Metric {
            name: name.to_string(),
            value,
            metric_type,
            tags,
            timestamp: SystemTime::now(),
        };
        
        // Update metrics history
        {
            let mut history = self.history.write().unwrap();
            history.add_metric(&metric);
        }
        
        // Add to buffer for sending to backend
        let mut metrics_buffer = self.metrics_buffer.lock().unwrap();
        metrics_buffer.push(metric);
        
        if metrics_buffer.len() >= self.buffer_size || self.last_flush.elapsed() > self.flush_interval {
            self.flush_metrics(&mut metrics_buffer);
        }
    }
    
    fn should_sample(&self) -> bool {
        rand::thread_rng().gen_range(0.0..1.0) <= self.sampling_rate
    }
    
    fn flush_metrics(&self, metrics_buffer: &mut Vec<Metric>) {
        if metrics_buffer.is_empty() {
            return;
        }
        
        let metrics_count = metrics_buffer.len();
        debug!("Flushing {} metrics", metrics_count);
        
        // Send metrics to telemetry service if enabled
        if let Some(client) = &self.telemetry_client {
            if client.is_telemetry_enabled() {
                match client.send_metrics(metrics_buffer) {
                    Ok(_) => debug!("Successfully sent {} metrics to telemetry", metrics_count),
                    Err(e) => warn!("Failed to send metrics to telemetry: {}", e),
                }
            }
        }
        
        // Clear buffer
        metrics_buffer.clear();
    }
    
    // Get a snapshot of current metrics for the dashboard
    pub fn get_metrics_snapshot(&self) -> Vec<Metric> {
        let metrics_buffer = self.metrics_buffer.lock().unwrap();
        metrics_buffer.clone()
    }
    
    // Get metrics history for dashboard visualizations
    pub fn get_history(&self) -> Arc<RwLock<MetricsHistory>> {
        self.history.clone()
    }
    
    // Get timers report
    pub fn get_timers_report(&self) -> HashMap<String, TimerStats> {
        let history = self.history.read().unwrap();
        history.get_timers_report()
    }
    
    // Get counters report
    pub fn get_counters_report(&self) -> HashMap<String, f64> {
        let history = self.history.read().unwrap();
        history.get_counters_report()
    }
    
    // Get gauges report
    pub fn get_gauges_report(&self) -> HashMap<String, f64> {
        let history = self.history.read().unwrap();
        history.get_gauges_report()
    }
    
    // Get histograms report
    pub fn get_histograms_report(&self) -> HashMap<String, HistogramStats> {
        let history = self.history.read().unwrap();
        history.get_histograms_report()
    }
}

// Create a global metrics collector
lazy_static::lazy_static! {
    pub static ref METRICS_COLLECTOR: Arc<RwLock<Option<MetricsCollector>>> = Arc::new(RwLock::new(None));
}

// Initialize metrics collector
pub fn init_metrics(config: &ObservabilityConfig, telemetry_client: Option<Arc<TelemetryClient>>) {
    let collector = MetricsCollector::new(config, telemetry_client);
    let mut global_collector = METRICS_COLLECTOR.write().unwrap();
    *global_collector = Some(collector);
    debug!("Metrics collector initialized with sampling rate: {}", config.sampling_rate);
}

// Helper functions to record metrics
pub fn record_counter(name: &str, value: f64, tags: Option<HashMap<String, String>>) {
    if let Some(collector) = METRICS_COLLECTOR.read().unwrap().as_ref() {
        collector.record_counter(name, value, tags.unwrap_or_default());
    }
}

pub fn increment_counter(name: &str, tags: Option<HashMap<String, String>>) {
    if let Some(collector) = METRICS_COLLECTOR.read().unwrap().as_ref() {
        collector.increment_counter(name, tags.unwrap_or_default());
    }
}

pub fn record_gauge(name: &str, value: f64, tags: Option<HashMap<String, String>>) {
    if let Some(collector) = METRICS_COLLECTOR.read().unwrap().as_ref() {
        collector.record_gauge(name, value, tags.unwrap_or_default());
    }
}

pub fn record_histogram(name: &str, value: f64, tags: Option<HashMap<String, String>>) {
    if let Some(collector) = METRICS_COLLECTOR.read().unwrap().as_ref() {
        collector.record_histogram(name, value, tags.unwrap_or_default());
    }
}

pub fn time_operation<F, R>(name: &str, tags: Option<HashMap<String, String>>, f: F) -> R
where
    F: FnOnce() -> R,
{
    if let Some(collector) = METRICS_COLLECTOR.read().unwrap().as_ref() {
        collector.time(name, tags.unwrap_or_default(), f)
    } else {
        f()
    }
}

// Get metrics reports for dashboard
pub fn get_timers_report() -> Option<HashMap<String, TimerStats>> {
    METRICS_COLLECTOR.read().unwrap().as_ref().map(|collector| collector.get_timers_report())
}

pub fn get_counters_report() -> Option<HashMap<String, f64>> {
    METRICS_COLLECTOR.read().unwrap().as_ref().map(|collector| collector.get_counters_report())
}

pub fn get_gauges_report() -> Option<HashMap<String, f64>> {
    METRICS_COLLECTOR.read().unwrap().as_ref().map(|collector| collector.get_gauges_report())
}

pub fn get_histograms_report() -> Option<HashMap<String, HistogramStats>> {
    METRICS_COLLECTOR.read().unwrap().as_ref().map(|collector| collector.get_histograms_report())
}

// Macro for timing blocks of code
#[macro_export]
macro_rules! time_operation {
    ($name:expr, $tags:expr, $block:expr) => {
        {
            $crate::observability::metrics::time_operation($name, $tags, || $block)
        }
    };
}
</file>

<file path="src-common/src/observability/telemetry.rs">
use std::sync::{Arc, Mutex, RwLock, Once};
use chrono::{DateTime, Utc};
use log::{debug, info, warn, error};
use reqwest::Client as HttpClient;
use serde::{Serialize, Deserialize};
use std::collections::HashMap;
use std::time::{Duration, Instant};
use std::thread;
use uuid::Uuid;
use std::io::ErrorKind;

use crate::observability::logging::LogEntry;
use crate::observability::metrics::Metric;
use crate::error::Result;

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum TelemetryEventType {
    ApplicationStart,
    ApplicationExit,
    FeatureUsage,
    Error,
    Performance,
    UserAction,
    SystemInfo,
    ModelUsage,
    Network,
}

impl TelemetryEventType {
    pub fn as_str(&self) -> &'static str {
        match self {
            TelemetryEventType::ApplicationStart => "app_start",
            TelemetryEventType::ApplicationExit => "app_exit",
            TelemetryEventType::FeatureUsage => "feature_usage",
            TelemetryEventType::Error => "error",
            TelemetryEventType::Performance => "performance",
            TelemetryEventType::UserAction => "user_action",
            TelemetryEventType::SystemInfo => "system_info",
            TelemetryEventType::ModelUsage => "model_usage",
            TelemetryEventType::Network => "network",
        }
    }
    
    pub fn category(&self) -> &'static str {
        match self {
            TelemetryEventType::ApplicationStart | 
            TelemetryEventType::ApplicationExit => "app_lifecycle",
            TelemetryEventType::FeatureUsage => "feature_usage",
            TelemetryEventType::Error => "errors",
            TelemetryEventType::Performance => "performance",
            TelemetryEventType::UserAction => "user_actions",
            TelemetryEventType::SystemInfo => "system_info",
            TelemetryEventType::ModelUsage => "model_usage",
            TelemetryEventType::Network => "network",
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryEvent {
    pub id: String,
    pub timestamp: DateTime<Utc>,
    pub event_type: TelemetryEventType,
    pub name: String,
    pub value: Option<String>,
    pub properties: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryBatch {
    pub batch_id: String,
    pub client_id: String,
    pub app_version: String,
    pub platform: String,
    pub events: Vec<TelemetryEvent>,
    pub metrics: Vec<Metric>,
    pub logs: Vec<LogEntry>,
    pub timestamp: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryConfig {
    pub enabled: bool,
    pub client_id: String,
    pub collection_categories: HashMap<String, bool>,
    pub batch_size: usize,
    pub batch_interval_seconds: u64,
    pub server_url: String,
    pub send_usage_statistics: bool,
    pub privacy_policy_version: String,
    pub privacy_policy_accepted: bool,
}

impl Default for TelemetryConfig {
    fn default() -> Self {
        Self {
            enabled: false, // Disabled by default, requires explicit opt-in
            client_id: Uuid::new_v4().to_string(),
            collection_categories: [
                ("app_lifecycle".to_string(), true),
                ("feature_usage".to_string(), false),
                ("errors".to_string(), true),
                ("performance".to_string(), false),
                ("user_actions".to_string(), false),
                ("system_info".to_string(), false),
                ("logs".to_string(), false),
                ("model_usage".to_string(), false),
                ("network".to_string(), false),
            ].iter().cloned().collect(),
            batch_size: 100,
            batch_interval_seconds: 60,
            server_url: "https://telemetry.mcp-client.example.com/v1/telemetry".to_string(),
            send_usage_statistics: false,
            privacy_policy_version: "1.0.0".to_string(),
            privacy_policy_accepted: false,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum PrivacyLevel {
    Minimal,    // Only critical errors and basic usage
    Standard,   // Standard telemetry (default)
    Enhanced,   // Additional usage patterns and performance metrics
    Full,       // Complete telemetry including detailed usage
}

impl PrivacyLevel {
    pub fn to_category_map(&self) -> HashMap<String, bool> {
        match self {
            PrivacyLevel::Minimal => [
                ("app_lifecycle".to_string(), true),
                ("feature_usage".to_string(), false),
                ("errors".to_string(), true),
                ("performance".to_string(), false),
                ("user_actions".to_string(), false),
                ("system_info".to_string(), false),
                ("logs".to_string(), false),
                ("model_usage".to_string(), false),
                ("network".to_string(), false),
            ].iter().cloned().collect(),
            
            PrivacyLevel::Standard => [
                ("app_lifecycle".to_string(), true),
                ("feature_usage".to_string(), true),
                ("errors".to_string(), true),
                ("performance".to_string(), true),
                ("user_actions".to_string(), false),
                ("system_info".to_string(), true),
                ("logs".to_string(), false),
                ("model_usage".to_string(), true),
                ("network".to_string(), true),
            ].iter().cloned().collect(),
            
            PrivacyLevel::Enhanced => [
                ("app_lifecycle".to_string(), true),
                ("feature_usage".to_string(), true),
                ("errors".to_string(), true),
                ("performance".to_string(), true),
                ("user_actions".to_string(), true),
                ("system_info".to_string(), true),
                ("logs".to_string(), false),
                ("model_usage".to_string(), true),
                ("network".to_string(), true),
            ].iter().cloned().collect(),
            
            PrivacyLevel::Full => [
                ("app_lifecycle".to_string(), true),
                ("feature_usage".to_string(), true),
                ("errors".to_string(), true),
                ("performance".to_string(), true),
                ("user_actions".to_string(), true),
                ("system_info".to_string(), true),
                ("logs".to_string(), true),
                ("model_usage".to_string(), true),
                ("network".to_string(), true),
            ].iter().cloned().collect(),
        }
    }
    
    pub fn from_category_map(map: &HashMap<String, bool>) -> Self {
        if *map.get("logs").unwrap_or(&false) {
            return PrivacyLevel::Full;
        }
        
        if *map.get("user_actions").unwrap_or(&false) {
            return PrivacyLevel::Enhanced;
        }
        
        if *map.get("feature_usage").unwrap_or(&false) {
            return PrivacyLevel::Standard;
        }
        
        PrivacyLevel::Minimal
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TelemetryStats {
    pub events_collected: u64,
    pub events_sent: u64,
    pub metrics_collected: u64,
    pub metrics_sent: u64,
    pub logs_collected: u64,
    pub logs_sent: u64,
    pub batches_sent: u64,
    pub batches_failed: u64,
    pub last_batch_sent: Option<DateTime<Utc>>,
}

impl Default for TelemetryStats {
    fn default() -> Self {
        Self {
            events_collected: 0,
            events_sent: 0,
            metrics_collected: 0,
            metrics_sent: 0,
            logs_collected: 0,
            logs_sent: 0,
            batches_sent: 0,
            batches_failed: 0,
            last_batch_sent: None,
        }
    }
}

pub struct TelemetryClient {
    config: Arc<RwLock<TelemetryConfig>>,
    event_buffer: Arc<Mutex<Vec<TelemetryEvent>>>,
    metrics_buffer: Arc<Mutex<Vec<Metric>>>,
    logs_buffer: Arc<Mutex<Vec<LogEntry>>>,
    is_running: Arc<RwLock<bool>>,
    last_flush: Arc<Mutex<Instant>>,
    http_client: HttpClient,
    stats: Arc<RwLock<TelemetryStats>>,
}

impl TelemetryClient {
    fn new(config: TelemetryConfig) -> Self {
        let config = Arc::new(RwLock::new(config));
        let event_buffer = Arc::new(Mutex::new(Vec::new()));
        let metrics_buffer = Arc::new(Mutex::new(Vec::new()));
        let logs_buffer = Arc::new(Mutex::new(Vec::new()));
        let is_running = Arc::new(RwLock::new(false));
        let last_flush = Arc::new(Mutex::new(Instant::now()));
        let http_client = HttpClient::builder()
            .timeout(Duration::from_secs(5))
            .build()
            .unwrap_or_else(|_| HttpClient::new());
        let stats = Arc::new(RwLock::new(TelemetryStats::default()));
        
        Self {
            config,
            event_buffer,
            metrics_buffer,
            logs_buffer,
            is_running,
            last_flush,
            http_client,
            stats,
        }
    }
    
    pub fn start_telemetry_worker(&self) {
        let config = Arc::clone(&self.config);
        let event_buffer = Arc::clone(&self.event_buffer);
        let metrics_buffer = Arc::clone(&self.metrics_buffer);
        let logs_buffer = Arc::clone(&self.logs_buffer);
        let is_running = Arc::clone(&self.is_running);
        let last_flush = Arc::clone(&self.last_flush);
        let http_client = self.http_client.clone();
        let stats = Arc::clone(&self.stats);
        
        // Set running state
        *is_running.write().unwrap() = true;
        
        thread::spawn(move || {
            debug!("Telemetry worker thread started");
            while *is_running.read().unwrap() {
                // Get batch interval from config
                let interval_seconds = {
                    let config = config.read().unwrap();
                    config.batch_interval_seconds
                };
                
                // Sleep for the batch interval
                thread::sleep(Duration::from_secs(interval_seconds));
                
                // Check if telemetry is enabled
                let is_enabled = {
                    let config = config.read().unwrap();
                    config.enabled && config.privacy_policy_accepted
                };
                
                if !is_enabled {
                    continue;
                }
                
                // Check if it's time to flush
                let should_flush = {
                    let last = last_flush.lock().unwrap();
                    last.elapsed() > Duration::from_secs(interval_seconds)
                };
                
                if !should_flush {
                    continue;
                }
                
                // Prepare batch for sending
                let batch = {
                    let config = config.read().unwrap();
                    let mut events = event_buffer.lock().unwrap();
                    let mut metrics = metrics_buffer.lock().unwrap();
                    let mut logs = logs_buffer.lock().unwrap();
                    
                    // Skip if all buffers are empty
                    if events.is_empty() && metrics.is_empty() && logs.is_empty() {
                        continue;
                    }
                    
                    // Update stats
                    {
                        let mut stats_guard = stats.write().unwrap();
                        stats_guard.events_collected += events.len() as u64;
                        stats_guard.metrics_collected += metrics.len() as u64;
                        stats_guard.logs_collected += logs.len() as u64;
                    }
                    
                    // Create batch
                    let batch = TelemetryBatch {
                        batch_id: Uuid::new_v4().to_string(),
                        client_id: config.client_id.clone(),
                        app_version: env!("CARGO_PKG_VERSION").to_string(),
                        platform: std::env::consts::OS.to_string(),
                        events: std::mem::take(events.as_mut()),
                        metrics: std::mem::take(metrics.as_mut()),
                        logs: std::mem::take(logs.as_mut()),
                        timestamp: Utc::now(),
                    };
                    
                    batch
                };
                
                // Send batch to server
                match Self::send_batch(&http_client, &batch, &config.read().unwrap().server_url) {
                    Ok(_) => {
                        // Successfully sent batch
                        let mut last = last_flush.lock().unwrap();
                        *last = Instant::now();
                        
                        // Update stats
                        {
                            let mut stats_guard = stats.write().unwrap();
                            stats_guard.events_sent += batch.events.len() as u64;
                            stats_guard.metrics_sent += batch.metrics.len() as u64;
                            stats_guard.logs_sent += batch.logs.len() as u64;
                            stats_guard.batches_sent += 1;
                            stats_guard.last_batch_sent = Some(Utc::now());
                        }
                        
                        debug!("Successfully sent telemetry batch: events={}, metrics={}, logs={}",
                               batch.events.len(), batch.metrics.len(), batch.logs.len());
                    }
                    Err(error) => {
                        // Failed to send batch, log error
                        warn!("Failed to send telemetry batch: {}", error);
                        
                        // Update stats
                        {
                            let mut stats_guard = stats.write().unwrap();
                            stats_guard.batches_failed += 1;
                        }
                        
                        // Return items to buffers
                        let mut events = event_buffer.lock().unwrap();
                        let mut metrics = metrics_buffer.lock().unwrap();
                        let mut logs = logs_buffer.lock().unwrap();
                        
                        events.extend(batch.events);
                        metrics.extend(batch.metrics);
                        logs.extend(batch.logs);
                    }
                }
            }
            
            debug!("Telemetry worker thread stopped");
        });
    }
    
    pub fn stop_telemetry_worker(&self) {
        let mut is_running = self.is_running.write().unwrap();
        *is_running = false;
    }
    
    async fn send_batch_async(http_client: &HttpClient, batch: &TelemetryBatch, server_url: &str) -> Result<()> {
        // Serialize batch
        let serialized = serde_json::to_string(batch)
            .map_err(|e| std::io::Error::new(ErrorKind::InvalidData, format!("Failed to serialize batch: {}", e)))?;
        
        // In a real implementation, this would send the data to a telemetry server
        // For this example, we'll just simulate sending
        debug!("Sending telemetry batch: {} bytes to {}", serialized.len(), server_url);
        
        // Simulate network request in dev/test mode
        if cfg!(feature = "dev") || server_url.contains("example.com") {
            // Simulate network delay
            tokio::time::sleep(Duration::from_millis(100)).await;
            return Ok(());
        }
        
        // Real HTTP request in production
        let response = http_client.post(server_url)
            .header("Content-Type", "application/json")
            .body(serialized)
            .send()
            .await
            .map_err(|e| std::io::Error::new(ErrorKind::Other, format!("Failed to send batch: {}", e)))?;
        
        if !response.status().is_success() {
            return Err(std::io::Error::new(
                ErrorKind::Other,
                format!("Server returned error: {}", response.status()),
            ).into());
        }
        
        Ok(())
    }
    
    fn send_batch(http_client: &HttpClient, batch: &TelemetryBatch, server_url: &str) -> Result<()> {
        // Create a runtime for async request
        let rt = tokio::runtime::Builder::new_current_thread()
            .enable_all()
            .build()?;
        
        rt.block_on(Self::send_batch_async(http_client, batch, server_url))
    }
    
    pub fn track_event(&self, event_type: TelemetryEventType, name: &str, value: Option<String>, properties: Option<HashMap<String, String>>) {
        // Check if telemetry is enabled
        let is_enabled = {
            let config = self.config.read().unwrap();
            config.enabled && config.privacy_policy_accepted
        };
        
        if !is_enabled {
            return;
        }
        
        // Check if this category is enabled
        let category_enabled = {
            let config = self.config.read().unwrap();
            *config.collection_categories.get(event_type.category()).unwrap_or(&false)
        };
        
        if !category_enabled {
            return;
        }
        
        // Create event
        let event = TelemetryEvent {
            id: Uuid::new_v4().to_string(),
            timestamp: Utc::now(),
            event_type: event_type.clone(),
            name: name.to_string(),
            value,
            properties: properties.unwrap_or_default(),
        };
        
        // Add to buffer
        let mut buffer = self.event_buffer.lock().unwrap();
        buffer.push(event);
        
        // Check if buffer is full
        let batch_size = {
            let config = self.config.read().unwrap();
            config.batch_size
        };
        
        if buffer.len() >= batch_size {
            // Force a flush
            let mut last = self.last_flush.lock().unwrap();
            *last = Instant::now() - Duration::from_secs(3600); // Set to 1 hour ago to force flush
        }
    }
    
    pub fn track_feature_usage(&self, feature_name: &str, properties: Option<HashMap<String, String>>) {
        self.track_event(
            TelemetryEventType::FeatureUsage,
            feature_name,
            None,
            properties,
        );
    }
    
    pub fn track_error(&self, error_name: &str, error_message: &str, properties: Option<HashMap<String, String>>) {
        let mut props = properties.unwrap_or_default();
        props.insert("message".to_string(), error_message.to_string());
        
        self.track_event(
            TelemetryEventType::Error,
            error_name,
            None,
            Some(props),
        );
    }
    
    pub fn track_performance(&self, operation_name: &str, duration_ms: f64, properties: Option<HashMap<String, String>>) {
        self.track_event(
            TelemetryEventType::Performance,
            operation_name,
            Some(duration_ms.to_string()),
            properties,
        );
    }
    
    pub fn track_model_usage(&self, model_name: &str, token_count: u32, properties: Option<HashMap<String, String>>) {
        let mut props = properties.unwrap_or_default();
        props.insert("token_count".to_string(), token_count.to_string());
        
        self.track_event(
            TelemetryEventType::ModelUsage,
            model_name,
            None,
            Some(props),
        );
    }
    
    pub fn send_metrics(&self, metrics: &[Metric]) -> Result<()> {
        // Check if telemetry is enabled
        let is_enabled = {
            let config = self.config.read().unwrap();
            config.enabled && 
            config.privacy_policy_accepted && 
            *config.collection_categories.get("performance").unwrap_or(&false)
        };
        
        if !is_enabled {
            return Ok(());
        }
        
        // Add to buffer
        let mut buffer = self.metrics_buffer.lock().unwrap();
        buffer.extend_from_slice(metrics);
        
        // Check if buffer is full
        let batch_size = {
            let config = self.config.read().unwrap();
            config.batch_size
        };
        
        if buffer.len() >= batch_size {
            // Force a flush
            let mut last = self.last_flush.lock().unwrap();
            *last = Instant::now() - Duration::from_secs(3600); // Set to 1 hour ago to force flush
        }
        
        Ok(())
    }
    
    pub fn send_log(&self, log: &LogEntry) -> Result<()> {
        // Check if telemetry is enabled
        let is_enabled = {
            let config = self.config.read().unwrap();
            config.enabled && 
            config.privacy_policy_accepted && 
            *config.collection_categories.get("logs").unwrap_or(&false)
        };
        
        if !is_enabled {
            return Ok(());
        }
        
        // Add to buffer
        let mut buffer = self.logs_buffer.lock().unwrap();
        buffer.push(log.clone());
        
        // Check if buffer is full
        let batch_size = {
            let config = self.config.read().unwrap();
            config.batch_size
        };
        
        if buffer.len() >= batch_size {
            // Force a flush
            let mut last = self.last_flush.lock().unwrap();
            *last = Instant::now() - Duration::from_secs(3600); // Set to 1 hour ago to force flush
        }
        
        Ok(())
    }
    
    pub fn update_config(&self, new_config: TelemetryConfig) {
        let mut config = self.config.write().unwrap();
        *config = new_config;
    }
    
    pub fn get_config(&self) -> TelemetryConfig {
        let config = self.config.read().unwrap();
        config.clone()
    }
    
    pub fn set_privacy_level(&self, level: PrivacyLevel) {
        let mut config = self.config.write().unwrap();
        config.collection_categories = level.to_category_map();
    }
    
    pub fn get_privacy_level(&self) -> PrivacyLevel {
        let config = self.config.read().unwrap();
        PrivacyLevel::from_category_map(&config.collection_categories)
    }
    
    pub fn accept_privacy_policy(&self, version: &str) {
        let mut config = self.config.write().unwrap();
        config.privacy_policy_version = version.to_string();
        config.privacy_policy_accepted = true;
    }
    
    pub fn is_telemetry_enabled(&self) -> bool {
        let config = self.config.read().unwrap();
        config.enabled && config.privacy_policy_accepted
    }
    
    pub fn delete_telemetry_data(&self) -> Result<()> {
        // Clear local buffers
        {
            let mut events = self.event_buffer.lock().unwrap();
            events.clear();
        }
        
        {
            let mut metrics = self.metrics_buffer.lock().unwrap();
            metrics.clear();
        }
        
        {
            let mut logs = self.logs_buffer.lock().unwrap();
            logs.clear();
        }
        
        // Generate a new client ID
        {
            let mut config = self.config.write().unwrap();
            config.client_id = Uuid::new_v4().to_string();
        }
        
        // Reset stats
        {
            let mut stats = self.stats.write().unwrap();
            *stats = TelemetryStats::default();
        }
        
        // In a real implementation, this would also send a deletion request to the server
        info!("Telemetry data deletion requested - client ID has been reset");
        
        // Send deletion request to server
        if let Ok(server_url) = self.get_deletion_endpoint() {
            let client_id = {
                let config = self.config.read().unwrap();
                config.client_id.clone()
            };
            
            let rt = tokio::runtime::Builder::new_current_thread()
                .enable_all()
                .build()?;
            
            rt.block_on(async {
                let _ = self.http_client.delete(&server_url)
                    .header("Content-Type", "application/json")
                    .json(&serde_json::json!({ "client_id": client_id }))
                    .send()
                    .await;
            });
        }
        
        Ok(())
    }
    
    fn get_deletion_endpoint(&self) -> Result<String> {
        let config = self.config.read().unwrap();
        let base_url = &config.server_url;
        
        // Append "/delete" to the base URL
        let url = if base_url.ends_with('/') {
            format!("{}delete", base_url)
        } else {
            format!("{}/delete", base_url)
        };
        
        Ok(url)
    }
    
    pub fn get_stats(&self) -> TelemetryStats {
        let stats = self.stats.read().unwrap();
        stats.clone()
    }
    
    // Singleton instance getter
    pub fn get_instance() -> Arc<Self> {
        static mut INSTANCE: Option<Arc<TelemetryClient>> = None;
        static ONCE: Once = Once::new();
        
        unsafe {
            ONCE.call_once(|| {
                // Create default config
                let config = TelemetryConfig::default();
                
                // Create instance
                let client = TelemetryClient::new(config);
                client.start_telemetry_worker();
                
                INSTANCE = Some(Arc::new(client));
            });
            
            INSTANCE.clone().unwrap()
        }
    }
}

impl Drop for TelemetryClient {
    fn drop(&mut self) {
        // Track application exit event
        self.track_event(
            TelemetryEventType::ApplicationExit,
            "application_exit",
            None,
            None,
        );
        
        // Stop worker thread
        self.stop_telemetry_worker();
    }
}

// Initialize telemetry
pub fn init_telemetry() -> Arc<TelemetryClient> {
    let client = TelemetryClient::get_instance();
    
    // Track application start event
    client.track_event(
        TelemetryEventType::ApplicationStart,
        "application_start",
        None,
        None,
    );
    
    // Track system info
    let mut system_info = HashMap::new();
    system_info.insert("os".to_string(), std::env::consts::OS.to_string());
    system_info.insert("arch".to_string(), std::env::consts::ARCH.to_string());
    system_info.insert("version".to_string(), env!("CARGO_PKG_VERSION").to_string());
    
    client.track_event(
        TelemetryEventType::SystemInfo,
        "system_info",
        None,
        Some(system_info),
    );
    
    client
}

// Tauri commands
#[cfg(feature = "tauri")]
#[tauri::command]
pub fn get_telemetry_config() -> TelemetryConfig {
    let client = TelemetryClient::get_instance();
    client.get_config()
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn update_telemetry_config(config: TelemetryConfig) -> Result<()> {
    let client = TelemetryClient::get_instance();
    client.update_config(config);
    Ok(())
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn set_privacy_level(level: String) -> Result<()> {
    let client = TelemetryClient::get_instance();
    let privacy_level = match level.as_str() {
        "minimal" => PrivacyLevel::Minimal,
        "standard" => PrivacyLevel::Standard,
        "enhanced" => PrivacyLevel::Enhanced,
        "full" => PrivacyLevel::Full,
        _ => return Err(std::io::Error::new(ErrorKind::InvalidInput, "Invalid privacy level").into()),
    };
    client.set_privacy_level(privacy_level);
    Ok(())
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn get_privacy_level() -> String {
    let client = TelemetryClient::get_instance();
    match client.get_privacy_level() {
        PrivacyLevel::Minimal => "minimal".to_string(),
        PrivacyLevel::Standard => "standard".to_string(),
        PrivacyLevel::Enhanced => "enhanced".to_string(),
        PrivacyLevel::Full => "full".to_string(),
    }
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn delete_telemetry_data() -> Result<()> {
    let client = TelemetryClient::get_instance();
    client.delete_telemetry_data()
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn get_telemetry_stats() -> TelemetryStats {
    let client = TelemetryClient::get_instance();
    client.get_stats()
}

#[cfg(feature = "tauri")]
#[tauri::command]
pub fn accept_privacy_policy(version: String) -> Result<()> {
    let client = TelemetryClient::get_instance();
    client.accept_privacy_policy(&version);
    Ok(())
}

// Helper function to track feature usage
pub fn track_feature_usage(feature_name: &str, properties: Option<HashMap<String, String>>) {
    let client = TelemetryClient::get_instance();
    client.track_feature_usage(feature_name, properties);
}

// Helper function to track errors
pub fn track_error(error_name: &str, error_message: &str, properties: Option<HashMap<String, String>>) {
    let client = TelemetryClient::get_instance();
    client.track_error(error_name, error_message, properties);
}

// Helper function to track performance
pub fn track_performance(operation_name: &str, duration_ms: f64, properties: Option<HashMap<String, String>>) {
    let client = TelemetryClient::get_instance();
    client.track_performance(operation_name, duration_ms, properties);
}

// Helper function to track model usage
pub fn track_model_usage(model_name: &str, token_count: u32, properties: Option<HashMap<String, String>>) {
    let client = TelemetryClient::get_instance();
    client.track_model_usage(model_name, token_count, properties);
}

// Macro for timing and tracking performance
#[macro_export]
macro_rules! track_performance {
    ($name:expr, $block:expr) => {
        {
            let start = std::time::Instant::now();
            let result = $block;
            let duration = start.elapsed();
            $crate::observability::telemetry::track_performance(
                $name, 
                duration.as_secs_f64() * 1000.0,
                None,
            );
            result
        }
    };
    ($name:expr, $properties:expr, $block:expr) => {
        {
            let start = std::time::Instant::now();
            let result = $block;
            let duration = start.elapsed();
            $crate::observability::telemetry::track_performance(
                $name, 
                duration.as_secs_f64() * 1000.0,
                $properties,
            );
            result
        }
    };
}
</file>

<file path="src-frontend/index.html">
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/icons/icon.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Papin - an MCP Client</title>
    <!-- Preload critical styles -->
    <link rel="preload" href="/src/index.css" as="style" />
    <link rel="preload" href="/src/components/Shell.css" as="style" />
    <!-- Font preloading -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      rel="preload"
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"
      as="style"
    />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"
    />
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>
</file>

<file path="src-frontend/src/App.css">
/* Reset and base styles */
*, *::before, *::after {
  box-sizing: border-box;
  margin: 0;
  padding: 0;
}

body {
  font-family: var(--font-family);
  font-size: var(--font-size-md);
  line-height: 1.5;
  color: var(--color-on-background);
  background-color: var(--color-background);
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#root {
  height: 100vh;
  display: flex;
  flex-direction: column;
}

/* Common utility classes */
.fade-in {
  animation: fadeIn 0.3s ease;
}

.visually-hidden {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  white-space: nowrap;
  border-width: 0;
}

/* Animations */
@keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}

@keyframes slideUp {
  from {
    transform: translateY(20px);
    opacity: 0;
  }
  to {
    transform: translateY(0);
    opacity: 1;
  }
}

@keyframes slideDown {
  from {
    transform: translateY(-20px);
    opacity: 0;
  }
  to {
    transform: translateY(0);
    opacity: 1;
  }
}

/* Scrollbar styling */
::-webkit-scrollbar {
  width: 8px;
  height: 8px;
}

::-webkit-scrollbar-track {
  background: var(--color-surface);
}

::-webkit-scrollbar-thumb {
  background-color: var(--color-on-surface-variant);
  opacity: 0.5;
  border-radius: 20px;
}

::-webkit-scrollbar-thumb:hover {
  background-color: var(--color-on-surface);
}
</file>

<file path="src-frontend/src/App.tsx">
import React, { useState, useEffect } from 'react';
import Shell, { LoadState } from './components/Shell';
import AppShell from './components/AppShell';
import { ThemeProvider } from './theme/ThemeContext';
import { AnimationProvider } from './animation';
import { KeyboardProvider } from './keyboard';
import { AccessibilityProvider } from './accessibility';
import { TourProvider } from './tours';
import { HelpProvider } from './help';
import { ProgressiveDisclosureProvider } from './disclosure';
import { A11yButton } from './accessibility';
import './App.css';

// Import base styles and animation utilities
import './theme/variables.css';

// Main application component
function App() {
  const [loadState, setLoadState] = useState<LoadState>(LoadState.ShellLoading);
  
  useEffect(() => {
    // Simulate load state progression from the backend
    // In a real app, this would come from backend events
    const progressLoadState = async () => {
      // Wait for shell to be ready
      await sleep(100);
      setLoadState(LoadState.ShellReady);
      
      // Core services loading
      await sleep(200);
      setLoadState(LoadState.CoreServicesLoading);
      
      // Core services ready
      await sleep(150);
      setLoadState(LoadState.CoreServicesReady);
      
      // Secondary loading
      await sleep(200);
      setLoadState(LoadState.SecondaryLoading);
      
      // Everything loaded
      await sleep(100);
      setLoadState(LoadState.FullyLoaded);
    };
    
    progressLoadState();
  }, []);
  
  return (
    <ThemeProvider>
      <AnimationProvider>
        <KeyboardProvider>
          <AccessibilityProvider>
            <TourProvider>
              <HelpProvider>
                <ProgressiveDisclosureProvider>
                  {loadState !== LoadState.FullyLoaded ? (
                    <Shell />
                  ) : (
                    <>
                      <AppShell loadState={loadState} />
                      <A11yButton />
                    </>
                  )}
                </ProgressiveDisclosureProvider>
              </HelpProvider>
            </TourProvider>
          </AccessibilityProvider>
        </KeyboardProvider>
      </AnimationProvider>
    </ThemeProvider>
  );
}

// Helper for simulating loading delays
const sleep = (ms: number) => new Promise(resolve => setTimeout(resolve, ms));

export default App;
</file>

<file path="src-frontend/src/components/help/helpContent.ts">
export const helpTopics = [
  {
    id: 'getting-started',
    title: 'Getting Started with Papin',
    id: 'getting-started',
    title: 'Getting Started with Papin',
    category: 'Basics',
    summary: 'Learn the basics of Papin, from installation to your first conversation.',
    intro: {
      beginner: "Welcome to Papin! This guide will help you get started with the basics. Papin is a desktop application that helps you communicate with AI models, even when you're offline.",
      intermediate: "This guide covers the initial setup and configuration of Papin, including customization options and account management.",
      advanced: "This technical guide details Papin's architecture, setup procedures, and configuration options for optimal performance."
    },
    content: [
      {
        title: 'System Requirements',
        beginner: "Papin works on most modern computers. You'll need Windows 10 or newer, macOS 10.15 or newer, or a recent version of Linux. Your computer should have at least 4GB of RAM (8GB recommended) and 500MB of free disk space for the application itself, plus extra space for offline models if you choose to use them.",
        intermediate: "Papin is compatible with Windows 10/11, macOS 10.15+, and major Linux distributions. Minimum requirements include 4GB RAM, 500MB storage for the application, and additional space for local models and conversation history. For optimal performance with local models, 16GB RAM and a multi-core CPU are recommended.",
        advanced: "Papin's system requirements vary based on usage patterns and enabled features. Base installation requires 500MB of storage and 4GB RAM minimum. Local model usage increases requirements substantially: small models require 2GB additional RAM, medium models 4GB, and large models 8-16GB. Multi-threading is leveraged for inference acceleration on systems with >4 cores. GPU acceleration is available for CUDA-compatible NVIDIA GPUs (minimum 4GB VRAM) and Apple Silicon."
      },
      {
        title: 'Installation Process',
        beginner: [
          {
            title: 'Windows Installation',
            content: "To install Papin on Windows, download the installer from the official website, run the .msi file, and follow the on-screen instructions. Once installed, you can find Papin in your Start menu.",
            steps: [
              "Download the installer from the official website",
              "Run the .msi installer file",
              "Follow the on-screen instructions",
              "Launch Papin from the Start menu or desktop shortcut"
            ]
          },
          {
            title: 'macOS Installation',
            content: "For Mac users, download the .dmg file from the official website, open it, and drag Papin to your Applications folder. Then you can launch it from your Applications folder.",
            steps: [
              "Download the .dmg file from the official website",
              "Open the file and drag Papin to your Applications folder",
              "Launch Papin from your Applications folder",
              "If prompted about security, go to System Preferences > Security & Privacy to allow the app"
            ]
          },
          {
            title: 'Linux Installation',
            content: "For Linux, download the appropriate package for your distribution (.deb, .rpm, or .AppImage) and install it using your package manager or by making the AppImage executable.",
            steps: [
              "Download the appropriate package for your distribution",
              "For .deb packages: sudo dpkg -i papin_1.0.0.deb",
              "For .rpm packages: sudo rpm -i papin_1.0.0.rpm",
              "For .AppImage: Make the file executable with 'chmod +x Papin-1.0.0.AppImage' and run it"
            ]
          }
        ],
        intermediate: [
          {
            title: 'Installation Options',
            content: "Papin offers several installation options across platforms, including portable installations and silent deployment options for enterprise environments.",
            note: "Enterprise users can use the --silent flag for automated deployment across multiple workstations."
          },
          {
            title: 'Cross-Platform Considerations',
            content: "When installing on multiple platforms, be aware that user data synchronization requires the same account across devices. Platform-specific features may vary slightly, particularly around system integration points.",
            tip: "For consistent cross-platform experiences, use the cloud synchronization feature to keep your settings and conversations in sync."
          },
          {
            title: 'Verification and Updates',
            content: "After installation, verify application integrity using the built-in diagnostic tool. Papin supports both automatic and manual updates, configurable in the settings panel.",
            code: "# Check application integrity\npapin --verify\n\n# Check for updates manually\npapin --check-update"
          }
        ],
        advanced: [
          {
            title: 'Command-Line Installation',
            content: "Advanced users can leverage command-line installation and configuration options.",
            code: "# Windows (PowerShell)\n$env:PAPIN_INSTALL_DIR = \"C:\\CustomPath\"\n$env:PAPIN_CONFIG_PRESET = \"developer\"\n.\\papin-installer.exe /S\n\n# macOS/Linux\nPAPIN_INSTALL_DIR=\"/opt/papin\" PAPIN_CONFIG_PRESET=\"developer\" ./papin-installer.sh"
          },
          {
            title: 'Enterprise Deployment',
            content: "For enterprise deployment, Papin supports system-wide installation with customizable policy configurations.",
            code: "# Deploy with custom policy file\npapin-installer --system-wide --policy-file=/path/to/policy.json"
          },
          {
            title: 'Installation Verification',
            content: "Cryptographic verification of installation artifacts ensures integrity.",
            code: "# Verify package signature\ngpg --verify papin-1.0.0.AppImage.sig papin-1.0.0.AppImage\n\n# Verify checksum\nsha256sum -c papin-1.0.0.AppImage.sha256"
          }
        ]
      },
      {
        title: 'Initial Configuration',
        beginner: "When you first open Papin, you'll need to create an account or sign in with an existing one. Follow the welcome wizard to set up your preferences, like which AI models you want to use and whether you want to enable offline mode. Don't worry, you can always change these settings later!",
        intermediate: "The initial configuration process includes account creation, preference setting, and optional local model downloads. Consider your typical usage pattern when selecting which offline capabilities to enable, as larger models require significant disk space. Authentication supports standard email/password, SSO, and OAuth integration with popular identity providers.",
        advanced: "Initial configuration can be automated via CLI flags or configuration files. Enterprise deployments can leverage LDAP/SAML integration and policy-based configuration management. Advanced users should consider configuring custom model endpoints, network proxy settings, and fine-tuned resource allocation based on hardware specifications."
      },
      {
        title: 'Your First Conversation',
        beginner: "Now that you're set up, let's start your first conversation! Click the 'New Conversation' button, choose which AI model you'd like to talk to, and type your message in the box at the bottom. Press Enter or click the Send button, and the AI will respond in the conversation window. It's that simple!",
        intermediate: "Initiating a conversation involves selecting an appropriate model based on your needs. Consider the context and complexity of your query when selecting a model. For technical queries, the specialized models may yield better results than general-purpose ones. Initial conversations are automatically titled based on content, but you can rename them for better organization.",
        advanced: "Conversations are backed by persistent storage with automatic checkpointing. Each message exchange generates a complete context window evaluation with adjustable parameters including temperature, top_p, and frequency penalty. Advanced users can modify these parameters via the API or configuration files to optimize for specific use cases."
      }
    ],
    faq: [
      {
        question: "Do I need an internet connection to use Papin?",
        beginner: "While Papin works best with an internet connection, you can still use it offline if you've set up offline mode. Some advanced features might not be available without internet, but you can still have conversations with AI models you've downloaded.",
        intermediate: "Papin functions in both online and offline modes. For offline use, you'll need to download models in advance and enable offline mode in settings. Syncing and cloud features require connectivity, but all core conversation functionality works offline with downloaded models.",
        advanced: "Papin implements a connection-aware state machine that automatically transitions between online and offline operation modes. Online operation enables cloud model access, synchronization, and telemetry collection. Offline operation leverages local model inference, persistent storage, and queues changes for future synchronization. Network requirements can be fine-tuned using the bandwidth management settings."
      },
      {
        question: "How do I update Papin?",
        beginner: "Papin checks for updates automatically when you're connected to the internet. When an update is available, you'll see a notification. Just click 'Update Now' to get the latest version!",
        intermediate: "Updates are managed through the built-in updater system accessible via Help > Check for Updates. You can configure update behavior in Settings > Application > Updates, including automatic download, silent installation, and update channel selection (stable, beta, etc.).",
        advanced: "Papin leverages a differential update system to minimize bandwidth usage. Updates are cryptographically signed and verified before installation. Enterprise environments can control updates through policy files and can stage updates for controlled rollout using the canary system. Custom update endpoints are supported for air-gapped networks."
      }
    ],
    relatedTopics: ['offline-capabilities', 'ui-navigation', 'account-management']
  },
  {
    id: 'conversations',
    title: 'Managing Conversations',
    category: 'Core Features',
    summary: 'Learn how to create, organize, and get the most out of your AI conversations.',
    intro: {
      beginner: "Conversations are at the heart of Papin. This guide will show you how to create, manage, and organize your conversations with AI models.",
      intermediate: "This comprehensive guide covers conversation management techniques, organization strategies, and effective prompt engineering.",
      advanced: "This technical overview explains the conversation architecture, storage mechanisms, and advanced conversation manipulation capabilities."
    },
    content: [
      {
        title: 'Creating New Conversations',
        beginner: "To start a new conversation, click the 'New Conversation' button in the top-left corner of the app or use the keyboard shortcut Ctrl+N (Cmd+N on Mac). You'll be able to choose which AI model you want to talk to before starting the conversation.",
        intermediate: "When creating new conversations, consider the specific model capabilities needed for your task. Specialized models offer better performance for domain-specific tasks, while general models excel at versatility. Conversation templates provide standardized starting points for common workflows.",
        advanced: "Conversation initialization triggers a series of template resolution steps, model capability verification, and resource allocation processes. Advanced users can script conversation creation using the API with pre-specified parameters, system prompts, and initial message sequences."
      },
      {
        title: 'Conversation History and Navigation',
        beginner: "All your conversations are saved automatically and appear in the sidebar on the left side of the app. Click any conversation to continue where you left off. You can use the search bar at the top of the sidebar to find specific conversations by typing keywords.",
        intermediate: "Conversation history leverages efficient storage mechanisms with full-text search capabilities. The contextual search supports filtering by date ranges, models used, and specific content types like code or data requests. Organization features include folders, tags, and starring important conversations.",
        advanced: "The conversation history implements a hybrid storage architecture utilizing SQLite for metadata and structured information with file-system based storage for conversation content. This enables efficient indexing while maintaining readability of raw data. The search system uses an inverted index with BM25 ranking for relevance-based retrieval."
      },
      {
        title: 'Organizing with Folders and Tags',
        beginner: "Keep your conversations organized by using folders and tags. To create a folder, right-click in the sidebar and select 'New Folder'. You can drag conversations into folders. To add tags, right-click on a conversation and select 'Add Tags'.",
        intermediate: [
          {
            title: 'Creating an Organization System',
            content: "Design an organization system that matches your workflow. Project-based folders work well for focused work, while tags can cross-cut across projects for thematic organization."
          },
          {
            title: 'Smart Folders',
            content: "Create smart folders that automatically collect conversations matching specific criteria. Configure rules based on content, models, dates, or tags."
          },
          {
            title: 'Bulk Organization',
            content: "Use multi-select (Ctrl+click or Shift+click) to organize multiple conversations at once. Bulk operations include moving to folders, adding tags, and exporting."
          }
        ],
        advanced: [
          {
            title: 'Custom Organization Schemes',
            content: "Create custom organization schemes using the advanced filtering API. Schemes can combine multiple metadata attributes with content-based filters.",
            code: "const filter = {\n  metadata: {\n    models: ['gpt4', 'claude-3'],\n    after: '2023-06-01',\n    tags: ['research', 'machine-learning']\n  },\n  content: {\n    contains: ['neural networks', 'transformer architecture'],\n    excludes: ['personal', 'confidential']\n  }\n};"
          },
          {
            title: 'Programmatic Organization',
            content: "Leverage the API for programmatic organization of conversations based on content analysis.",
            code: "// Auto-categorize conversations based on content analysis\nconst conversations = await api.getConversations();\nfor (const conv of conversations) {\n  const category = await analyzer.categorize(conv.content);\n  await api.addTag(conv.id, category);\n}"
          }
        ]
      },
      {
        title: 'Conversation Settings and Preferences',
        beginner: "Each conversation has its own settings you can adjust. Click the gear icon in the top-right corner of a conversation to change the AI model, adjust how the AI responds, or enable features like code highlighting.",
        intermediate: "Conversation settings include model selection, parameter tuning, and feature toggles. Model parameters control response characteristics like creativity, precision, and length. Context settings manage how much conversation history is included in each request. Accessibility settings include text-to-speech and alternative text rendering options.",
        advanced: "Each conversation maintains a complete set of configurable parameters that influence model behavior, rendering, and storage. These can be accessed and modified programmatically through the API. Parameter modifications during a conversation create change points that are tracked in the conversation metadata, enabling analysis of parameter impact on model outputs."
      },
      {
        title: 'Effective Prompt Engineering',
        beginner: "To get the best responses, try to be clear and specific in your messages. If you want the AI to respond in a certain way, don't be afraid to ask directly. For example, 'Please explain this like I'm 10 years old' or 'Can you format your response as a bullet-point list?'",
        intermediate: [
          {
            title: 'Structuring Effective Prompts',
            content: "Structure prompts with clear instructions, relevant context, and specific output formats. Break complex tasks into smaller steps for better results. Prime the model with examples of the desired output format."
          },
          {
            title: 'Role-Based Prompting',
            content: "Assign specific roles to the AI to obtain specialized perspectives. For example, 'Respond as a cybersecurity expert' or 'Answer from the perspective of a historian specializing in ancient Rome.'"
          },
          {
            title: 'Chain-of-Thought Prompting',
            content: "For complex reasoning tasks, encourage the model to work through the problem step-by-step by explicitly asking it to show its reasoning process."
          }
        ],
        advanced: [
          {
            title: 'Model-Specific Optimization',
            content: "Different model architectures respond optimally to different prompting techniques. Instruction-tuned models benefit from clear directives, while knowledge-heavy models may need careful context priming.",
            code: "// Example system prompt structure for optimal performance\nconst systemPrompt = {\n  role: \"system\",\n  content: `You are an expert in ${domain} with the following characteristics:\n  - ${characteristic1}\n  - ${characteristic2}\n  - ${characteristic3}\n\nRespond in the following format:\n${formatExample}`\n};"
          },
          {
            title: 'Retrieval-Augmented Generation',
            content: "Implement RAG techniques by augmenting prompts with relevant external knowledge, either from local document stores or integrated knowledge bases.",
            code: "// RAG implementation pseudo-code\nasync function enhancedPrompt(userQuery) {\n  const relevantDocs = await vectorStore.similaritySearch(userQuery);\n  return `Answer based on the following information:\\n\\n${relevantDocs.join('\\n\\n')}\\n\\nUser question: ${userQuery}`;\n}"
          },
          {
            title: 'Prompt Optimization',
            content: "Systematically optimize prompts through iterative refinement and testing against benchmark tasks.",
            code: "// Prompt testing framework\nconst promptVariants = [\n  { template: 'Explain ${concept} simply', name: 'Simple' },\n  { template: 'Explain ${concept} step by step', name: 'Steps' },\n  { template: 'Explain ${concept} with analogies', name: 'Analogy' }\n];\n\nconst results = await testPrompts(promptVariants, testCases);"
          }
        ]
      }
    ],
    examples: [
      {
        title: "Creating a Project-Based Folder Structure",
        description: "This example shows how to organize conversations for a research project:",
        steps: [
          "Right-click in the sidebar and select 'New Folder', name it 'Research Project'",
          "Inside that folder, create subfolders for 'Literature Review', 'Methodology', and 'Analysis'",
          "Start new conversations for specific research questions and drag them to appropriate folders",
          "Add tags like 'urgent', 'to-review', or 'completed' to track progress"
        ]
      },
      {
        title: "Effective Prompt for Complex Explanation",
        description: "Example of how to structure a prompt for a complex topic explanation:",
        code: "I need an explanation of quantum computing.\n\nPlease structure your response in the following way:\n1. Start with a simple analogy that a high school student would understand\n2. Explain the key principles and terminology\n3. Describe 2-3 practical applications\n4. Suggest resources for learning more\n\nKeep technical jargon to a minimum and explain any specialized terms."
      }
    ],
    relatedTopics: ['ui-navigation', 'offline-capabilities', 'local-llm']
  },
  {
    id: 'troubleshooting',
    title: 'Troubleshooting',
    category: 'Support',
    summary: 'Find solutions to common problems and learn how to diagnose issues with Papin.',
    intro: {
      beginner: "Having trouble with Papin? This guide will help you solve common problems and get back to using the app quickly.",
      intermediate: "This comprehensive troubleshooting guide covers diagnostic processes, common issues, and advanced resolution techniques for Papin-related problems.",
      advanced: "This technical guide details Papin's diagnostics architecture, error handling mechanisms, and advanced debugging methodologies for resolving complex issues."
    },
    content: [
      {
        title: 'Common Issues and Solutions',
        beginner: [
          {
            title: 'Application Won\'t Start',
            content: "If Papin won't start, try these steps: 1) Restart your computer, 2) Make sure your computer meets the minimum requirements, 3) Reinstall Papin with a fresh download from the official website, 4) Check if your antivirus might be blocking the app."
          },
          {
            title: 'Slow Performance',
            content: "If Papin is running slowly, try: 1) Close other applications to free up memory, 2) Use a smaller or cloud-based model instead of a large local model, 3) Check your internet connection if using cloud models, 4) Enable 'Performance Mode' in Settings."
          },
          {
            title: 'Offline Mode Not Working',
            content: "If offline mode isn't working properly: 1) Make sure you've downloaded local models, 2) Check that offline mode is enabled in Settings, 3) Verify you have enough storage space for models, 4) Try restarting the application."
          },
          {
            title: 'Sync Problems',
            content: "If your conversations aren't syncing between devices: 1) Check your internet connection, 2) Make sure you're signed in to the same account on all devices, 3) Try manually triggering a sync with Ctrl+Shift+S, 4) Check if there are sync conflicts to resolve in Settings > Sync > Conflicts."
          }
        ],
        intermediate: [
          {
            title: 'Diagnostic Process',
            content: "When troubleshooting issues, follow this systematic diagnostic process: 1) Identify the specific symptoms and when they occur, 2) Check application logs for relevant errors, 3) Verify system requirements and resource availability, 4) Test in different environments or configurations to isolate variables, 5) Apply targeted solutions based on diagnostic findings."
          },
          {
            title: 'Application Crashes',
            content: "For application crashes, examine crash logs located in your user directory under .papin/logs. Key indicators include memory access violations (suggesting memory issues), uncaught exceptions (indicating bug or edge case), or resource exhaustion (showing insufficient system resources). The log analysis tool in Help > Diagnostics > Log Analysis can help interpret these files."
          },
          {
            title: 'Model Loading Failures',
            content: "Model loading failures typically manifest as timeout errors, validation errors, or memory allocation failures. These can be resolved by verifying file integrity, ensuring sufficient resources (particularly RAM), and confirming compatibility between model versions and application versions. The model verification tool can diagnose specific model issues."
          },
          {
            title: 'Network-Related Issues',
            content: "Network problems often present as timeout errors, authentication failures, or sync conflicts. Diagnosing network issues involves checking connectivity to required endpoints, verifying API authentication, and examining network traffic patterns. The network diagnostic tool in Help > Diagnostics can perform comprehensive connectivity testing."
          }
        ],
        advanced: [
          {
            title: 'Telemetry Analysis',
            content: "Leverage built-in telemetry for advanced diagnostics and troubleshooting.",
            code: "// Retrieve detailed telemetry for specific subsystem\nconst telemetry = await api.diagnostics.getTelemetry({\n  subsystem: 'inference',\n  timeRange: { start: '-1h', end: 'now' },\n  includeMetrics: true,\n  includeLogs: true,\n  includeTraces: true,\n  samplingRate: 1.0 // Collect everything\n});\n\n// Analyze for anomalies\nconst anomalies = api.diagnostics.detectAnomalies(telemetry, {\n  algorithm: 'isolation-forest',\n  sensitivity: 0.8\n});"
          },
          {
            title: 'System State Inspection',
            content: "Inspect application state for inconsistencies and corruption using the state debugging tools.",
            code: "// Inspect and verify state integrity\nconst stateReport = await api.diagnostics.inspectState({\n  components: ['models', 'conversations', 'settings', 'sync'],\n  verifyIntegrity: true,\n  repairMode: 'report-only' // vs. 'auto-repair'\n});\n\n// Check for specific state corruption patterns\nif (stateReport.corruptedEntities.length > 0) {\n  // Handle corrupted state entities\n  const fixOptions = api.diagnostics.generateRepairPlan(stateReport);\n  console.log(fixOptions);\n}"
          },
          {
            title: 'Debugging Mode',
            content: "Enable comprehensive debugging mode for troubleshooting complex issues.",
            code: "// Enable debugging mode\napi.debug.enable({\n  level: 'trace', // vs. 'debug', 'info', etc.\n  subsystems: ['all'],\n  file: true,\n  console: true,\n  telemetry: false,\n  retention: '7d',\n  includeSymbolTables: true\n});\n\n// Inject diagnostic hooks\napi.debug.injectHooks({\n  beforeInference: (params) => {\n    console.log('Inference params:', params);\n    return params; // Can modify params here\n  },\n  afterInference: (result, metrics) => {\n    console.log('Inference metrics:', metrics);\n    return result; // Can modify result here\n  }\n});"
          }
        ]
      },
      {
        title: 'Using the Log Viewer',
        beginner: "The Log Viewer helps you (or support staff) figure out what's going wrong. To access it, go to Help > View Logs. You'll see a list of events that have happened in the app. If you're reporting a problem to support, they might ask you to share these logs to help diagnose the issue.",
        intermediate: "The Log Viewer provides a structured interface for examining application logs with filtering, searching, and export capabilities. Log entries are categorized by severity (debug, info, warning, error, critical) and subsystem (UI, models, sync, etc.). The contextual information includes timestamps, session IDs, and relevant state details. Use the export function to generate log packages for support tickets, which automatically redacts sensitive information.",
        advanced: "The logging infrastructure implements structured logging with contextual enrichment, correlation IDs, and causality tracking. Log rotation policies balance diagnostic value with storage efficiency, while real-time filtering leverages indexes for rapid query execution. Advanced filtering allows complex expressions combining predicates on multiple fields, with support for regular expressions and temporal constraints."
      },
      {
        title: 'Diagnostic Tools',
        beginner: "Papin includes several tools to help diagnose problems. Go to Help > Diagnostics to find tools for checking your network connection, testing model loading, and analyzing system compatibility. These tools can automatically detect common issues and suggest solutions.",
        intermediate: [
          {
            title: 'Network Diagnostics',
            content: "The network diagnostics tool performs comprehensive connectivity testing for all required services. It validates DNS resolution, endpoint accessibility, authentication, and throughput to identify specific network-related issues. The tool can distinguish between general connectivity problems and service-specific issues."
          },
          {
            title: 'System Compatibility Check',
            content: "The compatibility checker validates system specifications against application requirements, testing CPU capabilities, memory subsystems, storage performance, and GPU compatibility. It identifies potential hardware bottlenecks and suggests optimizations specific to your configuration."
          },
          {
            title: 'Model Verification',
            content: "The model verification tool validates local model files for integrity, ensuring they haven't been corrupted or tampered with. It verifies checksums, tests loading into memory, and performs basic inference tests to confirm functionality."
          },
          {
            title: 'Database Integrity Check',
            content: "The database checker validates the integrity of conversation storage and application settings, identifying and repairing corruption when possible. It verifies referential integrity, checks for orphaned data, and validates schema compliance."
          }
        ],
        advanced: [
          {
            title: 'Diagnostic API',
            content: "Access the diagnostic system programmatically for automated troubleshooting.",
            code: "// Comprehensive system diagnostics\nconst diagnosticSuite = api.diagnostics.createSuite({\n  modules: ['system', 'network', 'storage', 'models', 'database'],\n  depth: 'comprehensive', // vs. 'basic' or 'extended'\n  repair: false, // Don't auto-repair issues\n  timeout: 300000 // 5 minutes\n});\n\nconst results = await diagnosticSuite.run();\nconst report = diagnosticSuite.generateReport('markdown');"
          },
          {
            title: 'Performance Profiling',
            content: "Use built-in profiling tools to identify performance bottlenecks.",
            code: "// CPU profiling\nconst cpuProfile = await api.diagnostics.profileCPU({\n  duration: 30000, // 30 seconds\n  sampleInterval: 1, // 1ms\n  includeNative: true,\n  threads: 'all'\n});\n\n// Memory profiling\nconst memProfile = await api.diagnostics.profileMemory({\n  trackAllocations: true,\n  stackDepth: 20,\n  gcBefore: true\n});\n\n// Generate flame graph\nconst flamegraph = api.diagnostics.generateFlameGraph(cpuProfile);"
          },
          {
            title: 'Custom Diagnostic Modules',
            content: "Extend the diagnostic system with custom modules for specialized troubleshooting.",
            code: "// Define custom diagnostic module\napi.diagnostics.registerModule({\n  id: 'custom-pipeline-validator',\n  name: 'Custom Pipeline Validation',\n  description: 'Validates custom processing pipeline configuration',\n  run: async (options) => {\n    const results = { issues: [] };\n    // Custom validation logic...\n    return results;\n  },\n  interpretResults: (results) => {\n    // Generate human-readable interpretation\n    return { summary: '...', details: '...', severity: 'warning' };\n  }\n});"
          }
        ]
      },
      {
        title: 'Contacting Support',
        beginner: "If you can't solve a problem yourself, you can contact support for help. Go to Help > Contact Support or email support@papin.app. When reporting an issue, try to include: 1) What you were doing when the problem occurred, 2) What exactly happened, 3) Any error messages you saw, and 4) Your system information (Help > About).",
        intermediate: "When contacting support, provide comprehensive diagnostic information for faster resolution. Use the Support Package Generator (Help > Generate Support Package) to automatically collect relevant logs, system information, and diagnostic reports in a privacy-preserving format. Include reproducible steps for the issue, noting both the expected and actual behavior. For complex issues, consider scheduling a live troubleshooting session using the in-app booking tool.",
        advanced: "For advanced issues requiring engineering involvement, prepare a technical case file using the Support Debug Environment. This creates an isolated instance that captures a complete snapshot of the application state, execution traces, and environmental context. Configure the debugging telemetry level to balance diagnostic detail with privacy requirements. Enterprise customers can leverage the dedicated support channel with SLA-backed response times and escalation paths to engineering teams."
      }
    ],
    faq: [
      {
        question: "Why is Papin using so much memory?",
        beginner: "Papin uses memory for running AI models, especially local models that run on your computer. Large models need more memory to work properly. If memory usage is a concern, try using smaller models or cloud models instead of large local models.",
        intermediate: "Memory usage in Papin is primarily driven by model inference, particularly when using local models. The memory footprint includes model weights, KV caches, and temporary tensors required during inference. Memory usage scales with model size, context length, and batch size. To reduce memory consumption, consider using smaller models, quantized models, or enabling memory-efficient inference options in Settings > Performance > Advanced.",
        advanced: "Memory utilization in Papin follows a multi-tier allocation strategy optimized for inference workloads. The primary contributors to memory usage include model weights (static allocation based on model size and quantization), KV cache (dynamic allocation scaling with context length), inference tensors (temporary allocations dependent on batch size and model architecture), and application overhead. Advanced memory management techniques include weight sharing, disk offloading for inactive models, and dynamic precision adjustment based on available resources."
      },
      {
        question: "How can I make Papin run faster?",
        beginner: "To improve performance, try: 1) Close other applications to free up memory and CPU, 2) Use smaller models or cloud models instead of large local models, 3) Enable 'Performance Mode' in Settings > Performance, 4) Limit the number of open conversations, and 5) Make sure your computer meets the recommended system requirements.",
        intermediate: "Performance optimization involves multiple factors. Consider adjusting the following: 1) Model loading strategy (preload vs. on-demand), 2) Thread allocation for inference, 3) GPU acceleration settings if available, 4) Memory limits for model caching, and 5) Background task scheduling. For the most significant improvement, focus on selecting the right model size for your hardware capabilities.",
        advanced: "Comprehensive performance optimization involves a systems approach. Analyze bottlenecks using the profiling tools to identify whether the constraints are compute, memory, or I/O related. Consider implementing custom inference configurations tuned to your specific hardware, enabling specialized acceleration libraries, and leveraging quantized models with hardware-specific optimizations. For multi-model workflows, implement strategic model unloading and resource arbitration to prevent resource contention."
      }
    ],
    relatedTopics: ['performance-monitoring', 'local-llm', 'offline-capabilities']
  },
  {
    id: 'offline-capabilities',
    title: 'Offline Capabilities',
    category: 'Core Features',
    summary: 'Learn how to use Papin even when you don\'t have an internet connection.',
    intro: {
      beginner: "One of Papin's best features is that you can use it without an internet connection. This guide will show you how to set up and use offline mode.",
      intermediate: "This guide covers Papin's comprehensive offline capabilities, including local model management, synchronization, and offline workflow optimization.",
      advanced: "This technical documentation details the architecture of Papin's offline systems, including local inference engines, state management, and synchronization protocols."
    },
    content: [
      {
        title: 'Understanding Offline Mode',
        beginner: "Offline mode lets you use Papin even when you don't have internet access. It works by downloading AI models to your computer, so you can have conversations without connecting to the cloud. Your conversations are saved locally and can sync when you're back online.",
        intermediate: "Papin's offline capabilities leverage a local-first architecture that prioritizes availability and responsiveness. The system maintains functional equivalence between online and offline modes, with graceful degradation for features that require connectivity. This approach ensures consistent user experience regardless of network status.",
        advanced: "The offline architecture implements a sophisticated state machine that manages transitions between connectivity states. It employs optimistic concurrency control for operations, persistent queue management for deferred synchronization, and differential reconciliation algorithms to resolve conflicts. The system maintains a comprehensive event log to ensure data consistency across state transitions."
      },
      {
        title: 'Setting Up Offline Mode',
        beginner: [
          {
            title: 'Enabling Offline Mode',
            content: "To enable offline mode, go to Settings > Offline and toggle the 'Enable Offline Mode' switch. You'll then be asked to choose which local models you want to download."
          },
          {
            title: 'Choosing Models',
            content: "Start with smaller models if you're concerned about disk space. They download faster and use less storage, but might not be as capable as larger models."
          },
          {
            title: 'Download Process',
            content: "Downloading models might take some time depending on your internet speed and the model size. You can continue using Papin while downloads are in progress."
          }
        ],
        intermediate: [
          {
            title: 'Optimizing Model Selection',
            content: "Select models based on your typical offline usage patterns. Consider the trade-offs between model size, capability, and resource requirements. For general use, a medium-sized general model and a small specialized model often provide good coverage."
          },
          {
            title: 'Storage Management',
            content: "Monitor storage usage through the Models dashboard. Consider external storage options for larger models if your system has limited internal storage. Compression options can reduce storage requirements at the cost of slightly increased load times."
          },
          {
            title: 'Background Downloads',
            content: "Configure download scheduling to optimize for network conditions. Enable background downloading to continue model updates even when the application is closed. Set bandwidth limits to prevent network congestion during downloads."
          }
        ],
        advanced: [
          {
            title: 'Custom Model Integration',
            content: "Integrate custom or fine-tuned models into the offline system using the model registry API.",
            code: "// Register a custom local model\nconst modelConfig = {\n  id: 'custom-model-v1',\n  path: '/path/to/model/weights',\n  quantization: 'int8',\n  contextSize: 8192,\n  inferenceParams: {\n    threads: 4,\n    batchSize: 512\n  }\n};\napi.models.registerLocalModel(modelConfig);"
          },
          {
            title: 'Advanced Resource Allocation',
            content: "Configure fine-grained resource allocation for offline inference to optimize performance based on hardware capabilities.",
            code: "// Optimized resource configuration\nconst resourceConfig = {\n  memoryLimits: {\n    maxRam: '8GB',\n    maxVram: '4GB',\n    swapBuffer: '2GB'\n  },\n  computeAllocation: {\n    cpuThreads: 6,\n    cudaDevices: [0, 1],\n    prioritization: 'efficiency' // or 'speed'\n  }\n};"
          },
          {
            title: 'Model Sharding',
            content: "Implement model sharding to distribute large models across limited resources.",
            code: "// Configure model sharding\nconst shardingConfig = {\n  enabled: true,\n  shardSize: '2GB',\n  strategy: 'balanced', // 'memory-optimized' or 'latency-optimized'\n  diskCache: true,\n  prefetchWindow: 3\n};"
          }
        ]
      },
      {
        title: 'Working in Offline Mode',
        beginner: "When you're offline, Papin will automatically switch to using local models. You'll see an indicator in the top-right corner showing your connection status. Simply continue your conversations as normal! New conversations and changes are saved on your device.",
        intermediate: "During offline operation, Papin maintains the full feature set with local computational resources. The system automatically manages model loading and unloading based on usage patterns and available resources. Conversations are stored in a local database with automatic checkpointing for reliability. When connectivity is restored, the synchronization process reconciles local and cloud states.",
        advanced: "Offline operation relies on a complete local infrastructure including inference engines, storage services, and state management systems. The inference subsystem implements dynamic load balancing across available compute resources, with prioritization mechanisms for interactive requests. The storage layer employs a multi-tier architecture with optimized caching policies for high-throughput operation."
      },
      {
        title: 'Synchronization Process',
        beginner: "When you reconnect to the internet, Papin automatically syncs your offline conversations with your cloud account. You can see the sync status in the status bar at the bottom of the app. If you want to manually trigger a sync, click the sync icon or use Ctrl+Shift+S.",
        intermediate: [
          {
            title: 'Understanding the Sync Process',
            content: "Synchronization manages bidirectional data flow between local storage and cloud services. Changes are tracked through a distributed version control system and reconciled using intelligent conflict resolution strategies. The process prioritizes user data integrity while minimizing bandwidth usage."
          },
          {
            title: 'Managing Sync Conflicts',
            content: "When the same conversation is modified in multiple locations, conflict resolution intelligently merges changes. For direct conflicts, the system presents options to keep either version or merge manually. Conflict resolution preferences can be configured in advanced settings."
          },
          {
            title: 'Selective Synchronization',
            content: "Configure which content gets synchronized based on tags, folders, or conversation properties. Sensitive conversations can be marked as local-only to prevent cloud storage. Bandwidth usage can be optimized through selective sync policies."
          }
        ],
        advanced: [
          {
            title: 'Sync Protocol Architecture',
            content: "The synchronization system implements a CRDT-based protocol for eventual consistency across devices. Vector clocks track causality between events, enabling accurate conflict detection even with unreliable connectivity.",
            code: "// Simplified representation of the sync protocol\ninterface SyncOperation {\n  id: string;\n  vectorClock: Map<DeviceId, number>;\n  operation: 'create' | 'update' | 'delete';\n  resource: string;\n  payload: any;\n  parentOperations: string[];\n}"
          },
          {
            title: 'Efficient Delta Synchronization',
            content: "To minimize bandwidth usage, the system transmits only differential changes using an optimized binary format. This includes structural diffing for conversation content and model-specific optimizations for large assets.",
            code: "// Configure delta sync options\nconst deltaOptions = {\n  compression: 'zstd',\n  diffAlgorithm: 'structural', // vs. 'naive' or 'hybrid'\n  chunkSize: 262144, // bytes\n  deduplicate: true,\n  verifyChecksums: true\n};"
          },
          {
            title: 'Background Synchronization',
            content: "The system implements sophisticated background sync strategies with progressive backoff, network awareness, and battery optimization.",
            code: "// Advanced sync strategy configuration\nconst syncStrategy = {\n  initialRetryDelay: 1000, // ms\n  maxRetryDelay: 3600000, // 1 hour\n  backoffMultiplier: 1.5,\n  networkConditions: {\n    requireWifi: false,\n    meterednessAware: true,\n    minimumBandwidth: 0.5 // Mbps\n  }\n};"
          }
        ]
      }
    ],
    examples: [
      {
        title: "Optimizing for Travel Use",
        description: "Configure Papin for optimal offline use while traveling:",
        steps: [
          "Download small and medium-sized models before your trip",
          "Enable 'Aggressive caching' in Settings > Offline > Advanced",
          "Configure 'Bandwidth-aware sync' to prevent large syncs on hotel Wi-Fi",
          "Enable 'Power-saving mode' to optimize battery usage",
          "Set up scheduled synchronization for when you expect to have good connectivity"
        ]
      },
      {
        title: "Handling Specialized Tasks Offline",
        description: "Create a customized offline setup for specialized development tasks:",
        code: "// Custom offline configuration for code-focused workflows\nconst codeOptimizedConfig = {\n  models: ['papin-code-assistant-medium'],\n  contextSize: 16384,\n  templateOverrides: {\n    systemPrompts: {\n      codingAssistant: 'You are a helpful programming assistant...',\n      documentationHelper: 'You are a documentation specialist...',\n      bugDebugger: 'You are an expert in debugging code issues...'\n    }\n  },\n  caching: {\n    preferLocalExamples: true,\n    storeCodeSnippets: true,\n    indexingLevel: 'comprehensive'\n  }\n};"
      }
    ],
    relatedTopics: ['local-llm', 'performance-monitoring', 'troubleshooting']
  }
];
</file>

<file path="src-frontend/src/components/offline/OfflineSettings.tsx">
import React, { useState, useEffect, useMemo } from 'react';
import { invoke } from '@tauri-apps/api/tauri';
import LLMMetricsPrivacyNotice from './LLMMetricsPrivacyNotice';
import { 
  Box, Button, FormControl, FormLabel, Select, TextField, 
  Typography, Paper, Divider, CircularProgress, 
  List, ListItem, ListItemText, ListItemSecondaryAction,
  IconButton, Alert, Snackbar, Tabs, Tab, Switch, FormControlLabel,
  MenuItem, InputAdornment, Grid, Card, CardContent, Chip, Accordion,
  AccordionSummary, AccordionDetails, Dialog, DialogTitle, DialogContent,
  DialogActions, LinearProgress
} from '@mui/material';
import RefreshIcon from '@mui/icons-material/Refresh';
import DownloadIcon from '@mui/icons-material/Download';
import DeleteIcon from '@mui/icons-material/Delete';
import CheckCircleIcon from '@mui/icons-material/CheckCircle';
import ErrorIcon from '@mui/icons-material/Error';
import PauseCircleIcon from '@mui/icons-material/PauseCircle';
import WifiIcon from '@mui/icons-material/Wifi';
import WifiOffIcon from '@mui/icons-material/WifiOff';
import ExpandMoreIcon from '@mui/icons-material/ExpandMore';
import SettingsIcon from '@mui/icons-material/Settings';
import KeyIcon from '@mui/icons-material/Key';
import HttpIcon from '@mui/icons-material/Http';
import MemoryIcon from '@mui/icons-material/Memory';
import StorageIcon from '@mui/icons-material/Storage';
import ModelTrainingIcon from '@mui/icons-material/ModelTraining';
import ChatIcon from '@mui/icons-material/Chat';
import ImageIcon from '@mui/icons-material/Image';
import CodeIcon from '@mui/icons-material/Code';
import LanguageIcon from '@mui/icons-material/Language';
import InfoIcon from '@mui/icons-material/Info';

// Types
interface ModelInfo {
  id: string;
  name: string;
  description: string;
  size_bytes: number;
  is_downloaded: boolean;
  provider_metadata: Record<string, any>;
  provider: string;
  supports_text_generation: boolean;
  supports_completion: boolean;
  supports_chat: boolean;
  supports_embeddings: boolean;
  supports_image_generation: boolean;
  quantization?: string;
  parameter_count_b?: number;
  context_length?: number;
  model_family?: string;
  created_at?: string;
  tags: string[];
  license?: string;
}

interface DownloadStatus {
  status: string;
  NotStarted?: {};
  InProgress?: { 
    percent: number,
    bytes_downloaded?: number,
    total_bytes?: number,
    eta_seconds?: number,
    bytes_per_second?: number
  };
  Completed?: {
    completed_at?: string,
    duration_seconds?: number
  };
  Failed?: { 
    reason: string,
    error_code?: string,
    failed_at?: string
  };
  Cancelled?: {
    cancelled_at?: string
  };
}

interface CommandResponse<T> {
  success: boolean;
  error?: string;
  data?: T;
}

interface ProviderInfo {
  provider_type: string;
  name: string;
  description: string;
  version: string;
  default_endpoint: string;
  supports_text_generation: boolean;
  supports_chat: boolean;
  supports_embeddings: boolean;
  requires_api_key: boolean;
}

interface AvailabilityResult {
  available: boolean;
  version?: string;
  error?: string;
  response_time_ms?: number;
}

interface ProviderConfig {
  provider_type: string;
  endpoint_url: string;
  api_key?: string;
  default_model?: string;
  enable_advanced_config: boolean;
  advanced_config: Record<string, any>;
}

interface OfflineConfig {
  enabled: boolean;
  auto_switch: boolean;
  llm_config: ProviderConfig;
  max_history_size: number;
  enable_debug: boolean;
}

// Helper functions
const formatBytes = (bytes: number): string => {
  if (bytes === 0) return '0 Bytes';
  
  const k = 1024;
  const sizes = ['Bytes', 'KB', 'MB', 'GB', 'TB'];
  const i = Math.floor(Math.log(bytes) / Math.log(k));
  
  return parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + ' ' + sizes[i];
};

const getDownloadStatusText = (status: DownloadStatus): string => {
  switch (status.status) {
    case 'NotStarted':
      return 'Not Downloaded';
    case 'InProgress':
      if (status.InProgress?.percent !== undefined) {
        return `Downloading ${status.InProgress.percent.toFixed(2)}%`;
      }
      return 'Downloading...';
    case 'Completed':
      return 'Downloaded';
    case 'Failed':
      return `Failed: ${status.Failed?.reason}`;
    case 'Cancelled':
      return 'Cancelled';
    default:
      return 'Unknown';
  }
};

const getDownloadStatusIcon = (status: DownloadStatus) => {
  switch (status.status) {
    case 'NotStarted':
      return null;
    case 'InProgress':
      return <CircularProgress size={20} variant="determinate" value={status.InProgress?.percent || 0} />;
    case 'Completed':
      return <CheckCircleIcon color="success" />;
    case 'Failed':
      return <ErrorIcon color="error" />;
    case 'Cancelled':
      return <PauseCircleIcon color="warning" />;
    default:
      return null;
  }
};

const formatEta = (seconds?: number): string => {
  if (seconds === undefined) return 'Unknown';
  
  if (seconds < 60) {
    return `${Math.round(seconds)}s`;
  } else if (seconds < 3600) {
    const mins = Math.floor(seconds / 60);
    const secs = Math.round(seconds % 60);
    return `${mins}m ${secs}s`;
  } else {
    const hours = Math.floor(seconds / 3600);
    const mins = Math.floor((seconds % 3600) / 60);
    return `${hours}h ${mins}m`;
  }
};

// Convert provider_type string to display name
const getProviderDisplayName = (providerType: string): string => {
  switch (providerType) {
    case 'Ollama':
      return 'Ollama';
    case 'LocalAI':
      return 'LocalAI';
    case 'LlamaCpp':
      return 'llama.cpp';
    case 'Custom':
      return 'Custom Provider';
    default:
      if (providerType.startsWith('Custom(')) {
        return providerType.substring(7, providerType.length - 1);
      }
      return providerType;
  }
};

// Main component
const OfflineSettings: React.FC = () => {
  // State for offline mode
  const [isOfflineMode, setIsOfflineMode] = useState<boolean>(false);
  const [autoSwitchMode, setAutoSwitchMode] = useState<boolean>(true);
  const [isConnected, setIsConnected] = useState<boolean>(true);
  
  // State for providers
  const [availableProviders, setAvailableProviders] = useState<ProviderInfo[]>([]);
  const [providerAvailability, setProviderAvailability] = useState<Record<string, AvailabilityResult>>({});
  const [selectedProviderType, setSelectedProviderType] = useState<string>('Ollama');
  const [providerEndpoint, setProviderEndpoint] = useState<string>('http://localhost:11434');
  const [apiKey, setApiKey] = useState<string>('');
  const [defaultModel, setDefaultModel] = useState<string>('');
  const [enableAdvancedConfig, setEnableAdvancedConfig] = useState<boolean>(false);
  const [advancedConfig, setAdvancedConfig] = useState<Record<string, any>>({});
  
  // State for models
  const [availableModels, setAvailableModels] = useState<ModelInfo[]>([]);
  const [downloadedModels, setDownloadedModels] = useState<ModelInfo[]>([]);
  const [downloadStatus, setDownloadStatus] = useState<Record<string, DownloadStatus>>({});
  
  // UI state
  const [loading, setLoading] = useState<boolean>(false);
  const [error, setError] = useState<string | null>(null);
  const [success, setSuccess] = useState<string | null>(null);
  const [tabIndex, setTabIndex] = useState<number>(0);
  const [configChanged, setConfigChanged] = useState<boolean>(false);
  const [openModelInfoDialog, setOpenModelInfoDialog] = useState<boolean>(false);
  const [selectedModelInfo, setSelectedModelInfo] = useState<ModelInfo | null>(null);
  
  // Load initial data
  useEffect(() => {
    fetchOfflineConfig();
    checkNetworkStatus();
    fetchProviders();
    
    // Set up interval to refresh download status and network status
    const intervalId = setInterval(() => {
      refreshDownloadStatus();
      checkNetworkStatus();
    }, 5000);
    
    return () => clearInterval(intervalId);
  }, []);
  
  // Effect to update configChanged flag
  useEffect(() => {
    setConfigChanged(true);
  }, [selectedProviderType, providerEndpoint, apiKey, defaultModel, enableAdvancedConfig, advancedConfig, isOfflineMode, autoSwitchMode]);
  
  // Effect to fetch models when provider changes
  useEffect(() => {
    fetchModels();
  }, [selectedProviderType, providerEndpoint]);
  
  // Check network status
  const checkNetworkStatus = async () => {
    try {
      const response: CommandResponse<boolean> = await invoke('check_network');
      if (response.success && response.data !== undefined) {
        setIsConnected(response.data);
      }
    } catch (err) {
      console.error('Failed to check network status:', err);
      setIsConnected(false);
    }
  };
  
  // Fetch provider information
  const fetchProviders = async () => {
    try {
      // Get all providers
      const allProvidersResponse: CommandResponse<ProviderInfo[]> = await invoke('get_all_providers');
      if (allProvidersResponse.success && allProvidersResponse.data) {
        setAvailableProviders(allProvidersResponse.data);
      }
      
      // Get provider availability
      const availabilityResponse: CommandResponse<Record<string, AvailabilityResult>> = await invoke('get_all_provider_availability');
      if (availabilityResponse.success && availabilityResponse.data) {
        setProviderAvailability(availabilityResponse.data);
      }
    } catch (err) {
      console.error('Failed to fetch providers:', err);
      setError(`Failed to fetch providers: ${err}`);
    }
  };
  
  // Fetch offline configuration
  const fetchOfflineConfig = async () => {
    setLoading(true);
    try {
      const configResponse: CommandResponse<OfflineConfig> = await invoke('get_offline_config');
      
      if (configResponse.success && configResponse.data) {
        const config = configResponse.data;
        
        // Update state
        setIsOfflineMode(config.enabled);
        setAutoSwitchMode(config.auto_switch);
        
        const llmConfig = config.llm_config;
        setSelectedProviderType(llmConfig.provider_type);
        setProviderEndpoint(llmConfig.endpoint_url);
        setApiKey(llmConfig.api_key || '');
        setDefaultModel(llmConfig.default_model || '');
        setEnableAdvancedConfig(llmConfig.enable_advanced_config);
        setAdvancedConfig(llmConfig.advanced_config);
        
        // Reset changed flag as we just loaded the config
        setConfigChanged(false);
        
        // Fetch models for the current provider
        fetchModels();
      }
    } catch (err) {
      console.error('Failed to fetch offline config:', err);
      setError(`Failed to fetch offline configuration: ${err}`);
    } finally {
      setLoading(false);
    }
  };
  
  // Save offline configuration
  const saveOfflineConfig = async () => {
    setLoading(true);
    try {
      const config: OfflineConfig = {
        enabled: isOfflineMode,
        auto_switch: autoSwitchMode,
        llm_config: {
          provider_type: selectedProviderType,
          endpoint_url: providerEndpoint,
          api_key: apiKey || undefined,
          default_model: defaultModel || undefined,
          enable_advanced_config: enableAdvancedConfig,
          advanced_config: advancedConfig,
        },
        max_history_size: 100, // Default value
        enable_debug: false,   // Default value
      };
      
      const response: CommandResponse<boolean> = await invoke('update_offline_config', { config });
      
      if (response.success) {
        setSuccess('Configuration saved successfully');
        setConfigChanged(false);
        fetchModels(); // Refresh models with the new configuration
      } else if (response.error) {
        setError(`Failed to save configuration: ${response.error}`);
      }
    } catch (err) {
      console.error('Failed to save configuration:', err);
      setError(`Failed to save configuration: ${err}`);
    } finally {
      setLoading(false);
    }
  };
  
  // Toggle offline mode
  const toggleOfflineMode = async () => {
    setIsOfflineMode(!isOfflineMode);
  };
  
  // Toggle auto switch mode
  const toggleAutoSwitchMode = async () => {
    setAutoSwitchMode(!autoSwitchMode);
  };
  
  // Check if the selected provider is available
  const isSelectedProviderAvailable = useMemo(() => {
    const availability = providerAvailability[selectedProviderType];
    return availability?.available || false;
  }, [selectedProviderType, providerAvailability]);
  
  // Fetch models from the backend
  const fetchModels = async () => {
    if (!isSelectedProviderAvailable) {
      // Don't try to fetch models if the provider is not available
      setAvailableModels([]);
      setDownloadedModels([]);
      return;
    }
    
    setLoading(true);
    try {
      const availableResponse: CommandResponse<ModelInfo[]> = await invoke('list_available_models');
      const downloadedResponse: CommandResponse<ModelInfo[]> = await invoke('list_downloaded_models');
      
      if (availableResponse.success && availableResponse.data) {
        setAvailableModels(availableResponse.data);
      } else if (availableResponse.error) {
        console.error('Error fetching available models:', availableResponse.error);
      }
      
      if (downloadedResponse.success && downloadedResponse.data) {
        setDownloadedModels(downloadedResponse.data);
      } else if (downloadedResponse.error) {
        console.error('Error fetching downloaded models:', downloadedResponse.error);
      }
      
      refreshDownloadStatus();
    } catch (err) {
      console.error('Failed to fetch models:', err);
      setError(`Failed to fetch models: ${err}`);
    } finally {
      setLoading(false);
    }
  };
  
  // Refresh download status for all models
  const refreshDownloadStatus = async () => {
    // Skip if no models or provider not available
    if (availableModels.length === 0 || !isSelectedProviderAvailable) {
      return;
    }
    
    const newStatus: Record<string, DownloadStatus> = {};
    
    for (const model of availableModels) {
      try {
        const response: CommandResponse<DownloadStatus> = await invoke('get_download_status', { modelId: model.id });
        if (response.success && response.data) {
          newStatus[model.id] = response.data;
        }
      } catch (err) {
        console.error(`Failed to get download status for ${model.id}:`, err);
      }
    }
    
    setDownloadStatus(newStatus);
  };
  
  // Download a model
  const downloadModel = async (modelId: string) => {
    try {
      const response: CommandResponse<boolean> = await invoke('download_model', { modelId });
      
      if (response.success) {
        // Update the status immediately to show download started
        setDownloadStatus(prev => ({
          ...prev,
          [modelId]: { 
            status: 'InProgress',
            InProgress: { percent: 0 } 
          },
        }));
        
        setSuccess(`Started downloading model ${modelId}`);
        
        // Start polling the download status
        const intervalId = setInterval(async () => {
          try {
            const statusResponse: CommandResponse<DownloadStatus> = await invoke('get_download_status', { modelId });
            if (statusResponse.success && statusResponse.data) {
              setDownloadStatus(prev => ({
                ...prev,
                [modelId]: statusResponse.data,
              }));
              
              // Check if download is complete or failed
              if (statusResponse.data.status === 'Completed' || 
                  statusResponse.data.status === 'Failed' || 
                  statusResponse.data.status === 'Cancelled') {
                clearInterval(intervalId);
                if (statusResponse.data.status === 'Completed') {
                  setSuccess(`Model ${modelId} downloaded successfully`);
                  fetchModels(); // Refresh model lists
                } else if (statusResponse.data.status === 'Failed') {
                  setError(`Failed to download model ${modelId}: ${statusResponse.data.Failed?.reason}`);
                }
              }
            }
          } catch (err) {
            console.error(`Failed to get download status for ${modelId}:`, err);
          }
        }, 2000);
        
        // Cleanup interval after 10 minutes (failsafe)
        setTimeout(() => clearInterval(intervalId), 10 * 60 * 1000);
      } else if (response.error) {
        setError(`Failed to download model: ${response.error}`);
      }
    } catch (err) {
      setError(`Failed to download model: ${err}`);
    }
  };
  
  // Cancel a model download
  const cancelDownload = async (modelId: string) => {
    try {
      const response: CommandResponse<boolean> = await invoke('cancel_download', { modelId });
      
      if (response.success) {
        setSuccess(`Cancelled download of model ${modelId}`);
        
        // Update status immediately
        setDownloadStatus(prev => ({
          ...prev,
          [modelId]: { 
            status: 'Cancelled',
            Cancelled: { cancelled_at: new Date().toISOString() } 
          },
        }));
      } else if (response.error) {
        setError(`Failed to cancel download: ${response.error}`);
      }
    } catch (err) {
      setError(`Failed to cancel download: ${err}`);
    }
  };
  
  // Delete a model
  const deleteModel = async (modelId: string) => {
    try {
      const response: CommandResponse<boolean> = await invoke('delete_model', { modelId });
      
      if (response.success) {
        setSuccess(`Deleted model ${modelId}`);
        fetchModels(); // Refresh model lists
      } else if (response.error) {
        setError(`Failed to delete model: ${response.error}`);
      }
    } catch (err) {
      setError(`Failed to delete model: ${err}`);
    }
  };
  
  // Open model info dialog
  const openModelInfo = (model: ModelInfo) => {
    setSelectedModelInfo(model);
    setOpenModelInfoDialog(true);
  };
  
  // Handle tab change
  const handleTabChange = (_: React.SyntheticEvent, newValue: number) => {
    setTabIndex(newValue);
  };
  
  // Filter models by provider
  const filteredAvailableModels = useMemo(() => {
    return availableModels.filter(model => 
      model.provider === selectedProviderType || 
      // Special case for legacy models without provider field
      (model.provider === undefined && selectedProviderType === 'Ollama')
    );
  }, [availableModels, selectedProviderType]);
  
  const filteredDownloadedModels = useMemo(() => {
    return downloadedModels.filter(model => 
      model.provider === selectedProviderType ||
      // Special case for legacy models without provider field
      (model.provider === undefined && selectedProviderType === 'Ollama')
    );
  }, [downloadedModels, selectedProviderType]);

  return (
    <Box sx={{ padding: 3, maxWidth: 1000, margin: '0 auto' }}>
      <Typography variant="h4" gutterBottom>
        Offline Settings
      </Typography>
      
      {/* Network Status */}
      <Paper sx={{ p: 3, mb: 4 }}>
        <Box sx={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', mb: 3 }}>
          <Typography variant="h6">
            Network Status
          </Typography>
          
          <Box sx={{ display: 'flex', alignItems: 'center' }}>
            {isConnected ? (
              <>
                <WifiIcon color="success" sx={{ mr: 1 }} />
                <Typography color="success.main">Connected</Typography>
              </>
            ) : (
              <>
                <WifiOffIcon color="error" sx={{ mr: 1 }} />
                <Typography color="error.main">Disconnected</Typography>
              </>
            )}
            <IconButton 
              size="small" 
              sx={{ ml: 2 }}
              onClick={checkNetworkStatus}
            >
              <RefreshIcon />
            </IconButton>
          </Box>
        </Box>
        
        <Grid container spacing={3}>
          <Grid item xs={12} md={6}>
            <FormControlLabel
              control={
                <Switch
                  checked={isOfflineMode}
                  onChange={toggleOfflineMode}
                  color="primary"
                />
              }
              label="Enable Offline Mode"
            />
            
            <Typography variant="body2" color="text.secondary" sx={{ mt: 1 }}>
              {isOfflineMode 
                ? "The application will use local LLMs for text generation." 
                : "The application will use cloud services for text generation."}
            </Typography>
          </Grid>
          
          <Grid item xs={12} md={6}>
            <FormControlLabel
              control={
                <Switch
                  checked={autoSwitchMode}
                  onChange={toggleAutoSwitchMode}
                  color="primary"
                />
              }
              label="Auto-switch based on connectivity"
            />
            
            <Typography variant="body2" color="text.secondary" sx={{ mt: 1 }}>
              {autoSwitchMode 
                ? "The application will automatically switch between online and offline modes based on network connectivity." 
                : "You will need to manually switch between online and offline modes."}
            </Typography>
          </Grid>
        </Grid>
      </Paper>
      
      {/* Provider Selection */}
      <Paper sx={{ p: 3, mb: 4 }}>
        <Typography variant="h6" gutterBottom>
          Local LLM Provider
        </Typography>
        
        <Grid container spacing={3}>
          <Grid item xs={12} md={6}>
            <FormControl fullWidth sx={{ mb: 2 }}>
              <FormLabel>Provider Type</FormLabel>
              <Select
                value={selectedProviderType}
                onChange={(e) => setSelectedProviderType(e.target.value)}
              >
                {availableProviders.map((provider) => (
                  <MenuItem 
                    key={provider.provider_type} 
                    value={provider.provider_type}
                    disabled={!providerAvailability[provider.provider_type]?.available}
                  >
                    {getProviderDisplayName(provider.provider_type)}
                    {providerAvailability[provider.provider_type]?.available && 
                      <CheckCircleIcon color="success" fontSize="small" sx={{ ml: 1 }} />
                    }
                  </MenuItem>
                ))}
              </Select>
              {!isSelectedProviderAvailable && (
                <Alert severity="warning" sx={{ mt: 1 }}>
                  This provider is not available. Make sure it's installed and running at the specified endpoint.
                </Alert>
              )}
            </FormControl>
            
            {selectedProviderType && availableProviders.find(p => p.provider_type === selectedProviderType) && (
              <Card variant="outlined" sx={{ mb: 2, mt: 2 }}>
                <CardContent>
                  <Typography variant="subtitle1">
                    {getProviderDisplayName(selectedProviderType)}
                  </Typography>
                  <Typography variant="body2" color="text.secondary">
                    {availableProviders.find(p => p.provider_type === selectedProviderType)?.description}
                  </Typography>
                  
                  <Box sx={{ mt: 2, display: 'flex', flexWrap: 'wrap', gap: 1 }}>
                    {availableProviders.find(p => p.provider_type === selectedProviderType)?.supports_text_generation && (
                      <Chip icon={<LanguageIcon />} label="Text Generation" size="small" color="primary" variant="outlined" />
                    )}
                    {availableProviders.find(p => p.provider_type === selectedProviderType)?.supports_chat && (
                      <Chip icon={<ChatIcon />} label="Chat" size="small" color="primary" variant="outlined" />
                    )}
                    {availableProviders.find(p => p.provider_type === selectedProviderType)?.supports_embeddings && (
                      <Chip icon={<MemoryIcon />} label="Embeddings" size="small" color="primary" variant="outlined" />
                    )}
                  </Box>
                </CardContent>
              </Card>
            )}
          </Grid>
          
          <Grid item xs={12} md={6}>
            <FormControl fullWidth sx={{ mb: 2 }}>
              <FormLabel>Endpoint URL</FormLabel>
              <TextField
                value={providerEndpoint}
                onChange={(e) => setProviderEndpoint(e.target.value)}
                placeholder={
                  selectedProviderType === 'Ollama' 
                    ? "http://localhost:11434" 
                    : selectedProviderType === 'LocalAI'
                      ? "http://localhost:8080"
                      : "http://localhost:8000"
                }
                InputProps={{
                  startAdornment: (
                    <InputAdornment position="start">
                      <HttpIcon />
                    </InputAdornment>
                  ),
                }}
              />
            </FormControl>
            
            {availableProviders.find(p => p.provider_type === selectedProviderType)?.requires_api_key && (
              <FormControl fullWidth sx={{ mb: 2 }}>
                <FormLabel>API Key</FormLabel>
                <TextField
                  type="password"
                  value={apiKey}
                  onChange={(e) => setApiKey(e.target.value)}
                  placeholder="Enter API key"
                  InputProps={{
                    startAdornment: (
                      <InputAdornment position="start">
                        <KeyIcon />
                      </InputAdornment>
                    ),
                  }}
                />
              </FormControl>
            )}
            
            <FormControl fullWidth sx={{ mb: 3 }}>
              <FormLabel>Default Model</FormLabel>
              <Select
                value={defaultModel}
                onChange={(e) => setDefaultModel(e.target.value)}
                displayEmpty
              >
                <MenuItem value="">
                  <em>None</em>
                </MenuItem>
                {filteredDownloadedModels.map((model) => (
                  <MenuItem key={model.id} value={model.id}>
                    {model.name}
                  </MenuItem>
                ))}
              </Select>
              <Typography variant="body2" color="text.secondary" sx={{ mt: 1 }}>
                The default model will be used when no specific model is specified.
              </Typography>
            </FormControl>
            
            <Accordion 
              expanded={enableAdvancedConfig}
              onChange={() => setEnableAdvancedConfig(!enableAdvancedConfig)}
              sx={{ mb: 2 }}
            >
              <AccordionSummary expandIcon={<ExpandMoreIcon />}>
                <Box sx={{ display: 'flex', alignItems: 'center' }}>
                  <SettingsIcon sx={{ mr: 1 }} />
                  <Typography>Advanced Configuration</Typography>
                </Box>
              </AccordionSummary>
              <AccordionDetails>
                <Typography variant="body2" color="text.secondary" sx={{ mb: 2 }}>
                  These settings are for advanced users only. Incorrect settings may cause issues.
                </Typography>
                
                {/* This could be expanded with provider-specific advanced settings */}
                <Alert severity="info">
                  Advanced configuration options are not currently implemented.
                </Alert>
              </AccordionDetails>
            </Accordion>
          </Grid>
        </Grid>
        
        <Box sx={{ mt: 3, display: 'flex', justifyContent: 'space-between' }}>
          <Button
            variant="outlined"
            onClick={fetchProviders}
            startIcon={<RefreshIcon />}
            disabled={loading}
          >
            Refresh Providers
          </Button>
          
          <Button
            variant="contained"
            onClick={saveOfflineConfig}
            disabled={loading || !configChanged}
          >
            {loading ? <CircularProgress size={24} /> : 'Save Configuration'}
          </Button>
        </Box>
      </Paper>
      
      {/* LLM Performance Metrics */}
      <Paper sx={{ p: 3, mb: 4 }}>
        <Typography variant="h6" gutterBottom>
          LLM Performance Metrics
        </Typography>
        
        <Typography variant="body2" color="text.secondary" sx={{ mb: 2 }}>
          Help improve the performance of local LLM providers by allowing anonymous collection of metrics data.
          This data helps us optimize the application and understand which models and providers work best.
        </Typography>
        
        <LLMMetricsPrivacyNotice />
      </Paper>
      
      {/* Models */}
      <Paper sx={{ p: 3 }}>
        <Box sx={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', mb: 2 }}>
          <Typography variant="h6">
            Models
          </Typography>
          
          <Button 
            startIcon={<RefreshIcon />}
            onClick={fetchModels}
            disabled={loading || !isSelectedProviderAvailable}
          >
            Refresh
          </Button>
        </Box>
        
        {!isSelectedProviderAvailable ? (
          <Alert severity="warning">
            Provider is not available. Make sure it's installed and running at {providerEndpoint}.
          </Alert>
        ) : (
          <>
            <Tabs value={tabIndex} onChange={handleTabChange} sx={{ mb: 2 }}>
              <Tab label="Available Models" />
              <Tab label="Downloaded Models" />
            </Tabs>
            
            <Divider sx={{ mb: 2 }} />
            
            {loading && <LinearProgress sx={{ mb: 2 }} />}
            
            {tabIndex === 0 && !loading && (
              <>
                {filteredAvailableModels.length === 0 ? (
                  <Alert severity="info">No models available. Make sure your provider is running.</Alert>
                ) : (
                  <List>
                    {filteredAvailableModels.map((model) => {
                      const status = downloadStatus[model.id] || { status: 'NotStarted' };
                      const isDownloaded = status.status === 'Completed';
                      const isDownloading = status.status === 'InProgress';
                      
                      return (
                        <ListItem 
                          key={model.id} 
                          divider
                          secondaryAction={
                            <Box sx={{ display: 'flex', alignItems: 'center' }}>
                              {getDownloadStatusIcon(status)}
                              
                              <Box sx={{ ml: 1 }}>
                                {!isDownloaded && !isDownloading && (
                                  <IconButton edge="end" onClick={() => downloadModel(model.id)}>
                                    <DownloadIcon />
                                  </IconButton>
                                )}
                                
                                {isDownloading && (
                                  <IconButton edge="end" onClick={() => cancelDownload(model.id)}>
                                    <PauseCircleIcon />
                                  </IconButton>
                                )}
                                
                                {isDownloaded && (
                                  <IconButton edge="end" onClick={() => deleteModel(model.id)}>
                                    <DeleteIcon />
                                  </IconButton>
                                )}
                              </Box>
                              
                              <IconButton edge="end" onClick={() => openModelInfo(model)}>
                                <InfoIcon />
                              </IconButton>
                            </Box>
                          }
                        >
                          <ListItemText
                            primary={
                              <Box sx={{ display: 'flex', alignItems: 'center' }}>
                                <Typography>{model.name}</Typography>
                                {model.parameter_count_b && (
                                  <Chip 
                                    label={`${model.parameter_count_b}B`} 
                                    size="small" 
                                    color="primary" 
                                    sx={{ ml: 1 }}
                                  />
                                )}
                                {model.quantization && (
                                  <Chip 
                                    label={model.quantization} 
                                    size="small" 
                                    color="secondary" 
                                    sx={{ ml: 1 }}
                                  />
                                )}
                              </Box>
                            }
                            secondary={
                              <>
                                <Typography component="span" variant="body2" color="text.primary">
                                  {formatBytes(model.size_bytes)}
                                </Typography>
                                {" • "}
                                {getDownloadStatusText(status)}
                                
                                {status.status === 'InProgress' && status.InProgress?.eta_seconds !== undefined && (
                                  <>
                                    {" • "}
                                    ETA: {formatEta(status.InProgress.eta_seconds)}
                                  </>
                                )}
                                
                                {status.status === 'InProgress' && status.InProgress?.bytes_per_second !== undefined && (
                                  <>
                                    {" • "}
                                    Speed: {formatBytes(status.InProgress.bytes_per_second ?? 0)}/s
                                  </>
                                )}
                                
                                <Box sx={{ mt: 1 }}>
                                  {model.description}
                                </Box>
                                
                                {model.tags.length > 0 && (
                                  <Box sx={{ mt: 1, display: 'flex', flexWrap: 'wrap', gap: 0.5 }}>
                                    {model.tags.map((tag, index) => (
                                      <Chip 
                                        key={index} 
                                        label={tag} 
                                        size="small" 
                                        variant="outlined"
                                        sx={{ 
                                          fontSize: '0.7rem', 
                                          height: '20px',
                                          '& .MuiChip-label': { px: 1 } 
                                        }}
                                      />
                                    ))}
                                  </Box>
                                )}
                              </>
                            }
                          />
                        </ListItem>
                      );
                    })}
                  </List>
                )}
              </>
            )}
            
            {tabIndex === 1 && !loading && (
              <>
                {filteredDownloadedModels.length === 0 ? (
                  <Alert severity="info">No models downloaded yet.</Alert>
                ) : (
                  <List>
                    {filteredDownloadedModels.map((model) => (
                      <ListItem 
                        key={model.id} 
                        divider
                        secondaryAction={
                          <Box sx={{ display: 'flex', alignItems: 'center' }}>
                            <IconButton edge="end" onClick={() => deleteModel(model.id)}>
                              <DeleteIcon />
                            </IconButton>
                            <IconButton edge="end" onClick={() => openModelInfo(model)}>
                              <InfoIcon />
                            </IconButton>
                          </Box>
                        }
                      >
                        <ListItemText
                          primary={
                            <Box sx={{ display: 'flex', alignItems: 'center' }}>
                              <Typography>{model.name}</Typography>
                              {model.parameter_count_b && (
                                <Chip 
                                  label={`${model.parameter_count_b}B`} 
                                  size="small" 
                                  color="primary" 
                                  sx={{ ml: 1 }}
                                />
                              )}
                              {model.quantization && (
                                <Chip 
                                  label={model.quantization} 
                                  size="small" 
                                  color="secondary" 
                                  sx={{ ml: 1 }}
                                />
                              )}
                              {model.id === defaultModel && (
                                <Chip 
                                  label="Default" 
                                  size="small" 
                                  color="success" 
                                  sx={{ ml: 1 }}
                                />
                              )}
                            </Box>
                          }
                          secondary={
                            <>
                              <Typography component="span" variant="body2" color="text.primary">
                                {formatBytes(model.size_bytes)}
                              </Typography>
                              
                              <Box sx={{ mt: 1 }}>
                                {model.description}
                              </Box>
                              
                              {model.tags.length > 0 && (
                                <Box sx={{ mt: 1, display: 'flex', flexWrap: 'wrap', gap: 0.5 }}>
                                  {model.tags.map((tag, index) => (
                                    <Chip 
                                      key={index} 
                                      label={tag} 
                                      size="small" 
                                      variant="outlined"
                                      sx={{ 
                                        fontSize: '0.7rem', 
                                        height: '20px',
                                        '& .MuiChip-label': { px: 1 } 
                                      }}
                                    />
                                  ))}
                                </Box>
                              )}
                              
                              <Box sx={{ mt: 1, display: 'flex', flexWrap: 'wrap', gap: 0.5 }}>
                                {model.supports_text_generation && (
                                  <Chip icon={<LanguageIcon fontSize="small" />} label="Text" size="small" variant="outlined" />
                                )}
                                {model.supports_chat && (
                                  <Chip icon={<ChatIcon fontSize="small" />} label="Chat" size="small" variant="outlined" />
                                )}
                                {model.supports_embeddings && (
                                  <Chip icon={<MemoryIcon fontSize="small" />} label="Embeddings" size="small" variant="outlined" />
                                )}
                                {model.supports_image_generation && (
                                  <Chip icon={<ImageIcon fontSize="small" />} label="Images" size="small" variant="outlined" />
                                )}
                              </Box>
                            </>
                          }
                        />
                      </ListItem>
                    ))}
                  </List>
                )}
              </>
            )}
          </>
        )}
      </Paper>
      
      {/* Model Info Dialog */}
      <Dialog 
        open={openModelInfoDialog} 
        onClose={() => setOpenModelInfoDialog(false)}
        fullWidth
        maxWidth="md"
      >
        <DialogTitle>
          Model Information: {selectedModelInfo?.name}
        </DialogTitle>
        <DialogContent dividers>
          {selectedModelInfo && (
            <Grid container spacing={2}>
              <Grid item xs={12} md={6}>
                <Typography variant="subtitle1">General Information</Typography>
                <List dense>
                  <ListItem>
                    <ListItemText primary="ID" secondary={selectedModelInfo.id} />
                  </ListItem>
                  <ListItem>
                    <ListItemText primary="Name" secondary={selectedModelInfo.name} />
                  </ListItem>
                  <ListItem>
                    <ListItemText primary="Provider" secondary={getProviderDisplayName(selectedModelInfo.provider)} />
                  </ListItem>
                  <ListItem>
                    <ListItemText primary="Description" secondary={selectedModelInfo.description} />
                  </ListItem>
                  <ListItem>
                    <ListItemText primary="Size" secondary={formatBytes(selectedModelInfo.size_bytes)} />
                  </ListItem>
                  {selectedModelInfo.created_at && (
                    <ListItem>
                      <ListItemText 
                        primary="Created At" 
                        secondary={new Date(selectedModelInfo.created_at).toLocaleString()} 
                      />
                    </ListItem>
                  )}
                  {selectedModelInfo.license && (
                    <ListItem>
                      <ListItemText primary="License" secondary={selectedModelInfo.license} />
                    </ListItem>
                  )}
                </List>
              </Grid>
              
              <Grid item xs={12} md={6}>
                <Typography variant="subtitle1">Model Specifications</Typography>
                <List dense>
                  {selectedModelInfo.parameter_count_b && (
                    <ListItem>
                      <ListItemText 
                        primary="Parameter Count" 
                        secondary={`${selectedModelInfo.parameter_count_b} billion`} 
                      />
                    </ListItem>
                  )}
                  {selectedModelInfo.quantization && (
                    <ListItem>
                      <ListItemText primary="Quantization" secondary={selectedModelInfo.quantization} />
                    </ListItem>
                  )}
                  {selectedModelInfo.context_length && (
                    <ListItem>
                      <ListItemText 
                        primary="Context Length" 
                        secondary={`${selectedModelInfo.context_length} tokens`} 
                      />
                    </ListItem>
                  )}
                  {selectedModelInfo.model_family && (
                    <ListItem>
                      <ListItemText primary="Model Family" secondary={selectedModelInfo.model_family} />
                    </ListItem>
                  )}
                </List>
                
                <Typography variant="subtitle1" sx={{ mt: 2 }}>Capabilities</Typography>
                <Box sx={{ mt: 1, display: 'flex', flexWrap: 'wrap', gap: 1 }}>
                  {selectedModelInfo.supports_text_generation && (
                    <Chip icon={<LanguageIcon />} label="Text Generation" color="primary" variant="outlined" />
                  )}
                  {selectedModelInfo.supports_chat && (
                    <Chip icon={<ChatIcon />} label="Chat" color="primary" variant="outlined" />
                  )}
                  {selectedModelInfo.supports_completion && (
                    <Chip icon={<CodeIcon />} label="Completion" color="primary" variant="outlined" />
                  )}
                  {selectedModelInfo.supports_embeddings && (
                    <Chip icon={<MemoryIcon />} label="Embeddings" color="primary" variant="outlined" />
                  )}
                  {selectedModelInfo.supports_image_generation && (
                    <Chip icon={<ImageIcon />} label="Image Generation" color="primary" variant="outlined" />
                  )}
                </Box>
                
                {selectedModelInfo.tags.length > 0 && (
                  <>
                    <Typography variant="subtitle1" sx={{ mt: 2 }}>Tags</Typography>
                    <Box sx={{ mt: 1, display: 'flex', flexWrap: 'wrap', gap: 0.5 }}>
                      {selectedModelInfo.tags.map((tag, index) => (
                        <Chip key={index} label={tag} size="small" />
                      ))}
                    </Box>
                  </>
                )}
              </Grid>
              
              {Object.keys(selectedModelInfo.provider_metadata).length > 0 && (
                <Grid item xs={12}>
                  <Typography variant="subtitle1">Provider-Specific Metadata</Typography>
                  <Box sx={{ mt: 1, maxHeight: '200px', overflow: 'auto', bgcolor: '#f5f5f5', p: 2, borderRadius: 1 }}>
                    <pre>{JSON.stringify(selectedModelInfo.provider_metadata, null, 2)}</pre>
                  </Box>
                </Grid>
              )}
            </Grid>
          )}
        </DialogContent>
        <DialogActions>
          <Button onClick={() => setOpenModelInfoDialog(false)}>
            Close
          </Button>
        </DialogActions>
      </Dialog>
      
      {/* Notifications */}
      <Snackbar
        open={!!error}
        autoHideDuration={6000}
        onClose={() => setError(null)}
      >
        <Alert onClose={() => setError(null)} severity="error">
          {error}
        </Alert>
      </Snackbar>
      
      <Snackbar
        open={!!success}
        autoHideDuration={6000}
        onClose={() => setSuccess(null)}
      >
        <Alert onClose={() => setSuccess(null)} severity="success">
          {success}
        </Alert>
      </Snackbar>
    </Box>
  );
};

export default OfflineSettings;
</file>

<file path="src-frontend/src/index.css">
/* Modern CSS Reset */
*, *::before, *::after {
  box-sizing: border-box;
}

* {
  margin: 0;
}

html, body {
  height: 100%;
}

body {
  line-height: 1.5;
  -webkit-font-smoothing: antialiased;
}

img, picture, video, canvas, svg {
  display: block;
  max-width: 100%;
}

input, button, textarea, select {
  font: inherit;
}

p, h1, h2, h3, h4, h5, h6 {
  overflow-wrap: break-word;
}

#root {
  isolation: isolate;
  height: 100%;
}
</file>

<file path="src-frontend/src/lazy/Chat.css">
.chat-container {
  display: flex;
  flex-direction: column;
  height: 100%;
  overflow: hidden;
  background-color: var(--color-background);
}

.chat-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: var(--spacing-md) var(--spacing-lg);
  border-bottom: 1px solid var(--color-border);
  background-color: var(--color-surface);
}

.chat-header h2 {
  margin: 0;
  font-size: var(--font-size-xl);
  font-weight: 600;
}

.chat-status {
  display: flex;
  align-items: center;
  font-size: var(--font-size-sm);
  font-weight: 500;
}

.chat-status::before {
  content: '';
  display: inline-block;
  width: 8px;
  height: 8px;
  margin-right: var(--spacing-xs);
  border-radius: 50%;
}

.chat-status.online::before {
  background-color: var(--color-success);
}

.chat-status.offline::before {
  background-color: var(--color-error);
}

.chat-status.connecting::before {
  background-color: var(--color-warning);
}

.chat-messages {
  flex: 1;
  padding: var(--spacing-lg);
  overflow-y: auto;
  display: flex;
  flex-direction: column;
  gap: var(--spacing-md);
}

.message {
  display: flex;
  gap: var(--spacing-md);
  max-width: 85%;
  animation: fadeIn 0.3s ease;
}

.message.user {
  align-self: flex-end;
  flex-direction: row-reverse;
}

.message.assistant, .message.system {
  align-self: flex-start;
}

.message-avatar {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 2.5rem;
  height: 2.5rem;
  border-radius: 50%;
  font-weight: 600;
  flex-shrink: 0;
}

.message.user .message-avatar {
  background-color: var(--color-secondary);
  color: white;
}

.message.assistant .message-avatar {
  background-color: var(--color-primary);
  color: white;
}

.message.system .message-avatar {
  background-color: var(--color-on-surface-variant);
  color: white;
}

.message-content-wrapper {
  display: flex;
  flex-direction: column;
  gap: var(--spacing-xxs);
}

.message-content {
  padding: var(--spacing-sm) var(--spacing-md);
  border-radius: var(--radius-md);
  font-size: var(--font-size-md);
  line-height: 1.5;
}

.message.user .message-content {
  background-color: var(--color-secondary);
  color: white;
  border-top-right-radius: 0;
}

.message.assistant .message-content {
  background-color: var(--color-surface-variant);
  color: var(--color-on-surface);
  border-top-left-radius: 0;
}

.message.system .message-content {
  background-color: var(--color-surface);
  color: var(--color-on-surface-variant);
  font-style: italic;
  border: 1px solid var(--color-border);
}

.message-time {
  font-size: var(--font-size-xs);
  color: var(--color-on-surface-variant);
  opacity: 0.8;
}

.message.user .message-time {
  text-align: right;
}

.chat-input-container {
  display: flex;
  align-items: flex-end;
  gap: var(--spacing-sm);
  padding: var(--spacing-md) var(--spacing-lg);
  background-color: var(--color-surface);
  border-top: 1px solid var(--color-border);
}

.chat-input {
  flex: 1;
  min-height: 56px;
  max-height: 200px;
  padding: var(--spacing-sm) var(--spacing-md);
  background-color: var(--color-background);
  color: var(--color-on-background);
  border: 1px solid var(--color-border);
  border-radius: var(--radius-md);
  font-family: var(--font-family);
  font-size: var(--font-size-md);
  line-height: 1.5;
  resize: none;
  transition: all var(--transition-fast);
}

.chat-input:focus {
  outline: none;
  border-color: var(--color-primary);
  box-shadow: 0 0 0 2px var(--color-primary-light);
}

.send-button {
  height: 56px;
  flex-shrink: 0;
}

.chat-loading {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  height: 100%;
  gap: var(--spacing-md);
}

.loading-spinner {
  width: 2rem;
  height: 2rem;
  border: 3px solid rgba(0, 0, 0, 0.1);
  border-radius: 50%;
  border-top-color: var(--color-primary);
  animation: spin 1s linear infinite;
}

.typing-indicator {
  display: flex;
  align-items: center;
  justify-content: center;
  height: 24px;
  gap: 4px;
}

.typing-indicator span {
  display: inline-block;
  width: 8px;
  height: 8px;
  background-color: var(--color-on-surface-variant);
  border-radius: 50%;
  animation: pulse 1.5s infinite ease-in-out;
}

.typing-indicator span:nth-child(2) {
  animation-delay: 0.2s;
}

.typing-indicator span:nth-child(3) {
  animation-delay: 0.4s;
}

@keyframes fadeIn {
  from {
    opacity: 0;
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

@keyframes spin {
  to {
    transform: rotate(360deg);
  }
}

@keyframes pulse {
  0%, 100% {
    transform: scale(0.8);
    opacity: 0.5;
  }
  50% {
    transform: scale(1.2);
    opacity: 1;
  }
}

.fade-in {
  animation: fadeIn 0.3s ease;
}
</file>

<file path="src-frontend/src/lazy/Chat.tsx">
import React, { useState, useEffect, useRef } from 'react';
import { invoke } from '@tauri-apps/api/tauri';
import { Button } from '../components/ui/Button';
import './Chat.css';

// Define message types
interface Message {
  id: string;
  role: 'user' | 'assistant' | 'system';
  content: string;
  timestamp: number;
}

// Chat component - lazy loaded after shell is ready
const Chat: React.FC = () => {
  const [loaded, setLoaded] = useState(false);
  const [loading, setLoading] = useState(false);
  const [messages, setMessages] = useState<Message[]>([]);
  const [inputValue, setInputValue] = useState('');
  const messagesEndRef = useRef<HTMLDivElement>(null);
  const inputRef = useRef<HTMLTextAreaElement>(null);
  
  // Generate a unique ID for messages
  const generateId = () => {
    return `msg_${Date.now()}_${Math.floor(Math.random() * 1000)}`;
  };
  
  // Scroll to the latest message
  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };
  
  // Auto-resize the textarea based on content
  const autoResizeTextarea = () => {
    if (inputRef.current) {
      // Reset height to auto to correctly calculate the new height
      inputRef.current.style.height = 'auto';
      // Set the new height based on the scroll height
      const newHeight = Math.min(
        Math.max(56, inputRef.current.scrollHeight), // Min height 56px
        200 // Max height 200px
      );
      inputRef.current.style.height = `${newHeight}px`;
    }
  };
  
  // Load initial chat data
  useEffect(() => {
    const loadChat = async () => {
      try {
        // In a real app, we would fetch messages from backend
        // const initialMessages = await invoke<Message[]>('get_chat_messages');
        
        // For now, just add a system welcome message
        const welcomeMessage: Message = {
          id: generateId(),
          role: 'system',
          content: 'Welcome to Claude MCP client. How can I help you today?',
          timestamp: Date.now(),
        };
        
        setMessages([welcomeMessage]);
        setLoaded(true);
      } catch (error) {
        console.error('Failed to load chat:', error);
      }
    };
    
    loadChat();
  }, []);
  
  // Auto-resize textarea and scroll to bottom when messages change
  useEffect(() => {
    autoResizeTextarea();
    scrollToBottom();
  }, [messages, inputValue]);
  
  // Focus the textarea when the component loads
  useEffect(() => {
    if (loaded && inputRef.current) {
      inputRef.current.focus();
    }
  }, [loaded]);
  
  // Handle sending a message
  const handleSendMessage = async () => {
    if (!inputValue.trim()) return;
    
    const userMessage: Message = {
      id: generateId(),
      role: 'user',
      content: inputValue,
      timestamp: Date.now(),
    };
    
    setMessages(prev => [...prev, userMessage]);
    setInputValue('');
    setLoading(true);
    
    try {
      // In a real app, we would send the message to the backend
      // const response = await invoke<Message>('send_message', { content: inputValue });
      
      // For now, just simulate a response after a delay
      setTimeout(() => {
        const assistantMessage: Message = {
          id: generateId(),
          role: 'assistant',
          content: `I received your message: "${inputValue}". This is a placeholder response as the MCP backend is not yet connected.`,
          timestamp: Date.now(),
        };
        
        setMessages(prev => [...prev, assistantMessage]);
        setLoading(false);
      }, 1000);
    } catch (error) {
      console.error('Failed to send message:', error);
      setLoading(false);
      
      // Add an error message
      const errorMessage: Message = {
        id: generateId(),
        role: 'system',
        content: 'Failed to send message. Please try again.',
        timestamp: Date.now(),
      };
      
      setMessages(prev => [...prev, errorMessage]);
    }
  };
  
  // Handle textarea input changes
  const handleInputChange = (e: React.ChangeEvent<HTMLTextAreaElement>) => {
    setInputValue(e.target.value);
  };
  
  // Handle keyboard shortcuts
  const handleKeyDown = (e: React.KeyboardEvent<HTMLTextAreaElement>) => {
    // Send message on Enter without Shift
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      handleSendMessage();
    }
  };
  
  if (!loaded) {
    return (
      <div className="chat-container">
        <div className="chat-loading">
          <div className="loading-spinner"></div>
          <p>Loading conversation...</p>
        </div>
      </div>
    );
  }
  
  return (
    <div className="chat-container fade-in">
      <div className="chat-header">
        <h2>Claude MCP</h2>
        <div className="chat-status online">Connected</div>
      </div>
      
      <div className="chat-messages">
        {messages.map((message) => (
          <div 
            key={message.id} 
            className={`message ${message.role}`}
          >
            <div className="message-avatar">
              {message.role === 'user' ? 'U' : message.role === 'assistant' ? 'C' : 'S'}
            </div>
            <div className="message-content-wrapper">
              <div className="message-content">
                {message.content}
              </div>
              <div className="message-time">
                {new Date(message.timestamp).toLocaleTimeString(undefined, {
                  hour: '2-digit',
                  minute: '2-digit',
                })}
              </div>
            </div>
          </div>
        ))}
        {loading && (
          <div className="message assistant loading">
            <div className="message-avatar">C</div>
            <div className="message-content-wrapper">
              <div className="message-content">
                <div className="typing-indicator">
                  <span></span>
                  <span></span>
                  <span></span>
                </div>
              </div>
            </div>
          </div>
        )}
        <div ref={messagesEndRef} />
      </div>
      
      <div className="chat-input-container">
        <textarea 
          ref={inputRef}
          className="chat-input" 
          placeholder="Type a message..."
          value={inputValue}
          onChange={handleInputChange}
          onKeyDown={handleKeyDown}
          rows={1}
        />
        <Button 
          onClick={handleSendMessage}
          disabled={!inputValue.trim() || loading}
          className="send-button"
        >
          Send
        </Button>
      </div>
    </div>
  );
};

export default Chat;
</file>

<file path="src-frontend/src/lazy/Header.css">
.app-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  height: 4rem;
  padding: 0 var(--spacing-lg);
  background-color: var(--color-surface);
  border-bottom: 1px solid var(--color-border);
}

.header-left, .header-right {
  display: flex;
  align-items: center;
  gap: var(--spacing-lg);
}

.app-logo {
  display: flex;
  align-items: center;
  gap: var(--spacing-xs);
}

.logo-icon {
  width: 2rem;
  height: 2rem;
  background-color: var(--color-primary);
  border-radius: var(--radius-md);
  display: flex;
  align-items: center;
  justify-content: center;
}

.logo-text {
  font-size: var(--font-size-lg);
  font-weight: 600;
}

.main-nav {
  display: flex;
  align-items: center;
  gap: var(--spacing-md);
}

.nav-item {
  padding: var(--spacing-xs) var(--spacing-md);
  background: none;
  border: none;
  border-radius: var(--radius-md);
  font-size: var(--font-size-md);
  color: var(--color-on-surface-variant);
  cursor: pointer;
  transition: all var(--transition-fast);
}

.nav-item:hover {
  background-color: var(--color-surface-variant);
  color: var(--color-on-surface);
}

.nav-item.active {
  background-color: var(--color-primary);
  color: white;
}

.command-palette-button {
  display: flex;
  align-items: center;
  gap: var(--spacing-xs);
  padding: var(--spacing-xs) var(--spacing-md);
  background-color: var(--color-surface-variant);
  border: 1px solid var(--color-border);
  border-radius: var(--radius-md);
  color: var(--color-on-surface);
  font-size: var(--font-size-sm);
  cursor: pointer;
  transition: all var(--transition-fast);
}

.command-palette-button:hover {
  background-color: var(--color-background);
  border-color: var(--color-primary);
}

.command-icon {
  font-size: var(--font-size-md);
  font-weight: bold;
}

.keyboard-shortcut {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  min-width: 1.5rem;
  height: 1.25rem;
  padding: 0 var(--spacing-xxs);
  font-size: var(--font-size-xs);
  font-family: var(--font-family);
  background-color: var(--color-background);
  color: var(--color-on-surface-variant);
  border-radius: var(--radius-sm);
  border: 1px solid var(--color-border);
}
</file>

<file path="src-frontend/src/lazy/Header.tsx">
import React from 'react';
import ThemeToggle from '../theme/ThemeToggle';
import useCommandPalette from '../hooks/useCommandPalette';
import './Header.css';

interface HeaderProps {
  currentView: string;
  onViewChange: (view: string) => void;
}

const Header: React.FC<HeaderProps> = ({ currentView, onViewChange }) => {
  const { open: openCommandPalette } = useCommandPalette();
  
  return (
    <header className="app-header">
      <div className="header-left">
        <div className="app-logo">
          <div className="logo-icon"></div>
          <span className="logo-text">Claude MCP</span>
        </div>
        
        <nav className="main-nav">
          <button 
            className={`nav-item ${currentView === 'chat' ? 'active' : ''}`} 
            onClick={() => onViewChange('chat')}
          >
            Chat
          </button>
          <button 
            className={`nav-item ${currentView === 'settings' ? 'active' : ''}`} 
            onClick={() => onViewChange('settings')}
          >
            Settings
          </button>
        </nav>
      </div>
      
      <div className="header-right">
        <button className="command-palette-button" onClick={openCommandPalette}>
          <span className="command-icon">⌘</span>
          <span>Command Palette</span>
          <kbd className="keyboard-shortcut">Ctrl+K</kbd>
        </button>
        
        <ThemeToggle />
      </div>
    </header>
  );
};

export default Header;
</file>

<file path="src-frontend/src/lazy/Sidebar.css">
.sidebar {
  width: 280px;
  height: 100%;
  background-color: var(--color-surface);
  border-right: 1px solid var(--color-border);
  display: flex;
  flex-direction: column;
  overflow: hidden;
}

.sidebar-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: var(--spacing-md) var(--spacing-md);
  border-bottom: 1px solid var(--color-border);
}

.sidebar-header h3 {
  margin: 0;
  font-size: var(--font-size-md);
  font-weight: 600;
}

.new-chat-button {
  padding: var(--spacing-xs) var(--spacing-sm);
}

.conversation-list {
  flex: 1;
  overflow-y: auto;
  padding: var(--spacing-xs);
}

.conversation-item {
  padding: var(--spacing-sm);
  border-radius: var(--radius-md);
  margin-bottom: var(--spacing-xs);
  cursor: pointer;
  transition: background-color var(--transition-fast);
}

.conversation-item:hover {
  background-color: var(--color-surface-variant);
}

.conversation-title {
  font-weight: 500;
  margin-bottom: var(--spacing-xxs);
  color: var(--color-on-surface);
}

.conversation-preview {
  font-size: var(--font-size-sm);
  color: var(--color-on-surface-variant);
  margin-bottom: var(--spacing-xxs);
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}

.conversation-time {
  font-size: var(--font-size-xs);
  color: var(--color-on-surface-variant);
  opacity: 0.8;
}

.sidebar-nav {
  display: flex;
  flex-direction: column;
  padding: var(--spacing-xs);
}

.sidebar-nav-item {
  text-align: left;
  padding: var(--spacing-sm) var(--spacing-md);
  background: none;
  border: none;
  border-radius: var(--radius-md);
  font-size: var(--font-size-md);
  color: var(--color-on-surface-variant);
  cursor: pointer;
  transition: all var(--transition-fast);
  margin-bottom: var(--spacing-xxs);
}

.sidebar-nav-item:hover {
  background-color: var(--color-surface-variant);
  color: var(--color-on-surface);
}

.sidebar-nav-item.active {
  background-color: var(--color-primary);
  color: white;
}
</file>

<file path="src-frontend/src/lazy/Sidebar.tsx">
import React, { useState } from 'react';
import { Button } from '../components/ui/Button';
import './Sidebar.css';

interface SidebarProps {
  currentView: string;
}

// Define a conversation type for chat history
interface Conversation {
  id: string;
  title: string;
  timestamp: number;
  preview: string;
}

const Sidebar: React.FC<SidebarProps> = ({ currentView }) => {
  const [conversations, setConversations] = useState<Conversation[]>([
    {
      id: 'conv_1',
      title: 'Getting Started with MCP',
      timestamp: Date.now() - 1000 * 60 * 15, // 15 minutes ago
      preview: 'Learn about the Model Context Protocol...',
    },
    {
      id: 'conv_2',
      title: 'API Integration',
      timestamp: Date.now() - 1000 * 60 * 60 * 2, // 2 hours ago
      preview: 'How to integrate with Claude API...',
    },
    {
      id: 'conv_3',
      title: 'Streaming vs Non-Streaming',
      timestamp: Date.now() - 1000 * 60 * 60 * 24, // 1 day ago
      preview: 'Comparing streaming and non-streaming modes...',
    },
  ]);
  
  // Show appropriate sidebar content based on current view
  if (currentView === 'settings') {
    return (
      <aside className="sidebar">
        <div className="sidebar-header">
          <h3>Settings</h3>
        </div>
        <nav className="sidebar-nav">
          <button className="sidebar-nav-item active">General</button>
          <button className="sidebar-nav-item">API Keys</button>
          <button className="sidebar-nav-item">Models</button>
          <button className="sidebar-nav-item">Appearance</button>
          <button className="sidebar-nav-item">Advanced</button>
        </nav>
      </aside>
    );
  }
  
  // Default chat sidebar
  return (
    <aside className="sidebar">
      <div className="sidebar-header">
        <h3>Conversations</h3>
        <Button 
          variant="primary"
          size="sm"
          className="new-chat-button"
        >
          New Chat
        </Button>
      </div>
      
      <div className="conversation-list">
        {conversations.map((conversation) => (
          <div key={conversation.id} className="conversation-item">
            <div className="conversation-title">{conversation.title}</div>
            <div className="conversation-preview">{conversation.preview}</div>
            <div className="conversation-time">
              {formatTime(conversation.timestamp)}
            </div>
          </div>
        ))}
      </div>
    </aside>
  );
};

// Helper function to format timestamps
const formatTime = (timestamp: number): string => {
  const now = Date.now();
  const diff = now - timestamp;
  
  // Less than a minute
  if (diff < 1000 * 60) {
    return 'Just now';
  }
  
  // Less than an hour
  if (diff < 1000 * 60 * 60) {
    const minutes = Math.floor(diff / (1000 * 60));
    return `${minutes}m ago`;
  }
  
  // Less than a day
  if (diff < 1000 * 60 * 60 * 24) {
    const hours = Math.floor(diff / (1000 * 60 * 60));
    return `${hours}h ago`;
  }
  
  // Otherwise show the date
  return new Date(timestamp).toLocaleDateString();
};

export default Sidebar;
</file>

<file path="src-tauri/src/main.rs">
#![cfg_attr(
    all(not(debug_assertions), target_os = "windows"),
    windows_subsystem = "windows"
)]

mod commands;
mod monitoring;
mod services;
mod offline;

use std::collections::HashMap;
use std::sync::Once;
use std::time::Instant;
use chrono::Utc;

// Import observability modules
use mcp_client::observability::{
    metrics::{init_metrics, ObservabilityConfig as MetricsConfig, record_counter},
    logging::{init_logger, LogLevel},
    telemetry::{TelemetryClient, TelemetryEventType},
    canary::CANARY_SERVICE,
};

// Import feature flags
use mcp_client::feature_flags::{
    FEATURE_FLAG_MANAGER, FeatureFlag, RolloutStrategy,
    CANARY_GROUP_ALPHA, CANARY_GROUP_BETA, CANARY_GROUP_EARLY_ACCESS,
    FLAG_ADVANCED_TELEMETRY, FLAG_PERFORMANCE_DASHBOARD, FLAG_DEBUG_LOGGING,
    FLAG_RESOURCE_MONITORING, FLAG_CRASH_REPORTING
};

// Import monitoring tools
use monitoring::resources::RESOURCE_MONITOR;

// For startup performance tracking
static mut APP_START_TIME: Option<Instant> = None;
static START_TIME_INIT: Once = Once::new();

fn main() {
    // Record application start time
    unsafe {
        START_TIME_INIT.call_once(|| {
            APP_START_TIME = Some(Instant::now());
        });
    }

    // Initialize observability configuration
    let app_data_dir = app_dirs::app_dir(
        app_dirs::AppDataType::UserData,
        &app_info::AppInfo {
            name: "mcp-client",
            author: "acme",
        },
        "logs",
    )
    .expect("Failed to get app data directory");
    
    let log_file_path = app_data_dir.join("mcp-client.log");
    
    // Configure metrics
    let metrics_config = MetricsConfig {
        metrics_enabled: true,
        sampling_rate: 0.1, // 10% sampling rate
        buffer_size: 100,
        min_log_level: Some(LogLevel::Info as u8),
        log_file_path: Some(log_file_path.to_str().unwrap().to_string()),
        console_logging: Some(true),
        telemetry_enabled: Some(false), // Start with telemetry disabled, require opt-in
        log_telemetry: Some(false),
    };
    
    // Initialize metrics system
    init_metrics(&metrics_config);
    
    // Initialize logger
    init_logger(&metrics_config);
    
    // Log application start
    log_info!("main", "MCP Client starting up");
    
    // Get telemetry client
    let telemetry_client = TelemetryClient::get_instance();
    
    // Register shutdown handler
    register_shutdown_handler(telemetry_client.clone());
    
    // Initialize feature flags
    initialize_feature_flags();
    
    // Track startup as a telemetry event
    std::thread::spawn(move || {
        telemetry_client.track_event(
            TelemetryEventType::ApplicationStart,
            "app_start",
            None,
            Some(HashMap::from([
                ("version".to_string(), env!("CARGO_PKG_VERSION").to_string()),
                ("os".to_string(), std::env::consts::OS.to_string()),
            ])),
        );
    });

    // Build Tauri application
    tauri::Builder::default()
        .invoke_handler(tauri::generate_handler![
            // Resource monitoring commands
            monitoring::resources::get_resource_metrics,
            monitoring::resources::get_system_info,
            monitoring::resources::update_resource_metrics,
            monitoring::resources::get_uptime,
            monitoring::resources::report_startup_time,
            monitoring::resources::report_frame_rate,
            monitoring::resources::report_resource_metrics,
            monitoring::resources::report_page_metrics,
            
            // Logging commands
            mcp_client::observability::logging::get_recent_logs,
            mcp_client::observability::logging::export_logs,
            
            // Telemetry commands
            mcp_client::observability::telemetry::get_telemetry_config,
            mcp_client::observability::telemetry::update_telemetry_config,
            mcp_client::observability::telemetry::delete_telemetry_data,
            
            // Feature flag commands
            mcp_client::feature_flags::get_feature_flags,
            mcp_client::feature_flags::toggle_feature_flag,
            mcp_client::feature_flags::create_feature_flag,
            mcp_client::feature_flags::delete_feature_flag,
            
            // Canary release commands
            mcp_client::observability::canary::get_canary_groups,
            mcp_client::observability::canary::get_user_canary_group,
            mcp_client::observability::canary::opt_into_canary_group,
            mcp_client::observability::canary::opt_out_of_canary_group,
            mcp_client::observability::canary::toggle_canary_group,
            mcp_client::observability::canary::update_canary_percentage,
            mcp_client::observability::canary::get_canary_metrics,
            mcp_client::observability::canary::promote_canary_feature,
            mcp_client::observability::canary::rollback_canary_feature,
            mcp_client::observability::canary::create_canary_feature,
            mcp_client::observability::canary::toggle_canary_feature,
            
            // Offline LLM commands
            commands::offline::configure_llm,
            commands::offline::list_available_models,
            commands::offline::list_downloaded_models,
            commands::offline::get_model_info,
            commands::offline::download_model,
            commands::offline::get_download_status,
            commands::offline::is_model_loaded,
            commands::offline::load_model,
            commands::offline::delete_model,
            commands::offline::generate_text,
            commands::offline::check_network,
            commands::offline::get_offline_status,
            commands::offline::set_offline_mode
        ])
        .setup(|app| {
            // Register offline commands
            if let Err(e) = commands::offline::register_commands(app) {
                log_error!("main", "Failed to register offline commands: {}", e);
            }
        
            // Start resource monitor if feature is enabled
            if mcp_client::feature_enabled!(FLAG_RESOURCE_MONITORING) {
                let monitor = RESOURCE_MONITOR.lock().unwrap();
                monitor.start(1000); // Update every second
            }
            
            // Store app init time in window for frontend to access
            #[cfg(feature = "frontend")]
            {
                unsafe {
                    if let Some(start_time) = APP_START_TIME {
                        let elapsed = start_time.elapsed().as_millis() as f64;
                        
                        // Record startup time metric
                        let mut tags = HashMap::new();
                        tags.insert("type".to_string(), "backend".to_string());
                        record_counter("startup_time", elapsed, Some(tags));
                        
                        // Add to window for frontend to access
                        app.set_window_limits(app.get_window("main").unwrap(), None, None);
                        let window = app.get_window("main").unwrap();
                        window.eval(&format!("window.__APP_INIT_TIME__ = {}", elapsed)).unwrap();
                    }
                }
            }
            
            Ok(())
        })
        .run(tauri::generate_context!())
        .expect("error while running tauri application");
}

// Initialize feature flags
fn initialize_feature_flags() {
    let feature_flags = vec![
        FeatureFlag {
            id: FLAG_ADVANCED_TELEMETRY.to_string(),
            name: "Advanced Telemetry".to_string(),
            description: "Enable advanced telemetry collection for detailed performance insights".to_string(),
            enabled: true,
            rollout_strategy: RolloutStrategy::CanaryGroup(CANARY_GROUP_ALPHA.to_string(), 0.5),
            dependencies: vec![],
            created_at: Utc::now().timestamp(),
            updated_at: Utc::now().timestamp(),
            metadata: HashMap::new(),
        },
        FeatureFlag {
            id: FLAG_PERFORMANCE_DASHBOARD.to_string(),
            name: "Performance Dashboard".to_string(),
            description: "Access to the performance monitoring dashboard".to_string(),
            enabled: true,
            rollout_strategy: RolloutStrategy::CanaryGroup(CANARY_GROUP_BETA.to_string(),
            1.0),
            dependencies: vec![],
            created_at: Utc::now().timestamp(),
            updated_at: Utc::now().timestamp(),
            metadata: HashMap::new(),
        },
        FeatureFlag {
            id: FLAG_DEBUG_LOGGING.to_string(),
            name: "Debug Logging".to_string(),
            description: "Enable verbose debug logging for troubleshooting".to_string(),
            enabled: true,
            rollout_strategy: RolloutStrategy::CanaryGroup(CANARY_GROUP_ALPHA.to_string(), 1.0),
            dependencies: vec![],
            created_at: Utc::now().timestamp(),
            updated_at: Utc::now().timestamp(),
            metadata: HashMap::new(),
        },
        FeatureFlag {
            id: FLAG_RESOURCE_MONITORING.to_string(),
            name: "Resource Monitoring".to_string(),
            description: "Monitor system resource usage for the application".to_string(),
            enabled: true,
            rollout_strategy: RolloutStrategy::AllUsers,
            dependencies: vec![],
            created_at: Utc::now().timestamp(),
            updated_at: Utc::now().timestamp(),
            metadata: HashMap::new(),
        },
        FeatureFlag {
            id: FLAG_CRASH_REPORTING.to_string(),
            name: "Crash Reporting".to_string(),
            description: "Automatically send crash reports for analysis".to_string(),
            enabled: true,
            rollout_strategy: RolloutStrategy::PercentageRollout(0.5),
            dependencies: vec![],
            created_at: Utc::now().timestamp(),
            updated_at: Utc::now().timestamp(),
            metadata: HashMap::new(),
        },
    ];
    
    FEATURE_FLAG_MANAGER.load_flags(feature_flags);
    
    // Add features to canary groups
    let canary_service = CANARY_SERVICE.clone();
    let mut groups = canary_service.get_canary_groups();
    
    // Update alpha group
    if let Some(alpha_group) = groups.iter_mut().find(|g| g.name == CANARY_GROUP_ALPHA) {
        alpha_group.active_features = vec![
            FLAG_ADVANCED_TELEMETRY.to_string(),
            FLAG_DEBUG_LOGGING.to_string(),
        ];
    }
    
    // Update beta group
    if let Some(beta_group) = groups.iter_mut().find(|g| g.name == CANARY_GROUP_BETA) {
        beta_group.active_features = vec![
            FLAG_PERFORMANCE_DASHBOARD.to_string(),
        ];
    }
}

// Register shutdown handler
fn register_shutdown_handler(telemetry_client: Arc<TelemetryClient>) {
    ctrlc::set_handler(move || {
        // Log application exit
        log_info!("main", "MCP Client shutting down");
        
        // Track exit event
        telemetry_client.track_event(
            TelemetryEventType::ApplicationExit,
            "app_exit",
            None,
            None,
        );
        
        // Give telemetry a chance to send final batch
        std::thread::sleep(std::time::Duration::from_millis(500));
        
        // Exit application
        std::process::exit(0);
    }).expect("Error setting Ctrl-C handler");
}
</file>

<file path="src-tauri/tauri.conf.json">
{
  "build": {
    "beforeDevCommand": "npm run dev",
    "beforeBuildCommand": "npm run build",
    "devPath": "http://localhost:1420",
    "distDir": "../dist",
    "withGlobalTauri": false
  },
  "package": {
    "productName": "Papin",
    "version": "1.0.0"
  },
  "tauri": {
    "allowlist": {
      "all": false,
      "shell": {
        "all": false,
        "open": true
      },
      "dialog": {
        "all": true
      },
      "http": {
        "all": true,
        "request": true,
        "scope": ["https://**", "http://**"]
      },
      "fs": {
        "all": true,
        "scope": ["$APPDATA/**", "$APPCONFIG/**", "$APPLOCAL/**", "$DESKTOP/**", "$DOCUMENT/**"]
      },
      "protocol": {
        "all": false,
        "asset": true,
        "assetScope": ["**"]
      }
    },
    "bundle": {
      "active": true,
      "icon": [
        "icons/32x32.png",
        "icons/128x128.png",
        "icons/128x128@2x.png",
        "icons/icon.icns",
        "icons/icon.ico"
      ],
      "identifier": "com.mcp-client.app",
      "targets": ["deb", "rpm", "appimage", "msi", "dmg", "updater"],
      "publisher": "MCP Team"
    },
    "security": {
      "csp": "default-src 'self'; connect-src 'self' https://api.mcp-client.com https://update.mcp-client.com"
    },
    "updater": {
      "active": true,
      "endpoints": [
        "https://update.mcp-client.com/{{target}}/{{current_version}}"
      ],
      "dialog": true,
      "pubkey": "dW50cnVzdGVkIGNvbW1lbnQ6IG1pbmlzaWduIHB1YmxpYyBrZXk6IEMxNEYzODkyRjVCQjk3NjUKUldRTVVLVldVdXRrNC9WVklSVmorenBFODZIajVhUG16NnRKU2xEZ1JhRk9oNFpyRklBUkFBQUIKCg=="
    },
    "windows": [
      {
        "title": "Papin - an MCP Client",
        "width": 1200,
        "height": 800,
        "resizable": true,
        "fullscreen": false,
        "center": true
      }
    ]
  }
}
</file>

<file path="src-tui/src/error.rs">
use thiserror::Error;

/// Application error types
#[derive(Debug, Error)]
pub enum AppError {
    /// IO error
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
    
    /// MCP service error
    #[error("Service error: {0}")]
    Service(String),
    
    /// Application logic error
    #[error("Application error: {0}")]
    App(String),
}

/// Convert MCP error to AppError
impl From<mcp_common::error::McpError> for AppError {
    fn from(error: mcp_common::error::McpError) -> Self {
        AppError::Service(error.to_string())
    }
}
</file>

<file path="src-tui/src/event.rs">
use std::time::Duration;
use crossterm::event::{self, Event as CrosstermEvent, KeyEvent, MouseEvent};
use anyhow::Result;

/// Terminal events
#[derive(Debug, Clone)]
pub enum Event {
    /// Terminal tick
    Tick,
    /// Key press
    Key(KeyEvent),
    /// Mouse click/scroll
    Mouse(MouseEvent),
    /// Terminal resize
    Resize(u16, u16),
}

/// Event handler
pub struct EventHandler {
    /// Polling interval
    tick_rate: Duration,
}

impl EventHandler {
    /// Create a new event handler with the specified tick rate
    pub fn new(tick_rate: Duration) -> Self {
        Self { tick_rate }
    }
    
    /// Get the next event (blocking with timeout)
    pub fn next(&self) -> Result<Event> {
        if event::poll(self.tick_rate)? {
            match event::read()? {
                CrosstermEvent::Key(key) => Ok(Event::Key(key)),
                CrosstermEvent::Mouse(mouse) => Ok(Event::Mouse(mouse)),
                CrosstermEvent::Resize(width, height) => Ok(Event::Resize(width, height)),
                CrosstermEvent::FocusGained => self.next(),
                CrosstermEvent::FocusLost => self.next(),
                CrosstermEvent::Paste(_) => self.next(),
            }
        } else {
            Ok(Event::Tick)
        }
    }
}
</file>

<file path="src-tui/src/main.rs">
mod app;
mod error;
mod event;
mod ui;
mod util;

use std::sync::Arc;
use std::time::Duration;
use anyhow::Result;
use crossterm::{
    event::{DisableMouseCapture, EnableMouseCapture},
    execute,
    terminal::{disable_raw_mode, enable_raw_mode, EnterAlternateScreen, LeaveAlternateScreen},
};
use ratatui::backend::CrosstermBackend;
use ratatui::Terminal;

use app::{App, AppResult};
use event::{Event, EventHandler};
use mcp_common::{get_mcp_service, init_mcp_service, service::ChatService};

// Entry point
#[tokio::main]
async fn main() -> Result<()> {
    // Initialize logging
    env_logger::init();
    
    // Set up terminal
    enable_raw_mode()?;
    let mut stdout = std::io::stdout();
    execute!(stdout, EnterAlternateScreen, EnableMouseCapture)?;
    let backend = CrosstermBackend::new(stdout);
    let mut terminal = Terminal::new(backend)?;
    
    // Initialize services
    let mcp_service = init_mcp_service();
    let chat_service = Arc::new(ChatService::new(mcp_service));
    
    // Create app and run it
    let app = App::new(chat_service);
    let res = run_app(&mut terminal, app).await;
    
    // Restore terminal
    disable_raw_mode()?;
    execute!(
        terminal.backend_mut(),
        LeaveAlternateScreen,
        DisableMouseCapture
    )?;
    terminal.show_cursor()?;
    
    // Print any error
    if let Err(err) = res {
        eprintln!("{}", err);
    }
    
    Ok(())
}

// Run the application
async fn run_app<B: ratatui::backend::Backend>(
    terminal: &mut Terminal<B>,
    mut app: App,
) -> AppResult<()> {
    // Create an event handler
    let mut event_handler = EventHandler::new(Duration::from_millis(100));
    
    // Initialize the app
    app.initialize().await?;
    
    // Main loop
    loop {
        // Render the UI
        terminal.draw(|f| ui::draw(f, &app))?;
        
        // Handle events
        match event_handler.next()? {
            Event::Tick => {
                app.tick();
            }
            Event::Key(key_event) => {
                // Pass the key event to the app
                if app.handle_key_event(key_event).await? {
                    // If the app returns true, exit the application
                    return Ok(());
                }
            }
            Event::Mouse(mouse_event) => {
                app.handle_mouse_event(mouse_event);
            }
            Event::Resize(width, height) => {
                app.resize(width, height);
            }
        }
    }
}
</file>

<file path="src/feature_flags.rs">
use bitflags::bitflags;
use serde::{Deserialize, Serialize};
use std::str::FromStr;

bitflags! {
    /// Feature flags to control application behavior and enable/disable features
    #[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
    pub struct FeatureFlags: u32 {
        /// Enable experimental features (may be unstable)
        const EXPERIMENTAL = 0b0000_0000_0001;
        
        /// Enable development-only features (for testing)
        const DEV_FEATURES = 0b0000_0000_0010;
        
        /// Enable lazy loading of non-essential components
        const LAZY_LOAD = 0b0000_0000_0100;
        
        /// Enable plugin system
        const PLUGINS = 0b0000_0000_1000;
        
        /// Enable conversation history features
        const HISTORY = 0b0000_0001_0000;
        
        /// Enable advanced UI components
        const ADVANCED_UI = 0b0000_0010_0000;
        
        /// Enable analytics and telemetry
        const ANALYTICS = 0b0000_0100_0000;
        
        /// Enable auto-updates
        const AUTO_UPDATE = 0b0000_1000_0000;
        
        /// Enable real-time collaboration features
        const COLLABORATION = 0b0001_0000_0000;
        
        /// Default configuration for production builds
        const DEFAULT = Self::LAZY_LOAD.bits() | Self::PLUGINS.bits() | 
                        Self::HISTORY.bits() | Self::ADVANCED_UI.bits() | 
                        Self::AUTO_UPDATE.bits() | Self::COLLABORATION.bits();
        
        /// Minimal configuration for fastest startup
        const MINIMAL = Self::LAZY_LOAD.bits();
    }
}

impl Default for FeatureFlags {
    fn default() -> Self {
        FeatureFlags::DEFAULT
    }
}

impl FromStr for FeatureFlags {
    type Err = String;

    fn from_str(s: &str) -> Result<Self, Self::Err> {
        let mut flags = FeatureFlags::empty();
        
        for flag_str in s.split(',') {
            let flag_str = flag_str.trim().to_uppercase();
            match flag_str.as_str() {
                "EXPERIMENTAL" => flags |= FeatureFlags::EXPERIMENTAL,
                "DEV" | "DEV_FEATURES" => flags |= FeatureFlags::DEV_FEATURES,
                "LAZY_LOAD" => flags |= FeatureFlags::LAZY_LOAD,
                "PLUGINS" => flags |= FeatureFlags::PLUGINS,
                "HISTORY" => flags |= FeatureFlags::HISTORY,
                "ADVANCED_UI" => flags |= FeatureFlags::ADVANCED_UI,
                "ANALYTICS" => flags |= FeatureFlags::ANALYTICS,
                "AUTO_UPDATE" => flags |= FeatureFlags::AUTO_UPDATE,
                "COLLABORATION" => flags |= FeatureFlags::COLLABORATION,
                "DEFAULT" => flags |= FeatureFlags::DEFAULT,
                "MINIMAL" => flags |= FeatureFlags::MINIMAL,
                "" => continue,
                _ => return Err(format!("Unknown feature flag: {}", flag_str)),
            }
        }
        
        Ok(flags)
    }
}

/// FeatureManager handles the runtime management of feature flags
pub struct FeatureManager {
    flags: FeatureFlags,
}

impl FeatureManager {
    /// Create a new feature manager with the given flags
    pub fn new(flags: FeatureFlags) -> Self {
        FeatureManager { flags }
    }
    
    /// Create a new feature manager with default flags
    pub fn default() -> Self {
        FeatureManager { flags: FeatureFlags::default() }
    }
    
    /// Check if a feature is enabled
    pub fn is_enabled(&self, feature: FeatureFlags) -> bool {
        self.flags.contains(feature)
    }
    
    /// Enable a feature
    pub fn enable(&mut self, feature: FeatureFlags) {
        self.flags |= feature;
    }
    
    /// Disable a feature
    pub fn disable(&mut self, feature: FeatureFlags) {
        self.flags &= !feature;
    }
    
    /// Get the current feature flags
    pub fn flags(&self) -> FeatureFlags {
        self.flags
    }
    
    /// Load feature flags from environment
    pub fn from_env() -> Self {
        let env_flags = std::env::var("CLAUDE_MCP_FEATURES").unwrap_or_default();
        let flags = FeatureFlags::from_str(&env_flags).unwrap_or_else(|e| {
            eprintln!("Error parsing feature flags: {}", e);
            FeatureFlags::default()
        });
        
        Self::new(flags)
    }
    
    /// Get a minimal configuration for fastest startup
    pub fn minimal() -> Self {
        Self::new(FeatureFlags::MINIMAL)
    }
}

/// Helper function for reading features from config
pub fn parse_feature_config(config_str: &str) -> FeatureFlags {
    FeatureFlags::from_str(config_str).unwrap_or_default()
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_default_flags() {
        let flags = FeatureFlags::default();
        assert!(flags.contains(FeatureFlags::LAZY_LOAD));
        assert!(flags.contains(FeatureFlags::PLUGINS));
        assert!(flags.contains(FeatureFlags::HISTORY));
        assert!(flags.contains(FeatureFlags::ADVANCED_UI));
        assert!(flags.contains(FeatureFlags::AUTO_UPDATE));
        assert!(flags.contains(FeatureFlags::COLLABORATION));
        
        assert!(!flags.contains(FeatureFlags::EXPERIMENTAL));
        assert!(!flags.contains(FeatureFlags::DEV_FEATURES));
    }
    
    #[test]
    fn test_minimal_flags() {
        let flags = FeatureFlags::MINIMAL;
        assert!(flags.contains(FeatureFlags::LAZY_LOAD));
        
        assert!(!flags.contains(FeatureFlags::PLUGINS));
        assert!(!flags.contains(FeatureFlags::HISTORY));
        assert!(!flags.contains(FeatureFlags::ADVANCED_UI));
        assert!(!flags.contains(FeatureFlags::AUTO_UPDATE));
        assert!(!flags.contains(FeatureFlags::COLLABORATION));
        assert!(!flags.contains(FeatureFlags::EXPERIMENTAL));
        assert!(!flags.contains(FeatureFlags::DEV_FEATURES));
    }
    
    #[test]
    fn test_from_str() {
        let flags = FeatureFlags::from_str("EXPERIMENTAL,LAZY_LOAD,COLLABORATION").unwrap();
        assert!(flags.contains(FeatureFlags::EXPERIMENTAL));
        assert!(flags.contains(FeatureFlags::LAZY_LOAD));
        assert!(flags.contains(FeatureFlags::COLLABORATION));
        assert!(!flags.contains(FeatureFlags::PLUGINS));
    }
    
    #[test]
    fn test_feature_manager() {
        let mut manager = FeatureManager::default();
        assert!(manager.is_enabled(FeatureFlags::LAZY_LOAD));
        assert!(manager.is_enabled(FeatureFlags::COLLABORATION));
        
        manager.disable(FeatureFlags::COLLABORATION);
        assert!(!manager.is_enabled(FeatureFlags::COLLABORATION));
        
        manager.enable(FeatureFlags::EXPERIMENTAL);
        assert!(manager.is_enabled(FeatureFlags::EXPERIMENTAL));
    }
}
</file>

<file path="src/offline/llm/mod.rs">
use std::path::PathBuf;
use std::sync::{Arc, Mutex};
use log::{info, warn, error, debug};
use serde::{Serialize, Deserialize};
use std::time::{Duration, Instant};
use std::collections::HashMap;

// LLM discovery subsystem
pub mod discovery;

// LLM migration subsystem
pub mod migration;

/// Configuration for a local LLM
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMConfig {
    /// Model identifier
    pub model_id: String,
    /// Path to the model files
    pub model_path: PathBuf,
    /// Context size in tokens
    pub context_size: usize,
    /// Maximum output length in tokens
    pub max_output_length: usize,
    /// Inference parameters
    pub parameters: LLMParameters,
    /// Whether the model is enabled
    pub enabled: bool,
    /// Model memory usage in MB
    pub memory_usage_mb: usize,
}

/// Parameters for LLM inference
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMParameters {
    /// Temperature for sampling
    pub temperature: f32,
    /// Top-p sampling value
    pub top_p: f32,
    /// Top-k sampling value
    pub top_k: usize,
    /// Repetition penalty
    pub repetition_penalty: f32,
    /// Whether to use mirostat sampling
    pub use_mirostat: bool,
    /// Mirostat tau value
    pub mirostat_tau: f32,
    /// Mirostat eta value
    pub mirostat_eta: f32,
}

impl Default for LLMParameters {
    fn default() -> Self {
        Self {
            temperature: 0.7,
            top_p: 0.9,
            top_k: 40,
            repetition_penalty: 1.1,
            use_mirostat: false,
            mirostat_tau: 5.0,
            mirostat_eta: 0.1,
        }
    }
}

/// Information about an available model
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelInfo {
    /// Model identifier
    pub id: String,
    /// Model name for display
    pub name: String,
    /// Model size in MB
    pub size_mb: usize,
    /// Model context size in tokens
    pub context_size: usize,
    /// Whether the model is installed
    pub installed: bool,
    /// Model download URL
    pub download_url: Option<String>,
    /// Model description
    pub description: String,
}

/// Status of a model download
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DownloadStatus {
    /// Model identifier
    pub model_id: String,
    /// Download progress (0.0 to 1.0)
    pub progress: f32,
    /// Bytes downloaded
    pub bytes_downloaded: usize,
    /// Total bytes to download
    pub total_bytes: usize,
    /// Download speed in bytes per second
    pub speed_bps: usize,
    /// Estimated time remaining in seconds
    pub eta_seconds: u64,
    /// Whether the download is complete
    pub complete: bool,
    /// Error message if download failed
    pub error: Option<String>,
}

/// Local LLM handler for offline inference
pub struct LocalLLM {
    /// Model identifier
    pub name: String,
    /// Context size in tokens
    pub context_size: usize,
    /// Processing speed (tokens per second)
    pub speed: usize,
    /// Current configuration
    config: Arc<Mutex<LLMConfig>>,
    /// Available models
    available_models: Arc<Mutex<HashMap<String, ModelInfo>>>,
    /// Active downloads
    downloads: Arc<Mutex<HashMap<String, DownloadStatus>>>,
}

impl LocalLLM {
    /// Create a new local LLM manager
    pub fn new_manager() -> Self {
        // Default configuration
        let config = LLMConfig {
            model_id: "default".to_string(),
            model_path: PathBuf::from("models/default"),
            context_size: 4096,
            max_output_length: 2048,
            parameters: LLMParameters::default(),
            enabled: true,
            memory_usage_mb: 512,
        };
        
        // Default available models
        let mut available_models = HashMap::new();
        available_models.insert("small".to_string(), ModelInfo {
            id: "small".to_string(),
            name: "Small (512MB)".to_string(),
            size_mb: 512,
            context_size: 2048,
            installed: true,
            download_url: None,
            description: "Small model for basic tasks. Fast but limited capabilities.".to_string(),
        });
        
        available_models.insert("medium".to_string(), ModelInfo {
            id: "medium".to_string(),
            name: "Medium (1.5GB)".to_string(),
            size_mb: 1536,
            context_size: 4096,
            installed: true,
            download_url: None,
            description: "Medium model balancing performance and quality.".to_string(),
        });
        
        available_models.insert("large".to_string(), ModelInfo {
            id: "large".to_string(),
            name: "Large (4GB)".to_string(),
            size_mb: 4096,
            context_size: 8192,
            installed: false,
            download_url: Some("https://models.mcp-client.com/large-v1.0".to_string()),
            description: "Large model for advanced tasks. High quality but slower.".to_string(),
        });
        
        Self {
            name: "manager".to_string(),
            context_size: 4096,
            speed: 1000,
            config: Arc::new(Mutex::new(config)),
            available_models: Arc::new(Mutex::new(available_models)),
            downloads: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    /// Create a new local LLM
    pub fn new(name: &str, context_size: usize, speed: usize) -> Self {
        // Default configuration
        let config = LLMConfig {
            model_id: name.to_string(),
            model_path: PathBuf::from(format!("models/{}", name)),
            context_size,
            max_output_length: context_size / 2,
            parameters: LLMParameters::default(),
            enabled: true,
            memory_usage_mb: context_size / 8, // Rough estimate
        };
        
        Self {
            name: name.to_string(),
            context_size,
            speed,
            config: Arc::new(Mutex::new(config)),
            available_models: Arc::new(Mutex::new(HashMap::new())),
            downloads: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    /// Generate text based on the input
    pub fn generate(&self, input: &str, output_tokens: usize) -> String {
        debug!("Generating text with model {}", self.name);
        
        // Get configuration
        let config = self.config.lock().unwrap();
        if !config.enabled {
            warn!("Model {} is disabled", self.name);
            return "Model is disabled".to_string();
        }
        
        // Simulate inference based on model speed
        let tokens_per_second = self.speed;
        let input_tokens = input.split_whitespace().count();
        let total_tokens = input_tokens + output_tokens;
        
        if total_tokens > self.context_size {
            warn!("Input + output ({}) exceeds context size ({})", total_tokens, self.context_size);
            return "Input is too long for the model's context size".to_string();
        }
        
        // Simulate token processing time
        let estimated_time = output_tokens as f32 / tokens_per_second as f32;
        let sleep_duration = Duration::from_secs_f32(estimated_time);
        
        info!("Generating {} tokens (estimated time: {:.2}s)", output_tokens, estimated_time);
        let start = Instant::now();
        
        // Simulate inference process
        std::thread::sleep(sleep_duration);
        
        // Generate mock output
        let output = format!("Generated response from the {} model.\n\n", self.name);
        let output = output + &"This is a simulated response for testing purposes. ".repeat(output_tokens / 10 + 1);
        
        let elapsed = start.elapsed();
        debug!("Generation completed in {:.2?} ({:.2} tokens/second)", 
               elapsed, 
               output_tokens as f32 / elapsed.as_secs_f32());
        
        output
    }
    
    /// Get the list of available models
    pub fn list_models(&self) -> Vec<ModelInfo> {
        self.available_models.lock().unwrap().values().cloned().collect()
    }
    
    /// Get information about a specific model
    pub fn get_model_info(&self, model_id: &str) -> Option<ModelInfo> {
        self.available_models.lock().unwrap().get(model_id).cloned()
    }
    
    /// Download a model
    pub fn download_model(&self, model_id: &str) -> Result<String, String> {
        let models = self.available_models.lock().unwrap();
        
        // Check if model exists
        if let Some(model) = models.get(model_id) {
            if model.installed {
                return Err(format!("Model {} is already installed", model_id));
            }
            
            if model.download_url.is_none() {
                return Err(format!("Model {} has no download URL", model_id));
            }
            
            // Create download status
            let download_id = format!("download_{}", model_id);
            let status = DownloadStatus {
                model_id: model_id.to_string(),
                progress: 0.0,
                bytes_downloaded: 0,
                total_bytes: model.size_mb * 1024 * 1024,
                speed_bps: 0,
                eta_seconds: 0,
                complete: false,
                error: None,
            };
            
            // Start download task
            let model_id = model_id.to_string();
            let download_id_clone = download_id.clone();
            let downloads = self.downloads.clone();
            let available_models = self.available_models.clone();
            
            {
                let mut downloads = downloads.lock().unwrap();
                downloads.insert(download_id.clone(), status);
            }
            
            // Simulate download in a separate thread
            std::thread::spawn(move || {
                let model_size_bytes = {
                    let models = available_models.lock().unwrap();
                    models.get(&model_id).unwrap().size_mb * 1024 * 1024
                };
                
                // Simulate download speed (1-5 MB/s)
                let download_speed = rand::random::<usize>() % 4000000 + 1000000;
                let download_time_seconds = model_size_bytes / download_speed;
                let update_interval = Duration::from_millis(500);
                let steps = (download_time_seconds * 1000 / 500) as usize;
                let bytes_per_step = model_size_bytes / steps;
                
                let mut bytes_downloaded = 0;
                
                for i in 0..steps {
                    // Update download progress
                    bytes_downloaded += bytes_per_step;
                    if bytes_downloaded > model_size_bytes {
                        bytes_downloaded = model_size_bytes;
                    }
                    
                    let progress = bytes_downloaded as f32 / model_size_bytes as f32;
                    let eta = (model_size_bytes - bytes_downloaded) / download_speed;
                    
                    // Update status
                    {
                        let mut downloads = downloads.lock().unwrap();
                        if let Some(status) = downloads.get_mut(&download_id_clone) {
                            status.progress = progress;
                            status.bytes_downloaded = bytes_downloaded;
                            status.speed_bps = download_speed;
                            status.eta_seconds = eta as u64;
                        } else {
                            // Download was cancelled
                            return;
                        }
                    }
                    
                    // Sleep for update interval
                    std::thread::sleep(update_interval);
                }
                
                // Mark download as complete
                {
                    let mut downloads = downloads.lock().unwrap();
                    if let Some(status) = downloads.get_mut(&download_id_clone) {
                        status.progress = 1.0;
                        status.bytes_downloaded = model_size_bytes;
                        status.complete = true;
                    }
                }
                
                // Update model installation status
                {
                    let mut models = available_models.lock().unwrap();
                    if let Some(model) = models.get_mut(&model_id) {
                        model.installed = true;
                    }
                }
                
                // Clean up download status after a delay
                std::thread::sleep(Duration::from_secs(10));
                {
                    let mut downloads = downloads.lock().unwrap();
                    downloads.remove(&download_id_clone);
                }
            });
            
            Ok(download_id)
        } else {
            Err(format!("Model {} not found", model_id))
        }
    }
    
    /// Get the status of a model download
    pub fn get_download_status(&self, download_id: &str) -> Option<DownloadStatus> {
        self.downloads.lock().unwrap().get(download_id).cloned()
    }
    
    /// Cancel a model download
    pub fn cancel_download(&self, download_id: &str) -> Result<String, String> {
        let mut downloads = self.downloads.lock().unwrap();
        
        if downloads.remove(download_id).is_some() {
            Ok(format!("Download {} cancelled", download_id))
        } else {
            Err(format!("Download {} not found", download_id))
        }
    }
    
    /// Set the active model
    pub fn set_active_model(&self, model_id: &str) -> Result<String, String> {
        let available_models = self.available_models.lock().unwrap();
        
        if let Some(model) = available_models.get(model_id) {
            if !model.installed {
                return Err(format!("Model {} is not installed", model_id));
            }
            
            // Update configuration
            let mut config = self.config.lock().unwrap();
            config.model_id = model_id.to_string();
            config.model_path = PathBuf::from(format!("models/{}", model_id));
            config.context_size = model.context_size;
            config.max_output_length = model.context_size / 2;
            config.memory_usage_mb = model.size_mb;
            
            Ok(format!("Model {} set as active", model_id))
        } else {
            Err(format!("Model {} not found", model_id))
        }
    }
    
    /// Get the current configuration
    pub fn get_config(&self) -> LLMConfig {
        self.config.lock().unwrap().clone()
    }
    
    /// Update the configuration
    pub fn update_config(&self, config: LLMConfig) -> Result<String, String> {
        let available_models = self.available_models.lock().unwrap();
        
        // Validate model ID
        if !available_models.contains_key(&config.model_id) {
            return Err(format!("Model {} not found", config.model_id));
        }
        
        // Update configuration
        *self.config.lock().unwrap() = config;
        
        Ok("Configuration updated".to_string())
    }
    
    /// Unload the model to free memory
    pub fn unload(&self) -> Result<String, String> {
        let mut config = self.config.lock().unwrap();
        
        if !config.enabled {
            return Err("Model is already disabled".to_string());
        }
        
        config.enabled = false;
        
        Ok(format!("Model {} unloaded", self.name))
    }
    
    /// Load the model into memory
    pub fn load(&self) -> Result<String, String> {
        let mut config = self.config.lock().unwrap();
        
        if config.enabled {
            return Err("Model is already enabled".to_string());
        }
        
        // Simulate loading time based on model size
        let load_time = config.memory_usage_mb as f32 / 1024.0; // ~1 second per GB
        std::thread::sleep(Duration::from_secs_f32(load_time));
        
        config.enabled = true;
        
        Ok(format!("Model {} loaded", self.name))
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_model_generation() {
        let llm = LocalLLM::new("test", 4096, 1000);
        
        // Test basic generation
        let input = "This is a test input";
        let output = llm.generate(input, 50);
        
        assert!(output.contains("Generated response"));
        assert!(output.len() > input.len());
    }
    
    #[test]
    fn test_model_context_limit() {
        let llm = LocalLLM::new("test", 100, 1000);
        
        // Create input that exceeds context limit
        let input = "test ".repeat(50);
        let output = llm.generate(&input, 60);
        
        assert!(output.contains("too long"));
    }
    
    #[test]
    fn test_model_disabled() {
        let llm = LocalLLM::new("test", 4096, 1000);
        
        // Disable the model
        llm.unload().unwrap();
        
        // Try to generate
        let output = llm.generate("Test input", 10);
        assert!(output.contains("disabled"));
    }
    
    #[test]
    fn test_list_models() {
        let llm = LocalLLM::new_manager();
        
        let models = llm.list_models();
        assert_eq!(models.len(), 3);
        
        // Check that the expected models are present
        let model_ids: Vec<String> = models.iter().map(|m| m.id.clone()).collect();
        assert!(model_ids.contains(&"small".to_string()));
        assert!(model_ids.contains(&"medium".to_string()));
        assert!(model_ids.contains(&"large".to_string()));
    }
    
    #[test]
    fn test_model_download() {
        let llm = LocalLLM::new_manager();
        
        // Try to download the large model (not installed by default)
        let result = llm.download_model("large");
        assert!(result.is_ok());
        
        let download_id = result.unwrap();
        
        // Check download status
        let status = llm.get_download_status(&download_id);
        assert!(status.is_some());
        
        let status = status.unwrap();
        assert_eq!(status.model_id, "large");
        assert!(!status.complete);
        
        // Cancel the download
        let result = llm.cancel_download(&download_id);
        assert!(result.is_ok());
        
        // Check that the download was cancelled
        let status = llm.get_download_status(&download_id);
        assert!(status.is_none());
    }
}
</file>

<file path="src/offline/mod.rs">
pub mod llm;
pub mod checkpointing;
pub mod sync;

use std::sync::{Arc, Mutex};
use std::time::Duration;
use serde::{Serialize, Deserialize};
use log::{debug, info, warn, error};

use self::llm::LocalLLM;
use self::checkpointing::CheckpointManager;
use self::sync::{SyncManager, SyncConfig};
use self::llm::discovery::{DiscoveryService, DiscoveryConfig};
use self::llm::migration::{MigrationService, MigrationConfig, MigrationStatus, MigrationOptions};

/// Offline mode status
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub enum OfflineStatus {
    /// Online mode (normal operation)
    Online,
    /// Offline mode (using local capabilities)
    Offline,
    /// Transitioning from online to offline
    GoingOffline,
    /// Transitioning from offline to online
    GoingOnline,
}

/// Offline mode configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OfflineConfig {
    /// Whether offline mode is enabled
    pub enabled: bool,
    /// Whether to automatically switch to offline mode when network is unavailable
    pub auto_switch: bool,
    /// Whether to use local LLM in offline mode
    pub use_local_llm: bool,
    /// How often to check network connectivity (in seconds)
    pub connectivity_check_interval: u64,
    /// Network timeout threshold (in milliseconds)
    pub network_timeout_ms: u64,
    /// Maximum number of checkpoints to keep
    pub max_checkpoints: usize,
    /// Sync configuration
    pub sync: SyncConfig,
}

impl Default for OfflineConfig {
    fn default() -> Self {
        Self {
            enabled: true,
            auto_switch: true,
            use_local_llm: true,
            connectivity_check_interval: 30,
            network_timeout_ms: 5000,
            max_checkpoints: 10,
            sync: SyncConfig::default(),
        }
    }
}

/// Main offline mode manager
pub struct OfflineManager {
    status: Arc<Mutex<OfflineStatus>>,
    config: Arc<Mutex<OfflineConfig>>,
    llm: Arc<LocalLLM>,
    checkpoint_manager: Arc<Mutex<CheckpointManager>>,
    sync_manager: Arc<SyncManager>,
    llm_discovery: Arc<DiscoveryService>,
    llm_migration: Arc<MigrationService>,
    running: Arc<Mutex<bool>>,
}

impl Default for OfflineManager {
    fn default() -> Self {
        Self::new()
    }
}

impl OfflineManager {
    /// Create a new offline manager
    pub fn new() -> Self {
        Self {
            status: Arc::new(Mutex::new(OfflineStatus::Online)),
            config: Arc::new(Mutex::new(OfflineConfig::default())),
            llm: Arc::new(LocalLLM::new_manager()),
            checkpoint_manager: Arc::new(Mutex::new(CheckpointManager::new())),
            sync_manager: Arc::new(SyncManager::new()),
            llm_discovery: Arc::new(DiscoveryService::new()),
            llm_migration: Arc::new(MigrationService::new()),
            running: Arc::new(Mutex::new(false)),
        }
    }
    
    /// Start the offline manager
    pub fn start(&self) {
        let mut running = self.running.lock().unwrap();
        if *running {
            return;
        }
        *running = true;
        
        // Start sync manager
        self.sync_manager.start();
        
        // Initialize checkpoint manager
        {
            let mut checkpoint_manager = self.checkpoint_manager.lock().unwrap();
            if let Err(e) = checkpoint_manager.initialize() {
                error!("Failed to initialize checkpoint manager: {}", e);
            }
        }
        
        // Start LLM provider discovery service
        let discovery_service = self.llm_discovery.clone();
        tokio::spawn(async move {
            if let Err(e) = discovery_service.start_background_scanner().await {
                error!("Failed to start LLM provider discovery service: {}", e);
            }
        });
        
        // Start connectivity monitoring
        let status = self.status.clone();
        let config = self.config.clone();
        let running_clone = self.running.clone();
        
        // Initialize migration system
        let migration_service = self.llm_migration.clone();
        tokio::spawn(async move {
            match migration_service.detect_legacy_system().await {
                Ok(true) => {
                    info!("Legacy LLM system detected. Migration may be required.");
                    
                    // Check if auto-migrate is enabled
                    let migration_config = migration_service.get_config();
                    if migration_config.auto_migrate {
                        info!("Automatic migration is enabled. Starting migration...");
                        if let Err(e) = llm::migration::run_migration(&migration_service).await {
                            error!("Automatic migration failed: {}", e);
                        }
                    }
                },
                Ok(false) => {
                    debug!("No legacy LLM system detected.");
                },
                Err(e) => {
                    error!("Error detecting legacy LLM system: {}", e);
                }
            }
        });
        
        std::thread::spawn(move || {
            while *running_clone.lock().unwrap() {
                // Check network connectivity
                let is_online = Self::check_network_connectivity();
                let current_status = { *status.lock().unwrap() };
                let config_values = { config.lock().unwrap().clone() };
                
                if config_values.auto_switch {
                    // Automatically switch modes based on connectivity
                    if is_online && current_status == OfflineStatus::Offline {
                        // Going back online
                        debug!("Network connectivity restored, switching to online mode");
                        
                        {
                            let mut status_lock = status.lock().unwrap();
                            *status_lock = OfflineStatus::GoingOnline;
                        }
                        
                        // Perform sync
                        // (In a real implementation, we would initiate sync here)
                        std::thread::sleep(Duration::from_millis(1000));
                        
                        {
                            let mut status_lock = status.lock().unwrap();
                            *status_lock = OfflineStatus::Online;
                        }
                        
                        info!("Switched to online mode");
                    } else if !is_online && current_status == OfflineStatus::Online {
                        // Going offline
                        debug!("Network connectivity lost, switching to offline mode");
                        
                        {
                            let mut status_lock = status.lock().unwrap();
                            *status_lock = OfflineStatus::GoingOffline;
                        }
                        
                        // Create checkpoint
                        // (In a real implementation, we would create a checkpoint here)
                        std::thread::sleep(Duration::from_millis(1000));
                        
                        {
                            let mut status_lock = status.lock().unwrap();
                            *status_lock = OfflineStatus::Offline;
                        }
                        
                        info!("Switched to offline mode");
                    }
                }
                
                // Sleep for the configured interval
                std::thread::sleep(Duration::from_secs(config_values.connectivity_check_interval));
            }
        });
    }
    
    /// Stop the offline manager
    pub fn stop(&self) {
        let mut running = self.running.lock().unwrap();
        *running = false;
        
        // Stop sync manager
        self.sync_manager.stop();
        
        // Stop LLM provider discovery service
        self.llm_discovery.stop_background_scanner();
    }
    
    /// Check network connectivity
    fn check_network_connectivity() -> bool {
        // Simple ping to check connectivity
        let result = std::process::Command::new("ping")
            .args(&["-c", "1", "-W", "2", "8.8.8.8"])
            .output();
        
        match result {
            Ok(output) => output.status.success(),
            Err(_) => false,
        }
    }
    
    /// Manually switch to offline mode
    pub fn go_offline(&self) -> Result<(), String> {
        let current_status = { *self.status.lock().unwrap() };
        
        if current_status == OfflineStatus::Offline {
            return Err("Already in offline mode".to_string());
        }
        
        if current_status == OfflineStatus::GoingOffline {
            return Err("Already transitioning to offline mode".to_string());
        }
        
        // Update status
        {
            let mut status = self.status.lock().unwrap();
            *status = OfflineStatus::GoingOffline;
        }
        
        // Create checkpoint
        // (In a real implementation, we would create a checkpoint here)
        std::thread::sleep(Duration::from_millis(1000));
        
        // Update status
        {
            let mut status = self.status.lock().unwrap();
            *status = OfflineStatus::Offline;
        }
        
        info!("Manually switched to offline mode");
        Ok(())
    }
    
    /// Manually switch to online mode
    pub fn go_online(&self) -> Result<(), String> {
        let current_status = { *self.status.lock().unwrap() };
        
        if current_status == OfflineStatus::Online {
            return Err("Already in online mode".to_string());
        }
        
        if current_status == OfflineStatus::GoingOnline {
            return Err("Already transitioning to online mode".to_string());
        }
        
        // Check connectivity
        if !Self::check_network_connectivity() {
            return Err("Network is not available".to_string());
        }
        
        // Update status
        {
            let mut status = self.status.lock().unwrap();
            *status = OfflineStatus::GoingOnline;
        }
        
        // Perform sync
        // (In a real implementation, we would initiate sync here)
        std::thread::sleep(Duration::from_millis(1000));
        
        // Update status
        {
            let mut status = self.status.lock().unwrap();
            *status = OfflineStatus::Online;
        }
        
        info!("Manually switched to online mode");
        Ok(())
    }
    
    /// Get current offline status
    pub fn get_status(&self) -> OfflineStatus {
        *self.status.lock().unwrap()
    }
    
    /// Get offline configuration
    pub fn get_config(&self) -> OfflineConfig {
        self.config.lock().unwrap().clone()
    }
    
    /// Update offline configuration
    pub fn update_config(&self, config: OfflineConfig) {
        // Update sync config
        self.sync_manager.update_config(config.sync.clone());
        
        // Update main config
        *self.config.lock().unwrap() = config;
    }
    
    /// Get the local LLM manager
    pub fn get_llm(&self) -> Arc<LocalLLM> {
        self.llm.clone()
    }
    
    /// Get the checkpoint manager
    pub fn get_checkpoint_manager(&self) -> Arc<Mutex<CheckpointManager>> {
        self.checkpoint_manager.clone()
    }
    
    /// Get the sync manager
    pub fn get_sync_manager(&self) -> Arc<SyncManager> {
        self.sync_manager.clone()
    }
    
    /// Get the LLM discovery service
    pub fn get_llm_discovery(&self) -> Arc<DiscoveryService> {
        self.llm_discovery.clone()
    }
    
    /// Get the LLM migration service
    pub fn get_llm_migration(&self) -> Arc<MigrationService> {
        self.llm_migration.clone()
    }
    
    /// Scan for LLM providers
    pub async fn scan_for_llm_providers(&self) -> Result<(), String> {
        match self.llm_discovery.scan_for_providers().await {
            Ok(_) => Ok(()),
            Err(e) => Err(format!("Failed to scan for LLM providers: {}", e)),
        }
    }
    
    /// Run LLM migration
    pub async fn run_llm_migration(&self, options: MigrationOptions) -> Result<llm::migration::MigrationNotification, String> {
        match self.llm_migration.run_migration(options).await {
            Ok(notification) => Ok(notification),
            Err(e) => Err(format!("Failed to run LLM migration: {}", e)),
        }
    }
    
    /// Get available LLM providers
    pub fn get_available_providers(&self) -> Vec<crate::commands::offline::llm::ProviderInfo> {
        self.llm_discovery.create_provider_infos()
    }
    
    /// Get LLM provider suggestions
    pub fn get_provider_suggestions(&self) -> Vec<llm::discovery::ProviderSuggestion> {
        self.llm_discovery.get_suggestions()
    }
    
    /// Get LLM provider configurations
    pub fn get_provider_configs(&self) -> Vec<crate::commands::offline::llm::ProviderConfig> {
        self.llm_discovery.create_provider_configs()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_offline_manager_creation() {
        let manager = OfflineManager::new();
        
        // Check initial status
        assert_eq!(manager.get_status(), OfflineStatus::Online);
        
        // Check default config
        let config = manager.get_config();
        assert!(config.enabled);
        assert!(config.auto_switch);
        assert!(config.use_local_llm);
    }
    
    #[test]
    fn test_manual_offline_switching() {
        let manager = OfflineManager::new();
        
        // Switch to offline mode
        let result = manager.go_offline();
        assert!(result.is_ok());
        assert_eq!(manager.get_status(), OfflineStatus::Offline);
        
        // Try to switch to offline again (should fail)
        let result = manager.go_offline();
        assert!(result.is_err());
        
        // Switch back to online mode (might fail if network is not available)
        let _ = manager.go_online();
    }
}
</file>

<file path="src/protocols/mcp/session.rs">
use crate::models::messages::MessageError;
use crate::protocols::mcp::error::McpError;
use crate::protocols::mcp::message::{McpMessage, McpMessagePayload};
use crate::protocols::mcp::types::McpMessageType;
use crate::protocols::mcp::McpConfig;
use crate::observability::metrics::{increment_counter, record_gauge, record_histogram, time_operation};

use log::{debug, error, info, warn};
use std::collections::HashMap;
use std::fs::{self, File};
use std::io::{Read, Write};
use std::path::{Path, PathBuf};
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, Instant, SystemTime};
use tokio::sync::mpsc::{self, Receiver, Sender};
use tokio::sync::mpsc::{UnboundedReceiver, UnboundedSender};
use tokio::sync::oneshot;
use tokio::time::timeout;
use uuid::Uuid;
use serde::{Serialize, Deserialize};

/// Maximum time without activity before a session times out
const DEFAULT_SESSION_TIMEOUT: Duration = Duration::from_secs(3600); // 1 hour

/// Session status
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum SessionStatus {
    /// Session is initializing
    Initializing,
    
    /// Session is active
    Active,
    
    /// Session is idle
    Idle,
    
    /// Session has timed out
    TimedOut,
    
    /// Session has been terminated
    Terminated,
}

/// Persistent session data for storage
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SessionPersistentData {
    /// Session ID
    pub id: String,
    
    /// Status at time of persistence
    pub status: SessionStatus,
    
    /// Creation timestamp as ISO string
    pub created_at: String,
    
    /// Last activity timestamp as ISO string
    pub last_activity: String,
    
    /// User ID associated with this session
    pub user_id: Option<String>,
    
    /// Authentication token
    pub auth_token: Option<String>,
    
    /// Session timeout in seconds
    pub timeout_seconds: u64,
    
    /// Session metadata
    pub metadata: HashMap<String, String>,
    
    /// Server-side session ID
    pub server_session_id: Option<String>,
    
    /// Conversation history
    pub conversation_history: Vec<String>,
}

/// Session data
#[derive(Debug)]
pub struct Session {
    /// Session ID
    pub id: String,
    
    /// Current session status
    pub status: Arc<RwLock<SessionStatus>>,
    
    /// Last activity timestamp
    pub last_activity: Arc<RwLock<Instant>>,
    
    /// Creation timestamp
    pub created_at: SystemTime,
    
    /// User ID associated with this session
    pub user_id: Option<String>,
    
    /// Authentication token
    pub auth_token: Option<String>,
    
    /// Session timeout
    pub timeout: Duration,
    
    /// Session metadata
    pub metadata: Arc<RwLock<HashMap<String, String>>>,
    
    /// Conversation history
    pub conversation_history: Arc<RwLock<Vec<String>>>,
}

impl Session {
    /// Create a new session
    pub fn new(id: Option<String>, timeout: Option<Duration>) -> Self {
        let session_id = id.unwrap_or_else(|| Uuid::new_v4().to_string());
        let session_timeout = timeout.unwrap_or(DEFAULT_SESSION_TIMEOUT);
        
        let session = Self {
            id: session_id,
            status: Arc::new(RwLock::new(SessionStatus::Initializing)),
            last_activity: Arc::new(RwLock::new(Instant::now())),
            created_at: SystemTime::now(),
            user_id: None,
            auth_token: None,
            timeout: session_timeout,
            metadata: Arc::new(RwLock::new(HashMap::new())),
            conversation_history: Arc::new(RwLock::new(Vec::new())),
        };
        
        // Record metric for session creation
        increment_counter("session.created", None);
        
        session
    }
    
    /// Create session from persistent data
    pub fn from_persistent_data(data: SessionPersistentData) -> Self {
        let session = Self {
            id: data.id,
            status: Arc::new(RwLock::new(data.status)),
            last_activity: Arc::new(RwLock::new(Instant::now())),
            created_at: SystemTime::now(), // Cannot reliably restore original timestamp
            user_id: data.user_id,
            auth_token: data.auth_token,
            timeout: Duration::from_secs(data.timeout_seconds),
            metadata: Arc::new(RwLock::new(data.metadata)),
            conversation_history: Arc::new(RwLock::new(data.conversation_history)),
        };
        
        // Set server session ID if available
        if let Some(server_id) = data.server_session_id {
            session.set_metadata("server_session_id".to_string(), server_id);
        }
        
        // Record metric for session restoration
        increment_counter("session.restored", None);
        
        session
    }
    
    /// Check if the session is active
    pub fn is_active(&self) -> bool {
        let status = self.status.read().unwrap();
        *status == SessionStatus::Active
    }
    
    /// Check if the session has timed out
    pub fn is_timed_out(&self) -> bool {
        let last_activity = self.last_activity.read().unwrap();
        let elapsed = last_activity.elapsed();
        
        // Record session age metric
        record_histogram("session.age_seconds", elapsed.as_secs_f64(), None);
        
        elapsed >= self.timeout
    }
    
    /// Update the session status
    pub fn update_status(&self, status: SessionStatus) {
        let mut status_guard = self.status.write().unwrap();
        *status_guard = status;
        
        // Record metric for status change
        increment_counter(&format!("session.status.{:?}", status), None);
    }
    
    /// Update the last activity timestamp
    pub fn update_activity(&self) {
        let mut last_activity = self.last_activity.write().unwrap();
        *last_activity = Instant::now();
        
        // Record activity update
        increment_counter("session.activity_updated", None);
    }
    
    /// Set metadata value
    pub fn set_metadata(&self, key: String, value: String) {
        let mut metadata = self.metadata.write().unwrap();
        metadata.insert(key, value);
    }
    
    /// Get metadata value
    pub fn get_metadata(&self, key: &str) -> Option<String> {
        let metadata = self.metadata.read().unwrap();
        metadata.get(key).cloned()
    }
    
    /// Add a message to conversation history
    pub fn add_to_history(&self, message_json: String) {
        let mut history = self.conversation_history.write().unwrap();
        history.push(message_json);
        
        // Record history size metric
        record_gauge("session.history_size", history.len() as f64, None);
    }
    
    /// Get conversation history
    pub fn get_history(&self) -> Vec<String> {
        let history = self.conversation_history.read().unwrap();
        history.clone()
    }
    
    /// Generate heartbeat message
    pub fn generate_heartbeat(&self) -> McpMessage {
        McpMessage {
            id: Uuid::new_v4().to_string(),
            version: "v1".to_string(),
            type_: McpMessageType::Ping,
            payload: McpMessagePayload::Ping {},
        }
    }
    
    /// Convert session to persistent data format
    pub fn to_persistent_data(&self) -> SessionPersistentData {
        let metadata = self.metadata.read().unwrap().clone();
        let history = self.conversation_history.read().unwrap().clone();
        let status = *self.status.read().unwrap();
        
        let created_at = match self.created_at.duration_since(SystemTime::UNIX_EPOCH) {
            Ok(duration) => {
                let secs = duration.as_secs();
                chrono::NaiveDateTime::from_timestamp_opt(secs as i64, 0)
                    .map(|dt| dt.format("%Y-%m-%dT%H:%M:%SZ").to_string())
                    .unwrap_or_else(|| "unknown".to_string())
            },
            Err(_) => "unknown".to_string(),
        };
        
        // Get last activity as ISO string
        let last_activity = {
            let now = SystemTime::now();
            let last_activity_duration = self.last_activity.read().unwrap().elapsed();
            
            match now.checked_sub(last_activity_duration) {
                Some(last_activity_time) => {
                    match last_activity_time.duration_since(SystemTime::UNIX_EPOCH) {
                        Ok(duration) => {
                            let secs = duration.as_secs();
                            chrono::NaiveDateTime::from_timestamp_opt(secs as i64, 0)
                                .map(|dt| dt.format("%Y-%m-%dT%H:%M:%SZ").to_string())
                                .unwrap_or_else(|| "unknown".to_string())
                        },
                        Err(_) => "unknown".to_string(),
                    }
                },
                None => "unknown".to_string(),
            }
        };
        
        SessionPersistentData {
            id: self.id.clone(),
            status,
            created_at,
            last_activity,
            user_id: self.user_id.clone(),
            auth_token: self.auth_token.clone(),
            timeout_seconds: self.timeout.as_secs(),
            metadata,
            server_session_id: self.get_metadata("server_session_id"),
            conversation_history: history,
        }
    }
}

/// Session manager for handling multiple sessions
pub struct SessionManager {
    /// Active sessions
    sessions: Arc<Mutex<HashMap<String, Arc<Session>>>>,
    
    /// Session timeout duration
    timeout_duration: Duration,
    
    /// Session cleanup interval
    cleanup_interval: Duration,
    
    /// Session persistence enabled
    persistence_enabled: bool,
    
    /// Session storage directory
    storage_dir: PathBuf,
    
    /// Protocol configuration
    config: McpConfig,
}

impl SessionManager {
    /// Create a new session manager
    pub fn new(config: McpConfig) -> Self {
        let timeout = config.session_timeout.unwrap_or(DEFAULT_SESSION_TIMEOUT);
        let storage_dir = config.session_storage_dir.clone().unwrap_or_else(|| {
            let mut dir = dirs::data_local_dir().unwrap_or_else(|| PathBuf::from("."));
            dir.push("mcp");
            dir.push("sessions");
            dir
        });
        
        let manager = Self {
            sessions: Arc::new(Mutex::new(HashMap::new())),
            timeout_duration: timeout,
            cleanup_interval: Duration::from_secs(300), // 5 minutes
            persistence_enabled: config.enable_session_persistence.unwrap_or(true),
            storage_dir,
            config,
        };
        
        // Create storage directory if it doesn't exist
        if manager.persistence_enabled {
            if let Err(e) = fs::create_dir_all(&manager.storage_dir) {
                error!("Failed to create session storage directory: {}", e);
            }
            
            // Load persisted sessions
            if let Err(e) = manager.load_persisted_sessions() {
                error!("Failed to load persisted sessions: {}", e);
            }
        }
        
        manager
    }
    
    /// Start the session cleanup task
    pub fn start_cleanup_task(&self) {
        let sessions = self.sessions.clone();
        let interval = self.cleanup_interval;
        let persistence_enabled = self.persistence_enabled;
        let storage_dir = self.storage_dir.clone();
        
        tokio::spawn(async move {
            loop {
                // Sleep for the cleanup interval
                tokio::time::sleep(interval).await;
                
                // Find and remove expired sessions
                let mut to_remove = Vec::new();
                {
                    let sessions_guard = sessions.lock().unwrap();
                    for (id, session) in sessions_guard.iter() {
                        if session.is_timed_out() {
                            to_remove.push(id.clone());
                        }
                    }
                }
                
                if !to_remove.is_empty() {
                    let mut sessions_guard = sessions.lock().unwrap();
                    for id in &to_remove {
                        if let Some(session) = sessions_guard.remove(id) {
                            info!("Session {} timed out and was removed", id);
                            session.update_status(SessionStatus::TimedOut);
                            
                            // Attempt to persist session data if enabled
                            if persistence_enabled {
                                if let Err(e) = Self::persist_session_to_disk(&session, &storage_dir) {
                                    error!("Failed to persist session {}: {}", id, e);
                                }
                            }
                            
                            // Record session timeout metric
                            increment_counter("session.timeout", None);
                        }
                    }
                    
                    debug!("Cleaned up {} expired sessions", to_remove.len());
                    
                    // Record active sessions metric
                    record_gauge("session.active_count", sessions_guard.len() as f64, None);
                }
            }
        });
    }
    
    /// Create a new session
    pub fn create_session(&self, id: Option<String>) -> Arc<Session> {
        let session = Arc::new(Session::new(id, Some(self.timeout_duration)));
        
        let mut sessions = self.sessions.lock().unwrap();
        sessions.insert(session.id.clone(), session.clone());
        
        debug!("Created new session with ID: {}", session.id);
        
        // Record active sessions metric
        record_gauge("session.active_count", sessions.len() as f64, None);
        
        session
    }
    
    /// Get session by ID
    pub fn get_session(&self, id: &str) -> Option<Arc<Session>> {
        let sessions = self.sessions.lock().unwrap();
        sessions.get(id).cloned()
    }
    
    /// Terminate a session
    pub fn terminate_session(&self, id: &str) -> bool {
        let mut sessions = self.sessions.lock().unwrap();
        let result = sessions.remove(id).is_some();
        
        if result {
            debug!("Terminated session with ID: {}", id);
            increment_counter("session.terminated", None);
            
            // Record active sessions metric
            record_gauge("session.active_count", sessions.len() as f64, None);
        }
        
        result
    }
    
    /// Get all active sessions
    pub fn get_active_sessions(&self) -> Vec<Arc<Session>> {
        let sessions = self.sessions.lock().unwrap();
        sessions
            .values()
            .filter(|s| s.is_active())
            .cloned()
            .collect()
    }
    
    /// Count active sessions
    pub fn count_active_sessions(&self) -> usize {
        let sessions = self.sessions.lock().unwrap();
        sessions.values().filter(|s| s.is_active()).count()
    }
    
    /// Check if persistence is enabled
    pub fn is_persistence_enabled(&self) -> bool {
        self.persistence_enabled
    }
    
    /// Get session file path
    fn get_session_file_path(id: &str, storage_dir: &Path) -> PathBuf {
        let mut path = storage_dir.to_path_buf();
        path.push(format!("{}.json", id));
        path
    }
    
    /// Persist session to disk
    fn persist_session_to_disk(session: &Session, storage_dir: &Path) -> Result<(), String> {
        // Create persistent data representation
        let persistent_data = session.to_persistent_data();
        
        // Create storage directory if it doesn't exist
        if !storage_dir.exists() {
            if let Err(e) = fs::create_dir_all(storage_dir) {
                return Err(format!("Failed to create session storage directory: {}", e));
            }
        }
        
        // Get file path
        let file_path = Self::get_session_file_path(&session.id, storage_dir);
        
        // Serialize to JSON
        let json = match serde_json::to_string_pretty(&persistent_data) {
            Ok(json) => json,
            Err(e) => return Err(format!("Failed to serialize session data: {}", e)),
        };
        
        // Write to file
        let mut file = match File::create(&file_path) {
            Ok(file) => file,
            Err(e) => return Err(format!("Failed to create session file: {}", e)),
        };
        
        if let Err(e) = file.write_all(json.as_bytes()) {
            return Err(format!("Failed to write session data: {}", e));
        }
        
        debug!("Persisted session {} to {}", session.id, file_path.display());
        increment_counter("session.persisted", None);
        
        Ok(())
    }
    
    /// Load persisted sessions
    fn load_persisted_sessions(&self) -> Result<(), String> {
        // Check if storage directory exists
        if !self.storage_dir.exists() {
            return Ok(());
        }
        
        // Read directory entries
        let entries = match fs::read_dir(&self.storage_dir) {
            Ok(entries) => entries,
            Err(e) => return Err(format!("Failed to read session storage directory: {}", e)),
        };
        
        // Load each session file
        let mut loaded_count = 0;
        for entry in entries {
            if let Ok(entry) = entry {
                let path = entry.path();
                
                // Skip non-JSON files
                if path.extension().map_or(true, |ext| ext != "json") {
                    continue;
                }
                
                // Read file
                let mut file = match File::open(&path) {
                    Ok(file) => file,
                    Err(e) => {
                        warn!("Failed to open session file {}: {}", path.display(), e);
                        continue;
                    }
                };
                
                let mut content = String::new();
                if let Err(e) = file.read_to_string(&mut content) {
                    warn!("Failed to read session file {}: {}", path.display(), e);
                    continue;
                }
                
                // Parse JSON
                match serde_json::from_str::<SessionPersistentData>(&content) {
                    Ok(data) => {
                        // Restore session
                        let session = Arc::new(Session::from_persistent_data(data));
                        
                        // Store in sessions map
                        let mut sessions = self.sessions.lock().unwrap();
                        sessions.insert(session.id.clone(), session);
                        
                        loaded_count += 1;
                    }
                    Err(e) => {
                        warn!("Failed to parse session file {}: {}", path.display(), e);
                    }
                }
            }
        }
        
        info!("Loaded {} persisted sessions", loaded_count);
        
        // Record loaded sessions metric
        increment_counter("session.loaded_from_disk", Some({
            let mut map = HashMap::new();
            map.insert("count".to_string(), loaded_count.to_string());
            map
        }));
        
        Ok(())
    }
    
    /// Persist a session
    pub fn persist_session(&self, session: &Session) -> Result<(), String> {
        if !self.persistence_enabled {
            return Err("Session persistence is disabled".to_string());
        }
        
        Self::persist_session_to_disk(session, &self.storage_dir)
    }
    
    /// Recover a session from persistent storage
    pub fn recover_session(&self, id: &str) -> Result<Arc<Session>, String> {
        if !self.persistence_enabled {
            return Err("Session persistence is disabled".to_string());
        }
        
        // Check if session is already active
        {
            let sessions = self.sessions.lock().unwrap();
            if let Some(session) = sessions.get(id) {
                return Ok(session.clone());
            }
        }
        
        // Get file path
        let file_path = Self::get_session_file_path(id, &self.storage_dir);
        
        // Check if file exists
        if !file_path.exists() {
            return Err(format!("No persisted session found with ID: {}", id));
        }
        
        // Read file
        let mut file = match File::open(&file_path) {
            Ok(file) => file,
            Err(e) => return Err(format!("Failed to open session file: {}", e)),
        };
        
        let mut content = String::new();
        if let Err(e) = file.read_to_string(&mut content) {
            return Err(format!("Failed to read session file: {}", e));
        }
        
        // Parse JSON
        let data = match serde_json::from_str::<SessionPersistentData>(&content) {
            Ok(data) => data,
            Err(e) => return Err(format!("Failed to parse session data: {}", e)),
        };
        
        // Create session
        let session = Arc::new(Session::from_persistent_data(data));
        
        // Store in sessions map
        let mut sessions = self.sessions.lock().unwrap();
        sessions.insert(session.id.clone(), session.clone());
        
        // Record session recovery metric
        increment_counter("session.recovered_from_disk", None);
        
        Ok(session)
    }
}

/// Session message handler
pub struct SessionMessageHandler {
    /// Session
    session: Arc<Session>,
    
    /// Response channels for requests
    response_channels: Arc<Mutex<HashMap<String, oneshot::Sender<Result<McpMessage, McpError>>>>>,
    
    /// Event channel
    event_sender: UnboundedSender<McpMessage>,
}

impl SessionMessageHandler {
    /// Create a new session message handler
    pub fn new(session: Arc<Session>) -> (Self, UnboundedReceiver<McpMessage>) {
        let (event_sender, event_receiver) = mpsc::unbounded_channel();
        
        (
            Self {
                session,
                response_channels: Arc::new(Mutex::new(HashMap::new())),
                event_sender,
            },
            event_receiver,
        )
    }
    
    /// Register a response channel for a request
    pub fn register_response_channel(
        &self,
        request_id: String,
        channel: oneshot::Sender<Result<McpMessage, McpError>>,
    ) {
        let mut channels = self.response_channels.lock().unwrap();
        channels.insert(request_id, channel);
        
        // Record pending requests metric
        record_gauge("session.pending_requests", channels.len() as f64, None);
    }
    
    /// Handle an incoming message
    pub fn handle_message(&self, message: McpMessage) -> Result<(), McpError> {
        // Measure message handling time
        time_operation!("session.message_handling", None, {
            // Update session activity
            self.session.update_activity();
            
            // Record message to conversation history if it's a significant message
            match message.type_ {
                McpMessageType::CompletionRequest | 
                McpMessageType::CompletionResponse => {
                    if let Ok(json) = serde_json::to_string(&message) {
                        self.session.add_to_history(json);
                    }
                },
                _ => {}
            }
            
            match message.type_ {
                McpMessageType::CompletionResponse => {
                    self.handle_completion_response(message)
                }
                McpMessageType::StreamingMessage => {
                    self.handle_streaming_message(message)
                }
                McpMessageType::StreamingEnd => {
                    self.handle_streaming_end(message)
                }
                McpMessageType::Error => {
                    self.handle_error_message(message)
                }
                McpMessageType::Pong => {
                    // Heartbeat response, just log it
                    debug!("Received pong message");
                    increment_counter("session.heartbeat_response", None);
                    Ok(())
                }
                McpMessageType::AuthResponse => {
                    self.handle_auth_response(message)
                }
                McpMessageType::SessionRecoveryResponse => {
                    self.handle_session_recovery_response(message)
                }
                _ => {
                    // Unsupported message type, send as event
                    debug!("Received message of type: {:?}", message.type_);
                    increment_counter(&format!("session.message.type.{:?}", message.type_).to_lowercase(), None);
                    
                    self.event_sender.send(message).map_err(|_| {
                        McpError::ProtocolError("Failed to send message to event channel".to_string())
                    })?;
                    Ok(())
                }
            }
        })
    }
    
    /// Handle a completion response message
    fn handle_completion_response(&self, message: McpMessage) -> Result<(), McpError> {
        // Record metric for completion response
        increment_counter("session.completion_response", None);
        
        let mut channels = self.response_channels.lock().unwrap();
        if let Some(channel) = channels.remove(&message.id) {
            channel.send(Ok(message)).map_err(|_| {
                McpError::ProtocolError("Failed to send response to channel".to_string())
            })?;
        } else {
            // No channel found, send as event
            self.event_sender.send(message).map_err(|_| {
                McpError::ProtocolError("Failed to send message to event channel".to_string())
            })?;
        }
        
        // Update pending requests metric
        record_gauge("session.pending_requests", channels.len() as f64, None);
        
        Ok(())
    }
    
    /// Handle a streaming message
    fn handle_streaming_message(&self, message: McpMessage) -> Result<(), McpError> {
        // Record metric for streaming message
        increment_counter("session.streaming_message", None);
        
        // Extract streaming ID from payload
        if let McpMessagePayload::StreamingMessage { streaming_id, chunk, .. } = &message.payload {
            // Record chunk size metric
            record_histogram("session.streaming_chunk_size", chunk.len() as f64, None);
            
            // Use the streaming ID as the key for finding the response channel
            // This is different from the message ID, which is unique per message
            let mut channels = self.response_channels.lock().unwrap();
            if let Some(channel) = channels.get(streaming_id) {
                let _ = channel.send(Ok(message.clone()));
            } else {
                // No channel found, send as event
                self.event_sender.send(message).map_err(|_| {
                    McpError::ProtocolError("Failed to send message to event channel".to_string())
                })?;
            }
        } else {
            return Err(McpError::ProtocolError(
                "Invalid streaming message format".to_string(),
            ));
        }
        
        Ok(())
    }
    
    /// Handle a streaming end message
    fn handle_streaming_end(&self, message: McpMessage) -> Result<(), McpError> {
        // Record metric for streaming end
        increment_counter("session.streaming_end", None);
        
        // Extract streaming ID from payload
        if let McpMessagePayload::StreamingEnd { streaming_id } = &message.payload {
            // Use the streaming ID as the key for finding the response channel
            let mut channels = self.response_channels.lock().unwrap();
            if let Some(channel) = channels.remove(streaming_id) {
                let _ = channel.send(Ok(message.clone()));
                
                // Update pending requests metric
                record_gauge("session.pending_requests", channels.len() as f64, None);
            } else {
                // No channel found, send as event
                self.event_sender.send(message).map_err(|_| {
                    McpError::ProtocolError("Failed to send message to event channel".to_string())
                })?;
            }
        } else {
            return Err(McpError::ProtocolError(
                "Invalid streaming end message format".to_string(),
            ));
        }
        
        Ok(())
    }
    
    /// Handle an error message
    fn handle_error_message(&self, message: McpMessage) -> Result<(), McpError> {
        // Record metric for error message
        increment_counter("session.error_message", None);
        
        // Extract request ID from payload
        if let McpMessagePayload::Error { request_id, code, message: error_msg, .. } = &message.payload {
            // Record error type metric
            increment_counter(&format!("session.error.{:?}", code).to_lowercase(), None);
            
            let mut channels = self.response_channels.lock().unwrap();
            if let Some(channel) = channels.remove(request_id) {
                let error = McpError::ProtocolError(
                    format!("{:?}: {}", code, error_msg),
                );
                let _ = channel.send(Err(error));
                
                // Update pending requests metric
                record_gauge("session.pending_requests", channels.len() as f64, None);
            } else {
                // No channel found, send as event
                self.event_sender.send(message).map_err(|_| {
                    McpError::ProtocolError("Failed to send message to event channel".to_string())
                })?;
            }
        } else {
            return Err(McpError::ProtocolError(
                "Invalid error message format".to_string(),
            ));
        }
        
        Ok(())
    }
    
    /// Handle an authentication response message
    fn handle_auth_response(&self, message: McpMessage) -> Result<(), McpError> {
        // Record metric for auth response
        increment_counter("session.auth_response", None);
        
        // Parse authentication response
        if let McpMessagePayload::AuthResponse { success, session_id } = &message.payload {
            if *success {
                if let Some(sid) = session_id {
                    // Store session ID in metadata
                    self.session.set_metadata("server_session_id".to_string(), sid.clone());
                    
                    // Update session status
                    self.session.update_status(SessionStatus::Active);
                    
                    info!("Successfully authenticated, server session ID: {}", sid);
                    increment_counter("session.auth_success", None);
                } else {
                    warn!("Authentication successful but no session ID provided");
                    
                    // Update session status anyway
                    self.session.update_status(SessionStatus::Active);
                    
                    increment_counter("session.auth_success_no_id", None);
                }
            } else {
                // Authentication failed
                self.session.update_status(SessionStatus::Terminated);
                
                increment_counter("session.auth_failed", None);
                
                return Err(McpError::AuthenticationFailed(
                    "Authentication failed".to_string(),
                ));
            }
        } else {
            return Err(McpError::ProtocolError(
                "Invalid authentication response format".to_string(),
            ));
        }
        
        // Also send to any waiting response channel
        let mut channels = self.response_channels.lock().unwrap();
        if let Some(channel) = channels.remove(&message.id) {
            let _ = channel.send(Ok(message));
            
            // Update pending requests metric
            record_gauge("session.pending_requests", channels.len() as f64, None);
        }
        
        Ok(())
    }
    
    /// Handle a session recovery response message
    fn handle_session_recovery_response(&self, message: McpMessage) -> Result<(), McpError> {
        // Record metric for session recovery response
        increment_counter("session.recovery_response", None);
        
        // Parse session recovery response
        if let McpMessagePayload::SessionRecoveryResponse { success, error, session_id } = &message.payload {
            if *success {
                // Update session status
                self.session.update_status(SessionStatus::Active);
                
                // Store server session ID if provided
                if let Some(sid) = session_id {
                    self.session.set_metadata("server_session_id".to_string(), sid.clone());
                    info!("Successfully recovered session, server session ID: {}", sid);
                } else {
                    info!("Successfully recovered session");
                }
                
                increment_counter("session.recovery_success", None);
            } else {
                // Recovery failed
                let error_msg = error.clone().unwrap_or_else(|| "Unknown error".to_string());
                self.session.update_status(SessionStatus::Terminated);
                
                warn!("Session recovery failed: {}", error_msg);
                increment_counter("session.recovery_failed", None);
                
                return Err(McpError::SessionError(
                    format!("Session recovery failed: {}", error_msg),
                ));
            }
        } else {
            return Err(McpError::ProtocolError(
                "Invalid session recovery response format".to_string(),
            ));
        }
        
        // Also send to any waiting response channel
        let mut channels = self.response_channels.lock().unwrap();
        if let Some(channel) = channels.remove(&message.id) {
            let _ = channel.send(Ok(message));
            
            // Update pending requests metric
            record_gauge("session.pending_requests", channels.len() as f64, None);
        }
        
        Ok(())
    }
    
    /// Cancel a specific request
    pub fn cancel_request(&self, request_id: &str) {
        let mut channels = self.response_channels.lock().unwrap();
        if let Some(channel) = channels.remove(request_id) {
            let _ = channel.send(Err(McpError::RequestCancelled));
            increment_counter("session.request_cancelled", None);
        }
        
        // Update pending requests metric
        record_gauge("session.pending_requests", channels.len() as f64, None);
    }
    
    /// Cancel all pending requests
    pub fn cancel_all_requests(&self, error: McpError) {
        let mut channels = self.response_channels.lock().unwrap();
        let count = channels.len();
        
        for (_, channel) in channels.drain() {
            let _ = channel.send(Err(error.clone()));
        }
        
        if count > 0 {
            info!("Cancelled {} pending requests", count);
            
            // Record metric for mass cancellation
            increment_counter("session.requests_mass_cancelled", Some({
                let mut map = HashMap::new();
                map.insert("count".to_string(), count.to_string());
                map
            }));
            
            // Update pending requests metric
            record_gauge("session.pending_requests", 0.0, None);
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_session_creation() {
        let session = Session::new(None, None);
        assert_eq!(*session.status.read().unwrap(), SessionStatus::Initializing);
        assert!(!session.is_active());
        assert!(!session.is_timed_out());
    }
    
    #[test]
    fn test_session_activity() {
        let session = Session::new(None, Some(Duration::from_millis(100)));
        
        // Initial state
        assert!(!session.is_timed_out());
        
        // Wait for timeout
        std::thread::sleep(Duration::from_millis(110));
        
        // Should be timed out now
        assert!(session.is_timed_out());
        
        // Update activity
        session.update_activity();
        
        // Should not be timed out after update
        assert!(!session.is_timed_out());
    }
    
    #[test]
    fn test_session_metadata() {
        let session = Session::new(None, None);
        
        // Set metadata
        session.set_metadata("test_key".to_string(), "test_value".to_string());
        
        // Get metadata
        let value = session.get_metadata("test_key");
        assert_eq!(value, Some("test_value".to_string()));
        
        // Get non-existent metadata
        let value = session.get_metadata("non_existent");
        assert_eq!(value, None);
    }
    
    #[test]
    fn test_session_persistence() {
        let session = Session::new(None, None);
        
        // Add some metadata
        session.set_metadata("test_key".to_string(), "test_value".to_string());
        
        // Convert to persistent data
        let persistent_data = session.to_persistent_data();
        
        // Check data
        assert_eq!(persistent_data.id, session.id);
        assert_eq!(persistent_data.status, *session.status.read().unwrap());
        assert_eq!(persistent_data.metadata.get("test_key"), Some(&"test_value".to_string()));
        
        // Convert back to session
        let restored_session = Session::from_persistent_data(persistent_data);
        
        // Check restored data
        assert_eq!(restored_session.id, session.id);
        assert_eq!(*restored_session.status.read().unwrap(), *session.status.read().unwrap());
        assert_eq!(restored_session.get_metadata("test_key"), Some("test_value".to_string()));
    }
}
</file>

<file path="src/security/data_flow.rs">
// Data Flow Tracking and Visualization
//
// This module tracks the flow of data throughout the application and visualizes it for users.
// It shows users where their data goes, what systems process it, and provides transparency
// about how their information is handled.

use std::collections::{HashMap, HashSet, VecDeque};
use std::fs;
use std::path::PathBuf;
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, SystemTime};

use log::{debug, info, warn, error};
use serde::{Serialize, Deserialize};

use crate::config::config_path;
use crate::error::Result;
use crate::observability::metrics::{record_counter, record_gauge};
use crate::security::DataClassification;

const DATA_FLOW_LOG_FILE: &str = "data_flow_log.json";
const MAX_LOG_ENTRIES: usize = 1000;

/// A data flow event
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataFlowEvent {
    /// Unique ID for the event
    pub id: String,
    
    /// Type of operation (e.g., "api_request", "file_save", "encryption")
    pub operation: String,
    
    /// Data item name or description
    pub data_item: String,
    
    /// Classification level of the data
    pub classification: DataClassification,
    
    /// Source location/system
    pub source: String,
    
    /// Destination location/system
    pub destination: String,
    
    /// Whether user consent was obtained for this data flow
    pub consent_obtained: bool,
    
    /// When the data flow occurred
    pub timestamp: SystemTime,
    
    /// Whether the data was encrypted during transfer
    pub encrypted: bool,
    
    /// Additional metadata about the data flow
    pub metadata: HashMap<String, String>,
}

/// A node in the data flow graph
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataFlowNode {
    /// Unique ID for the node
    pub id: String,
    
    /// Name of the system or component
    pub name: String,
    
    /// Type of node (e.g., "local_storage", "remote_api", "memory")
    pub node_type: String,
    
    /// Whether the node is internal to the app
    pub internal: bool,
    
    /// Location (e.g., "local", "cloud", "third_party")
    pub location: String,
    
    /// Additional metadata about the node
    pub metadata: HashMap<String, String>,
}

/// Data flow graph for visualization
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataFlowGraph {
    /// Nodes in the graph
    pub nodes: Vec<DataFlowNode>,
    
    /// Edges in the graph (source_id, destination_id, data_classifications)
    pub edges: Vec<(String, String, Vec<DataClassification>)>,
    
    /// Data items flowing through the graph
    pub data_items: HashMap<String, DataClassification>,
}

/// Data Flow Manager
pub struct DataFlowManager {
    /// Whether data flow tracking is enabled
    enabled: bool,
    
    /// Recent data flow events
    events: Arc<RwLock<VecDeque<DataFlowEvent>>>,
    
    /// Data flow graph
    graph: Arc<RwLock<DataFlowGraph>>,
    
    /// Known nodes in the system
    nodes: Arc<RwLock<HashMap<String, DataFlowNode>>>,
    
    /// Lock for file operations
    file_lock: Arc<Mutex<()>>,
}

impl DataFlowManager {
    /// Create a new Data Flow Manager
    pub fn new(enabled: bool) -> Result<Self> {
        Ok(Self {
            enabled,
            events: Arc::new(RwLock::new(VecDeque::with_capacity(MAX_LOG_ENTRIES))),
            graph: Arc::new(RwLock::new(DataFlowGraph {
                nodes: Vec::new(),
                edges: Vec::new(),
                data_items: HashMap::new(),
            })),
            nodes: Arc::new(RwLock::new(HashMap::new())),
            file_lock: Arc::new(Mutex::new(())),
        })
    }
    
    /// Start the data flow tracking service
    pub fn start_service(&self) -> Result<()> {
        if !self.enabled {
            info!("Data flow tracking is disabled");
            return Ok(());
        }
        
        // Load previous data flow events if available
        self.load_events()?;
        
        // Initialize known nodes
        self.initialize_known_nodes()?;
        
        // Build the graph from events
        self.rebuild_graph()?;
        
        info!("Data flow tracking service started with {} events", self.events.read().unwrap().len());
        record_gauge("security.data_flow.event_count", self.events.read().unwrap().len() as f64, None);
        record_counter("security.data_flow.service_started", 1.0, None);
        
        Ok(())
    }
    
    /// Initialize known nodes in the system
    fn initialize_known_nodes(&self) -> Result<()> {
        let mut nodes = self.nodes.write().unwrap();
        
        // Add standard nodes
        self.add_node_internal(&mut nodes, "local_app", "Papin", "application", true, "local", HashMap::new())?;
        self.add_node_internal(&mut nodes, "local_storage", "Local Storage", "storage", true, "local", HashMap::new())?;
        self.add_node_internal(&mut nodes, "memory", "Application Memory", "memory", true, "local", HashMap::new())?;
        self.add_node_internal(&mut nodes, "secure_enclave", "Secure Enclave", "secure_storage", true, "local", HashMap::new())?;
        self.add_node_internal(&mut nodes, "cloud_sync", "Cloud Sync Service", "api", false, "cloud", HashMap::new())?;
        self.add_node_internal(&mut nodes, "local_llm", "Local LLM", "model", true, "local", HashMap::new())?;
        self.add_node_internal(&mut nodes, "cloud_llm", "Cloud LLM", "model", false, "cloud", HashMap::new())?;
        self.add_node_internal(&mut nodes, "telemetry", "Telemetry Service", "api", false, "cloud", HashMap::new())?;
        self.add_node_internal(&mut nodes, "file_system", "File System", "storage", false, "local", HashMap::new())?;
        self.add_node_internal(&mut nodes, "clipboard", "System Clipboard", "memory", false, "local", HashMap::new())?;
        
        Ok(())
    }
    
    /// Add a node to the graph
    fn add_node_internal(
        &self,
        nodes: &mut HashMap<String, DataFlowNode>,
        id: &str,
        name: &str,
        node_type: &str,
        internal: bool,
        location: &str,
        metadata: HashMap<String, String>,
    ) -> Result<()> {
        let node = DataFlowNode {
            id: id.to_string(),
            name: name.to_string(),
            node_type: node_type.to_string(),
            internal,
            location: location.to_string(),
            metadata,
        };
        
        nodes.insert(id.to_string(), node);
        
        Ok(())
    }
    
    /// Add a custom node to the graph
    pub fn add_node(
        &self,
        id: &str,
        name: &str,
        node_type: &str,
        internal: bool,
        location: &str,
        metadata: HashMap<String, String>,
    ) -> Result<()> {
        if !self.enabled {
            return Ok(());
        }
        
        let mut nodes = self.nodes.write().unwrap();
        self.add_node_internal(&mut nodes, id, name, node_type, internal, location, metadata)?;
        
        // Rebuild the graph
        drop(nodes);
        self.rebuild_graph()?;
        
        Ok(())
    }
    
    /// Enable or disable data flow tracking
    pub fn set_enabled(&self, enabled: bool) -> Result<()> {
        if self.enabled == enabled {
            return Ok(()); // No change needed
        }
        
        // Update enabled state
        let mut this = unsafe { &mut *(self as *const Self as *mut Self) };
        this.enabled = enabled;
        
        if enabled {
            // Start tracking
            info!("Enabling data flow tracking");
            self.load_events()?;
            self.initialize_known_nodes()?;
            self.rebuild_graph()?;
            
            record_counter("security.data_flow.enabled", 1.0, None);
        } else {
            // Stop tracking
            info!("Disabling data flow tracking");
            self.events.write().unwrap().clear();
            
            // Clear graph
            *self.graph.write().unwrap() = DataFlowGraph {
                nodes: Vec::new(),
                edges: Vec::new(),
                data_items: HashMap::new(),
            };
            
            record_counter("security.data_flow.disabled", 1.0, None);
        }
        
        Ok(())
    }
    
    /// Load data flow events from disk
    fn load_events(&self) -> Result<()> {
        let log_path = config_path(DATA_FLOW_LOG_FILE);
        
        if !log_path.exists() {
            // No events to load
            return Ok(());
        }
        
        let log_data = fs::read_to_string(&log_path)
            .map_err(|e| format!("Failed to read data flow log: {}", e))?;
        
        let events: Vec<DataFlowEvent> = serde_json::from_str(&log_data)
            .map_err(|e| format!("Failed to parse data flow log: {}", e))?;
        
        let mut event_queue = self.events.write().unwrap();
        event_queue.clear();
        
        for event in events {
            event_queue.push_back(event);
        }
        
        // Limit size
        while event_queue.len() > MAX_LOG_ENTRIES {
            event_queue.pop_front();
        }
        
        debug!("Loaded {} data flow events", event_queue.len());
        
        Ok(())
    }
    
    /// Save data flow events to disk
    fn save_events(&self) -> Result<()> {
        if !self.enabled {
            return Ok(());
        }
        
        let _lock = self.file_lock.lock().unwrap();
        let log_path = config_path(DATA_FLOW_LOG_FILE);
        
        let events: Vec<DataFlowEvent> = self.events.read().unwrap().iter().cloned().collect();
        
        let log_data = serde_json::to_string_pretty(&events)
            .map_err(|e| format!("Failed to serialize data flow log: {}", e))?;
        
        fs::write(&log_path, log_data)
            .map_err(|e| format!("Failed to write data flow log: {}", e))?;
        
        debug!("Saved {} data flow events", events.len());
        
        Ok(())
    }
    
    /// Track a data flow event
    pub fn track_data_flow(
        &self,
        operation: &str,
        data_item: &str,
        classification: DataClassification,
        destination: &str,
    ) -> Result<()> {
        if !self.enabled {
            return Ok(());
        }
        
        // Create event
        let event = DataFlowEvent {
            id: uuid::Uuid::new_v4().to_string(),
            operation: operation.to_string(),
            data_item: data_item.to_string(),
            classification,
            source: "local_app".to_string(), // Default source is the app itself
            destination: destination.to_string(),
            consent_obtained: true, // Default to true, should be overridden as needed
            timestamp: SystemTime::now(),
            encrypted: destination != "memory" && destination != "local_app", // Default to encrypted for external flows
            metadata: HashMap::new(),
        };
        
        // Add event to queue
        {
            let mut events = self.events.write().unwrap();
            events.push_back(event.clone());
            
            // Limit size
            while events.len() > MAX_LOG_ENTRIES {
                events.pop_front();
            }
        }
        
        // Update the graph
        self.update_graph(&event)?;
        
        // Save events periodically (every 10 events)
        if self.events.read().unwrap().len() % 10 == 0 {
            self.save_events()?;
        }
        
        record_counter("security.data_flow.event_tracked", 1.0, None);
        
        Ok(())
    }
    
    /// Track a data flow event with more details
    pub fn track_data_flow_detailed(
        &self,
        operation: &str,
        data_item: &str,
        classification: DataClassification,
        source: &str,
        destination: &str,
        consent_obtained: bool,
        encrypted: bool,
        metadata: HashMap<String, String>,
    ) -> Result<()> {
        if !self.enabled {
            return Ok(());
        }
        
        // Create event
        let event = DataFlowEvent {
            id: uuid::Uuid::new_v4().to_string(),
            operation: operation.to_string(),
            data_item: data_item.to_string(),
            classification,
            source: source.to_string(),
            destination: destination.to_string(),
            consent_obtained,
            timestamp: SystemTime::now(),
            encrypted,
            metadata,
        };
        
        // Add event to queue
        {
            let mut events = self.events.write().unwrap();
            events.push_back(event.clone());
            
            // Limit size
            while events.len() > MAX_LOG_ENTRIES {
                events.pop_front();
            }
        }
        
        // Update the graph
        self.update_graph(&event)?;
        
        // Save events periodically (every 10 events)
        if self.events.read().unwrap().len() % 10 == 0 {
            self.save_events()?;
        }
        
        record_counter("security.data_flow.event_tracked", 1.0, None);
        
        Ok(())
    }
    
    /// Update the data flow graph with a new event
    fn update_graph(&self, event: &DataFlowEvent) -> Result<()> {
        let nodes = self.nodes.read().unwrap();
        let mut graph = self.graph.write().unwrap();
        
        // Add source node if needed
        if !graph.nodes.iter().any(|n| n.id == event.source) {
            if let Some(node) = nodes.get(&event.source) {
                graph.nodes.push(node.clone());
            } else {
                // Unknown node, create a generic one
                graph.nodes.push(DataFlowNode {
                    id: event.source.clone(),
                    name: event.source.clone(),
                    node_type: "unknown".to_string(),
                    internal: false,
                    location: "unknown".to_string(),
                    metadata: HashMap::new(),
                });
            }
        }
        
        // Add destination node if needed
        if !graph.nodes.iter().any(|n| n.id == event.destination) {
            if let Some(node) = nodes.get(&event.destination) {
                graph.nodes.push(node.clone());
            } else {
                // Unknown node, create a generic one
                graph.nodes.push(DataFlowNode {
                    id: event.destination.clone(),
                    name: event.destination.clone(),
                    node_type: "unknown".to_string(),
                    internal: false,
                    location: "unknown".to_string(),
                    metadata: HashMap::new(),
                });
            }
        }
        
        // Add or update edge
        let edge_exists = graph.edges.iter().any(|(src, dst, _)| 
            *src == event.source && *dst == event.destination);
            
        if edge_exists {
            // Update existing edge
            for (_, _, classifications) in graph.edges.iter_mut() {
                if !classifications.contains(&event.classification) {
                    classifications.push(event.classification);
                }
            }
        } else {
            // Add new edge
            graph.edges.push((
                event.source.clone(),
                event.destination.clone(),
                vec![event.classification],
            ));
        }
        
        // Add data item
        graph.data_items.insert(event.data_item.clone(), event.classification);
        
        Ok(())
    }
    
    /// Rebuild the data flow graph from all events
    fn rebuild_graph(&self) -> Result<()> {
        let nodes = self.nodes.read().unwrap();
        let events = self.events.read().unwrap();
        let mut graph = self.graph.write().unwrap();
        
        // Clear existing graph
        *graph = DataFlowGraph {
            nodes: Vec::new(),
            edges: Vec::new(),
            data_items: HashMap::new(),
        };
        
        // Add all nodes (will deduplicate later)
        for (_, node) in nodes.iter() {
            graph.nodes.push(node.clone());
        }
        
        // Add all events to the graph
        for event in events.iter() {
            // Add source node if needed
            if !graph.nodes.iter().any(|n| n.id == event.source) {
                if let Some(node) = nodes.get(&event.source) {
                    graph.nodes.push(node.clone());
                } else {
                    // Unknown node, create a generic one
                    graph.nodes.push(DataFlowNode {
                        id: event.source.clone(),
                        name: event.source.clone(),
                        node_type: "unknown".to_string(),
                        internal: false,
                        location: "unknown".to_string(),
                        metadata: HashMap::new(),
                    });
                }
            }
            
            // Add destination node if needed
            if !graph.nodes.iter().any(|n| n.id == event.destination) {
                if let Some(node) = nodes.get(&event.destination) {
                    graph.nodes.push(node.clone());
                } else {
                    // Unknown node, create a generic one
                    graph.nodes.push(DataFlowNode {
                        id: event.destination.clone(),
                        name: event.destination.clone(),
                        node_type: "unknown".to_string(),
                        internal: false,
                        location: "unknown".to_string(),
                        metadata: HashMap::new(),
                    });
                }
            }
            
            // Add or update edge
            let edge_idx = graph.edges.iter().position(|(src, dst, _)| 
                *src == event.source && *dst == event.destination);
                
            if let Some(idx) = edge_idx {
                // Update existing edge
                let (_, _, classifications) = &mut graph.edges[idx];
                if !classifications.contains(&event.classification) {
                    classifications.push(event.classification);
                }
            } else {
                // Add new edge
                graph.edges.push((
                    event.source.clone(),
                    event.destination.clone(),
                    vec![event.classification],
                ));
            }
            
            // Add data item
            graph.data_items.insert(event.data_item.clone(), event.classification);
        }
        
        // Remove duplicates from nodes
        let mut unique_nodes = Vec::new();
        let mut seen_ids = HashSet::new();
        
        for node in graph.nodes.drain(..) {
            if !seen_ids.contains(&node.id) {
                seen_ids.insert(node.id.clone());
                unique_nodes.push(node);
            }
        }
        
        graph.nodes = unique_nodes;
        
        Ok(())
    }
    
    /// Get recent data flow events
    pub fn get_recent_events(&self, limit: Option<usize>) -> Result<Vec<DataFlowEvent>> {
        let events = self.events.read().unwrap();
        let limit = limit.unwrap_or(100).min(events.len());
        
        // Get the last 'limit' events
        let result: Vec<DataFlowEvent> = events.iter()
            .rev()
            .take(limit)
            .cloned()
            .collect();
            
        Ok(result)
    }
    
    /// Get the current data flow graph
    pub fn get_data_flow_graph(&self) -> Result<DataFlowGraph> {
        Ok(self.graph.read().unwrap().clone())
    }
    
    /// Clear all data flow events
    pub fn clear_events(&self) -> Result<()> {
        if !self.enabled {
            return Ok(());
        }
        
        // Clear events
        self.events.write().unwrap().clear();
        
        // Clear graph
        *self.graph.write().unwrap() = DataFlowGraph {
            nodes: Vec::new(),
            edges: Vec::new(),
            data_items: HashMap::new(),
        };
        
        // Initialize graph with nodes
        self.rebuild_graph()?;
        
        // Save empty events
        self.save_events()?;
        
        info!("Cleared all data flow events");
        record_counter("security.data_flow.events_cleared", 1.0, None);
        
        Ok(())
    }
    
    /// Search for data flow events
    pub fn search_events(
        &self,
        data_item: Option<&str>,
        classification: Option<DataClassification>,
        source: Option<&str>,
        destination: Option<&str>,
        operation: Option<&str>,
        start_time: Option<SystemTime>,
        end_time: Option<SystemTime>,
    ) -> Result<Vec<DataFlowEvent>> {
        let events = self.events.read().unwrap();
        
        let results: Vec<DataFlowEvent> = events.iter()
            .filter(|event| {
                // Match all provided filters
                (data_item.is_none() || data_item.unwrap() == event.data_item) &&
                (classification.is_none() || classification.unwrap() == event.classification) &&
                (source.is_none() || source.unwrap() == event.source) &&
                (destination.is_none() || destination.unwrap() == event.destination) &&
                (operation.is_none() || operation.unwrap() == event.operation) &&
                (start_time.is_none() || start_time.unwrap() <= event.timestamp) &&
                (end_time.is_none() || end_time.unwrap() >= event.timestamp)
            })
            .cloned()
            .collect();
            
        Ok(results)
    }
    
    /// Get data flow statistics
    pub fn get_statistics(&self) -> Result<DataFlowStatistics> {
        let events = self.events.read().unwrap();
        let graph = self.graph.read().unwrap();
        
        // Count by classification
        let mut count_by_classification = HashMap::new();
        count_by_classification.insert(DataClassification::Public, 0);
        count_by_classification.insert(DataClassification::Personal, 0);
        count_by_classification.insert(DataClassification::Sensitive, 0);
        count_by_classification.insert(DataClassification::Confidential, 0);
        
        for event in events.iter() {
            *count_by_classification.entry(event.classification).or_insert(0) += 1;
        }
        
        // Count by destination
        let mut count_by_destination = HashMap::new();
        for event in events.iter() {
            *count_by_destination.entry(event.destination.clone()).or_insert(0) += 1;
        }
        
        // Count by operation
        let mut count_by_operation = HashMap::new();
        for event in events.iter() {
            *count_by_operation.entry(event.operation.clone()).or_insert(0) += 1;
        }
        
        // Calculate stats
        let stats = DataFlowStatistics {
            total_events: events.len(),
            node_count: graph.nodes.len(),
            edge_count: graph.edges.len(),
            data_item_count: graph.data_items.len(),
            classification_counts: count_by_classification,
            destination_counts: count_by_destination,
            operation_counts: count_by_operation,
            external_destinations: graph.nodes.iter()
                .filter(|node| !node.internal)
                .map(|node| node.id.clone())
                .collect(),
        };
        
        Ok(stats)
    }
}

/// Statistics about data flows
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataFlowStatistics {
    /// Total number of events
    pub total_events: usize,
    
    /// Number of nodes in the graph
    pub node_count: usize,
    
    /// Number of edges in the graph
    pub edge_count: usize,
    
    /// Number of unique data items
    pub data_item_count: usize,
    
    /// Count of events by classification
    pub classification_counts: HashMap<DataClassification, usize>,
    
    /// Count of events by destination
    pub destination_counts: HashMap<String, usize>,
    
    /// Count of events by operation
    pub operation_counts: HashMap<String, usize>,
    
    /// List of external destinations
    pub external_destinations: Vec<String>,
}
</file>

<file path="src/services/mod.rs">
pub mod ai;
pub mod api;
pub mod auth;
pub mod chat;
pub mod mcp;

// Export key service types
pub use ai::AiService;
pub use api::ApiService;
pub use auth::AuthService;
pub use chat::ChatService;
pub use mcp::McpService;
</file>

<file path="Cargo.toml">
[package]
name = "papin"
version = "1.0.0"
description = "Papin - an MCP Client"
authors = ["MCP Team"]
license = "MIT"
edition = "2021"
rust-version = "1.65"

[dependencies]
# Tauri and system dependencies
tauri = { version = "1.5", features = ["dialog-all", "fs-all", "http-all", "shell-open", "updater", "protocol-asset"] }
tauri-build = { version = "1.5", features = [] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tokio = { version = "1.35", features = ["full"] }
tokio-stream = "0.1"
axum = { version = "0.7", features = ["multipart"] }

# Logging and observability
log = "0.4"
env_logger = "0.10"
tracing = "0.1"
tracing-subscriber = "0.3"
metrics = "0.21"
metrics-exporter-prometheus = "0.12"
chrono = { version = "0.4", features = ["serde"] }

# Network and API
reqwest = { version = "0.11", features = ["json", "stream", "multipart"] }
url = "2.5"
http = "0.2"
hyper = { version = "1.1", features = ["full"] }

# Utilities
anyhow = "1.0"
thiserror = "1.0"
uuid = { version = "1.6", features = ["v4", "serde"] }
parking_lot = "0.12"
once_cell = "1.19"
lazy_static = "1.4"
regex = "1.10"
rand = "0.8"
zstd = "0.13"
flate2 = "1.0"
lru = "0.12"
tempfile = "3.8"
dirs = "5.0"

# Memory optimization (optional)
mimalloc = { version = "0.1", optional = true }

# Performance benchmarking
criterion = { version = "0.5", optional = true }

# System information
sys-info = "0.9"

[dev-dependencies]
mockall = "0.12"
tempfile = "3.8"
criterion = "0.5"
insta = "1.34"
test-log = "0.2"
proptest = "1.4"

[features]
default = []
memory-optimizations = ["mimalloc"]
benchmarking = ["criterion"]
telemetry = []
canary = []
# Custom protocol for deep linking support
custom-protocol = ["tauri/custom-protocol"]

[build-dependencies]
tauri-build = { version = "1.5", features = [] }

[profile.release]
panic = "abort"
codegen-units = 1
lto = true
strip = true
opt-level = "s"

[profile.dev.package."*"]
opt-level = 2

[profile.release.package."*"]
opt-level = 3

# [[bench]]
# name = "performance_bench"
# harness = false
# required-features = ["benchmarking"]

[package.metadata.bundle]
name = "Papin"
identifier = "com.papin.app"
icon = ["icons/32x32.png", "icons/128x128.png", "icons/128x128@2x.png", "icons/icon.icns", "icons/icon.ico"]
copyright = "© 2025 MCP Team"
category = "DeveloperTool"
short_description = "Papin - an MCP Client"
long_description = """
Papin is a cross-platform desktop application for accessing and interacting with the MCP system.
It provides a rich set of features, robust offline capabilities, and excellent performance.
"""
deb_depends = ["libssl3", "libwebkit2gtk-4.0-37"]
osx_minimum_system_version = "10.15"
osx_frameworks = ["WebKit", "AppKit", "Foundation"]
</file>

<file path="src-frontend/package.json">
{
  "name": "papin",
  "private": true,
  "version": "0.1.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "tsc && vite build",
    "preview": "vite preview",
    "tauri": "npx @tauri-apps/cli",
    "tauri:dev": "npx @tauri-apps/cli dev"
  },
  "dependencies": {
    "@tauri-apps/api": "^1.4.0",
    "react": "^18.2.0",
    "react-dom": "^18.2.0"
  },
  "devDependencies": {
    "@tauri-apps/cli": "^1.4.0",
    "@types/node": "^20.1.4",
    "@types/react": "^18.2.6",
    "@types/react-dom": "^18.2.4",
    "@typescript-eslint/eslint-plugin": "^5.59.5",
    "@typescript-eslint/parser": "^5.59.5",
    "@vitejs/plugin-react": "^4.0.0",
    "eslint": "^8.40.0",
    "eslint-config-prettier": "^8.8.0",
    "eslint-plugin-import": "^2.27.5",
    "eslint-plugin-jsx-a11y": "^6.7.1",
    "eslint-plugin-react": "^7.32.2",
    "eslint-plugin-react-hooks": "^4.6.0",
    "typescript": "^5.0.4",
    "vite": "^4.3.5"
  }
}
</file>

<file path="src-frontend/src/components/AppShell.css">
/* AppShell styles */

.app-shell {
  display: flex;
  flex-direction: column;
  height: 100vh;
  width: 100vw;
  overflow: hidden;
}

.app-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 0 16px;
  height: 56px;
  background-color: #2196f3;
  color: white;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

.app-logo {
  font-size: 18px;
  font-weight: 600;
}

.app-subtitle {
  font-size: 14px;
  font-weight: 400;
  margin-left: 8px;
  opacity: 0.8;
}

.app-toolbar {
  display: flex;
  align-items: center;
  gap: 8px;
}

.app-content {
  display: flex;
  flex: 1;
  overflow: hidden;
  position: relative;
}

.app-sidebar {
  width: 240px;
  height: 100%;
  background-color: #f5f5f5;
  border-right: 1px solid #e0e0e0;
  overflow-y: auto;
}

.app-main {
  flex: 1;
  height: 100%;
  overflow-y: auto;
  position: relative;
  padding: 20px;
}

.conversation-container {
  max-width: 800px;
  margin: 0 auto;
}

.message {
  padding: 12px 16px;
  margin-bottom: 16px;
  background-color: #f9f9f9;
  border-radius: 8px;
  border: 1px solid #e0e0e0;
}
</file>

<file path="src-frontend/src/components/AppShell.tsx">
// AppShell.tsx
//
// This is a sample implementation of the AppShell component with collaboration features.
// It demonstrates how to integrate collaboration into the main UI.

import React, { useRef, useEffect, useState } from 'react';
import { invoke } from '@tauri-apps/api/tauri';
import { FeatureFlags } from '../../src/feature_flags';
import HelpButton from './help/HelpButton';

// Import collaboration components
import { 
  CollaborationProvider, 
  CollaborationPanel,
  CursorOverlay,
  SelectionOverlay,
  ConnectionStatus
} from './collaboration';

// Import styles
import './AppShell.css';
import '../styles/collaboration.css';
import HelpButton from './help/HelpButton';

interface AppShellProps {
  featureFlags: FeatureFlags;
  // Other props would go here in a real implementation
}

const AppShell: React.FC<AppShellProps> = ({ featureFlags }) => {
  const mainContentRef = useRef<HTMLDivElement>(null);
  const editorRef = useRef<HTMLDivElement>(null);
  const [showCollaborationPanel, setShowCollaborationPanel] = useState<boolean>(false);
  const [currentConversationId, setCurrentConversationId] = useState<string>('conversation-123'); // Example ID
  const [collaborationEnabled, setCollaborationEnabled] = useState<boolean>(false);
  const [connectionStatus, setConnectionStatus] = useState<ConnectionStatus>(ConnectionStatus.Disconnected);
  const [helpVisible, setHelpVisible] = useState<boolean>(false);
  
  // Initialize feature flags
  useEffect(() => {
    // Check if collaboration feature is enabled
    if (featureFlags.contains(FeatureFlags.COLLABORATION)) {
      setCollaborationEnabled(true);
    }
  }, [featureFlags]);
  
  // Initialize collaboration system
  useEffect(() => {
    if (collaborationEnabled) {
      const initCollaboration = async () => {
        try {
          // Initialize the collaboration system with default config
          await invoke('init_collaboration_system', { config: null });
          
          // Get the initial connection status
          const status = await invoke<ConnectionStatus>('get_connection_status');
          setConnectionStatus(status);
        } catch (error) {
          console.error('Failed to initialize collaboration system:', error);
        }
      };
      
      initCollaboration();
    }
  }, [collaborationEnabled]);
  
  // Set up keyboard shortcut for help
  useEffect(() => {
    const handleKeyDown = (e: KeyboardEvent) => {
      // F1 key for help
      if (e.key === 'F1') {
        e.preventDefault();
        setHelpVisible(true);
      }
    };
    
    window.addEventListener('keydown', handleKeyDown);
    return () => {
      window.removeEventListener('keydown', handleKeyDown);
    };
  }, []);
  
  // Poll for connection status updates
  useEffect(() => {
    if (!collaborationEnabled) return;
    
    const interval = setInterval(async () => {
      try {
        const status = await invoke<ConnectionStatus>('get_connection_status');
        setConnectionStatus(status);
      } catch (error) {
        console.error('Failed to get connection status:', error);
      }
    }, 5000);
    
    return () => clearInterval(interval);
  }, [collaborationEnabled]);
  
  // Update current conversation ID when conversation changes
  const handleConversationChange = (conversationId: string) => {
    setCurrentConversationId(conversationId);
  };
  
  // Toggle collaboration panel
  const toggleCollaborationPanel = () => {
    setShowCollaborationPanel(!showCollaborationPanel);
  };
  
  // Render collaboration status indicator
  const renderCollaborationStatus = () => {
    if (!collaborationEnabled) return null;
    
    let color = '#9E9E9E';
    let title = 'Collaboration: Disconnected';
    
    switch (connectionStatus) {
      case ConnectionStatus.Connected:
        color = '#4CAF50';
        title = 'Collaboration: Connected';
        break;
      case ConnectionStatus.Connecting:
        color = '#FFC107';
        title = 'Collaboration: Connecting';
        break;
      case ConnectionStatus.Limited:
        color = '#FF9800';
        title = 'Collaboration: Limited Connectivity';
        break;
      case ConnectionStatus.Error:
        color = '#F44336';
        title = 'Collaboration: Error';
        break;
      default:
        break;
    }
    
    return (
      <button
        onClick={toggleCollaborationPanel}
        title={title}
        className="collaboration-status-button"
      >
        <svg 
          width="20" 
          height="20" 
          viewBox="0 0 24 24" 
          fill="none" 
          stroke="currentColor" 
          strokeWidth="2" 
          strokeLinecap="round" 
          strokeLinejoin="round"
        >
          <path d="M17 21v-2a4 4 0 0 0-4-4H5a4 4 0 0 0-4 4v2"></path>
          <circle cx="9" cy="7" r="4"></circle>
          <path d="M23 21v-2a4 4 0 0 0-3-3.87"></path>
          <path d="M16 3.13a4 4 0 0 1 0 7.75"></path>
        </svg>
        
        {/* Status indicator dot */}
        <div 
          className="collaboration-status-indicator"
          style={{ backgroundColor: color }}
        />
      </button>
    );
  };
  
  return (
    <CollaborationProvider>
      <div className="app-shell">
        {/* App toolbar with collaboration button */}
        <header className="app-header">
          <div className="app-logo">Papin</div>
          <div className="app-subtitle">an MCP Client</div>
          <div className="app-toolbar">
            {/* Other toolbar buttons would go here */}
            {collaborationEnabled && renderCollaborationStatus()}
            <HelpButton 
              isOpen={helpVisible} 
              onOpenChange={(open) => setHelpVisible(open)} 
            />
          </div>
        </header>
        
        <div className="app-content">
          {/* Main sidebar */}
          <div className="app-sidebar">
            {/* Sidebar content would go here */}
          </div>
          
          {/* Main content area with conversation */}
          <div className="app-main" ref={mainContentRef}>
            <div className="conversation-container" ref={editorRef}>
              {/* Conversation content would go here */}
              <div id="message-1" className="message">
                This is a sample message that can be edited collaboratively.
              </div>
              <div id="message-2" className="message">
                Multiple users can see each other's cursors and selections when collaboration is enabled.
              </div>
            </div>
            
            {/* Collaboration cursor overlay */}
            {collaborationEnabled && (
              <CursorOverlay containerRef={mainContentRef} />
            )}
            
            {/* Collaboration selection overlay */}
            {collaborationEnabled && (
              <SelectionOverlay editorRef={editorRef} />
            )}
          </div>
          
          {/* Collaboration panel (shown when toggled) */}
          {collaborationEnabled && showCollaborationPanel && (
            <div className="collaboration-panel-container">
              <CollaborationPanel conversationId={currentConversationId} />
            </div>
          )}
        </div>
      </div>
    </CollaborationProvider>
  );
};

export default AppShell;
</file>

<file path="src-frontend/src/components/dashboard/ResourceDashboard.tsx">
import React, { useState, useEffect } from 'react';
import { invoke } from '@tauri-apps/api/tauri';
import { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip, Legend, ResponsiveContainer, BarChart, Bar } from 'recharts';
import './ResourceDashboard.css';
import LLMPerformanceDashboard from './llm/LLMPerformanceDashboard';

interface MetricTimeSeries {
  timestamp: string;
  value: number;
}

interface HistogramStats {
  min: number;
  max: number;
  avg: number;
  count: number;
  p50: number;
  p90: number;
  p99: number;
}

interface TimerStats {
  min_ms: number;
  max_ms: number;
  avg_ms: number;
  count: number;
  p50_ms: number;
  p90_ms: number;
  p99_ms: number;
}

interface SystemResources {
  cpu_usage: number;
  memory_usage: number;
  memory_total: number;
  disk_usage: number;
  disk_total: number;
  network_rx: number;
  network_tx: number;
}

interface TelemetryStats {
  events_collected: number;
  events_sent: number;
  metrics_collected: number;
  metrics_sent: number;
  logs_collected: number;
  logs_sent: number;
  batches_sent: number;
  batches_failed: number;
  last_batch_sent?: string;
}

interface MetricsData {
  counters: Record<string, number>;
  gauges: Record<string, number>;
  histograms: Record<string, HistogramStats>;
  timers: Record<string, TimerStats>;
  
  cpu_history: MetricTimeSeries[];
  memory_history: MetricTimeSeries[];
  api_latency_history: MetricTimeSeries[];
  
  system: SystemResources;
  telemetry: TelemetryStats;
}

const ResourceDashboard: React.FC = () => {
  const [activeTab, setActiveTab] = useState<'overview' | 'cpu' | 'memory' | 'network' | 'api' | 'telemetry' | 'llm'>('overview');
  const [metrics, setMetrics] = useState<MetricsData | null>(null);
  const [loading, setLoading] = useState<boolean>(true);
  const [error, setError] = useState<string | null>(null);
  const [autoRefresh, setAutoRefresh] = useState<boolean>(true);
  const [refreshInterval, setRefreshInterval] = useState<number>(5000);

  // Fetch metrics data
  const fetchMetrics = async () => {
    try {
      setLoading(true);
      
      // Invoke Tauri commands to get metrics
      const [
        counters,
        gauges,
        histograms,
        timers,
        cpuHistory,
        memoryHistory,
        apiLatencyHistory,
        systemResources,
        telemetryStats
      ] = await Promise.all([
        invoke('get_counters_report'),
        invoke('get_gauges_report'),
        invoke('get_histograms_report'),
        invoke('get_timers_report'),
        invoke('get_metric_history', { metricName: 'system.cpu_usage', metricType: 'Gauge' }),
        invoke('get_metric_history', { metricName: 'system.memory_usage', metricType: 'Gauge' }),
        invoke('get_metric_history', { metricName: 'api.latency', metricType: 'Timer' }),
        invoke('get_system_resources'),
        invoke('get_telemetry_stats')
      ]);
      
      setMetrics({
        counters: counters as Record<string, number>,
        gauges: gauges as Record<string, number>,
        histograms: histograms as Record<string, HistogramStats>,
        timers: timers as Record<string, TimerStats>,
        cpu_history: cpuHistory as MetricTimeSeries[],
        memory_history: memoryHistory as MetricTimeSeries[],
        api_latency_history: apiLatencyHistory as MetricTimeSeries[],
        system: systemResources as SystemResources,
        telemetry: telemetryStats as TelemetryStats
      });
      
      setError(null);
    } catch (err) {
      console.error('Error fetching metrics:', err);
      setError(`Failed to fetch metrics: ${err}`);
    } finally {
      setLoading(false);
    }
  };

  // Set up auto-refresh
  useEffect(() => {
    fetchMetrics();
    
    let intervalId: number;
    
    if (autoRefresh) {
      intervalId = window.setInterval(fetchMetrics, refreshInterval);
    }
    
    return () => {
      if (intervalId) {
        clearInterval(intervalId);
      }
    };
  }, [autoRefresh, refreshInterval]);

  // Format bytes to a human-readable format
  const formatBytes = (bytes: number): string => {
    if (bytes === 0) return '0 B';
    
    const k = 1024;
    const sizes = ['B', 'KB', 'MB', 'GB', 'TB'];
    const i = Math.floor(Math.log(bytes) / Math.log(k));
    
    return `${parseFloat((bytes / Math.pow(k, i)).toFixed(2))} ${sizes[i]}`;
  };

  // Format milliseconds for display
  const formatTime = (ms: number): string => {
    if (ms < 1) {
      return `${(ms * 1000).toFixed(2)} μs`;
    } else if (ms < 1000) {
      return `${ms.toFixed(2)} ms`;
    } else {
      return `${(ms / 1000).toFixed(2)} s`;
    }
  };

  // Render overview tab
  const renderOverview = () => {
    if (!metrics) return null;
    
    return (
      <div className="overview-grid">
        <div className="metric-card">
          <h3>CPU Usage</h3>
          <div className="metric-value">{metrics.system.cpu_usage.toFixed(1)}%</div>
          <ResponsiveContainer width="100%" height={100}>
            <LineChart data={metrics.cpu_history}>
              <Line type="monotone" dataKey="value" stroke="#8884d8" dot={false} />
            </LineChart>
          </ResponsiveContainer>
        </div>
        
        <div className="metric-card">
          <h3>Memory Usage</h3>
          <div className="metric-value">
            {formatBytes(metrics.system.memory_usage)} / {formatBytes(metrics.system.memory_total)}
          </div>
          <div className="meter">
            <div 
              className="meter-fill" 
              style={{ width: `${(metrics.system.memory_usage / metrics.system.memory_total) * 100}%` }}
            />
          </div>
        </div>
        
        <div className="metric-card">
          <h3>Disk Usage</h3>
          <div className="metric-value">
            {formatBytes(metrics.system.disk_usage)} / {formatBytes(metrics.system.disk_total)}
          </div>
          <div className="meter">
            <div 
              className="meter-fill" 
              style={{ width: `${(metrics.system.disk_usage / metrics.system.disk_total) * 100}%` }}
            />
          </div>
        </div>
        
        <div className="metric-card">
          <h3>Network Traffic</h3>
          <div className="metric-value">
            ↓ {formatBytes(metrics.system.network_rx)}/s &nbsp; ↑ {formatBytes(metrics.system.network_tx)}/s
          </div>
        </div>
        
        <div className="metric-card">
          <h3>API Latency</h3>
          <div className="metric-value">
            {metrics.timers['api.latency'] ? formatTime(metrics.timers['api.latency'].avg_ms) : 'N/A'}
          </div>
          <ResponsiveContainer width="100%" height={100}>
            <LineChart data={metrics.api_latency_history}>
              <Line type="monotone" dataKey="value" stroke="#82ca9d" dot={false} />
            </LineChart>
          </ResponsiveContainer>
        </div>
        
        <div className="metric-card">
          <h3>Messages</h3>
          <div className="metric-value">
            {metrics.counters['message.sent'] || 0} sent / {metrics.counters['message.received'] || 0} received
          </div>
        </div>
      </div>
    );
  };

  // Render CPU tab
  const renderCpuTab = () => {
    if (!metrics) return null;
    
    return (
      <div className="detailed-metrics">
        <div className="metric-detail-card">
          <h3>CPU Usage Over Time</h3>
          <ResponsiveContainer width="100%" height={300}>
            <LineChart data={metrics.cpu_history}>
              <CartesianGrid strokeDasharray="3 3" />
              <XAxis 
                dataKey="timestamp" 
                tickFormatter={(timestamp) => new Date(timestamp).toLocaleTimeString()} 
              />
              <YAxis domain={[0, 100]} />
              <Tooltip
                formatter={(value: number) => `${value.toFixed(1)}%`}
                labelFormatter={(timestamp) => new Date(timestamp).toLocaleString()}
              />
              <Legend />
              <Line name="CPU Usage %" type="monotone" dataKey="value" stroke="#8884d8" dot={false} />
            </LineChart>
          </ResponsiveContainer>
        </div>
        
        <div className="metric-detail-card">
          <h3>Process CPU Usage</h3>
          <div className="stats-grid">
            {Object.entries(metrics.gauges)
              .filter(([key]) => key.includes('cpu'))
              .map(([key, value]) => (
                <div key={key} className="stat-item">
                  <div className="stat-label">{key.replace('system.', '')}</div>
                  <div className="stat-value">{value.toFixed(1)}%</div>
                </div>
              ))}
          </div>
        </div>
      </div>
    );
  };

  // Render Memory tab
  const renderMemoryTab = () => {
    if (!metrics) return null;
    
    // Calculate memory usage breakdown
    const usageBreakdown = [
      { name: 'Application', value: metrics.gauges['memory.app'] || 0 },
      { name: 'System', value: metrics.gauges['memory.system'] || 0 },
      { name: 'Other', value: metrics.system.memory_usage - (metrics.gauges['memory.app'] || 0) - (metrics.gauges['memory.system'] || 0) }
    ].filter(item => item.value > 0);
    
    return (
      <div className="detailed-metrics">
        <div className="metric-detail-card">
          <h3>Memory Usage Over Time</h3>
          <ResponsiveContainer width="100%" height={300}>
            <LineChart data={metrics.memory_history}>
              <CartesianGrid strokeDasharray="3 3" />
              <XAxis 
                dataKey="timestamp" 
                tickFormatter={(timestamp) => new Date(timestamp).toLocaleTimeString()}
              />
              <YAxis 
                domain={[0, metrics.system.memory_total]} 
                tickFormatter={(value) => formatBytes(value)}
              />
              <Tooltip
                formatter={(value: number) => formatBytes(value)}
                labelFormatter={(timestamp) => new Date(timestamp).toLocaleString()}
              />
              <Legend />
              <Line name="Memory Usage" type="monotone" dataKey="value" stroke="#82ca9d" dot={false} />
            </LineChart>
          </ResponsiveContainer>
        </div>
        
        <div className="metric-detail-card">
          <h3>Memory Usage Breakdown</h3>
          <ResponsiveContainer width="100%" height={300}>
            <BarChart data={usageBreakdown}>
              <CartesianGrid strokeDasharray="3 3" />
              <XAxis dataKey="name" />
              <YAxis tickFormatter={(value) => formatBytes(value)} />
              <Tooltip formatter={(value: number) => formatBytes(value)} />
              <Legend />
              <Bar name="Memory Usage" dataKey="value" fill="#8884d8" />
            </BarChart>
          </ResponsiveContainer>
        </div>
      </div>
    );
  };

  // Render Network tab
  const renderNetworkTab = () => {
    if (!metrics) return null;
    
    return (
      <div className="detailed-metrics">
        <div className="metric-detail-card">
          <h3>Network Throughput</h3>
          <div className="stats-grid">
            <div className="stat-item">
              <div className="stat-label">Download Rate</div>
              <div className="stat-value">{formatBytes(metrics.system.network_rx)}/s</div>
            </div>
            <div className="stat-item">
              <div className="stat-label">Upload Rate</div>
              <div className="stat-value">{formatBytes(metrics.system.network_tx)}/s</div>
            </div>
            <div className="stat-item">
              <div className="stat-label">Total Downloaded</div>
              <div className="stat-value">{formatBytes(metrics.counters['network.rx.total'] || 0)}</div>
            </div>
            <div className="stat-item">
              <div className="stat-label">Total Uploaded</div>
              <div className="stat-value">{formatBytes(metrics.counters['network.tx.total'] || 0)}</div>
            </div>
          </div>
        </div>
        
        <div className="metric-detail-card">
          <h3>API Requests</h3>
          <div className="stats-grid">
            <div className="stat-item">
              <div className="stat-label">Total Requests</div>
              <div className="stat-value">{metrics.counters['api.requests.total'] || 0}</div>
            </div>
            <div className="stat-item">
              <div className="stat-label">Successful Requests</div>
              <div className="stat-value">{metrics.counters['api.requests.success'] || 0}</div>
            </div>
            <div className="stat-item">
              <div className="stat-label">Failed Requests</div>
              <div className="stat-value">{metrics.counters['api.requests.failure'] || 0}</div>
            </div>
            <div className="stat-item">
              <div className="stat-label">Error Rate</div>
              <div className="stat-value">
                {metrics.counters['api.requests.total'] 
                  ? ((metrics.counters['api.requests.failure'] || 0) / metrics.counters['api.requests.total'] * 100).toFixed(2)
                  : 0}%
              </div>
            </div>
          </div>
        </div>
      </div>
    );
  };

  // Render API tab
  const renderApiTab = () => {
    if (!metrics) return null;
    
    return (
      <div className="detailed-metrics">
        <div className="metric-detail-card">
          <h3>API Latency Over Time</h3>
          <ResponsiveContainer width="100%" height={300}>
            <LineChart data={metrics.api_latency_history}>
              <CartesianGrid strokeDasharray="3 3" />
              <XAxis 
                dataKey="timestamp" 
                tickFormatter={(timestamp) => new Date(timestamp).toLocaleTimeString()}
              />
              <YAxis tickFormatter={(value) => formatTime(value)} />
              <Tooltip
                formatter={(value: number) => formatTime(value)}
                labelFormatter={(timestamp) => new Date(timestamp).toLocaleString()}
              />
              <Legend />
              <Line name="API Latency" type="monotone" dataKey="value" stroke="#8884d8" dot={false} />
            </LineChart>
          </ResponsiveContainer>
        </div>
        
        <div className="metric-detail-card">
          <h3>API Endpoints</h3>
          <table className="metrics-table">
            <thead>
              <tr>
                <th>Endpoint</th>
                <th>Requests</th>
                <th>Avg. Latency</th>
                <th>Success Rate</th>
              </tr>
            </thead>
            <tbody>
              {Object.entries(metrics.timers)
                .filter(([key]) => key.startsWith('api.endpoint.'))
                .map(([key, value]) => {
                  const endpoint = key.replace('api.endpoint.', '');
                  const requests = metrics.counters[`api.endpoint.${endpoint}.count`] || 0;
                  const successes = metrics.counters[`api.endpoint.${endpoint}.success`] || 0;
                  const successRate = requests > 0 ? (successes / requests * 100).toFixed(1) : '100.0';
                  
                  return (
                    <tr key={key}>
                      <td>{endpoint}</td>
                      <td>{requests}</td>
                      <td>{formatTime(value.avg_ms)}</td>
                      <td className={Number(successRate) < 99 ? 'stat-warning' : ''}>{successRate}%</td>
                    </tr>
                  );
                })}
            </tbody>
          </table>
        </div>
      </div>
    );
  };

  // Render Telemetry tab
  const renderTelemetryTab = () => {
    if (!metrics) return null;
    
    return (
      <div className="detailed-metrics">
        <div className="metric-detail-card">
          <h3>Telemetry Overview</h3>
          <div className="stats-grid">
            <div className="stat-item">
              <div className="stat-label">Events Collected</div>
              <div className="stat-value">{metrics.telemetry.events_collected}</div>
            </div>
            <div className="stat-item">
              <div className="stat-label">Events Sent</div>
              <div className="stat-value">{metrics.telemetry.events_sent}</div>
            </div>
            <div className="stat-item">
              <div className="stat-label">Metrics Collected</div>
              <div className="stat-value">{metrics.telemetry.metrics_collected}</div>
            </div>
            <div className="stat-item">
              <div className="stat-label">Metrics Sent</div>
              <div className="stat-value">{metrics.telemetry.metrics_sent}</div>
            </div>
            <div className="stat-item">
              <div className="stat-label">Logs Collected</div>
              <div className="stat-value">{metrics.telemetry.logs_collected}</div>
            </div>
            <div className="stat-item">
              <div className="stat-label">Logs Sent</div>
              <div className="stat-value">{metrics.telemetry.logs_sent}</div>
            </div>
            <div className="stat-item">
              <div className="stat-label">Batches Sent</div>
              <div className="stat-value">{metrics.telemetry.batches_sent}</div>
            </div>
            <div className="stat-item">
              <div className="stat-label">Batches Failed</div>
              <div className="stat-value">{metrics.telemetry.batches_failed}</div>
            </div>
          </div>
        </div>
        
        <div className="metric-detail-card">
          <h3>Telemetry Status</h3>
          <div className="stats-detail">
            <div className="stat-row">
              <div className="stat-label">Last Batch Sent</div>
              <div className="stat-value">
                {metrics.telemetry.last_batch_sent 
                  ? new Date(metrics.telemetry.last_batch_sent).toLocaleString()
                  : 'Never'}
              </div>
            </div>
            <div className="stat-row">
              <div className="stat-label">Sending Rate</div>
              <div className="stat-value">
                {metrics.counters['telemetry.sending_rate'] 
                  ? `${metrics.counters['telemetry.sending_rate'].toFixed(2)} batches/minute`
                  : 'N/A'}
              </div>
            </div>
            <div className="stat-row">
              <div className="stat-label">Success Rate</div>
              <div className="stat-value">
                {metrics.telemetry.batches_sent + metrics.telemetry.batches_failed > 0
                  ? `${(metrics.telemetry.batches_sent / (metrics.telemetry.batches_sent + metrics.telemetry.batches_failed) * 100).toFixed(1)}%`
                  : 'N/A'}
              </div>
            </div>
            <div className="stat-row">
              <div className="stat-label">Connection Status</div>
              <div className="stat-value">
                <span className={metrics.gauges['telemetry.connected'] > 0 ? 'status-indicator connected' : 'status-indicator disconnected'}></span>
                {metrics.gauges['telemetry.connected'] > 0 ? 'Connected' : 'Disconnected'}
              </div>
            </div>
          </div>
        </div>
      </div>
    );
  };

  return (
    <div className="resource-dashboard">
      <div className="dashboard-header">
        <h2>System Resources & Performance</h2>
        <div className="dashboard-controls">
          <label className="refresh-control">
            <input 
              type="checkbox" 
              checked={autoRefresh} 
              onChange={(e) => setAutoRefresh(e.target.checked)} 
            />
            Auto-refresh
          </label>
          {autoRefresh && (
            <select 
              value={refreshInterval} 
              onChange={(e) => setRefreshInterval(Number(e.target.value))}
              className="interval-select"
            >
              <option value={1000}>Every 1s</option>
              <option value={5000}>Every 5s</option>
              <option value={10000}>Every 10s</option>
              <option value={30000}>Every 30s</option>
              <option value={60000}>Every minute</option>
            </select>
          )}
          <button 
            onClick={fetchMetrics} 
            className="refresh-button"
            disabled={loading}
          >
            {loading ? 'Refreshing...' : 'Refresh Now'}
          </button>
        </div>
      </div>
      
      <div className="dashboard-tabs">
        <button 
          className={activeTab === 'overview' ? 'active' : ''} 
          onClick={() => setActiveTab('overview')}
        >
          Overview
        </button>
        <button 
          className={activeTab === 'cpu' ? 'active' : ''} 
          onClick={() => setActiveTab('cpu')}
        >
          CPU
        </button>
        <button 
          className={activeTab === 'memory' ? 'active' : ''} 
          onClick={() => setActiveTab('memory')}
        >
          Memory
        </button>
        <button 
          className={activeTab === 'network' ? 'active' : ''} 
          onClick={() => setActiveTab('network')}
        >
          Network
        </button>
        <button 
          className={activeTab === 'api' ? 'active' : ''} 
          onClick={() => setActiveTab('api')}
        >
          API
        </button>
        <button 
          className={activeTab === 'telemetry' ? 'active' : ''} 
          onClick={() => setActiveTab('telemetry')}
        >
          Telemetry
        </button>
        <button 
          className={activeTab === 'llm' ? 'active' : ''} 
          onClick={() => setActiveTab('llm')}
        >
          LLM
        </button>
      </div>
      
      <div className="dashboard-content">
        {error && (
          <div className="error-message">
            {error}
          </div>
        )}
        
        {loading && !metrics ? (
          <div className="loading-indicator">
            <div className="spinner"></div>
            <p>Loading metrics data...</p>
          </div>
        ) : (
          <>
            {activeTab === 'overview' && renderOverview()}
            {activeTab === 'cpu' && renderCpuTab()}
            {activeTab === 'memory' && renderMemoryTab()}
            {activeTab === 'network' && renderNetworkTab()}
            {activeTab === 'api' && renderApiTab()}
            {activeTab === 'telemetry' && renderTelemetryTab()}
            {activeTab === 'llm' && <LLMPerformanceDashboard />}
          </>
        )}
      </div>
    </div>
  );
};

export default ResourceDashboard;
</file>

<file path="src-frontend/src/lazy/Settings.css">
.settings-container {
  height: 100%;
  overflow-y: auto;
  padding: var(--spacing-lg);
  background-color: var(--color-background);
}

.settings-content {
  max-width: 800px;
  margin: 0 auto;
}

.settings-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  margin-bottom: var(--spacing-lg);
}

.settings-heading {
  font-size: var(--font-size-xl);
  font-weight: 600;
  margin: 0;
  color: var(--color-on-surface);
}

.header-actions {
  display: flex;
  gap: var(--spacing-sm);
}

.settings-section {
  margin-bottom: var(--spacing-xl);
  padding: var(--spacing-lg);
  background-color: var(--color-surface);
  border-radius: var(--radius-lg);
  box-shadow: var(--shadow-sm);
}

.settings-section h2 {
  font-size: var(--font-size-lg);
  font-weight: 600;
  margin: 0 0 var(--spacing-lg) 0;
  padding-bottom: var(--spacing-sm);
  border-bottom: 1px solid var(--color-border);
  color: var(--color-on-surface);
  display: flex;
  align-items: center;
}

.settings-form {
  display: flex;
  flex-direction: column;
  gap: var(--spacing-lg);
}

.form-group {
  display: flex;
  flex-direction: column;
  gap: var(--spacing-xxs);
}

.form-label {
  font-size: var(--font-size-md);
  font-weight: 500;
  color: var(--color-on-surface);
}

.form-hint {
  font-size: var(--font-size-sm);
  color: var(--color-on-surface-variant);
  margin: var(--spacing-xxs) 0 0 0;
  display: flex;
  align-items: center;
  gap: var(--spacing-xs);
}

.select-input {
  height: 2.5rem;
  padding: 0 var(--spacing-md);
  font-size: var(--font-size-md);
  background-color: var(--color-background);
  color: var(--color-on-background);
  border: 1px solid var(--color-border);
  border-radius: var(--radius-md);
  transition: all var(--transition-fast);
}

.select-input:focus {
  outline: none;
  border-color: var(--color-primary);
  box-shadow: 0 0 0 2px var(--color-primary-light);
}

.checkbox-group {
  display: flex;
  flex-direction: column;
  gap: var(--spacing-xxs);
}

.checkbox-label {
  display: flex;
  align-items: center;
  gap: var(--spacing-xs);
  cursor: pointer;
  font-size: var(--font-size-md);
  color: var(--color-on-surface);
}

.checkbox-label input[type="checkbox"] {
  width: 1.25rem;
  height: 1.25rem;
  border: 1px solid var(--color-border);
  border-radius: var(--radius-sm);
  cursor: pointer;
}

.range-control {
  display: flex;
  align-items: center;
  gap: var(--spacing-md);
}

.range-input {
  flex: 1;
  height: 0.5rem;
  background-color: var(--color-surface-variant);
  border-radius: var(--radius-full);
  -webkit-appearance: none;
  appearance: none;
}

.range-input::-webkit-slider-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  background-color: var(--color-primary);
  border-radius: 50%;
  cursor: pointer;
}

.range-input::-moz-range-thumb {
  width: 1.25rem;
  height: 1.25rem;
  background-color: var(--color-primary);
  border: none;
  border-radius: 50%;
  cursor: pointer;
}

.range-value {
  font-size: var(--font-size-md);
  color: var(--color-on-surface);
  width: 2.5rem;
  text-align: center;
}

.textarea-input {
  width: 100%;
  min-height: 100px;
  padding: var(--spacing-sm) var(--spacing-md);
  background-color: var(--color-background);
  color: var(--color-on-background);
  border: 1px solid var(--color-border);
  border-radius: var(--radius-md);
  font-family: var(--font-family);
  font-size: var(--font-size-md);
  line-height: 1.5;
  resize: vertical;
  transition: all var(--transition-fast);
}

.textarea-input:focus {
  outline: none;
  border-color: var(--color-primary);
  box-shadow: 0 0 0 2px var(--color-primary-light);
}

.settings-actions {
  display: flex;
  flex-direction: column;
  align-items: flex-end;
  gap: var(--spacing-md);
  margin-top: var(--spacing-xl);
}

.settings-feedback {
  font-size: var(--font-size-md);
  color: var(--color-primary);
  margin: 0;
}

/* Interaction demos */
.interaction-demos {
  padding: var(--spacing-md);
  border: 1px dashed var(--color-border);
  border-radius: var(--radius-md);
}

.demo-row {
  display: flex;
  flex-wrap: wrap;
  gap: var(--spacing-md);
  margin-bottom: var(--spacing-md);
}

.demo-row:last-child {
  margin-bottom: 0;
}

.ripple-demo-button {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  padding: 0 var(--spacing-md);
  height: 2.5rem;
  background-color: var(--color-surface-variant);
  color: var(--color-on-surface);
  border: 1px solid var(--color-border);
  border-radius: var(--radius-md);
  font-size: var(--font-size-md);
  cursor: pointer;
  transition: all var(--transition-fast);
  text-align: center;
}

.ripple-demo-button:hover {
  background-color: var(--color-surface);
}
</file>

<file path="src-frontend/src/lazy/Settings.tsx">
import React, { useState, useEffect } from 'react';
import { Input, Button } from '../components/ui/index';
import { useTheme } from '../theme/ThemeContext';
import { useAnimation } from '../animation';
import { useKeyboard } from '../keyboard';
import { useAccessibility } from '../accessibility';
import { useTour, TourButton } from '../tours';
import { useHelp, HelpButton, HelpTrigger } from '../help';
import { 
  useDisclosure, 
  ProgressiveFeature, 
  LevelProgressIndicator, 
  AdvancedFeaturesToggle, 
  FeatureBadge, 
  LockedFeatureMessage 
} from '../disclosure';
import { 
  useMicroInteraction, 
  usePressEffect, 
  useRippleEffect, 
  useFeedback 
} from '../interactions';
import './Settings.css';

// Demo tour
const demoTour = {
  id: 'settings-tour',
  name: 'Settings Tour',
  steps: [
    {
      target: '.settings-heading',
      title: 'Settings Panel',
      content: 'This is the settings panel where you can configure the application.',
      placement: 'bottom'
    },
    {
      target: '.theme-settings',
      title: 'Theme Settings',
      content: 'Here you can customize the appearance of the application.',
      placement: 'right'
    },
    {
      target: '.accessibility-settings',
      title: 'Accessibility Settings',
      content: 'These settings help make the application more accessible to everyone.',
      placement: 'left'
    },
    {
      target: '.animations-settings',
      title: 'Animation Settings',
      content: 'Control how animations and transitions behave throughout the app.',
      placement: 'right'
    },
    {
      target: '.user-level-settings',
      title: 'User Level',
      content: 'As you use the app, you\'ll earn points and unlock new features.',
      placement: 'top'
    }
  ]
};

// Demo help topics
const demoHelpTopics = [
  {
    id: 'theme-help',
    title: 'Changing Themes',
    content: `
      <h2>How to Change Themes</h2>
      <p>You can select between light, dark, or system theme. The system theme will automatically match your device settings.</p>
      <p>To change the theme:</p>
      <ol>
        <li>Go to Settings</li>
        <li>Find the "Theme" section</li>
        <li>Select your preferred theme option</li>
        <li>Changes are applied immediately</li>
      </ol>
    `,
    category: 'Appearance',
    keywords: ['theme', 'dark mode', 'light mode', 'appearance']
  },
  {
    id: 'accessibility-help',
    title: 'Accessibility Features',
    content: `
      <h2>Accessibility Options</h2>
      <p>The application includes several accessibility features to make it usable for everyone:</p>
      <ul>
        <li><strong>High Contrast:</strong> Increases contrast for better readability</li>
        <li><strong>Large Text:</strong> Makes all text larger throughout the application</li>
        <li><strong>Reduced Motion:</strong> Minimizes animations and transitions</li>
        <li><strong>Screen Reader Support:</strong> Enhances compatibility with screen readers</li>
        <li><strong>Focus Indicators:</strong> Shows clear visual indicators when navigating with keyboard</li>
      </ul>
      <p>You can access these settings through the Accessibility button or in the Settings panel.</p>
    `,
    category: 'Accessibility',
    keywords: ['accessibility', 'a11y', 'screen reader', 'high contrast', 'large text'],
    related: ['keyboard-help']
  },
  {
    id: 'keyboard-help',
    title: 'Keyboard Navigation',
    content: `
      <h2>Keyboard Shortcuts</h2>
      <p>You can navigate the application efficiently using keyboard shortcuts:</p>
      <ul>
        <li><strong>Ctrl+K:</strong> Open command palette</li>
        <li><strong>Alt+A:</strong> Open accessibility panel</li>
        <li><strong>?:</strong> Show keyboard shortcuts</li>
        <li><strong>Esc:</strong> Close current panel or dialog</li>
      </ul>
      <p>Press ? at any time to see all available keyboard shortcuts.</p>
    `,
    category: 'Navigation',
    keywords: ['keyboard', 'shortcuts', 'navigation', 'hotkeys'],
    related: ['accessibility-help']
  }
];

const Settings: React.FC = () => {
  // Demo state
  const [apiKey, setApiKey] = useState('');
  const [selectedModel, setSelectedModel] = useState('claude-3-opus-20240229');
  const [feedbackMessage, setFeedbackMessage] = useState('');
  
  // Custom hooks for enhanced UI
  const { theme, setTheme } = useTheme();
  const { animationsEnabled, animationSpeed, toggleAnimations, setAnimationSpeed } = useAnimation();
  const { registerAction } = useKeyboard();
  const { settings: a11ySettings, updateSettings: updateA11ySettings } = useAccessibility();
  const { registerTour } = useTour();
  const { registerTopic } = useHelp();
  const { userLevel, addPoints } = useDisclosure();
  
  // Interactions
  const { trigger: triggerPulse, animationClass: pulseClass } = useMicroInteraction('pulse');
  const { props: pressProps } = usePressEffect();
  const { props: rippleProps, RippleEffect } = useRippleEffect();
  const { triggerSuccess, triggerError } = useFeedback();
  
  // Register keyboard shortcuts for the settings page
  useEffect(() => {
    const unregister = registerAction({
      key: 's',
      ctrlKey: true,
      description: 'Save settings',
      handler: () => handleSaveSettings(),
      scope: 'settings'
    });
    
    return unregister;
  }, [registerAction]);
  
  // Register demo tour
  useEffect(() => {
    registerTour(demoTour);
  }, [registerTour]);
  
  // Register help topics
  useEffect(() => {
    demoHelpTopics.forEach(topic => {
      registerTopic(topic);
    });
  }, [registerTopic]);
  
  // Demo save function that awards points
  const handleSaveSettings = () => {
    if (apiKey.length < 8) {
      setFeedbackMessage('API key must be at least 8 characters');
      triggerError('Invalid API key format');
      return;
    }
    
    // Success feedback
    setFeedbackMessage('Settings saved successfully!');
    triggerSuccess();
    
    // Award points for saving settings
    addPoints(15);
    
    // Trigger a pulse animation on the save button
    triggerPulse();
    
    // In a real app, we would send this to the backend
    console.log('Saving settings:', { apiKey, selectedModel });
  };
  
  return (
    <div className="settings-container">
      <div className="settings-content">
        <div className="settings-header">
          <h1 className="settings-heading">Settings</h1>
          <div className="header-actions">
            <HelpButton />
            <TourButton tourId="settings-tour" />
          </div>
        </div>
        
        {/* User level section with progressive disclosure */}
        <div className="settings-section user-level-settings">
          <h2>Your Profile Level</h2>
          <LevelProgressIndicator />
          <AdvancedFeaturesToggle />
        </div>
        
        {/* Basic settings section */}
        <div className="settings-section">
          <h2 className="settings-heading">General Settings</h2>
          
          <div className="settings-form">
            <div className="form-group">
              <label className="form-label">API Key</label>
              <Input
                type="password"
                value={apiKey}
                onChange={(e) => setApiKey(e.target.value)}
                placeholder="Enter your Anthropic API key"
                fullWidth
              />
              <p className="form-hint">
                Your API key is stored securely and never shared.
                <HelpTrigger topicId="api-key-help" />
              </p>
            </div>
            
            <div className="form-group">
              <label className="form-label">Default Model</label>
              <select 
                className="select-input"
                value={selectedModel}
                onChange={(e) => setSelectedModel(e.target.value)}
              >
                <option value="claude-3-opus-20240229">Claude 3 Opus</option>
                <option value="claude-3-sonnet-20240229">Claude 3 Sonnet</option>
                <option value="claude-3-haiku-20240307">Claude 3 Haiku</option>
              </select>
              <p className="form-hint">
                The model that will be used by default for new conversations.
              </p>
            </div>
          </div>
        </div>
        
        {/* Theme settings */}
        <div className="settings-section theme-settings">
          <h2 className="settings-heading">Theme Settings</h2>
          
          <div className="settings-form">
            <div className="form-group">
              <label className="form-label">Theme Mode</label>
              <select 
                className="select-input"
                value={theme}
                onChange={(e) => setTheme(e.target.value as 'light' | 'dark' | 'system')}
              >
                <option value="light">Light</option>
                <option value="dark">Dark</option>
                <option value="system">System (Follow OS)</option>
              </select>
              <p className="form-hint">
                Choose how the application should appear.
              </p>
            </div>
          </div>
        </div>
        
        {/* Accessibility settings */}
        <div className="settings-section accessibility-settings">
          <h2 className="settings-heading">
            Accessibility
            <HelpTrigger topicId="accessibility-help" />
          </h2>
          
          <div className="settings-form">
            <div className="form-group checkbox-group">
              <label className="checkbox-label">
                <input 
                  type="checkbox" 
                  checked={a11ySettings.highContrast}
                  onChange={(e) => updateA11ySettings({ highContrast: e.target.checked })}
                />
                <span>High Contrast</span>
              </label>
              <p className="form-hint">
                Increases contrast for better readability
              </p>
            </div>
            
            <div className="form-group checkbox-group">
              <label className="checkbox-label">
                <input 
                  type="checkbox" 
                  checked={a11ySettings.largeText}
                  onChange={(e) => updateA11ySettings({ largeText: e.target.checked })}
                />
                <span>Large Text</span>
              </label>
              <p className="form-hint">
                Increases font size throughout the application
              </p>
            </div>
            
            <div className="form-group checkbox-group">
              <label className="checkbox-label">
                <input 
                  type="checkbox" 
                  checked={a11ySettings.reducedMotion}
                  onChange={(e) => updateA11ySettings({ reducedMotion: e.target.checked })}
                />
                <span>Reduced Motion</span>
              </label>
              <p className="form-hint">
                Minimizes animations and transitions
              </p>
            </div>
            
            <div className="form-group checkbox-group">
              <label className="checkbox-label">
                <input 
                  type="checkbox" 
                  checked={a11ySettings.screenReader}
                  onChange={(e) => updateA11ySettings({ screenReader: e.target.checked })}
                />
                <span>Screen Reader Support</span>
              </label>
              <p className="form-hint">
                Enhances compatibility with screen readers
              </p>
            </div>
          </div>
        </div>
        
        {/* Animation settings */}
        <div className="settings-section animations-settings">
          <h2 className="settings-heading">Animations</h2>
          
          <div className="settings-form">
            <div className="form-group checkbox-group">
              <label className="checkbox-label">
                <input 
                  type="checkbox" 
                  checked={animationsEnabled}
                  onChange={toggleAnimations}
                />
                <span>Enable Animations</span>
              </label>
              <p className="form-hint">
                Turn on/off all animations throughout the application
              </p>
            </div>
            
            <div className="form-group">
              <label className="form-label">Animation Speed</label>
              <select 
                className="select-input"
                value={animationSpeed}
                onChange={(e) => setAnimationSpeed(e.target.value as 'normal' | 'slow' | 'fast')}
                disabled={!animationsEnabled}
              >
                <option value="slow">Slow</option>
                <option value="normal">Normal</option>
                <option value="fast">Fast</option>
              </select>
              <p className="form-hint">
                Control how quickly animations play
              </p>
            </div>
          </div>
        </div>
        
        {/* Advanced settings (Progressive disclosure) */}
        <ProgressiveFeature level="intermediate" fallback={<LockedFeatureMessage level="intermediate" />}>
          <div className="settings-section">
            <h2 className="settings-heading">
              Advanced Settings 
              <FeatureBadge level="intermediate" />
            </h2>
            
            <div className="settings-form">
              <div className="form-group">
                <label className="form-label">Temperature</label>
                <div className="range-control">
                  <input 
                    type="range" 
                    min="0" 
                    max="1" 
                    step="0.1" 
                    defaultValue="0.7" 
                    className="range-input"
                  />
                  <span className="range-value">0.7</span>
                </div>
                <p className="form-hint">
                  Controls randomness in responses. Lower values are more deterministic.
                </p>
              </div>
              
              <div className="form-group">
                <label className="form-label">Max Tokens</label>
                <Input
                  type="number"
                  defaultValue="4000"
                  fullWidth
                />
                <p className="form-hint">
                  Maximum number of tokens for model responses.
                </p>
              </div>
            </div>
          </div>
        </ProgressiveFeature>
        
        {/* Expert settings (Progressive disclosure) */}
        <ProgressiveFeature level="expert" fallback={<LockedFeatureMessage level="expert" />}>
          <div className="settings-section">
            <h2 className="settings-heading">
              Expert Settings 
              <FeatureBadge level="expert" />
            </h2>
            
            <div className="settings-form">
              <div className="form-group">
                <label className="form-label">System Prompt</label>
                <textarea 
                  className="textarea-input"
                  rows={4}
                  placeholder="Enter a system prompt that will be used for all conversations"
                />
                <p className="form-hint">
                  Custom system prompt to control model behavior.
                </p>
              </div>
              
              <div className="form-group checkbox-group">
                <label className="checkbox-label">
                  <input type="checkbox" defaultChecked />
                  <span>Enable Advanced API Features</span>
                </label>
                <p className="form-hint">
                  Enables experimental API features
                </p>
              </div>
            </div>
          </div>
        </ProgressiveFeature>
        
        {/* Microinteractions Demo */}
        <div className="settings-section">
          <h2 className="settings-heading">Interaction Examples</h2>
          
          <div className="interaction-demos">
            <div className="demo-row">
              <Button 
                className={pulseClass}
                onClick={triggerPulse}
              >
                Pulse Effect
              </Button>
              
              <Button 
                {...pressProps}
              >
                Press Effect
              </Button>
              
              <div {...rippleProps} className="ripple-demo-button">
                <RippleEffect />
                Ripple Effect
              </div>
            </div>
            
            <div className="demo-row">
              <Button 
                variant="primary"
                onClick={() => triggerSuccess('Success!')}
              >
                Success Feedback
              </Button>
              
              <Button 
                variant="danger"
                onClick={() => triggerError('Something went wrong')}
              >
                Error Feedback
              </Button>
            </div>
          </div>
        </div>
        
        {/* Save button with ripple effect */}
        <div className="settings-actions">
          <p className="settings-feedback">{feedbackMessage}</p>
          
          <div {...rippleProps}>
            <Button 
              variant="primary"
              onClick={handleSaveSettings}
              className={pulseClass}
            >
              Save Settings (Ctrl+S)
              <RippleEffect />
            </Button>
          </div>
        </div>
      </div>
    </div>
  );
};

export default Settings;
</file>

<file path="src/main.rs">
#![cfg_attr(
    all(not(debug_assertions), target_os = "windows"),
    windows_subsystem = "windows"
)]

mod collaboration;
mod commands;
mod feature_flags;
mod models;
mod protocols;
mod security;
mod services;
mod shell_loader;
mod utils;

use env_logger::Env;
use log::{error, info};
use std::sync::{Arc, Mutex};
use tauri::{Manager, WindowBuilder, WindowUrl};
use tokio::runtime::Runtime;

use crate::feature_flags::{FeatureFlags, FeatureManager};
use crate::security::{init_security_manager, SecurityConfig, PermissionLevel};
use crate::shell_loader::{launch_with_fast_shell, ShellLoader};
use crate::utils::config::Config;

// Global runtime handle for async operations
lazy_static::lazy_static! {
    static ref RUNTIME: Runtime = Runtime::new().expect("Failed to create Tokio runtime");
}

// Global feature manager
lazy_static::lazy_static! {
    static ref FEATURE_MANAGER: Arc<Mutex<FeatureManager>> = Arc::new(Mutex::new(FeatureManager::from_env()));
}

#[tauri::command]
async fn get_app_info() -> Result<serde_json::Value, String> {
    let config = Config::global();
    let config = config.lock().unwrap();
    
    let app_name = config.get_string("app_name").unwrap_or_else(|| "Claude MCP Client".to_string());
    let version = config.get_string("version").unwrap_or_else(|| "0.1.0".to_string());
    
    let info = serde_json::json!({
        "name": app_name,
        "version": version,
        "platform": "linux"
    });
    
    Ok(info)
}

#[tauri::command]
async fn get_enabled_features() -> Result<Vec<String>, String> {
    let feature_manager = FEATURE_MANAGER.lock().unwrap();
    let flags = feature_manager.flags();
    
    let mut enabled_features = Vec::new();
    
    if flags.contains(FeatureFlags::EXPERIMENTAL) {
        enabled_features.push("experimental".to_string());
    }
    
    if flags.contains(FeatureFlags::DEV_FEATURES) {
        enabled_features.push("dev".to_string());
    }
    
    if flags.contains(FeatureFlags::LAZY_LOAD) {
        enabled_features.push("lazy_load".to_string());
    }
    
    if flags.contains(FeatureFlags::PLUGINS) {
        enabled_features.push("plugins".to_string());
    }
    
    if flags.contains(FeatureFlags::HISTORY) {
        enabled_features.push("history".to_string());
    }
    
    if flags.contains(FeatureFlags::ADVANCED_UI) {
        enabled_features.push("advanced_ui".to_string());
    }
    
    if flags.contains(FeatureFlags::ANALYTICS) {
        enabled_features.push("analytics".to_string());
    }
    
    if flags.contains(FeatureFlags::AUTO_UPDATE) {
        enabled_features.push("auto_update".to_string());
    }
    
    Ok(enabled_features)
}

fn main() {
    // Initialize logging
    env_logger::Builder::from_env(Env::default().default_filter_or("info")).init();
    info!("Starting Claude MCP Client");
    
    // Load config
    let config = Config::global();
    
    // Build Tauri application
    let mut builder = tauri::Builder::default();
    
    // Register commands
    builder = commands::register_commands(builder);
    
    builder
        .setup(|app| {
            // Get the main window or create it
            let window = app.get_window("main").unwrap_or_else(|| {
                WindowBuilder::new(
                    app,
                    "main".to_string(),
                    WindowUrl::App("index.html".into())
                )
                .title("Claude MCP")
                .inner_size(1200.0, 800.0)
                .visible(false) // Keep window hidden until shell is ready
                .build()
                .expect("Failed to create main window")
            });
            
            // Store app handle in state
            let app_handle = app.handle();
            app.manage(Arc::new(Mutex::new(app_handle)));
            
            // Initialize security manager
            let security_config = SecurityConfig {
                e2ee_enabled: true,
                use_secure_enclave: true,
                data_flow_tracking_enabled: true,
                default_permission_level: PermissionLevel::AskFirstTime,
                interactive_permissions: true,
                anonymize_telemetry: true,
                encrypt_local_storage: true,
                credential_cache_duration: 600, // 10 minutes
                clipboard_security_enabled: true,
            };
            
            if let Err(e) = init_security_manager(Some(security_config)) {
                error!("Failed to initialize security manager: {}", e);
            } else {
                info!("Security manager initialized");
            }
            
            // Start shell loader (this happens in Tokio runtime)
            RUNTIME.spawn(async move {
                let config_lock = config.lock().unwrap();
                let shell_loader = launch_with_fast_shell(window, &config_lock).await;
                
                // Log startup time
                if let Some(elapsed) = shell_loader.elapsed() {
                    info!("Shell ready in {}ms", elapsed.as_millis());
                }
            });
            
            Ok(())
        })
        .invoke_handler(tauri::generate_handler![
            get_app_info,
            get_enabled_features,
        ])
        .run(tauri::generate_context!())
        .expect("Error running Tauri application");
}
</file>

<file path="src/protocols/mcp/protocol.rs">
use crate::models::messages::{Message, MessageError};
use crate::protocols::mcp::error::McpError;
use crate::protocols::mcp::message::{McpMessage, McpMessagePayload, McpResponseMessage};
use crate::protocols::mcp::session::{Session, SessionManager, SessionMessageHandler};
use crate::protocols::mcp::types::{McpCompletionRequest, McpMessageRole, McpMessageType};
use crate::protocols::mcp::{McpClient, McpConfig};
use crate::protocols::{ConnectionStatus, ProtocolHandler};
use crate::observability::metrics::{increment_counter, record_gauge, record_histogram, time_operation};
use async_trait::async_trait;
use log::{debug, error, info, warn};
use std::collections::HashMap;
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, Instant, SystemTime};
use tokio::sync::mpsc::{self, Receiver, Sender};
use tokio::sync::mpsc::{UnboundedReceiver, UnboundedSender};
use tokio::sync::oneshot;
use tokio::time::timeout;
use uuid::Uuid;

/// Connection state
#[derive(Debug, Clone, PartialEq, Eq)]
enum ConnectionState {
    /// Not connected
    Disconnected,
    
    /// Connecting to server
    Connecting,
    
    /// Connected but not authenticated
    Connected,
    
    /// Connected and authenticated
    Authenticated,
    
    /// Connection error
    Error(String),
}

/// MCP Protocol Handler
pub struct McpProtocolHandler {
    /// MCP client
    client: Arc<McpClient>,
    
    /// Current connection status
    status: Arc<RwLock<ConnectionStatus>>,
    
    /// Connection state
    connection_state: Arc<RwLock<ConnectionState>>,
    
    /// Protocol configuration
    config: McpConfig,
    
    /// Session manager
    session_manager: Arc<SessionManager>,
    
    /// Current session
    current_session: Arc<RwLock<Option<Arc<Session>>>>,
    
    /// Session message handler
    message_handler: Arc<Mutex<Option<SessionMessageHandler>>>,
    
    /// Event receiver
    event_receiver: Arc<Mutex<Option<UnboundedReceiver<McpMessage>>>>,
    
    /// Last reconnection attempt
    last_reconnect_attempt: Arc<RwLock<Option<Instant>>>,
    
    /// Reconnection attempts
    reconnect_attempts: Arc<RwLock<u32>>,
    
    /// Request tracking map for streaming requests
    streaming_requests: Arc<Mutex<HashMap<String, Sender<Result<Message, MessageError>>>>>,
    
    /// Message routing table (message type -> handler function)
    message_router: Arc<Mutex<HashMap<McpMessageType, Box<dyn Fn(McpMessage) -> Result<(), McpError> + Send + Sync>>>>,
    
    /// Message queue for outgoing messages
    outgoing_queue: Arc<Mutex<Vec<McpMessage>>>,
    
    /// Flag to determine if the protocol handler is active
    is_active: Arc<RwLock<bool>>,
}

impl McpProtocolHandler {
    /// Create a new MCP protocol handler
    pub fn new(config: McpConfig) -> Self {
        let status = Arc::new(RwLock::new(ConnectionStatus::Disconnected));
        let client = Arc::new(McpClient::new(config.clone()));
        let session_manager = Arc::new(SessionManager::new(config.clone()));
        
        // Start session cleanup task
        session_manager.start_cleanup_task();
        
        Self {
            client,
            status,
            connection_state: Arc::new(RwLock::new(ConnectionState::Disconnected)),
            config,
            session_manager,
            current_session: Arc::new(RwLock::new(None)),
            message_handler: Arc::new(Mutex::new(None)),
            event_receiver: Arc::new(Mutex::new(None)),
            last_reconnect_attempt: Arc::new(RwLock::new(None)),
            reconnect_attempts: Arc::new(RwLock::new(0)),
            streaming_requests: Arc::new(Mutex::new(HashMap::new())),
            message_router: Arc::new(Mutex::new(HashMap::new())),
            outgoing_queue: Arc::new(Mutex::new(Vec::new())),
            is_active: Arc::new(RwLock::new(false)),
        }
    }
    
    /// Initialize the protocol handler
    pub fn initialize(&self) {
        // Set up message routing table
        self.setup_message_routing();
        
        // Start the outgoing message processor
        self.start_outgoing_processor();
        
        // Set active flag
        let mut is_active = self.is_active.write().unwrap();
        *is_active = true;
    }
    
    /// Set up message routing table
    fn setup_message_routing(&self) {
        let mut router = self.message_router.lock().unwrap();
        let message_handler = self.message_handler.clone();
        let streaming_requests = self.streaming_requests.clone();
        
        // Set up routing for each message type
        router.insert(
            McpMessageType::CompletionResponse,
            Box::new(move |message| {
                let handler_guard = message_handler.lock().unwrap();
                if let Some(handler) = handler_guard.as_ref() {
                    handler.handle_message(message)
                } else {
                    Err(McpError::ProtocolError("No message handler available".to_string()))
                }
            }),
        );
        
        // Set up routing for streaming messages
        let message_handler_clone = self.message_handler.clone();
        let streaming_requests_clone = self.streaming_requests.clone();
        
        router.insert(
            McpMessageType::StreamingMessage,
            Box::new(move |message| {
                // First, handle using the message handler
                let handler_guard = message_handler_clone.lock().unwrap();
                if let Some(handler) = handler_guard.as_ref() {
                    handler.handle_message(message.clone())?;
                }
                
                // Then, handle streaming delivery
                if let McpMessagePayload::StreamingMessage { streaming_id, chunk, is_final } = &message.payload {
                    let mut streaming_requests = streaming_requests_clone.lock().unwrap();
                    if let Some(sender) = streaming_requests.get(streaming_id) {
                        // Convert to application message format
                        let app_message = Message {
                            id: Uuid::new_v4().to_string(),
                            role: crate::models::messages::MessageRole::Assistant,
                            content: crate::models::messages::MessageContent {
                                parts: vec![crate::models::messages::ContentType::Text {
                                    text: chunk.to_string(),
                                }],
                            },
                            metadata: None,
                            created_at: SystemTime::now(),
                        };
                        
                        // Collect metrics
                        increment_counter("protocol.streaming.chunks", None);
                        record_histogram("protocol.streaming.chunk_size", chunk.len() as f64, None);
                        
                        // Try to send to application
                        if let Err(_) = sender.try_send(Ok(app_message)) {
                            warn!("Failed to send streaming chunk to application: channel full or closed");
                        }
                        
                        // If final, remove from tracking
                        if *is_final {
                            debug!("Received final chunk for streaming request {}", streaming_id);
                            streaming_requests.remove(streaming_id);
                            increment_counter("protocol.streaming.completed", None);
                        }
                    } else {
                        warn!("Received streaming chunk for unknown streaming ID: {}", streaming_id);
                    }
                }
                
                Ok(())
            }),
        );
        
        // Set up routing for streaming end
        let message_handler_clone = self.message_handler.clone();
        let streaming_requests_clone = self.streaming_requests.clone();
        
        router.insert(
            McpMessageType::StreamingEnd,
            Box::new(move |message| {
                // First, handle using the message handler
                let handler_guard = message_handler_clone.lock().unwrap();
                if let Some(handler) = handler_guard.as_ref() {
                    handler.handle_message(message.clone())?;
                }
                
                // Then, handle streaming cleanup
                if let McpMessagePayload::StreamingEnd { streaming_id } = &message.payload {
                    let mut streaming_requests = streaming_requests_clone.lock().unwrap();
                    if streaming_requests.remove(streaming_id).is_some() {
                        debug!("Removed streaming request {}", streaming_id);
                        increment_counter("protocol.streaming.ended", None);
                    }
                }
                
                Ok(())
            }),
        );
        
        // Set up routing for error messages
        let message_handler_clone = self.message_handler.clone();
        router.insert(
            McpMessageType::Error,
            Box::new(move |message| {
                let handler_guard = message_handler_clone.lock().unwrap();
                if let Some(handler) = handler_guard.as_ref() {
                    handler.handle_message(message)
                } else {
                    Err(McpError::ProtocolError("No message handler available".to_string()))
                }
            }),
        );
        
        // Set up routing for ping/pong
        let connection_state_clone = self.connection_state.clone();
        router.insert(
            McpMessageType::Pong,
            Box::new(move |_| {
                // Just update connection state
                let state = connection_state_clone.read().unwrap();
                if *state == ConnectionState::Authenticated {
                    debug!("Received pong message, connection is active");
                    // Record heartbeat success metric
                    increment_counter("protocol.heartbeat.success", None);
                } else {
                    warn!("Received pong message but connection is not authenticated");
                }
                Ok(())
            }),
        );
        
        // Set up routing for auth response
        let message_handler_clone = self.message_handler.clone();
        router.insert(
            McpMessageType::AuthResponse,
            Box::new(move |message| {
                let handler_guard = message_handler_clone.lock().unwrap();
                if let Some(handler) = handler_guard.as_ref() {
                    handler.handle_message(message)
                } else {
                    Err(McpError::ProtocolError("No message handler available".to_string()))
                }
            }),
        );
        
        // Add a default handler for unhandled message types
        let message_handler_clone = self.message_handler.clone();
        router.insert(
            McpMessageType::Unknown,
            Box::new(move |message| {
                warn!("Received unknown message type: {:?}", message.type_);
                // Record metric for unknown message types
                increment_counter("protocol.messages.unknown", None);
                
                // Try to handle with generic handler
                let handler_guard = message_handler_clone.lock().unwrap();
                if let Some(handler) = handler_guard.as_ref() {
                    handler.handle_message(message)
                } else {
                    Err(McpError::ProtocolError("No message handler available".to_string()))
                }
            }),
        );
    }
    
    /// Start outgoing message processor
    fn start_outgoing_processor(&self) {
        let outgoing_queue = self.outgoing_queue.clone();
        let client = self.client.clone();
        let is_active = self.is_active.clone();
        
        tokio::spawn(async move {
            while *is_active.read().unwrap() {
                // Get messages from queue
                let messages = {
                    let mut queue = outgoing_queue.lock().unwrap();
                    let messages = queue.clone();
                    queue.clear();
                    messages
                };
                
                // Send messages
                for message in messages {
                    match serde_json::to_string(&message) {
                        Ok(message_str) => {
                            if let Err(e) = client.send_raw(message_str).await {
                                error!("Failed to send message: {}", e);
                                increment_counter("protocol.messages.send_error", None);
                            } else {
                                debug!("Sent message: {:?}", message.type_);
                                increment_counter("protocol.messages.sent", None);
                                
                                // Record message size metrics
                                if let Ok(message_size) = serde_json::to_string(&message) {
                                    record_histogram("protocol.message.size", message_size.len() as f64, None);
                                }
                            }
                        }
                        Err(e) => {
                            error!("Failed to serialize message: {}", e);
                            increment_counter("protocol.messages.serialize_error", None);
                        }
                    }
                }
                
                // Sleep to avoid busy loop
                tokio::time::sleep(Duration::from_millis(10)).await;
            }
        });
    }
    
    /// Create a new session
    fn create_session(&self) -> Arc<Session> {
        let session = self.session_manager.create_session(None);
        
        // Create a message handler for the session
        let (handler, receiver) = SessionMessageHandler::new(session.clone());
        {
            let mut handler_guard = self.message_handler.lock().unwrap();
            *handler_guard = Some(handler);
        }
        {
            let mut receiver_guard = self.event_receiver.lock().unwrap();
            *receiver_guard = Some(receiver);
        }
        
        // Store session
        let mut session_guard = self.current_session.write().unwrap();
        *session_guard = Some(session.clone());
        
        // Record session creation metric
        increment_counter("protocol.session.created", None);
        
        session
    }
    
    /// Recover an existing session
    async fn recover_session(&self, session_id: &str) -> Result<Arc<Session>, McpError> {
        match self.session_manager.recover_session(session_id) {
            Ok(session) => {
                // Create a message handler for the session
                let (handler, receiver) = SessionMessageHandler::new(session.clone());
                {
                    let mut handler_guard = self.message_handler.lock().unwrap();
                    *handler_guard = Some(handler);
                }
                {
                    let mut receiver_guard = self.event_receiver.lock().unwrap();
                    *receiver_guard = Some(receiver);
                }
                
                // Store session
                let mut session_guard = self.current_session.write().unwrap();
                *session_guard = Some(session.clone());
                
                // Record session recovery metric
                increment_counter("protocol.session.recovered", None);
                
                // Send session recovery message to server
                self.send_session_recovery(session_id).await?;
                
                Ok(session)
            }
            Err(e) => {
                error!("Failed to recover session {}: {}", session_id, e);
                increment_counter("protocol.session.recovery_failed", None);
                Err(McpError::SessionError(format!("Failed to recover session: {}", e)))
            }
        }
    }
    
    /// Send session recovery message to server
    async fn send_session_recovery(&self, session_id: &str) -> Result<(), McpError> {
        let recovery_message = McpMessage {
            id: Uuid::new_v4().to_string(),
            version: "v1".to_string(),
            type_: McpMessageType::SessionRecovery,
            payload: McpMessagePayload::SessionRecovery {
                session_id: session_id.to_string(),
            },
        };
        
        // Set up response channel
        let (tx, rx) = oneshot::channel();
        
        // Register response channel
        {
            let handler_guard = self.message_handler.lock().unwrap();
            if let Some(handler) = handler_guard.as_ref() {
                handler.register_response_channel(recovery_message.id.clone(), tx);
            } else {
                return Err(McpError::ProtocolError(
                    "No message handler available".to_string(),
                ));
            }
        }
        
        // Send recovery request
        self.client
            .send_raw(serde_json::to_string(&recovery_message)?)
            .await?;
        
        // Wait for response with timeout
        match timeout(self.config.connection_timeout, rx).await {
            Ok(result) => match result {
                Ok(response) => match response {
                    Ok(msg) => {
                        if let McpMessagePayload::SessionRecoveryResponse { success, error } = msg.payload {
                            if success {
                                info!("Successfully recovered session {}", session_id);
                                increment_counter("protocol.session.recovery_success", None);
                                Ok(())
                            } else {
                                let error_msg = error.unwrap_or_else(|| "Unknown error".to_string());
                                warn!("Failed to recover session {}: {}", session_id, error_msg);
                                increment_counter("protocol.session.recovery_rejected", None);
                                Err(McpError::SessionError(format!("Session recovery failed: {}", error_msg)))
                            }
                        } else {
                            Err(McpError::ProtocolError(
                                "Unexpected response type".to_string(),
                            ))
                        }
                    }
                    Err(e) => Err(e),
                },
                Err(_) => Err(McpError::ProtocolError(
                    "Response channel closed".to_string(),
                )),
            },
            Err(_) => Err(McpError::Timeout(self.config.connection_timeout)),
        }
    }
    
    /// Persist the current session
    async fn persist_current_session(&self) -> Result<(), McpError> {
        let session_guard = self.current_session.read().unwrap();
        if let Some(session) = session_guard.as_ref() {
            if let Err(e) = self.session_manager.persist_session(session) {
                error!("Failed to persist session {}: {}", session.id, e);
                increment_counter("protocol.session.persistence_failed", None);
                return Err(McpError::SessionError(format!("Failed to persist session: {}", e)));
            }
            increment_counter("protocol.session.persisted", None);
            Ok(())
        } else {
            Err(McpError::SessionError("No active session to persist".to_string()))
        }
    }
    
    /// Authenticate with the server
    async fn authenticate(&self) -> Result<(), McpError> {
        // Record authentication attempt metric
        increment_counter("protocol.auth.attempt", None);
        
        // Generate authentication message
        let auth_message = McpMessage {
            id: Uuid::new_v4().to_string(),
            version: "v1".to_string(),
            type_: McpMessageType::AuthRequest,
            payload: McpMessagePayload::AuthRequest {
                api_key: self.config.api_key.clone(),
                organization_id: self.config.organization_id.clone(),
            },
        };
        
        // Set up response channel
        let (tx, rx) = oneshot::channel();
        
        // Register response channel
        {
            let handler_guard = self.message_handler.lock().unwrap();
            if let Some(handler) = handler_guard.as_ref() {
                handler.register_response_channel(auth_message.id.clone(), tx);
            } else {
                return Err(McpError::ProtocolError(
                    "No message handler available".to_string(),
                ));
            }
        }
        
        // Send authentication request
        self.client
            .send_raw(serde_json::to_string(&auth_message)?)
            .await?;
        
        // Wait for response with timeout
        match timeout(self.config.connection_timeout, rx).await {
            Ok(result) => match result {
                Ok(response) => match response {
                    Ok(msg) => {
                        if let McpMessagePayload::AuthResponse { success, session_id } = msg.payload {
                            if success {
                                // Update connection state
                                let mut state = self.connection_state.write().unwrap();
                                *state = ConnectionState::Authenticated;
                                
                                // Update connection status
                                let mut status = self.status.write().unwrap();
                                *status = ConnectionStatus::Connected;
                                
                                // Start heartbeat task
                                self.start_heartbeat_task();
                                
                                // Reset reconnection attempts
                                let mut attempts = self.reconnect_attempts.write().unwrap();
                                *attempts = 0;
                                
                                // Record successful authentication
                                increment_counter("protocol.auth.success", None);
                                
                                Ok(())
                            } else {
                                // Record failed authentication
                                increment_counter("protocol.auth.failed", None);
                                
                                Err(McpError::AuthenticationFailed(
                                    "Authentication failed".to_string(),
                                ))
                            }
                        } else {
                            Err(McpError::ProtocolError(
                                "Unexpected response type".to_string(),
                            ))
                        }
                    }
                    Err(e) => Err(e),
                },
                Err(_) => Err(McpError::ProtocolError(
                    "Response channel closed".to_string(),
                )),
            },
            Err(_) => {
                // Record timeout
                increment_counter("protocol.auth.timeout", None);
                
                Err(McpError::Timeout(self.config.connection_timeout))
            }
        }
    }
    
    /// Start the message handling task
    fn start_message_handler(&self) {
        let client_clone = self.client.clone();
        let message_router = self.message_router.clone();
        let connection_state_clone = self.connection_state.clone();
        let status_clone = self.status.clone();
        let config_clone = self.config.clone();
        let reconnect_handler = Arc::new(move || {
            let connection_state = connection_state_clone.clone();
            let status = status_clone.clone();
            let config = config_clone.clone();
            
            tokio::spawn(async move {
                // Update connection state
                {
                    let mut state = connection_state.write().unwrap();
                    *state = ConnectionState::Error("Connection lost, attempting to reconnect".to_string());
                }
                {
                    let mut status = status.write().unwrap();
                    *status = ConnectionStatus::Reconnecting;
                }
                
                // Implement exponential backoff for reconnection
                // This will be completed in the handle_reconnection method
            });
        });
        
        tokio::spawn(async move {
            loop {
                // Check connection state
                {
                    let state = connection_state_clone.read().unwrap();
                    if *state == ConnectionState::Disconnected {
                        break;
                    }
                }
                
                // Receive message from WebSocket
                match client_clone.receive_raw().await {
                    Ok(message_str) => {
                        // Record message received metric
                        increment_counter("protocol.messages.received", None);
                        
                        // Record message size metric
                        record_histogram("protocol.message.received_size", message_str.len() as f64, None);
                        
                        // Parse message
                        match serde_json::from_str::<McpMessage>(&message_str) {
                            Ok(message) => {
                                // Record message type metric
                                let metric_name = format!("protocol.messages.type.{:?}", message.type_);
                                increment_counter(&metric_name.to_lowercase(), None);
                                
                                // Route message to appropriate handler
                                let router = message_router.lock().unwrap();
                                let handler = router.get(&message.type_).or_else(|| router.get(&McpMessageType::Unknown));
                                
                                if let Some(handler) = handler {
                                    if let Err(e) = handler(message.clone()) {
                                        error!("Error handling message: {}", e);
                                        increment_counter("protocol.messages.handler_error", None);
                                    }
                                } else {
                                    error!("No handler found for message type: {:?}", message.type_);
                                    increment_counter("protocol.messages.no_handler", None);
                                }
                            }
                            Err(e) => {
                                error!("Error parsing message: {}", e);
                                increment_counter("protocol.messages.parse_error", None);
                            }
                        }
                    }
                    Err(e) => {
                        error!("Error receiving message: {}", e);
                        increment_counter("protocol.connection.error", None);
                        
                        // Update connection state and status
                        {
                            let mut state = connection_state_clone.write().unwrap();
                            *state = ConnectionState::Error(e.to_string());
                        }
                        {
                            let mut status = status_clone.write().unwrap();
                            *status = ConnectionStatus::ConnectionError(e.to_string());
                        }
                        
                        // Attempt to reconnect if enabled
                        if config_clone.auto_reconnect {
                            reconnect_handler();
                        }
                        
                        break;
                    }
                }
            }
        });
    }
    
    /// Start heartbeat task
    fn start_heartbeat_task(&self) {
        let client_clone = self.client.clone();
        let current_session_clone = self.current_session.clone();
        let connection_state_clone = self.connection_state.clone();
        let config_clone = self.config.clone();
        
        tokio::spawn(async move {
            let heartbeat_interval = Duration::from_secs(30); // 30 seconds between heartbeats
            let mut missed_heartbeats = 0;
            let max_missed_heartbeats = 3; // Allow up to 3 missed heartbeats before considering connection lost
            
            loop {
                // Sleep for heartbeat interval
                tokio::time::sleep(heartbeat_interval).await;
                
                // Check connection state
                {
                    let state = connection_state_clone.read().unwrap();
                    if *state != ConnectionState::Authenticated {
                        break;
                    }
                }
                
                // Generate heartbeat message
                let session_guard = current_session_clone.read().unwrap();
                if let Some(session) = session_guard.as_ref() {
                    let heartbeat = session.generate_heartbeat();
                    
                    // Send heartbeat
                    if let Err(e) = client_clone.send_raw(serde_json::to_string(&heartbeat).unwrap()).await {
                        error!("Error sending heartbeat: {}", e);
                        increment_counter("protocol.heartbeat.error", None);
                        
                        // Increment missed heartbeats
                        missed_heartbeats += 1;
                        
                        // Check if we've missed too many heartbeats
                        if missed_heartbeats >= max_missed_heartbeats {
                            error!("Too many missed heartbeats ({}/{}), considering connection lost", 
                                   missed_heartbeats, max_missed_heartbeats);
                            
                            // Update connection state
                            let mut state = connection_state_clone.write().unwrap();
                            *state = ConnectionState::Error("Connection lost due to missed heartbeats".to_string());
                            
                            // Attempt to reconnect if enabled
                            if config_clone.auto_reconnect {
                                // Connection lost, will trigger reconnection in message handler
                                break;
                            }
                        }
                    } else {
                        // Reset missed heartbeats counter on successful sending
                        missed_heartbeats = 0;
                        
                        // Record heartbeat sent metric
                        increment_counter("protocol.heartbeat.sent", None);
                    }
                } else {
                    break;
                }
            }
        });
    }
    
    /// Validate message before sending
    fn validate_message(&self, message: &Message) -> Result<(), MessageError> {
        // Record validation metric
        increment_counter("protocol.validation.attempt", None);
        
        // Check message ID
        if message.id.is_empty() {
            increment_counter("protocol.validation.error.empty_id", None);
            return Err(MessageError::ValidationError("Message ID cannot be empty".to_string()));
        }
        
        // Check message content
        if message.content.parts.is_empty() {
            increment_counter("protocol.validation.error.empty_content", None);
            return Err(MessageError::ValidationError("Message content cannot be empty".to_string()));
        }
        
        // Validate by message role
        match message.role {
            crate::models::messages::MessageRole::User => {
                // User messages must have text or image content
                let has_valid_content = message.content.parts.iter().any(|part| {
                    matches!(
                        part,
                        crate::models::messages::ContentType::Text { .. } |
                        crate::models::messages::ContentType::Image { .. }
                    )
                });
                
                if !has_valid_content {
                    increment_counter("protocol.validation.error.user_invalid_content", None);
                    return Err(MessageError::ValidationError(
                        "User messages must have text or image content".to_string(),
                    ));
                }
            }
            crate::models::messages::MessageRole::Assistant => {
                // Assistant messages must have text content
                let has_text = message.content.parts.iter().any(|part| {
                    matches!(part, crate::models::messages::ContentType::Text { .. })
                });
                
                if !has_text {
                    increment_counter("protocol.validation.error.assistant_no_text", None);
                    return Err(MessageError::ValidationError(
                        "Assistant messages must have text content".to_string(),
                    ));
                }
            }
            crate::models::messages::MessageRole::System => {
                // System messages must only have text content
                let all_text = message.content.parts.iter().all(|part| {
                    matches!(part, crate::models::messages::ContentType::Text { .. })
                });
                
                if !all_text {
                    increment_counter("protocol.validation.error.system_non_text", None);
                    return Err(MessageError::ValidationError(
                        "System messages must only have text content".to_string(),
                    ));
                }
            }
            crate::models::messages::MessageRole::Tool => {
                // Tool messages must have tool result content
                let has_tool_result = message.content.parts.iter().any(|part| {
                    matches!(part, crate::models::messages::ContentType::ToolResult { .. })
                });
                
                if !has_tool_result {
                    increment_counter("protocol.validation.error.tool_no_result", None);
                    return Err(MessageError::ValidationError(
                        "Tool messages must have tool result content".to_string(),
                    ));
                }
            }
        }
        
        // Validate message size
        if let Ok(json) = serde_json::to_string(message) {
            if json.len() > self.config.max_message_size {
                increment_counter("protocol.validation.error.message_too_large", None);
                return Err(MessageError::ValidationError(
                    format!("Message exceeds maximum size of {} bytes", self.config.max_message_size)
                ));
            }
        }
        
        // Record successful validation
        increment_counter("protocol.validation.success", None);
        
        Ok(())
    }
    
    /// Create a streaming channel
    async fn create_streaming_channel(
        &self,
        message: Message,
    ) -> Result<Receiver<Result<Message, MessageError>>, MessageError> {
        // Measure operation duration
        time_operation!("protocol.streaming.setup", None, {
            // Validate message
            self.validate_message(&message)?;
            
            // Generate streaming ID
            let streaming_id = Uuid::new_v4().to_string();
            
            // Create streaming channel
            let (tx, rx) = mpsc::channel(32);
            
            // Store channel
            {
                let mut streaming_requests = self.streaming_requests.lock().unwrap();
                streaming_requests.insert(streaming_id.clone(), tx);
                
                // Record active streaming channels metric
                record_gauge("protocol.streaming.active_channels", streaming_requests.len() as f64, None);
            }
            
            // Convert to MCP message
            let mut mcp_message = McpClient::convert_to_mcp_message(message)?;
            
            // Set streaming parameters
            if let McpMessagePayload::CompletionRequest(ref mut req) = mcp_message.payload {
                req.stream = Some(true);
                req.streaming_id = Some(streaming_id.clone());
            }
            
            // Send message
            match self.client
                .send_raw(serde_json::to_string(&mcp_message)?)
                .await
            {
                Ok(_) => {
                    // Record streaming start metric
                    increment_counter("protocol.streaming.started", None);
                },
                Err(e) => {
                    // Clean up streaming request on error
                    let mut streaming_requests = self.streaming_requests.lock().unwrap();
                    streaming_requests.remove(&streaming_id);
                    
                    // Record streaming error metric
                    increment_counter("protocol.streaming.start_error", None);
                    
                    return Err(MessageError::NetworkError(e.to_string()));
                }
            }
            
            Ok(rx)
        })
    }
    
    /// Cancel streaming request
    async fn cancel_streaming(&self, streaming_id: &str) -> Result<(), MessageError> {
        // Record cancel attempt metric
        increment_counter("protocol.streaming.cancel_attempt", None);
        
        // Remove channel
        let channel_existed = {
            let mut streaming_requests = self.streaming_requests.lock().unwrap();
            streaming_requests.remove(streaming_id).is_some()
        };
        
        if !channel_existed {
            warn!("Attempted to cancel non-existent streaming request: {}", streaming_id);
            increment_counter("protocol.streaming.cancel_nonexistent", None);
            return Err(MessageError::InvalidRequest(format!("Streaming request {} not found", streaming_id)));
        }
        
        // Create cancel message
        let cancel_message = McpMessage {
            id: Uuid::new_v4().to_string(),
            version: "v1".to_string(),
            type_: McpMessageType::CancelStream,
            payload: McpMessagePayload::CancelStream {
                streaming_id: streaming_id.to_string(),
            },
        };
        
        // Send cancel message
        match self.client
            .send_raw(serde_json::to_string(&cancel_message)?)
            .await
        {
            Ok(_) => {
                // Record successful cancellation
                increment_counter("protocol.streaming.cancelled", None);
                Ok(())
            }
            Err(e) => {
                // Record failed cancellation
                increment_counter("protocol.streaming.cancel_error", None);
                Err(MessageError::NetworkError(e.to_string()))
            }
        }
    }
    
    /// Initialize reconnection process
    async fn handle_reconnection(&self) -> Result<(), McpError> {
        // Record reconnection attempt metric
        increment_counter("protocol.reconnection.attempt", None);
        
        // Check if reconnection is enabled
        if !self.config.auto_reconnect {
            return Err(McpError::ConnectionError(
                "Automatic reconnection is disabled".to_string(),
            ));
        }
        
        // Check reconnection attempts
        {
            let attempts = self.reconnect_attempts.read().unwrap();
            if *attempts >= self.config.max_reconnect_attempts {
                increment_counter("protocol.reconnection.max_attempts_reached", None);
                return Err(McpError::ConnectionError(
                    format!("Maximum reconnection attempts reached ({})", self.config.max_reconnect_attempts)
                ));
            }
        }
        
        // Update reconnection attempts
        let current_attempt;
        {
            let mut attempts = self.reconnect_attempts.write().unwrap();
            *attempts += 1;
            current_attempt = *attempts;
            
            info!("Reconnection attempt {}/{}", current_attempt, self.config.max_reconnect_attempts);
        }
        
        // Calculate backoff delay
        let backoff_delay = {
            // Exponential backoff: base_delay * 2^attempt with jitter
            let base_ms = self.config.reconnect_backoff.as_millis() as u64;
            let exp_factor = 2u64.pow(current_attempt as u32 - 1);
            let delay_ms = base_ms * exp_factor;
            
            // Add jitter (±20%)
            let jitter_factor = 0.8 + (rand::random::<f64>() * 0.4); // 0.8 to 1.2
            let jittered_delay_ms = (delay_ms as f64 * jitter_factor) as u64;
            
            Duration::from_millis(jittered_delay_ms.min(30000)) // Cap at 30 seconds
        };
        
        // Update last reconnection attempt
        {
            let mut last_attempt = self.last_reconnect_attempt.write().unwrap();
            *last_attempt = Some(Instant::now());
            
            // Record backoff delay metric
            record_histogram("protocol.reconnection.backoff_ms", backoff_delay.as_millis() as f64, None);
        }
        
        // Wait for backoff delay
        debug!("Waiting for {} ms before reconnection attempt", backoff_delay.as_millis());
        tokio::time::sleep(backoff_delay).await;
        
        // Update connection state
        {
            let mut state = self.connection_state.write().unwrap();
            *state = ConnectionState::Connecting;
        }
        {
            let mut status = self.status.write().unwrap();
            *status = ConnectionStatus::Connecting;
        }
        
        // Attempt to reconnect
        match self.client.connect().await {
            Ok(_) => {
                // Update connection state
                {
                    let mut state = self.connection_state.write().unwrap();
                    *state = ConnectionState::Connected;
                }
                
                // Try to recover existing session if we have one
                let session_id = {
                    let session_guard = self.current_session.read().unwrap();
                    session_guard.as_ref().map(|s| s.id.clone())
                };
                
                if let Some(id) = session_id {
                    // Attempt session recovery
                    match self.recover_session(&id).await {
                        Ok(_) => {
                            debug!("Successfully recovered session during reconnection");
                            increment_counter("protocol.reconnection.session_recovered", None);
                        }
                        Err(e) => {
                            warn!("Failed to recover session during reconnection: {}", e);
                            increment_counter("protocol.reconnection.session_recovery_failed", None);
                            
                            // Create a new session instead
                            self.create_session();
                        }
                    }
                } else {
                    // Create a new session
                    self.create_session();
                }
                
                // Start message handler
                self.start_message_handler();
                
                // Authenticate
                match self.authenticate().await {
                    Ok(_) => {
                        // Reset reconnection attempts on success
                        {
                            let mut attempts = self.reconnect_attempts.write().unwrap();
                            *attempts = 0;
                        }
                        
                        // Record successful reconnection
                        increment_counter("protocol.reconnection.success", None);
                        
                        Ok(())
                    }
                    Err(e) => {
                        // Record authentication failure during reconnection
                        increment_counter("protocol.reconnection.auth_failed", None);
                        
                        Err(e)
                    }
                }
            }
            Err(e) => {
                // Update connection state
                {
                    let mut state = self.connection_state.write().unwrap();
                    *state = ConnectionState::Error(e.to_string());
                }
                {
                    let mut status = self.status.write().unwrap();
                    *status = ConnectionStatus::ConnectionError(e.to_string());
                }
                
                // Record failed reconnection attempt
                increment_counter("protocol.reconnection.failed", None);
                
                Err(e)
            }
        }
    }
    
    /// Process synchronous message (send and wait for response)
    async fn process_sync_message(&self, message: Message) -> Result<Message, MessageError> {
        // Measure operation duration
        time_operation!("protocol.sync_message.process", None, {
            // Validate message
            self.validate_message(&message)?;
            
            // Convert to MCP message
            let mcp_message = McpClient::convert_to_mcp_message(message)?;
            
            // Create response channel
            let (tx, rx) = oneshot::channel();
            
            // Register response channel
            {
                let handler_guard = self.message_handler.lock().unwrap();
                if let Some(handler) = handler_guard.as_ref() {
                    handler.register_response_channel(mcp_message.id.clone(), tx);
                } else {
                    return Err(MessageError::InvalidState("No message handler available".to_string()));
                }
            }
            
            // Send message
            match self.client
                .send_raw(serde_json::to_string(&mcp_message)?)
                .await
            {
                Ok(_) => {
                    // Record sync message sent metric
                    increment_counter("protocol.sync_message.sent", None);
                },
                Err(e) => {
                    // Clean up response channel on error
                    let handler_guard = self.message_handler.lock().unwrap();
                    if let Some(handler) = handler_guard.as_ref() {
                        handler.cancel_request(&mcp_message.id);
                    }
                    
                    // Record error metric
                    increment_counter("protocol.sync_message.send_error", None);
                    
                    return Err(MessageError::NetworkError(e.to_string()));
                }
            }
            
            // Wait for response with timeout
            match timeout(self.config.request_timeout, rx).await {
                Ok(result) => match result {
                    Ok(response) => match response {
                        Ok(mcp_response) => {
                            // Convert MCP response to application message
                            match McpClient::convert_from_mcp_message(mcp_response) {
                                Ok(app_message) => {
                                    // Record success metric
                                    increment_counter("protocol.sync_message.success", None);
                                    Ok(app_message)
                                },
                                Err(e) => {
                                    // Record conversion error
                                    increment_counter("protocol.sync_message.conversion_error", None);
                                    Err(e)
                                }
                            }
                        },
                        Err(e) => {
                            // Record error response
                            increment_counter("protocol.sync_message.response_error", None);
                            Err(MessageError::ProtocolError(e.to_string()))
                        }
                    },
                    Err(_) => {
                        // Record channel closed error
                        increment_counter("protocol.sync_message.channel_closed", None);
                        Err(MessageError::ProtocolError("Response channel closed".to_string()))
                    }
                },
                Err(_) => {
                    // Record timeout
                    increment_counter("protocol.sync_message.timeout", None);
                    Err(MessageError::Timeout(self.config.request_timeout.as_secs() as u32))
                }
            }
        })
    }
    
    /// Check if the handler is connected and authenticated
    pub fn is_connected(&self) -> bool {
        let state = self.connection_state.read().unwrap();
        *state == ConnectionState::Authenticated
    }
    
    /// Get connection state as string
    pub fn get_connection_state_str(&self) -> String {
        let state = self.connection_state.read().unwrap();
        match *state {
            ConnectionState::Disconnected => "Disconnected".to_string(),
            ConnectionState::Connecting => "Connecting".to_string(),
            ConnectionState::Connected => "Connected".to_string(),
            ConnectionState::Authenticated => "Authenticated".to_string(),
            ConnectionState::Error(ref msg) => format!("Error: {}", msg),
        }
    }
}

#[async_trait]
impl ProtocolHandler for McpProtocolHandler {
    fn protocol_name(&self) -> &'static str {
        "Model Context Protocol"
    }
    
    fn connection_status(&self) -> ConnectionStatus {
        self.status.read().unwrap().clone()
    }
    
    async fn connect(&self) -> Result<(), String> {
        // Initialize the protocol handler if not already done
        {
            let is_active = self.is_active.read().unwrap();
            if !*is_active {
                self.initialize();
            }
        }
        
        // Record connection attempt
        increment_counter("protocol.connection.attempt", None);
        
        // Update connection state and status
        {
            let mut state = self.connection_state.write().unwrap();
            *state = ConnectionState::Connecting;
        }
        {
            let mut status = self.status.write().unwrap();
            *status = ConnectionStatus::Connecting;
        }
        
        // Connect to WebSocket
        match self.client.connect().await {
            Ok(_) => {
                // Update connection state
                {
                    let mut state = self.connection_state.write().unwrap();
                    *state = ConnectionState::Connected;
                }
                
                // Create a new session
                self.create_session();
                
                // Start message handler
                self.start_message_handler();
                
                // Authenticate
                match self.authenticate().await {
                    Ok(_) => {
                        // Record successful connection
                        increment_counter("protocol.connection.success", None);
                        
                        Ok(())
                    }
                    Err(e) => {
                        let mut status = self.status.write().unwrap();
                        *status = ConnectionStatus::AuthFailed;
                        
                        // Record authentication failure
                        increment_counter("protocol.connection.auth_failed", None);
                        
                        Err(e.to_string())
                    }
                }
            }
            Err(e) => {
                // Update connection state and status
                {
                    let mut state = self.connection_state.write().unwrap();
                    *state = ConnectionState::Error(e.to_string());
                }
                {
                    let mut status = self.status.write().unwrap();
                    *status = ConnectionStatus::ConnectionError(e.to_string());
                }
                
                // Record connection failure
                increment_counter("protocol.connection.failed", None);
                
                Err(e.to_string())
            }
        }
    }
    
    async fn disconnect(&self) -> Result<(), String> {
        // Record disconnect attempt
        increment_counter("protocol.disconnect.attempt", None);
        
        // Persist current session if possible
        if let Err(e) = self.persist_current_session().await {
            warn!("Failed to persist session during disconnect: {}", e);
        }
        
        // Update connection state
        {
            let mut state = self.connection_state.write().unwrap();
            *state = ConnectionState::Disconnected;
        }
        
        // Cancel all streaming requests
        {
            let mut streaming_requests = self.streaming_requests.lock().unwrap();
            for (id, sender) in streaming_requests.drain() {
                debug!("Canceling streaming request {} due to disconnect", id);
                let _ = sender.send(Err(MessageError::ConnectionClosed)).await;
            }
        }
        
        // Cancel all pending requests
        {
            let handler_guard = self.message_handler.lock().unwrap();
            if let Some(handler) = handler_guard.as_ref() {
                handler.cancel_all_requests(McpError::ConnectionClosed);
            }
        }
        
        // Disconnect WebSocket
        match self.client.disconnect().await {
            Ok(_) => {
                // Update connection status
                let mut status = self.status.write().unwrap();
                *status = ConnectionStatus::Disconnected;
                
                // Set active flag to false
                {
                    let mut is_active = self.is_active.write().unwrap();
                    *is_active = false;
                }
                
                // Record successful disconnect
                increment_counter("protocol.disconnect.success", None);
                
                Ok(())
            }
            Err(e) => {
                // Record disconnect failure
                increment_counter("protocol.disconnect.error", None);
                
                Err(e.to_string())
            }
        }
    }
    
    async fn send_message(&self, message: Message) -> Result<(), MessageError> {
        // Check connection
        if !self.is_connected() {
            increment_counter("protocol.message.error.not_connected", None);
            return Err(MessageError::ConnectionClosed);
        }
        
        // Validate message
        self.validate_message(&message)?;
        
        // Convert to MCP message
        let mcp_message = McpClient::convert_to_mcp_message(message)?;
        
        // Queue message for sending
        {
            let mut queue = self.outgoing_queue.lock().unwrap();
            queue.push(mcp_message.clone());
        }
        
        // Record message queued metric
        increment_counter("protocol.message.queued", None);
        
        Ok(())
    }
    
    async fn receive_messages(&self) -> Result<Vec<Message>, MessageError> {
        // Check connection
        if !self.is_connected() {
            return Err(MessageError::ConnectionClosed);
        }
        
        // Get event receiver
        let mut event_receiver_guard = self.event_receiver.lock().unwrap();
        if let Some(receiver) = event_receiver_guard.as_mut() {
            let mut messages = Vec::new();
            
            // Poll for messages without blocking
            while let Ok(Some(mcp_message)) = tokio::time::timeout(Duration::from_millis(10), receiver.recv()).await {
                // Convert MCP message to application message
                if let Ok(app_message) = McpClient::convert_from_mcp_message(mcp_message) {
                    messages.push(app_message);
                }
            }
            
            // Record number of messages received
            if !messages.is_empty() {
                record_gauge("protocol.receive_messages.count", messages.len() as f64, None);
            }
            
            Ok(messages)
        } else {
            Ok(Vec::new())
        }
    }
    
    // Additional methods required by the interface
    
    /// Stream a message and get chunks
    async fn stream_message(&self, message: Message) -> Result<Receiver<Result<Message, MessageError>>, MessageError> {
        // Check connection
        if !self.is_connected() {
            return Err(MessageError::ConnectionClosed);
        }
        
        // Create streaming channel
        self.create_streaming_channel(message).await
    }
    
    /// Cancel a streaming request
    async fn cancel_stream(&self, streaming_id: &str) -> Result<(), MessageError> {
        // Check connection
        if !self.is_connected() {
            return Err(MessageError::ConnectionClosed);
        }
        
        // Cancel streaming
        self.cancel_streaming(streaming_id).await
    }
    
    /// Send a message and wait for a response
    async fn send_and_receive(&self, message: Message) -> Result<Message, MessageError> {
        // Check connection
        if !self.is_connected() {
            return Err(MessageError::ConnectionClosed);
        }
        
        // Process synchronous message
        self.process_sync_message(message).await
    }
}
</file>

<file path="src-tauri/Cargo.toml">
[package]
name = "claude-mcp-client"
version = "0.1.0"
description = "Claude MCP Client for Linux"
authors = ["Your Name"]
license = ""
repository = ""
edition = "2021"
rust-version = "1.62"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[build-dependencies]
tauri-build = { version = "1.4.0", features = [] }

[dependencies]
serde_json = "1.0"
serde = { version = "1.0", features = ["derive"] }
tauri = { version = "1.4.0", features = ["dialog-ask", "dialog-confirm", "dialog-message", "dialog-open", "dialog-save", "clipboard-read-text", "clipboard-write-text", "fs-exists", "fs-read-dir", "fs-read-file", "fs-write-file", "http-request", "notification-all", "window-center", "window-close", "window-hide", "window-maximize", "window-minimize", "window-request-user-attention", "window-set-always-on-top", "window-set-decorations", "window-set-focus", "window-set-fullscreen", "window-set-icon", "window-set-min-size", "window-set-position", "window-set-resizable", "window-set-size", "window-set-title", "window-show", "window-start-dragging", "window-unmaximize", "window-unminimize", "shell-open"] }
tokio = { version = "1.28.1", features = ["full"] }
reqwest = { version = "0.11", features = ["json", "stream"] }
once_cell = "1.18.0"
log = "0.4.19"
env_logger = "0.10.0"
rand = "0.8.5"
tauri-plugin-store = { git = "https://github.com/tauri-apps/plugins-workspace", branch = "v1" }
futures = "0.3.28"
lazy_static = "1.4.0"
bitflags = "2.3.3"
directories = "5.0.1"
thiserror = "1.0.40"
async-trait = "0.1.68"
url = "2.3.1"
futures-util = "0.3.28"
uuid = { version = "1.3.3", features = ["v4", "serde"] }
tokio-tungstenite = { version = "0.19.0", features = ["native-tls"] }
wasmer = "2.3.0"
wasmer-wasi = "2.3.0"
wasmtime = "6.0.1"
wasmtime-wasi = "6.0.1"
tempfile = "3.5.0"
zip = "0.6.4"
chrono = { version = "0.4.24", features = ["serde"] }
dirs = "5.0.1"
app-dirs = "1.0.2" # Added for the app data directory
app-info = "0.1.0" # Added for the app info
ctrlc = "3.2.5" # Added for shutdown handling
mcp-client = { path = "../src-common" } # Local crate for shared code

[features]
# this feature is used for production builds or when `devPath` points to the filesystem
# DO NOT REMOVE!!
custom-protocol = ["tauri/custom-protocol"]
# Enables experimental features
experimental = []
# Enables development-only features
dev = []
# Enables frontend integration
frontend = []
</file>

<file path="src/commands/mod.rs">
pub mod ai;
pub mod auth;
pub mod chat;
pub mod collaboration;
pub mod mcp;
pub mod offline;
pub mod security;
pub mod llm_metrics;

// Offline commands
pub mod offline {
    pub mod llm;
    
    // Re-export the existing module
    pub use super::offline::*;
}

use tauri::Wry;

/// Register all commands with Tauri
pub fn register_commands(builder: tauri::Builder<Wry>) -> tauri::Builder<Wry> {
    // Register each command module
    let builder = builder
        .invoke_handler(tauri::generate_handler![
            // Authentication commands
            auth::set_api_key,
            auth::validate_api_key,
            auth::get_organization_id,
            auth::logout,
            
            // Chat commands
            chat::get_available_models,
            chat::create_conversation,
            chat::get_conversation,
            chat::get_conversations,
            chat::delete_conversation,
            chat::get_messages,
            chat::send_message,
            
            // MCP commands
            mcp::connect,
            mcp::disconnect,
            mcp::get_connection_status,
            
            // AI commands
            ai::get_available_models,
            ai::set_network_status,
            ai::send_message,
            ai::stream_message,
            ai::cancel_streaming,
            ai::get_messages,
            ai::create_conversation,
            ai::delete_conversation,
        ]);
    
    // Register offline commands
    let builder = offline::register_offline_commands(builder);
    
    // Register offline LLM provider commands
    let builder = builder
        .invoke_handler(tauri::generate_handler![
            // Provider management
            offline::llm::get_all_providers,
            offline::llm::get_all_provider_availability,
            offline::llm::check_provider_availability,
            offline::llm::add_custom_provider,
            offline::llm::remove_custom_provider,
            offline::llm::get_active_provider,
            offline::llm::set_active_provider,
            offline::llm::get_provider_config,
            offline::llm::update_provider_config,
            
            // Model management
            offline::llm::list_available_models,
            offline::llm::list_downloaded_models,
            offline::llm::get_download_status,
            offline::llm::download_model,
            offline::llm::cancel_download,
            offline::llm::delete_model,
            
            // Text generation
            offline::llm::generate_text,
            
            // Provider discovery
            offline::llm::scan_for_providers,
            offline::llm::get_discovery_status,
            offline::llm::get_provider_suggestions,
            offline::llm::get_discovery_config,
            offline::llm::update_discovery_config,
            offline::llm::auto_configure_providers,
            
            // Provider migration
            offline::llm::check_legacy_system,
            offline::llm::get_migration_status,
            offline::llm::run_migration,
            offline::llm::get_migration_config,
            offline::llm::update_migration_config,
            offline::llm::opt_out_of_migration,
            offline::llm::get_model_mappings,
            offline::llm::get_provider_mappings,
        ]);
    
    // Register security commands
    let builder = builder
        .invoke_handler(tauri::generate_handler![
            // Security commands
            security::init_security,
            security::get_security_config,
            security::update_security_config,
            
            // Credentials commands
            security::store_secure_credential,
            security::get_secure_credential,
            security::delete_secure_credential,
            security::list_secure_credentials,
            
            // E2EE commands
            security::encrypt_data,
            security::decrypt_data,
            security::rotate_encryption_keys,
            
            // Permission commands
            security::check_permission_granted,
            security::request_app_permission,
            security::get_all_permissions,
            security::set_permission_level,
            security::reset_permission,
            security::reset_all_permissions,
            security::get_permission_statistics,
            
            // Data flow commands
            security::get_data_flow_graph,
            security::get_recent_data_flow_events,
            security::track_data_flow,
            security::clear_data_flow_events,
            security::get_data_flow_statistics,
            security::search_data_flow_events,
        ]);
        
    // Register LLM metrics commands
    let builder = builder
        .invoke_handler(tauri::generate_handler![
            // LLM metrics commands
            llm_metrics::get_llm_provider_metrics,
            llm_metrics::get_llm_model_metrics,
            llm_metrics::get_active_llm_provider,
            llm_metrics::get_default_llm_model,
            llm_metrics::get_llm_metrics_enabled,
            llm_metrics::get_llm_metrics_config,
            llm_metrics::update_llm_metrics_config,
            llm_metrics::accept_llm_metrics_privacy_notice,
            llm_metrics::reset_llm_metrics,
        ]);
    
    builder
}
</file>

<file path="README.md">
# MCP Client

The MCP Client is a cross-platform desktop application that provides access to the MCP system with robust offline capabilities and excellent performance.

## Features

- **Rich Conversation UI**: Fully-featured conversation interface with markdown, code blocks, and more
- **Offline Mode**: Continue working even without internet connectivity using embedded LLMs
- **Auto-Updates**: Seamlessly update the application with the latest features and fixes
- **Performance Optimizations**: Efficient memory usage and caching for responsive experience
- **Comprehensive Observability**: Detailed metrics, logging, and telemetry

## Getting Started

### Installation

#### Windows

- Download the MSI installer from the [releases page](https://github.com/your-org/mcp-client/releases)
- Run the installer and follow the on-screen instructions

#### macOS

- Download the DMG file from the [releases page](https://github.com/your-org/mcp-client/releases)
- Open the DMG file and drag the application to your Applications folder

#### Linux

For Debian/Ubuntu:
```bash
sudo apt install ./mcp-client_1.0.0_amd64.deb
```

For Fedora/RHEL:
```bash
sudo rpm -i mcp-client-1.0.0.x86_64.rpm
```

For other distributions, use the AppImage:
```bash
chmod +x MCP-Client-1.0.0.AppImage
./MCP-Client-1.0.0.AppImage
```

### Usage

Refer to the [User Guide](docs/USER_GUIDE.md) for detailed usage instructions.

## Development

### Prerequisites

- Node.js 16 or later
- Rust 1.65 or later
- Tauri CLI

### Setup

1. Clone the repository:
```bash
git clone https://github.com/your-org/mcp-client.git
cd mcp-client
```

2. Install dependencies:
```bash
npm install
```

3. Start the development server:
```bash
npm run tauri dev
```

### Building

```bash
# For all platforms
npm run tauri build

# For specific platforms
./installers/windows-build.ps1  # Windows
./installers/macos-build.sh     # macOS
./installers/linux-build.sh     # Linux
```

## Documentation

- [Installation Guide](docs/INSTALLATION.md)
- [User Guide](docs/USER_GUIDE.md)
- [Architecture Overview](docs/ARCHITECTURE.md)
- [API Documentation](docs/API.md)
- [Contribution Guidelines](CONTRIBUTING.md)

## Performance Benchmarks

Run the performance benchmarks:

```bash
cargo bench --features benchmarking
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgements

- [Tauri](https://tauri.app/) - Framework for building desktop applications
- [React](https://reactjs.org/) - UI framework
- [Rust](https://www.rust-lang.org/) - Systems programming language
</file>

</files>
